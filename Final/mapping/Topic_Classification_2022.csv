Unnamed: 0,Title,Keywords,Classification
0,A Fine-Grained Analysis on Distribution Shift,"Keywords: robustness, distribution shifts",Biological Applications in AI
1,"A New Perspective on ""How Graph Neural Networks Go Beyond Weisfeiler-Lehman?""","Keywords: Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman",Graph Neural Networks and Graph Theory
2,Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models,"Keywords: diffusion probabilistic models, generative models",Unclassified
3,Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks,"Keywords: out-of-distribution classification, symmetries, counterfactual invariances, geometric deep learning",Unclassified
4,BEiT: BERT Pre-Training of Image Transformers,"Keywords: self-supervised learning, pre-training, vision Transformer",Transformers and Sequence Modeling
5,Bootstrapped Meta-Learning,"Keywords: meta-learning, meta-gradients, meta-reinforcement learning",Reinforcement Learning and Control
6,Comparing Distributions by Measuring Differences that Affect Decision Making,"Keywords: probability divergence, two sample test, generative model",Unclassified
7,Coordination Among Neural Modules Through a Shared Global Workspace,"Keywords: slot based recurrent architectures, attention, transformers, latent bottleneck.",Transformers and Sequence Modeling
8,CycleMLP: A MLP-like Architecture for Dense Prediction,"Keywords: MLP, Dense Prediction",Unclassified
9,DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS,"Keywords: representation bottleneck, representation ability, interaction, explanation",Biological Applications in AI
10,Data-Efficient Graph Grammar Learning for Molecular Generation,"Keywords: molecular generation, graph grammar, data efficient generative model",Unclassified
11,Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme,"Keywords: speech, voice conversion, diffusion models, stochastic differential equations",Diffusion Models and Generative Modeling
12,Domino: Discovering Systematic Errors with Cross-Modal Embeddings,"Keywords: robustness, subgroup analysis, error analysis, multimodal, slice discovery",Multimodal Learning and Vision-Language Models
13,Efficiently Modeling Long Sequences with Structured State Spaces,"Keywords: sequence models, state space, RNN, CNN, Long Range Arena",Unclassified
14,Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation,"Keywords: tensor manipulations, tensor transformation, einops, einstein notation, einsum",Unclassified
15,Expressiveness and Approximation Properties of Graph Neural Networks,"Keywords: Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality",Graph Neural Networks and Graph Theory
16,Extending the WILDS Benchmark for Unsupervised Adaptation,"Keywords: distribution shifts, adaptation, unlabeled data",Unclassified
17,F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization,"Keywords: Neural Network Quantization, Fixed-Point Arithmetic",Model Compression and Quantization
18,Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space,"Abstract: Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.",Statistical Theory and High-Dimensional Data
19,Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution,"Keywords: fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization",Multitask and Transfer Learning
20,Finetuned Language Models are Zero-Shot Learners,"Keywords: natural language processing, zero-shot learning, language models",Unclassified
21,Frame Averaging for Invariant and Equivariant Network Design,"Keywords: Invariant and equivariant neural network, expressive power",Unclassified
22,GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation,"Keywords: molecular conformation generation, deep generative models, diffusion probabilistic models",Diffusion Models and Generative Modeling
23,Hyperparameter Tuning with Renyi Differential Privacy,"Keywords: differential privacy, hyperparameter tuning",Federated Learning and Privacy
24,Language modeling via stochastic processes,"Keywords: contrastive learning, language modelling, stochastic processes",Unclassified
25,Large Language Models Can Be Strong Differentially Private Learners,"Keywords: language model, differential privacy, language generation, fine-tuning, NLP",Large Language Models and Language Modeling
26,Learning Strides in Convolutional Neural Networks,"Keywords: Strides, Convolutional neural networks, Downsampling, Spectral representations, Fourier",Unclassified
27,MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling,"Keywords: Audio Synthesis, Generative Model, Hierarchical, DDSP, Music, Audio, Structured Models",Reinforcement Learning and Control
28,Meta-Learning with Fewer Tasks through Task Interpolation,"Keywords: meta-learning, task interpolation, meta-regularization",Hyperparameter Optimization and Meta-Learning
29,Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond,"Keywords: Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning",Optimization and Training Techniques
30,Natural Language Descriptions of Deep Visual Features,"Abstract: Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.",Reinforcement Learning and Control
31,Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path,"Keywords: neural collapse, deep learning theory, deep learning, inductive bias, equiangular tight frame, ETF, nearest class center, mean squared error loss, MSE loss, invariance, renormalization, gradient flow, dynamics, adversarial robustness",Adversarial Robustness and Security
32,Neural Structured Prediction for Inductive Node Classification,"Abstract: This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.",Graph Neural Networks and Graph Theory
33,Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization,"Keywords: Domain Adaptation, Transfer Learning, Societal Considerations of Representation Learning, Model Watermark",Self-Supervised Learning and Representation Learning
34,Open-Set Recognition: A Good Closed-Set Classifier is All You Need,"Keywords: open set recognition, image recognition, computer vision",Computer Vision and 3D Modeling
35,PiCO: Contrastive Label Disambiguation for Partial Label Learning,"Keywords: Partial Label Learning, Contrastive Learning, Prototype-based Disambiguation",Unclassified
36,Poisoning and Backdooring Contrastive Learning,"Keywords: Contrastive Learning, Poisoning attack, Backdoor attack, CLIP",Unclassified
37,ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics,"Keywords: inverse kinematics, deep learning, pose modeling",Unclassified
38,Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics,"Keywords: Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics",Reinforcement Learning and Control
39,Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting,"Keywords: sparse attention, pyramidal graph, Transformer, time series forecasting, long-range dependence, multiresolution",Transformers and Sequence Modeling
40,RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation,"Keywords: differentiable rendering, differentiable simulation, system identification","Simulation, Planning, and Agents"
41,Real-Time Neural Voice Camouflage,"Keywords: automatic speech recognition, predictive models, privacy",Audio and Speech Processing
42,Representational Continuity for Unsupervised Continual Learning,"Keywords: Continual Learning, Representational Learning, Deep Learning",Unclassified
43,Resolving Training Biases via Influence-based Data Relabeling,"Keywords: Training bias, influence functions, data relabeling",Optimization and Training Techniques
44,Sparse Communication via Mixed Distributions,"Abstract: Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call ""mixed random variables.'' Our starting point is a new ""direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (""sample-and-project'’) and an intrinsic one (based on face stratification). We experiment with both approaches on an emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables.",Biological Applications in AI
45,StyleAlign: Analysis and Applications of Aligned StyleGAN Models,"Keywords: StyleGAN, transfer learning, fine tuning, model alignment, image-to-image translation, image morphing",Multitask and Transfer Learning
46,The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions,"Keywords: Neural networks, global optimization, convex optimization, convex analysis",Optimization and Training Techniques
47,The Information Geometry of Unsupervised Reinforcement Learning,"Keywords: unsupervised skill learning, reward-free RL, mutual information, DIAYN",Reinforcement Learning and Control
48,Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design,"Keywords: Agent Design, Morphology Optimization, Reinforcement Learning",Reinforcement Learning and Control
49,Understanding over-squashing and bottlenecks on graphs via curvature,"Keywords: Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature",Graph Neural Networks and Graph Theory
50,Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling,"Keywords: Grammar Induction, Vision-Language Matching, Unsupervised Learning",Multimodal Learning and Vision-Language Models
51,Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion,"Keywords: Black-box variational inference, missing values, evidence upper bound",Diffusion Models and Generative Modeling
52,Vision-Based Manipulators Need to Also See from Their Hands,"Keywords: reinforcement learning, observation space, out-of-distribution generalization, visuomotor control, robotics, manipulation",Reinforcement Learning and Control
53,Weighted Training for Cross-Task Learning,"Keywords: Cross-task learning, Natural language processing, Representation learning",Optimization and Training Techniques
54,iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data,"Keywords: neuroscience, latent variable models, RNN, VAE, motor control, control theory, dynamical systems",Reinforcement Learning and Control
