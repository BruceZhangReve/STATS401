[{"Sentence Number":1,"Page":1,"Sentence":"Editor: Ivan Titov\n\nAbstract\n\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).","Similarity":0.9365032212,"Cluster Number":0},{"Sentence Number":2,"Page":1,"Sentence":"The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.","Similarity":0.7824258394,"Cluster Number":2},{"Sentence Number":3,"Page":1,"Sentence":"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.","Similarity":0.8591280572,"Cluster Number":0},{"Sentence Number":4,"Page":1,"Sentence":"Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks.","Similarity":0.8829750893,"Cluster Number":0},{"Sentence Number":5,"Page":1,"Sentence":"By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.","Similarity":0.8756681161,"Cluster Number":0},{"Sentence Number":6,"Page":1,"Sentence":"To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.","Similarity":0.854288187,"Cluster Number":1},{"Sentence Number":7,"Page":1,"Sentence":"1\n\nKeywords: transfer learning, natural language processing, multi-task learning, attention- based models, deep learning\n\n1.","Similarity":0.8818037013,"Cluster Number":0},{"Sentence Number":8,"Page":1,"Sentence":"Introduction\n\nTraining a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning.","Similarity":0.8939697638,"Cluster Number":0},{"Sentence Number":9,"Page":1,"Sentence":"This can be loosely viewed as developing general-purpose knowledge that allows the model to \u201cunderstand\u201d text.","Similarity":0.8821885931,"Cluster Number":0},{"Sentence Number":10,"Page":1,"Sentence":"This knowledge can range from low-level (e.g.","Similarity":0.7527196352,"Cluster Number":2},{"Sentence Number":11,"Page":1,"Sentence":"the spelling\n\n\u2217 .","Similarity":0.4890339563,"Cluster Number":4},{"Sentence Number":12,"Page":1,"Sentence":"Equal contribution.","Similarity":0.5594807169,"Cluster Number":4},{"Sentence Number":13,"Page":1,"Sentence":"A description of each author\u2019s contribution is available in Appendix A.","Similarity":0.7251233771,"Cluster Number":2},{"Sentence Number":14,"Page":1,"Sentence":"Correspondence to craffel@gmail.com .","Similarity":0.6374271001,"Cluster Number":3},{"Sentence Number":15,"Page":1,"Sentence":"1. https:\/\/github.com\/google-research\/text-to-text-transfer-transformer\n\n\u00a9 2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.","Similarity":0.8241959805,"Cluster Number":1},{"Sentence Number":16,"Page":1,"Sentence":"License: CC-BY 4.0, see https:\/\/creativecommons.org\/licenses\/by\/4.0\/ .","Similarity":0.6778363657,"Cluster Number":3},{"Sentence Number":17,"Page":1,"Sentence":"Attribution requirements are provided at http:\/\/jmlr.org\/papers\/v21\/20-074.html .","Similarity":0.720643005,"Cluster Number":2},{"Sentence Number":18,"Page":2,"Sentence":"Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n\nor meaning of words) to high-level (e.g.","Similarity":0.8128330367,"Cluster Number":1},{"Sentence Number":19,"Page":2,"Sentence":"that a tuba is too large to fit in most backpacks).","Similarity":0.7351051025,"Cluster Number":2},{"Sentence Number":20,"Page":2,"Sentence":"In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task.","Similarity":0.9087696149,"Cluster Number":0},{"Sentence Number":21,"Page":2,"Sentence":"For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors.","Similarity":0.8869582382,"Cluster Number":0},{"Sentence Number":22,"Page":2,"Sentence":"These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).","Similarity":0.7936117376,"Cluster Number":1},{"Sentence Number":23,"Page":2,"Sentence":"Recently, it has become increasingly common to pre-train the entire model on a data-rich task.","Similarity":0.8387012226,"Cluster Number":1},{"Sentence Number":24,"Page":2,"Sentence":"Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks.","Similarity":0.8402242948,"Cluster Number":1},{"Sentence Number":25,"Page":2,"Sentence":"In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).","Similarity":0.8552548548,"Cluster Number":1},{"Sentence Number":26,"Page":2,"Sentence":"In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.","Similarity":0.9432614822,"Cluster Number":0},{"Sentence Number":27,"Page":2,"Sentence":"This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019).","Similarity":0.8120655677,"Cluster Number":1},{"Sentence Number":28,"Page":2,"Sentence":"Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet\u2014for example, the Common Crawl project 2 produces about 20TB of text data extracted from web pages each month.","Similarity":0.901929528,"Cluster Number":0},{"Sentence Number":29,"Page":2,"Sentence":"This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e.","Similarity":0.853644549,"Cluster Number":1},{"Sentence Number":30,"Page":2,"Sentence":"it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).","Similarity":0.8165743343,"Cluster Number":1},{"Sentence Number":31,"Page":2,"Sentence":"This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more.","Similarity":0.8300331916,"Cluster Number":1},{"Sentence Number":32,"Page":2,"Sentence":"The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning.","Similarity":0.8494542937,"Cluster Number":1},{"Sentence Number":33,"Page":2,"Sentence":"Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.","Similarity":0.8033691124,"Cluster Number":1},{"Sentence Number":34,"Page":2,"Sentence":"The basic idea underlying our work is to treat every text processing problem as a \u201ctext-to-text\u201d problem, i.e.","Similarity":0.8831858152,"Cluster Number":0},{"Sentence Number":35,"Page":2,"Sentence":"taking text as input and producing new text as output.","Similarity":0.8321655771,"Cluster Number":1},{"Sentence Number":36,"Page":2,"Sentence":"This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al.","Similarity":0.8857269124,"Cluster Number":0},{"Sentence Number":37,"Page":2,"Sentence":"(2019b) tasks.","Similarity":0.6997619156,"Cluster Number":2},{"Sentence Number":38,"Page":2,"Sentence":"Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider.","Similarity":0.8867716876,"Cluster Number":0},{"Sentence Number":39,"Page":2,"Sentence":"We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document\n\n2. http:\/\/commoncrawl.org\n\n2","Similarity":0.838230219,"Cluster Number":1},{"Sentence Number":40,"Page":3,"Sentence":"Exploring the Limits of Transfer Learning\n\n\"translate English to German: That is good.\"","Similarity":0.7893471392,"Cluster Number":1},{"Sentence Number":41,"Page":3,"Sentence":"\"cola sentence: The course is jumping well.\"","Similarity":0.6812301263,"Cluster Number":3},{"Sentence Number":42,"Page":3,"Sentence":"\"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...\"\n\n\"stsb sentence1: The rhino grazed on the grass.","Similarity":0.753420374,"Cluster Number":2},{"Sentence Number":43,"Page":3,"Sentence":"sentence2: A rhino is grazing in a field.\"","Similarity":0.753414148,"Cluster Number":2},{"Sentence Number":44,"Page":3,"Sentence":"T5\n\n\"Das ist gut.\"","Similarity":0.6303426943,"Cluster Number":3},{"Sentence Number":45,"Page":3,"Sentence":"\"not acceptable\"\n\n\"six people hospitalized after a storm in attala county.\"","Similarity":0.6445636014,"Cluster Number":3},{"Sentence Number":46,"Page":3,"Sentence":"\"3.8\"\n\nFigure 1: A diagram of our text-to-text framework.","Similarity":0.8137711326,"Cluster Number":1},{"Sentence Number":47,"Page":3,"Sentence":"Every task we consider\u2014including translation, question answering, and classification\u2014is cast as feeding our model text as input and training it to generate some target text.","Similarity":0.9104707179,"Cluster Number":0},{"Sentence Number":48,"Page":3,"Sentence":"This allows us to use the same model, loss function, hyperparameters, etc.","Similarity":0.8382995713,"Cluster Number":1},{"Sentence Number":49,"Page":3,"Sentence":"across our diverse set of tasks.","Similarity":0.724094273,"Cluster Number":2},{"Sentence Number":50,"Page":3,"Sentence":"It also provides a standard testbed for the methods included in our empirical survey.","Similarity":0.7898385356,"Cluster Number":1},{"Sentence Number":51,"Page":3,"Sentence":"\u201cT5\u201d refers to our model, which we dub the \u201c T ext- t o- T ext T ransfer T ransformer\u201d.","Similarity":0.7395622369,"Cluster Number":2},{"Sentence Number":52,"Page":3,"Sentence":"summarization, and sentiment classification, to name a few.","Similarity":0.7909085255,"Cluster Number":1},{"Sentence Number":53,"Page":3,"Sentence":"With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.","Similarity":0.9062336086,"Cluster Number":0},{"Sentence Number":54,"Page":3,"Sentence":"We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands.","Similarity":0.7737094586,"Cluster Number":2},{"Sentence Number":55,"Page":3,"Sentence":"As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques.","Similarity":0.8219761441,"Cluster Number":1},{"Sentence Number":56,"Page":3,"Sentence":"We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider.","Similarity":0.8638079658,"Cluster Number":0},{"Sentence Number":57,"Page":3,"Sentence":"In order to perform experiments at this scale, we introduce the \u201cColossal Clean Crawled Corpus\u201d (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web.","Similarity":0.8650852235,"Cluster Number":0},{"Sentence Number":58,"Page":3,"Sentence":"Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.","Similarity":0.876143585,"Cluster Number":0},{"Sentence Number":59,"Page":3,"Sentence":"1\n\nThe remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider.","Similarity":0.858767173,"Cluster Number":0},{"Sentence Number":60,"Page":3,"Sentence":"In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP.","Similarity":0.854465693,"Cluster Number":1},{"Sentence Number":61,"Page":3,"Sentence":"At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks.","Similarity":0.8011638503,"Cluster Number":1},{"Sentence Number":62,"Page":3,"Sentence":"Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.","Similarity":0.7363897258,"Cluster Number":2},{"Sentence Number":63,"Page":3,"Sentence":"3","Similarity":0.4628240487,"Cluster Number":4},{"Sentence Number":64,"Page":4,"Sentence":"Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n\n2.","Similarity":0.6252198405,"Cluster Number":3},{"Sentence Number":65,"Page":4,"Sentence":"Setup\n\nBefore presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on.","Similarity":0.8616611357,"Cluster Number":0},{"Sentence Number":66,"Page":4,"Sentence":"We also introduce our approach for treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled Corpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text data.","Similarity":0.9129505805,"Cluster Number":0},{"Sentence Number":67,"Page":4,"Sentence":"We refer to our model and framework as the \u201c T ext- t o- T ext T ransfer T ransformer\u201d (T5).","Similarity":0.7892629911,"Cluster Number":1},{"Sentence Number":68,"Page":4,"Sentence":"2.1.","Similarity":0.6127899169,"Cluster Number":3},{"Sentence Number":69,"Page":4,"Sentence":"Model\n\nEarly results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017).","Similarity":0.8680161922,"Cluster Number":0},{"Sentence Number":70,"Page":4,"Sentence":"The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018).","Similarity":0.8168797662,"Cluster Number":1},{"Sentence Number":71,"Page":4,"Sentence":"Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture.","Similarity":0.7233502098,"Cluster Number":2},{"Sentence Number":72,"Page":4,"Sentence":"Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed.","Similarity":0.7785300534,"Cluster Number":2},{"Sentence Number":73,"Page":4,"Sentence":"Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials 3,4 for a more detailed introduction.","Similarity":0.8144876398,"Cluster Number":1},{"Sentence Number":74,"Page":4,"Sentence":"The primary building block of the Transformer is self-attention (Cheng et al., 2016).","Similarity":0.7883459337,"Cluster Number":2},{"Sentence Number":75,"Page":4,"Sentence":"Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence.","Similarity":0.834736295,"Cluster Number":1},{"Sentence Number":76,"Page":4,"Sentence":"The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks.","Similarity":0.8265951626,"Cluster Number":1},{"Sentence Number":77,"Page":4,"Sentence":"It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).","Similarity":0.8767358092,"Cluster Number":0},{"Sentence Number":78,"Page":4,"Sentence":"We empirically explore these architectural variants in Section 3.2.","Similarity":0.7964138498,"Cluster Number":1},{"Sentence Number":79,"Page":4,"Sentence":"Overall, our encoder-decoder Transformer implementation closely follows its originally- proposed form (Vaswani et al., 2017).","Similarity":0.8668227968,"Cluster Number":0},{"Sentence Number":80,"Page":4,"Sentence":"First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder.","Similarity":0.7177737405,"Cluster Number":2},{"Sentence Number":81,"Page":4,"Sentence":"The encoder consists of a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network.","Similarity":0.8257379142,"Cluster Number":1},{"Sentence Number":82,"Page":4,"Sentence":"Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.","Similarity":0.7777090557,"Cluster Number":2},{"Sentence Number":83,"Page":4,"Sentence":"We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.","Similarity":0.8344618187,"Cluster Number":1},{"Sentence Number":84,"Page":4,"Sentence":"After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent\u2019s input to its output.","Similarity":0.8322129131,"Cluster Number":1},{"Sentence Number":85,"Page":4,"Sentence":"Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack.","Similarity":0.8603205121,"Cluster Number":0},{"Sentence Number":86,"Page":4,"Sentence":"The decoder is similar in structure to the encoder except that it includes a standard attention\n\n3. http:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html 4. http:\/\/jalammar.github.io\/illustrated-transformer\/\n\n4","Similarity":0.8154430239,"Cluster Number":1},{"Sentence Number":87,"Page":5,"Sentence":"Exploring the Limits of Transfer Learning\n\nmechanism after each self-attention layer that attends to the output of the encoder.","Similarity":0.8089350951,"Cluster Number":1},{"Sentence Number":88,"Page":5,"Sentence":"The self-attention mechanism in the decoder also uses a form of autoregressive or causal self- attention, which only allows the model to attend to past outputs.","Similarity":0.8169563019,"Cluster Number":1},{"Sentence Number":89,"Page":5,"Sentence":"The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix.","Similarity":0.7748340905,"Cluster Number":2},{"Sentence Number":90,"Page":5,"Sentence":"All attention mechanisms in the Transformer are split up into independent \u201cheads\u201d whose outputs are concatenated before being further processed.","Similarity":0.7814723482,"Cluster Number":2},{"Sentence Number":91,"Page":5,"Sentence":"Since self-attention is order-independent (i.e.","Similarity":0.771111129,"Cluster Number":2},{"Sentence Number":92,"Page":5,"Sentence":"it is an operation on sets), it is common to provide an explicit position signal to the Transformer.","Similarity":0.6047480503,"Cluster Number":3},{"Sentence Number":93,"Page":5,"Sentence":"While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).","Similarity":0.8369804603,"Cluster Number":1},{"Sentence Number":94,"Page":5,"Sentence":"Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the \u201ckey\u201d and \u201cquery\u201d being compared in the self-attention mechanism.","Similarity":0.8903465944,"Cluster Number":0},{"Sentence Number":95,"Page":5,"Sentence":"We use a simplified form of position embeddings where each \u201cembedding\u201d is simply a scalar that is added to the corresponding logit used for computing the attention weights.","Similarity":0.8426653168,"Cluster Number":1},{"Sentence Number":96,"Page":5,"Sentence":"For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding.","Similarity":0.8545873748,"Cluster Number":1},{"Sentence Number":97,"Page":5,"Sentence":"Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.","Similarity":0.8395972097,"Cluster Number":1},{"Sentence Number":98,"Page":5,"Sentence":"In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding.","Similarity":0.8341641645,"Cluster Number":1},{"Sentence Number":99,"Page":5,"Sentence":"Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.","Similarity":0.846357979,"Cluster Number":1},{"Sentence Number":100,"Page":5,"Sentence":"To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al.","Similarity":0.8144851123,"Cluster Number":1},{"Sentence Number":101,"Page":5,"Sentence":"(2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme.","Similarity":0.8000372947,"Cluster Number":1},{"Sentence Number":102,"Page":5,"Sentence":"Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.","Similarity":0.8275182626,"Cluster Number":1},{"Sentence Number":103,"Page":5,"Sentence":"As part of our study, we experiment with the scalability of these models, i.e.","Similarity":0.7602935787,"Cluster Number":2},{"Sentence Number":104,"Page":5,"Sentence":"how their performance changes as they are made to have more parameters or layers.","Similarity":0.8187375357,"Cluster Number":1},{"Sentence Number":105,"Page":5,"Sentence":"Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation.","Similarity":0.8743098785,"Cluster Number":0},{"Sentence Number":106,"Page":5,"Sentence":"As a result, we use a combination of model and data parallelism and train models on \u201cslices\u201d of Cloud TPU Pods.","Similarity":0.8331761482,"Cluster Number":1},{"Sentence Number":107,"Page":5,"Sentence":"5 TPU pods are are multi-rack ML supercomputers that contain 1 , 024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines.","Similarity":0.7887643858,"Cluster Number":2},{"Sentence Number":108,"Page":5,"Sentence":"We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).","Similarity":0.8545661311,"Cluster Number":1},{"Sentence Number":109,"Page":5,"Sentence":"2.2.","Similarity":0.545220103,"Cluster Number":4},{"Sentence Number":110,"Page":5,"Sentence":"The Colossal Clean Crawled Corpus\n\nMuch of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning.","Similarity":0.899258937,"Cluster Number":0},{"Sentence Number":111,"Page":5,"Sentence":"In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data.","Similarity":0.8212984564,"Cluster Number":1},{"Sentence Number":112,"Page":5,"Sentence":"To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web.","Similarity":0.8461224081,"Cluster Number":1},{"Sentence Number":113,"Page":5,"Sentence":"Common\n\n5. https:\/\/cloud.google.com\/tpu\/\n\n5","Similarity":0.7177205511,"Cluster Number":2}]