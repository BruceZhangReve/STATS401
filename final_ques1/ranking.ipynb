{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fcb9c6-30cf-4046-8d30-124f349f0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert')\n",
    "\n",
    "# 加载模型\n",
    "model = BertModel.from_pretrained('bert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2cbc0d4-d41c-453d-91e8-cb5d62c3ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件到DataFrame\n",
    "df = pd.read_csv(\"paragraph.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "033ce199-d504-4f16-8f8a-778811656a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Number</th>\n",
       "      <th>Page</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The effectiveness of transfer learning has giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we explore the landscape of tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Our systematic study compares pre-training obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>By combining the insights from our exploration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>2.2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>The Colossal Clean Crawled Corpus\\n\\nMuch of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>In this paper, we are interested in measuring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>To generate data sets that satisfy our needs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence Number  Page                                           Sentence\n",
       "0                  1     1  Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...\n",
       "1                  2     1  The effectiveness of transfer learning has giv...\n",
       "2                  3     1  In this paper, we explore the landscape of tra...\n",
       "3                  4     1  Our systematic study compares pre-training obj...\n",
       "4                  5     1  By combining the insights from our exploration...\n",
       "..               ...   ...                                                ...\n",
       "108               23     5                                               2.2.\n",
       "109               24     5  The Colossal Clean Crawled Corpus\\n\\nMuch of t...\n",
       "110               25     5  In this paper, we are interested in measuring ...\n",
       "111               26     5  To generate data sets that satisfy our needs, ...\n",
       "112               27     5    Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5\n",
       "\n",
       "[113 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.data.path.append(\"nltk_data\")\n",
    "\n",
    "# 使用nltk将段落拆分为句子\n",
    "sentences = sent_tokenize(paragraph)\n",
    "# 输出分割后的句子\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a2f4408-2a32-4190-81fe-1531abd1c8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).', 'The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.', 'In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.', 'Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks.', 'By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.', 'To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.', '1\\n\\nKeywords: transfer learning, natural language processing, multi-task learning, attention- based models, deep learning\\n\\n1.', 'Introduction\\n\\nTraining a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning.', 'This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text.', 'This knowledge can range from low-level (e.g.', 'the spelling\\n\\n∗ .', 'Equal contribution.', 'A description of each author’s contribution is available in Appendix A.', 'Correspondence to craffel@gmail.com .', '1. https://github.com/google-research/text-to-text-transfer-transformer\\n\\n© 2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.', 'License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .', 'Attribution requirements are provided at http://jmlr.org/papers/v21/20-074.html .', 'Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\n\\nor meaning of words) to high-level (e.g.', 'that a tuba is too large to fit in most backpacks).', 'In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task.', 'For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors.', 'These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).', 'Recently, it has become increasingly common to pre-train the entire model on a data-rich task.', 'Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks.', 'In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).', 'In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.', 'This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019).', 'Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project 2 produces about 20TB of text data extracted from web pages each month.', 'This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e.', 'it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).', 'This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more.', 'The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning.', 'Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.', 'The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem, i.e.', 'taking text as input and producing new text as output.', 'This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al.', '(2019b) tasks.', 'Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider.', 'We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document\\n\\n2. http://commoncrawl.org\\n\\n2', 'Exploring the Limits of Transfer Learning\\n\\n\"translate English to German: That is good.\"', '\"cola sentence: The course is jumping well.\"', '\"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...\"\\n\\n\"stsb sentence1: The rhino grazed on the grass.', 'sentence2: A rhino is grazing in a field.\"', 'T5\\n\\n\"Das ist gut.\"', '\"not acceptable\"\\n\\n\"six people hospitalized after a storm in attala county.\"', '\"3.8\"\\n\\nFigure 1: A diagram of our text-to-text framework.', 'Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text.', 'This allows us to use the same model, loss function, hyperparameters, etc.', 'across our diverse set of tasks.', 'It also provides a standard testbed for the methods included in our empirical survey.', '“T5” refers to our model, which we dub the “ T ext- t o- T ext T ransfer T ransformer”.', 'summarization, and sentiment classification, to name a few.', 'With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.', 'We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands.', 'As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques.', 'We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider.', 'In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web.', 'Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.', '1\\n\\nThe remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider.', 'In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP.', 'At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks.', 'Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.', '3', 'Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\n\\n2.', 'Setup\\n\\nBefore presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on.', 'We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data.', 'We refer to our model and framework as the “ T ext- t o- T ext T ransfer T ransformer” (T5).', '2.1.', 'Model\\n\\nEarly results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the “Transformer” architecture (Vaswani et al., 2017).', 'The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018).', 'Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture.', 'Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed.', 'Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials 3,4 for a more detailed introduction.', 'The primary building block of the Transformer is self-attention (Cheng et al., 2016).', 'Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence.', 'The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks.', 'It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).', 'We empirically explore these architectural variants in Section 3.2.', 'Overall, our encoder-decoder Transformer implementation closely follows its originally- proposed form (Vaswani et al., 2017).', 'First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder.', 'The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network.', 'Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.', 'We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.', 'After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.', 'Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack.', 'The decoder is similar in structure to the encoder except that it includes a standard attention\\n\\n3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/\\n\\n4', 'Exploring the Limits of Transfer Learning\\n\\nmechanism after each self-attention layer that attends to the output of the encoder.', 'The self-attention mechanism in the decoder also uses a form of autoregressive or causal self- attention, which only allows the model to attend to past outputs.', 'The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix.', 'All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed.', 'Since self-attention is order-independent (i.e.', 'it is an operation on sets), it is common to provide an explicit position signal to the Transformer.', 'While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).', 'Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism.', 'We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.', 'For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding.', 'Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.', 'In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding.', 'Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.', 'To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al.', '(2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme.', 'Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.', 'As part of our study, we experiment with the scalability of these models, i.e.', 'how their performance changes as they are made to have more parameters or layers.', 'Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation.', 'As a result, we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.', '5 TPU pods are are multi-rack ML supercomputers that contain 1 , 024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines.', 'We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).', '2.2.', 'The Colossal Clean Crawled Corpus\\n\\nMuch of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning.', 'In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data.', 'To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web.', 'Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5']\n"
     ]
    }
   ],
   "source": [
    "# 初始化全局句子计数器\n",
    "global_sentence_number = 1\n",
    "\n",
    "# 初始化全局的句子列表\n",
    "sentences = []\n",
    "sentence_data = []\n",
    "\n",
    "# 遍历每一行，获取页码和内容\n",
    "for index, row in df.iterrows():\n",
    "    page = row['page']  # 获取所在的页码\n",
    "    paragraph = row['content']  # 获取段落内容\n",
    "    sentence_list = sent_tokenize(paragraph)  # 将段落分割为句子\n",
    "    \n",
    "    # 保存每个句子的编号和所在页码，不重置sentence_number\n",
    "    for sentence in sentence_list:\n",
    "        sentence_data.append({'Sentence Number': global_sentence_number, 'Page': page, 'Sentence': sentence})\n",
    "        sentences.append (sentence)\n",
    "        global_sentence_number += 1  # 增加全局句子编号\n",
    "\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "# 输出句子变量\n",
    "sentence_df\n",
    "print (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "643d3619-9159-4e11-aa62-75319b61efb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Number</th>\n",
       "      <th>Page</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The effectiveness of transfer learning has giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we explore the landscape of tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Our systematic study compares pre-training obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>By combining the insights from our exploration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "      <td>2.2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>The Colossal Clean Crawled Corpus\\n\\nMuch of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>In this paper, we are interested in measuring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>To generate data sets that satisfy our needs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence Number  Page                                           Sentence\n",
       "0                  1     1  Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...\n",
       "1                  2     1  The effectiveness of transfer learning has giv...\n",
       "2                  3     1  In this paper, we explore the landscape of tra...\n",
       "3                  4     1  Our systematic study compares pre-training obj...\n",
       "4                  5     1  By combining the insights from our exploration...\n",
       "..               ...   ...                                                ...\n",
       "108              109     5                                               2.2.\n",
       "109              110     5  The Colossal Clean Crawled Corpus\\n\\nMuch of t...\n",
       "110              111     5  In this paper, we are interested in measuring ...\n",
       "111              112     5  To generate data sets that satisfy our needs, ...\n",
       "112              113     5    Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5\n",
       "\n",
       "[113 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 确保nltk数据路径正确\n",
    "nltk.data.path.append(\"nltk_data\")\n",
    "\n",
    "# 创建用于存储每个句子的编号和所在的页码的数据\n",
    "sentence_data = []\n",
    "\n",
    "# 初始化全局句子计数器\n",
    "global_sentence_number = 1\n",
    "\n",
    "# 遍历每一行，获取页码和内容\n",
    "for index, row in df.iterrows():\n",
    "    page = row['page']  # 获取所在的页码\n",
    "    paragraph = row['content']  # 获取段落内容\n",
    "    sentences = sent_tokenize(paragraph)  # 将段落分割为句子\n",
    "    \n",
    "    # 保存每个句子的编号和所在页码，不重置sentence_number\n",
    "    for sentence in sentences:\n",
    "        sentence_data.append({'Sentence Number': global_sentence_number, 'Page': page, 'Sentence': sentence})\n",
    "        global_sentence_number += 1  # 增加全局句子编号\n",
    "\n",
    "# 创建新的DataFrame\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "\n",
    "# 输出结果\n",
    "sentence_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46f15270-1e1f-4d0d-a888-8c1d00b9dd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff5c090a-0673-4b49-b013-573f8e4c8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Editor: Ivan Titov\n",
      "\n",
      "Abstract\n",
      "\n",
      "Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. 1\n",
      "\n",
      "Keywords: transfer learning, natural language processing, multi-task learning, attention- based models, deep learning\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text. This knowledge can range from low-level (e.g. the spelling\n",
      "\n",
      "∗ . Equal contribution. A description of each author’s contribution is available in Appendix A. Correspondence to craffel@gmail.com . 1. https://github.com/google-research/text-to-text-transfer-transformer\n",
      "\n",
      "© 2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ . Attribution requirements are provided at http://jmlr.org/papers/v21/20-074.html . Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n",
      "\n",
      "or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b). Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks.\n",
      "1\n",
      "In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project 2 produces about 20TB of text data extracted from web pages each month. This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a). This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field. The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document\n",
      "\n",
      "2. http://commoncrawl.org\n",
      "\n",
      "2\n",
      "2\n",
      "Exploring the Limits of Transfer Learning\n",
      "\n",
      "\"translate English to German: That is good.\" \"cola sentence: The course is jumping well.\" \"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...\"\n",
      "\n",
      "\"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\" T5\n",
      "\n",
      "\"Das ist gut.\" \"not acceptable\"\n",
      "\n",
      "\"six people hospitalized after a storm in attala county.\" \"3.8\"\n",
      "\n",
      "Figure 1: A diagram of our text-to-text framework. Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. “T5” refers to our model, which we dub the “ T ext- t o- T ext T ransfer T ransformer”. summarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider. In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models. 1\n",
      "\n",
      "The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4. 3 Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n",
      "\n",
      "2.\n",
      "3\n",
      "Setup\n",
      "\n",
      "Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the “ T ext- t o- T ext T ransfer T ransformer” (T5). 2.1. Model\n",
      "\n",
      "Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the “Transformer” architecture (Vaswani et al., 2017). The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials 3,4 for a more detailed introduction. The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originally- proposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.\n",
      "4\n",
      "After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output. Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention\n",
      "\n",
      "3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/\n",
      "\n",
      "4 Exploring the Limits of Transfer Learning\n",
      "\n",
      "mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal self- attention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed. Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer. While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism. We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers. To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.\n",
      "5\n",
      "Final Summary: Transfer learning is where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task. In this paper, we explore the landscape of transfer learning techniques for NLP. We introduce a unified framework that converts all text-based language problems into a text-to-text format. We compare pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. Modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many common NLP benchmarks. The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem. Every task we consider is cast as feeding our model text as input and training it to generate some target text. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results. All models we study are based on the Transformer architecture. The Transformer consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. Our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 动态摘要的长度阈值\n",
    "max_char_length = 3000\n",
    "\n",
    "# 初始化变量\n",
    "current_chunk = \"\"\n",
    "summaries = []\n",
    "flag = 1\n",
    "# 动态汇总句子，按需生成摘要\n",
    "for sentence in sentences:\n",
    "    if len(current_chunk) + len(sentence) <= max_char_length:\n",
    "        # 如果当前汇总长度小于阈值，继续累加句子\n",
    "        current_chunk += \" \" + sentence\n",
    "    else:\n",
    "        # 如果超过阈值，对当前累加的文本进行摘要\n",
    "        print (current_chunk.strip())\n",
    "        print (flag)\n",
    "        flag += 1\n",
    "        summary = summarizer(current_chunk.strip(), min_length=25, do_sample=False)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "        \n",
    "        # 重置current_chunk并添加当前句子\n",
    "        current_chunk = sentence\n",
    "\n",
    "# 处理最后剩下的文本\n",
    "if current_chunk:\n",
    "    summary = summarizer(current_chunk.strip(), min_length=25, do_sample=False)\n",
    "    summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "# 合并所有摘要\n",
    "final_summary = \" \".join(summaries)\n",
    "print(\"Final Summary:\", final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97acf4c0-671e-419f-b1af-f1ae75af2ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning is where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task. Modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem.\n"
     ]
    }
   ],
   "source": [
    "summary_text = summarizer(final_summary, min_length=25, do_sample=False)\n",
    "summary_text = summary_text[0]['summary_text']\n",
    "print (summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a231c5df-71d6-4464-ade0-48225fd5a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: [0.93650322 0.78242584 0.85912806 0.88297509 0.87566812 0.85428819\n",
      " 0.8818037  0.89396976 0.88218859 0.75271964 0.48903396 0.55948072\n",
      " 0.72512338 0.6374271  0.82419598 0.67783637 0.720643   0.81283304\n",
      " 0.7351051  0.90876961 0.88695824 0.79361174 0.83870122 0.84022429\n",
      " 0.85525485 0.94326148 0.81206557 0.90192953 0.85364455 0.81657433\n",
      " 0.83003319 0.84945429 0.80336911 0.88318582 0.83216558 0.88572691\n",
      " 0.69976192 0.88677169 0.83823022 0.78934714 0.68123013 0.75342037\n",
      " 0.75341415 0.63034269 0.6445636  0.81377113 0.91047072 0.83829957\n",
      " 0.72409427 0.78983854 0.73956224 0.79090853 0.90623361 0.77370946\n",
      " 0.82197614 0.86380797 0.86508522 0.87614359 0.85876717 0.85446569\n",
      " 0.80116385 0.73638973 0.46282405 0.62521984 0.86166114 0.91295058\n",
      " 0.78926299 0.61278992 0.86801619 0.81687977 0.72335021 0.77853005\n",
      " 0.81448764 0.78834593 0.8347363  0.82659516 0.87673581 0.79641385\n",
      " 0.8668228  0.71777374 0.82573791 0.77770906 0.83446182 0.83221291\n",
      " 0.86032051 0.81544302 0.8089351  0.8169563  0.77483409 0.78147235\n",
      " 0.77111113 0.60474805 0.83698046 0.89034659 0.84266532 0.85458737\n",
      " 0.83959721 0.83416416 0.84635798 0.81448511 0.80003729 0.82751826\n",
      " 0.76029358 0.81873754 0.87430988 0.83317615 0.78876439 0.85456613\n",
      " 0.5452201  0.89925894 0.82129846 0.84612241 0.71772055]\n",
      "Ranked Sentences by Importance:\n",
      "In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.\n",
      "Editor: Ivan Titov\n",
      "\n",
      "Abstract\n",
      "\n",
      "Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).\n",
      "We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data.\n",
      "Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text.\n",
      "In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task.\n",
      "With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.\n",
      "Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project 2 produces about 20TB of text data extracted from web pages each month.\n",
      "The Colossal Clean Crawled Corpus\n",
      "\n",
      "Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning.\n",
      "Introduction\n",
      "\n",
      "Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning.\n",
      "Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism.\n",
      "For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors.\n",
      "Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider.\n",
      "This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al.\n",
      "The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem, i.e.\n",
      "Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks.\n",
      "This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text.\n",
      "1\n",
      "\n",
      "Keywords: transfer learning, natural language processing, multi-task learning, attention- based models, deep learning\n",
      "\n",
      "1.\n",
      "It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).\n",
      "Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.\n",
      "By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.\n",
      "Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation.\n",
      "Model\n",
      "\n",
      "Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the “Transformer” architecture (Vaswani et al., 2017).\n",
      "Overall, our encoder-decoder Transformer implementation closely follows its originally- proposed form (Vaswani et al., 2017).\n",
      "In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web.\n",
      "We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider.\n",
      "Setup\n",
      "\n",
      "Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on.\n",
      "Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack.\n",
      "In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.\n",
      "1\n",
      "\n",
      "The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider.\n",
      "In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).\n",
      "For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding.\n",
      "We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).\n",
      "In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP.\n",
      "To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.\n",
      "This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e.\n",
      "The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning.\n",
      "Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.\n",
      "To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web.\n",
      "We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.\n",
      "Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks.\n",
      "Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.\n",
      "Recently, it has become increasingly common to pre-train the entire model on a data-rich task.\n",
      "This allows us to use the same model, loss function, hyperparameters, etc.\n",
      "We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document\n",
      "\n",
      "2. http://commoncrawl.org\n",
      "\n",
      "2\n",
      "While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\n",
      "Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence.\n",
      "We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.\n",
      "In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding.\n",
      "As a result, we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.\n",
      "After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.\n",
      "taking text as input and producing new text as output.\n",
      "This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more.\n",
      "Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.\n",
      "The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks.\n",
      "The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network.\n",
      "1. https://github.com/google-research/text-to-text-transfer-transformer\n",
      "\n",
      "© 2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n",
      "As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques.\n",
      "In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data.\n",
      "how their performance changes as they are made to have more parameters or layers.\n",
      "The self-attention mechanism in the decoder also uses a form of autoregressive or causal self- attention, which only allows the model to attend to past outputs.\n",
      "The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018).\n",
      "it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).\n",
      "The decoder is similar in structure to the encoder except that it includes a standard attention\n",
      "\n",
      "3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/\n",
      "\n",
      "4\n",
      "Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials 3,4 for a more detailed introduction.\n",
      "To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al.\n",
      "\"3.8\"\n",
      "\n",
      "Figure 1: A diagram of our text-to-text framework.\n",
      "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n",
      "\n",
      "or meaning of words) to high-level (e.g.\n",
      "This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019).\n",
      "Exploring the Limits of Transfer Learning\n",
      "\n",
      "mechanism after each self-attention layer that attends to the output of the encoder.\n",
      "Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.\n",
      "At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks.\n",
      "(2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme.\n",
      "We empirically explore these architectural variants in Section 3.2.\n",
      "These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).\n",
      "summarization, and sentiment classification, to name a few.\n",
      "It also provides a standard testbed for the methods included in our empirical survey.\n",
      "Exploring the Limits of Transfer Learning\n",
      "\n",
      "\"translate English to German: That is good.\"\n",
      "We refer to our model and framework as the “ T ext- t o- T ext T ransfer T ransformer” (T5).\n",
      "5 TPU pods are are multi-rack ML supercomputers that contain 1 , 024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines.\n",
      "The primary building block of the Transformer is self-attention (Cheng et al., 2016).\n",
      "The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.\n",
      "All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed.\n",
      "Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed.\n",
      "Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.\n",
      "The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix.\n",
      "We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands.\n",
      "Since self-attention is order-independent (i.e.\n",
      "As part of our study, we experiment with the scalability of these models, i.e.\n",
      "\"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...\"\n",
      "\n",
      "\"stsb sentence1: The rhino grazed on the grass.\n",
      "sentence2: A rhino is grazing in a field.\"\n",
      "This knowledge can range from low-level (e.g.\n",
      "“T5” refers to our model, which we dub the “ T ext- t o- T ext T ransfer T ransformer”.\n",
      "Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.\n",
      "that a tuba is too large to fit in most backpacks).\n",
      "A description of each author’s contribution is available in Appendix A.\n",
      "across our diverse set of tasks.\n",
      "Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture.\n",
      "Attribution requirements are provided at http://jmlr.org/papers/v21/20-074.html .\n",
      "First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder.\n",
      "Common\n",
      "\n",
      "5. https://cloud.google.com/tpu/\n",
      "\n",
      "5\n",
      "(2019b) tasks.\n",
      "\"cola sentence: The course is jumping well.\"\n",
      "License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .\n",
      "\"not acceptable\"\n",
      "\n",
      "\"six people hospitalized after a storm in attala county.\"\n",
      "Correspondence to craffel@gmail.com .\n",
      "T5\n",
      "\n",
      "\"Das ist gut.\"\n",
      "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n",
      "\n",
      "2.\n",
      "2.1.\n",
      "it is an operation on sets), it is common to provide an explicit position signal to the Transformer.\n",
      "Equal contribution.\n",
      "2.2.\n",
      "the spelling\n",
      "\n",
      "∗ .\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 使用本地预训练的BERT模型进行句子嵌入\n",
    "nlp = pipeline(\"feature-extraction\", model=\"bert\", device=-1)\n",
    "\n",
    "# 获取每个句子的嵌入向量\n",
    "embeddings = [nlp(sentence)[0] for sentence in sentences]\n",
    "\n",
    "# 使用第一个token [CLS] 的嵌入作为句子的表示\n",
    "embeddings = [np.mean(np.array(embedding), axis=0) for embedding in embeddings]\n",
    "\n",
    "# 获取摘要文本的嵌入向量，并将其作为参考\n",
    "summary_embedding = np.mean(np.array(nlp(summary_text)[0]), axis=0)\n",
    "\n",
    "# 计算摘要嵌入与其他句子嵌入的相似度\n",
    "similarities = cosine_similarity([summary_embedding], embeddings)[0]\n",
    "\n",
    "# 输出相似度\n",
    "print(\"Similarities:\", similarities)\n",
    "\n",
    "# 排序句子\n",
    "ranked_sentences = [x for _, x in sorted(zip(similarities, sentences), reverse=True)]\n",
    "\n",
    "print(\"Ranked Sentences by Importance:\")\n",
    "for sentence in ranked_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "feebf690-eab0-4c76-9ab8-462cfe28826f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Number</th>\n",
       "      <th>Page</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...</td>\n",
       "      <td>0.936503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The effectiveness of transfer learning has giv...</td>\n",
       "      <td>0.782426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we explore the landscape of tra...</td>\n",
       "      <td>0.859128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Our systematic study compares pre-training obj...</td>\n",
       "      <td>0.882975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>By combining the insights from our exploration...</td>\n",
       "      <td>0.875668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "      <td>2.2.</td>\n",
       "      <td>0.545220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>The Colossal Clean Crawled Corpus\\n\\nMuch of t...</td>\n",
       "      <td>0.899259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>In this paper, we are interested in measuring ...</td>\n",
       "      <td>0.821298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>To generate data sets that satisfy our needs, ...</td>\n",
       "      <td>0.846122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5</td>\n",
       "      <td>0.717721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence Number  Page                                           Sentence  \\\n",
       "0                  1     1  Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...   \n",
       "1                  2     1  The effectiveness of transfer learning has giv...   \n",
       "2                  3     1  In this paper, we explore the landscape of tra...   \n",
       "3                  4     1  Our systematic study compares pre-training obj...   \n",
       "4                  5     1  By combining the insights from our exploration...   \n",
       "..               ...   ...                                                ...   \n",
       "108              109     5                                               2.2.   \n",
       "109              110     5  The Colossal Clean Crawled Corpus\\n\\nMuch of t...   \n",
       "110              111     5  In this paper, we are interested in measuring ...   \n",
       "111              112     5  To generate data sets that satisfy our needs, ...   \n",
       "112              113     5    Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5   \n",
       "\n",
       "     Similarity  \n",
       "0      0.936503  \n",
       "1      0.782426  \n",
       "2      0.859128  \n",
       "3      0.882975  \n",
       "4      0.875668  \n",
       "..          ...  \n",
       "108    0.545220  \n",
       "109    0.899259  \n",
       "110    0.821298  \n",
       "111    0.846122  \n",
       "112    0.717721  \n",
       "\n",
       "[113 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df['Similarity'] = similarities\n",
    "sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "809b388e-17d5-4460-bb58-6fa686e15c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustered Sentences by Sorted Average Similarity:\n",
      "\n",
      "Cluster 0 (Old Cluster 4) - Average Similarity: 0.8862\n",
      "Size: 29\n",
      "Editor: Ivan Titov\n",
      "\n",
      "Abstract\n",
      "\n",
      "Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).\n",
      "In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.\n",
      "Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks.\n",
      "By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.\n",
      "1\n",
      "\n",
      "Keywords: transfer learning, natural language processing, multi-task learning, attention- based models, deep learning\n",
      "\n",
      "1.\n",
      "Introduction\n",
      "\n",
      "Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning.\n",
      "This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text.\n",
      "In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task.\n",
      "For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors.\n",
      "In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.\n",
      "Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project 2 produces about 20TB of text data extracted from web pages each month.\n",
      "The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem, i.e.\n",
      "This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al.\n",
      "Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider.\n",
      "Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text.\n",
      "With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.\n",
      "We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider.\n",
      "In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web.\n",
      "Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.\n",
      "1\n",
      "\n",
      "The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider.\n",
      "Setup\n",
      "\n",
      "Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on.\n",
      "We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data.\n",
      "Model\n",
      "\n",
      "Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the “Transformer” architecture (Vaswani et al., 2017).\n",
      "It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019).\n",
      "Overall, our encoder-decoder Transformer implementation closely follows its originally- proposed form (Vaswani et al., 2017).\n",
      "Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack.\n",
      "Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism.\n",
      "Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation.\n",
      "The Colossal Clean Crawled Corpus\n",
      "\n",
      "Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning.\n",
      "\n",
      "Cluster 1 (Old Cluster 0) - Average Similarity: 0.8252\n",
      "Size: 49\n",
      "To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.\n",
      "1. https://github.com/google-research/text-to-text-transfer-transformer\n",
      "\n",
      "© 2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n",
      "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n",
      "\n",
      "or meaning of words) to high-level (e.g.\n",
      "These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).\n",
      "Recently, it has become increasingly common to pre-train the entire model on a data-rich task.\n",
      "Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks.\n",
      "In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).\n",
      "This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019).\n",
      "This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e.\n",
      "it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).\n",
      "This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more.\n",
      "The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning.\n",
      "Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.\n",
      "taking text as input and producing new text as output.\n",
      "We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document\n",
      "\n",
      "2. http://commoncrawl.org\n",
      "\n",
      "2\n",
      "Exploring the Limits of Transfer Learning\n",
      "\n",
      "\"translate English to German: That is good.\"\n",
      "\"3.8\"\n",
      "\n",
      "Figure 1: A diagram of our text-to-text framework.\n",
      "This allows us to use the same model, loss function, hyperparameters, etc.\n",
      "It also provides a standard testbed for the methods included in our empirical survey.\n",
      "summarization, and sentiment classification, to name a few.\n",
      "As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques.\n",
      "In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP.\n",
      "At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks.\n",
      "We refer to our model and framework as the “ T ext- t o- T ext T ransfer T ransformer” (T5).\n",
      "The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018).\n",
      "Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials 3,4 for a more detailed introduction.\n",
      "Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence.\n",
      "The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks.\n",
      "We empirically explore these architectural variants in Section 3.2.\n",
      "The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network.\n",
      "We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.\n",
      "After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.\n",
      "The decoder is similar in structure to the encoder except that it includes a standard attention\n",
      "\n",
      "3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/\n",
      "\n",
      "4\n",
      "Exploring the Limits of Transfer Learning\n",
      "\n",
      "mechanism after each self-attention layer that attends to the output of the encoder.\n",
      "The self-attention mechanism in the decoder also uses a form of autoregressive or causal self- attention, which only allows the model to attend to past outputs.\n",
      "While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\n",
      "We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.\n",
      "For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding.\n",
      "Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.\n",
      "In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding.\n",
      "Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.\n",
      "To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al.\n",
      "(2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme.\n",
      "Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.\n",
      "how their performance changes as they are made to have more parameters or layers.\n",
      "As a result, we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.\n",
      "We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).\n",
      "In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data.\n",
      "To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web.\n",
      "\n",
      "Cluster 2 (Old Cluster 3) - Average Similarity: 0.7511\n",
      "Size: 23\n",
      "The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.\n",
      "This knowledge can range from low-level (e.g.\n",
      "A description of each author’s contribution is available in Appendix A.\n",
      "Attribution requirements are provided at http://jmlr.org/papers/v21/20-074.html .\n",
      "that a tuba is too large to fit in most backpacks).\n",
      "(2019b) tasks.\n",
      "\"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...\"\n",
      "\n",
      "\"stsb sentence1: The rhino grazed on the grass.\n",
      "sentence2: A rhino is grazing in a field.\"\n",
      "across our diverse set of tasks.\n",
      "“T5” refers to our model, which we dub the “ T ext- t o- T ext T ransfer T ransformer”.\n",
      "We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands.\n",
      "Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.\n",
      "Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture.\n",
      "Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed.\n",
      "The primary building block of the Transformer is self-attention (Cheng et al., 2016).\n",
      "First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder.\n",
      "Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.\n",
      "The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix.\n",
      "All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed.\n",
      "Since self-attention is order-independent (i.e.\n",
      "As part of our study, we experiment with the scalability of these models, i.e.\n",
      "5 TPU pods are are multi-rack ML supercomputers that contain 1 , 024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines.\n",
      "Common\n",
      "\n",
      "5. https://cloud.google.com/tpu/\n",
      "\n",
      "5\n",
      "\n",
      "Cluster 3 (Old Cluster 2) - Average Similarity: 0.6393\n",
      "Size: 8\n",
      "Correspondence to craffel@gmail.com .\n",
      "License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .\n",
      "\"cola sentence: The course is jumping well.\"\n",
      "T5\n",
      "\n",
      "\"Das ist gut.\"\n",
      "\"not acceptable\"\n",
      "\n",
      "\"six people hospitalized after a storm in attala county.\"\n",
      "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n",
      "\n",
      "2.\n",
      "2.1.\n",
      "it is an operation on sets), it is common to provide an explicit position signal to the Transformer.\n",
      "\n",
      "Cluster 4 (Old Cluster 1) - Average Similarity: 0.5141\n",
      "Size: 4\n",
      "the spelling\n",
      "\n",
      "∗ .\n",
      "Equal contribution.\n",
      "3\n",
      "2.2.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "\n",
    "# 使用 MiniBatchKMeans 进行聚类\n",
    "n_clusters = 5  # 你想要的聚类数量\n",
    "mini_kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0, batch_size=10)\n",
    "cluster_labels = mini_kmeans.fit_predict(similarities.reshape(-1, 1))\n",
    "\n",
    "# 计算每个聚类的相似度平均值\n",
    "cluster_similarities = {i: [] for i in range(n_clusters)}\n",
    "clustered_sentences = {i: [] for i in range(n_clusters)}  # 存储每个簇的句子\n",
    "\n",
    "# 将句子和相似度根据聚类标签分组\n",
    "for label, similarity, sentence in zip(cluster_labels, similarities, sentences):\n",
    "    cluster_similarities[label].append(similarity)\n",
    "    clustered_sentences[label].append(sentence)\n",
    "\n",
    "# 输出每个聚类的平均相似度\n",
    "cluster_avg_similarities = {cluster: np.mean(sim_values) for cluster, sim_values in cluster_similarities.items()}\n",
    "\n",
    "# 对聚类按平均相似度排序\n",
    "sorted_clusters = sorted(cluster_avg_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_cluster_mapping = {old_label: new_label for new_label, (old_label, _) in enumerate(sorted_clusters)}\n",
    "\n",
    "# 更新 cluster_labels 为新的顺序编码\n",
    "new_cluster_labels = [sorted_cluster_mapping[label] for label in cluster_labels]\n",
    "\n",
    "# 将句子按新的簇标签分组\n",
    "new_clustered_sentences = {i: [] for i in range(n_clusters)}\n",
    "for new_label, sentence in zip(new_cluster_labels, sentences):\n",
    "    new_clustered_sentences[new_label].append(sentence)\n",
    "\n",
    "# 按新的簇标签输出排序后的句子\n",
    "# 输出排序后每个聚类的平均相似度和句子\n",
    "print(\"\\nClustered Sentences by Sorted Average Similarity:\")\n",
    "for new_label, (old_label, avg_similarity) in enumerate(sorted_clusters):\n",
    "    print(f\"\\nCluster {new_label} (Old Cluster {old_label}) - Average Similarity: {avg_similarity:.4f}\")\n",
    "    clustered_group = new_clustered_sentences[new_label]\n",
    "    print(f\"Size: {len(clustered_group)}\")\n",
    "    for sentence in clustered_group:\n",
    "        print(sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56feca71-04c9-4824-ad71-d1e78592cf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Number</th>\n",
       "      <th>Page</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Cluster Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...</td>\n",
       "      <td>0.936503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The effectiveness of transfer learning has giv...</td>\n",
       "      <td>0.782426</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we explore the landscape of tra...</td>\n",
       "      <td>0.859128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Our systematic study compares pre-training obj...</td>\n",
       "      <td>0.882975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>By combining the insights from our exploration...</td>\n",
       "      <td>0.875668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "      <td>2.2.</td>\n",
       "      <td>0.545220</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>The Colossal Clean Crawled Corpus\\n\\nMuch of t...</td>\n",
       "      <td>0.899259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>In this paper, we are interested in measuring ...</td>\n",
       "      <td>0.821298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>To generate data sets that satisfy our needs, ...</td>\n",
       "      <td>0.846122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5</td>\n",
       "      <td>0.717721</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence Number  Page                                           Sentence  \\\n",
       "0                  1     1  Editor: Ivan Titov\\n\\nAbstract\\n\\nTransfer lea...   \n",
       "1                  2     1  The effectiveness of transfer learning has giv...   \n",
       "2                  3     1  In this paper, we explore the landscape of tra...   \n",
       "3                  4     1  Our systematic study compares pre-training obj...   \n",
       "4                  5     1  By combining the insights from our exploration...   \n",
       "..               ...   ...                                                ...   \n",
       "108              109     5                                               2.2.   \n",
       "109              110     5  The Colossal Clean Crawled Corpus\\n\\nMuch of t...   \n",
       "110              111     5  In this paper, we are interested in measuring ...   \n",
       "111              112     5  To generate data sets that satisfy our needs, ...   \n",
       "112              113     5    Common\\n\\n5. https://cloud.google.com/tpu/\\n\\n5   \n",
       "\n",
       "     Similarity  Cluster Number  \n",
       "0      0.936503               0  \n",
       "1      0.782426               2  \n",
       "2      0.859128               0  \n",
       "3      0.882975               0  \n",
       "4      0.875668               0  \n",
       "..          ...             ...  \n",
       "108    0.545220               4  \n",
       "109    0.899259               0  \n",
       "110    0.821298               1  \n",
       "111    0.846122               1  \n",
       "112    0.717721               2  \n",
       "\n",
       "[113 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df['Cluster Number'] = new_cluster_labels\n",
    "sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91238e95-dfc9-45de-b32e-ddc3cb35c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.to_csv('clustered_sentences.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c306492d-59ba-4288-b039-7bd8b1af176e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clustered_sentences.json'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataframe to JSON format for use in the HTML page\n",
    "json_data = sentence_df.to_json(orient='records')\n",
    "\n",
    "# Save the JSON data to a file so it can be used later\n",
    "json_file_path = 'clustered_sentences.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "json_file_path  # Return the path of the saved JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdd578-633a-40e6-aee4-54e24c0ae641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
