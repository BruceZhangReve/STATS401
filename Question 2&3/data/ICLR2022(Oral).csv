"Papers_Title","Papers_Keywords","Papers_OfficialReview_list1_StrengthandWeakness","Papers_OfficialReview_list1_Rate","Papers_OfficialReview_list1_Confidence"
"Domino: Discovering Systematic Errors with Cross-Modal Embeddings","Keywords: robustness, subgroup analysis, error analysis, multimodal, slice discovery","Pros:
I believe that there is well backed motivation for work based off of the plentiful literature review. There is novel integration of CLIP as well as SDMs previously not combined before A variety of datasets seem to show that the proposed method is useful. Code is available for reproducibility.
Cons:
I would rather the the literature review section be in the intro rather than in the conclusion for flow. Then, the authors could summarize their work in the conclusion instead.
Are textual descriptions of the Slices actually actionable for real life experts? I would like if physicians found the textual descriptions of the MIMIC and EEG dataset slices useful.","8: accept, good paper","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Domino: Discovering Systematic Errors with Cross-Modal Embeddings","Keywords: robustness, subgroup analysis, error analysis, multimodal, slice discovery","The paper addresses interesting problems in the area of slice discovery, particularly underperforming clusters. Overall the Domino approach seems to work well for this task. There are ablation studies on the embeddings used and clustering algorithm chosen. The part about generating natural language explanation is not as convincing as it's ultimately using the text embeddings on a large corpus in a retrieval setting. Actually generating text from the given slice embeddings would have been more convincing. Currently the text seems to be restricted to single words from Fig 5. Though the idea introduced in the paper is quite interesting, the paper itself is organised in a very confusing manner. Related work is only introduced in section 6 on page 9 and seems incomplete. The Domino method is shown in Figure 1 on page 2 but the actual text describing it is on page 6. Another weakness is the novelty. Though the framework is novel, the individual models are not. Given the new evaluation framework, it would have been great to introduce some further technical novelty. In the experiments section, the results are described as-is without further interpretation. Do the results with the evaluation framework indicate specific trends? If so, why? Maybe further investigation on this would provide some ideas for future exploration - which are also missing in the paper.","8: accept, good paper","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Domino: Discovering Systematic Errors with Cross-Modal Embeddings","Keywords: robustness, subgroup analysis, error analysis, multimodal, slice discovery","General comments:
The paper addresses the problem of identifying on which subsets of data machine learning models make systematic errors. The authors term this as a ""slice discovery method"" (SDM). Two desirable properties for a SDM are outlined: First, providing a quantitative evaluation framework for measuring performance of SDMs and secondly, ensuring a solution that is ""coherent"". Here coherence is defined as being understandable by a domain experts, The proposed evaluation framework seems rather ad-hoc and poorly justified. The main idea, the DOMINO approach seems limited to images with captions. In the approach images and captions are separately embedded while preserving their similarity. This is followed by a mixture model which identifies errors in the model predictions, but it is not clear to me where the actual model predictions are trained. Also, the paper is often written in a confusing manner that makes it difficult to follow and understand the the contributions of the paper. Furthermore there is a lack of definitions for many of the terminology used in the paper.
Specific comments:
I find the term ""slice discovery method"" misleading, it is not commonly used term in this field.
The definition of the slice discovery problem (section 2) is rather imprecise and uses formulations such as ""exhibits degraded performance"". No precise definition is given what ""degraded"" means in this context.
After reading the definition of the slice discovery problem (section 2), I am still not clear whether a slice can refer to whole images only or part of the images (or frames in a video).","n/a","6: marginally above the acceptance threshold"
"Natural Language Descriptions of Deep Visual Features","Abstract: Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.","Strength
This paper is well-written and easy to follow. The authors provide sufficient technical details for readers to understand and reproduce their work. Data, code, and the trained model will be open source.
This paper picks up an intriguing topic that aims to interpret deep learning models by investigating hidden units and summarizing their exemplar activation by natural language descriptions. The proposed method is concise, straightforward, and well-motivated.
The natural language descriptions can capture categorical, relational, and logical structure across different levels in the learned features. It’s nice to see low-level features like edges (“the top boundaries of horizontal objects”), middle-level features like shapes (“Poles and legs”), and relatively high-level features like objects (“dog faces”) could all be generated quite well by this same model.
The results suggest generalizability across different model architectures, datasets, and tasks. This makes MILAN readily useful for many other potential applications, including the three interesting experiments shown in section 5-7.
Comments
The model is trained on a newly collected dataset MILANNOTATIONS. Each unit was annotated by three human participants. But the inter-annotator agreement among human annotations seems not quite high (Table 4, Figure 10), compared to the BERTScore between model-generated descriptions and human annotations (Table 2, Table 3). How did the authors handle this inter-annotator inconsistency during their model training? Is there any additional quality control/validation performed for this MILANNOTATIONS dataset?
Following the first point, I wonder if the authors would consider scaling up their methods by leveraging the existing large, multimodal datasets like GQA/Visual Genome or visual-language model trained on large paired image-text datasets like CLIP/ALIGN?
Fig. 3 is not referenced in the main text. And I think these failure modes are interesting, and taking a closer look at them might be inspiring for improving this model in future studies. Do authors have further comments or thoughts about this result?
The results in Fig. 4 are quite interesting. The bar chart suggests low-level visual features are more described by adjectives, middle-level units need more prepositions and verbs to describe relational features, and high-level units need more complex composition of words thus resulting in longer length and deeper parser trees. This probably aligns well with our intuition and expectation. For units that may contribute to those ""non-robust"" model behavior, are they described by more nouns with higher max word diff? Will the proposed MILAN model be able to detect those ""non-robust"" units and edit the network to improve its performance?
Minor:
What do different dots refer to in Fig. 5?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Natural Language Descriptions of Deep Visual Features","-''-","The paper is about visualization and explainability. It is good read and inspirational to me since I have been an advocate of the sub-field. To be able go deeper on making use of the intermediate stages of a network, for visibility as well as new AI product features. The summary above shows the short of the discoveries. I have to experiment with the technique myself to go further. However, it looks promising.
I also appreciate the amount of effort put on testing the system, on many architectures, datasets, and tasks. If there is more, I would like to have better characterization of the limits of the approach.
Grammars, typos, etc.: pg.1, par.1: convlution —> convolution","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Natural Language Descriptions of Deep Visual Features","-''-","I want to preface my review by stating that I am a domain expert in NLP and not CV. While I attempted a preliminary search for related work, I am not sure if there is prior work in specifically generating language descriptions of neurons (I understand that there is prior work in explaining neuron behaviour by examining inputs that activate it). Hence I may significantly modify my review based on the response of my peer reviewers.
This is a strong piece of work that clearly states its hypotheses and carefully designs experiments to test said hypotheses. The technical novel of the particular method is low, however I do not think that is the point of this work. What this paper does show is a novel way to interpret model behaviour, and allows for very useful downstream applications (e.g. fast filtering of neurons activated by a particular feature through text selection). I advocate for acceptance of this work, and propose some potential improvement. I do have some concerns about the scoping of this work. The title and description suggests a more general method, however experiments are purely based on images. I think the paper title should be more finely scoped (NL descriptions of Deep Image Features).
Major:
Do these transfer results generalize to other tasks? or are they specific to image models? Or are they specific to the fact that all of these image models are trained/evaluated on (two) datasets that are similar to each other in terms of image distribution? I think perhaps the paper makes this technique seem more portable than it actually is. For example I would have to collect data for my task distribution in order for this technique to work. If you can show this working with pretrained zero-shot image captioning models than this result would be a lot more convincing.
How do image captioning models do on the task of generating descriptions conditioned on inputs? The implicit hypothesis here is that they do not work well because they operate on higher levels of abstraction, but I would like to see empirical results of this by using caption models as baselines (e.g. zero-shot, fine-tuned).
What do generated descriptions look like? How long are the descriptions? What is the vocabulary size? How diverse are the descriptions?
What does the released dataset look like? Do you control for bias in the description? Diversity? Coverage?
This objective not only needs to maximize over all input space but also all description space. The former is talked about but the latter is skimmed over (by stating that beam search is performed). It's unclear whether beam search would yield useful description if the description is more complex (e.g. in a language task like question-answering).
It would be nice to show spurious feature filtering results on something that is not a contrived task (e.g. classification with spurious text labels on top left corner of image) and instead on something impactful (e.g. removing racial/gender bias by filtering out neurons with corresponding descriptions).
Minor:
typo; convlutional network
Questions:
Why is there so much variance across architecture pairs? For example large gains on AlexNet -> Places and ResNet -> ImageNet but small gains on ResNet -> Places and AlexNet -> ImageNet.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization","Keywords: Domain Adaptation, Transfer Learning, Societal Considerations of Representation Learning, Model Watermark","Pros:
The research direction is promising and important in the real world. Nowadays, AI companies will train their own deep models with abundant labelled data that costs a lot of resources. Thus, it is a good timing to research how to protect these models, which have become very important and practical.
This paper proposed a method that can be effective solutions to both model verification and authorization, which is general and is promising to be applied in other applications.
This paper is easy to follow. Experiments are enough to support the claims made in this paper. A plus should be that experiments are conducted with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets.
Cons:
The presentation should be improved. The first paragraph in intro is too long. It is better to divided it into several paragraphs to better demonstrate the key points of this paper.
I am not sure if it is necessary to list the contributions in the introduction. Such contributions have been described clearly in intro and abs. It seems that you do not need to restate them.
Key related works are missing. For an AI company, they need to be aware of many adversarial attacks, such as reprogramming attacks, model-inversion attacks. These works are also related to IP protection of deep learning. It would be better to conclude these attacks as related works as well. Some discussions should be also added for general readers of ICLR.
Some notations should be changed. For example, we will not use X or Y to present distributions, instead, we will use them to represent random variables. It is better to use \sP_X to represent the distribution corresponding to a random variable X. It is unnecessary to use GMMD, you can use MMD(P,Q; k), where k is a Gaussian kernel (you can follow the notations from recent deep kernel MMD papers).
How many times do you repeat your experiments? I did not see error bar/STD values of your methods. This should be provided to verify that the experimental results are stable.
If we consider to add bandwidth to your kernel function, how does the kernel bandwidth affect your results?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization","Keywords: Domain Adaptation, Transfer Learning, Societal Considerations of Representation Learning, Model Watermark","Basically, the authors design a clever technique for learning nuisance-dependent representations. Such a representation can be made to perform accurately for a particular source domain, but poorly for another target domain. Furthermore, the authors design a GAN type technique for generating samples outside the source domain to serve as a kind of generic target domain. This is obviously important, as one cannot know to which target domain the model would be later adapted to.
This is a very interesting paper, although I have to say I'm not an expert in this topic at all. Most of the paper is really nicely written and is pretty easy to follow. The experimental verification is clear and detailed, but mostly limited to small images, so it's hard to say how it actually performs in some real-life scenarios.
Couple questions come to mind:
Can you imagine uses of this to other kinds of models, e.g., language models, or is this mainly meaningful for image data?
It sounds like an NTL representation by nature is highly vulnerable to training data privacy attacks, like membership inference. Have you considered if one could use the NTL representation to particularly efficiently generate samples from (something close to) the training data distribution?","No particular concerns. The authors already addressed some in their submission.","8: accept, good paper"
"Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization","Keywords: Domain Adaptation, Transfer Learning, Societal Considerations of Representation Learning, Model Watermark","This paper could be significantly improved via addressing the following issues:
In Table 1, what is the number of training epoches when transfering MT to MM? Did you try to increase the epochs of fine-tuning? If you train for enough epochs, the model would eventually reach the original accuracy. The sensitivity analysis regarding the epoches of your fine-tuning is necessary when compared to training from scratch and the transfer learning from the original model to the target task.
The training complexity of using your NTL approach and the GAN training should be introduced in this paper? The computing time of the MMDs during each time step is at least twice your training time?
The propsoed methodology is well presented. However, the differences between the proposed model and realted SOTA works should be presented clearly.
Comparing Table 2 and Table 3, it can be seen that sometimes the source-only method shows greater performance compared to the target-specific method. The reasons why would this happen are interesting since providing the target-domain target should be more accurate when removing some part in the generalization space. However, the experiments seem does not agree with it.
A future research section should be added in the revision.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Neural Structured Prediction for Inductive Node Classification","Abstract: This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.","Strengths
Paper is well written
The proposed method is a novel contribution that combines ideas from CRFs and GNNs.
Weaknesses
I consider the experiment section to be a minor weakness. I understand that the method sits in between CRFs and GNNs, but it would be great to compare against methods beyond those from CRFs and GNNs, e.g., SSVMs and others.
Another minor weakness. The technical contribution is limited as it builds on old ideas from graphical models. My understanding is that the key to efficiency relies on solving the proxy problem, which, as the authors stated, is an idea that dates back to at least the early 2000s in the context of graphical models.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Neural Structured Prediction for Inductive Node Classification","-''-","Update following revision / author comments:
Thanks for clarifying and providing details on how the method compares to piecewise training. Having all of those details in the final paper would make it much stronger, and so I have raised my score.
For what it's worth, I'd suggest trying to describe the method in as general terms as possible so that people working on other structured prediction problems are more likely to learn about it and use it. The method is general and appears very promising, but currently the paper is very much written for graph problems, so I fear that researchers outside of the graph community may overlook it. Choosing a more specific name for the method could also help. The name ""Structured Markov Network"" is a bit too broad, as it just sounds like a Markov Network, which is a synonym for MRF. Maybe working ""proxy"" into the name you choose would be helpful.
Original review follows:
Strengths:
The methods are well-chosen for the task. The method is simple enough that other researchers may use it.
The empirical results are solid and interesting.
The paper is well-written.
Weaknesses:
The original piecewise training paper (Sutton & McCallum, 2009) is cited, but is not discussed in any detail. No connection is made between the proxy problem and piecewise training. I think a connection should be made and discussed, especially because the experiments do not involve actually solving the proxy problem but rather omit the marginal consistency constraint (""The last consistency constraint...can be ignored during optimization""... ""We also tried some constrained optimization methods to handle the consistency constraint, but they yield no improvement""). When refinement is not used (it is omitted in the main experiments since it doesn't consistently help) and when KL divergence is used for the divergence measure (as it is in the experiments), the actual training objective becomes even more similar to piecewise training. Piecewise training is fairly well-known in other communities like the vision community, e.g., Lin et al. (2016), so I think it's really important for this paper to draw a connection to piecewise training. I would suggest including piecewise training as a baseline to compare to, but I think that the SMN (without the marginal consistency constraint, without refinement, and when using KL) actually corresponds to a natural way to apply piecewise training to this problem.
After reading the paper, I was confused by one part of the results: Why would SMN outperform CRF? That may suggest that the approximations made during learning are beneficial, which deserves follow-up investigation. A related question is: why is refinement not helpful? The CRF-G* settings correspond to using the same models as the corresponding SMN-G* settings, but the former seek to directly solve the maximin game using loopy BP for inference during learning (I believe), rather than use the proxy problem. The SMN results are consistently better than the corresponding CRF ones. Why is the proxy problem superior? Perhaps the CRF objective is not as good for learning as the proxy problem? The proxy problem can be viewed as using ""local supervision"" on specific nodes and edges, which may be more learnable from supervised datasets than the traditional CRF objective which is log loss on labelings of entire graphs. Or maybe it's due to training stability: An SMN that solely solves the maximin game (is this the same as the CRF-G* models?) is described as being unstable in Sec 5.5, #5. Can you provide more details about that? Is the instability due to the use of loopy BP as the inference algorithm? How about if you pretrain the GNNs for the potentials with maximum likelihood?
Some more specific suggestions are below:
Spell out SMN (""Structured Markov Network"") the first time it appears in Sec 1. I would actually also suggest changing the terminology to something more specific. ""Structured Markov Network"" is a pretty generic term that would evoke several kinds of existing graphical models in people's minds. E.g., some people use the term ""Markov Network"" to refer to undirected graphical models in general, and so the addition of the term ""structured"" does not really add anything since graphical models are already ""structured"".
The definition of GNNs in 3.2 seems unnecessarily limited. A graph neural network does not have to produce distributions over anything -- it could simply represent a graph via autoencoder training with an L2 loss, for example. Please see surveys on GNNs, such as Zhou et al. (2021), which provide a richer characterization of GNNs. If you wish to define GNN in a more constrained way for purposes of this paper, then please add ""In the context of this paper, we define a GNN to be"".
Sec. 3.2: ""However, GNNs approximate only the marginal label distributions of nodes on training graphs, which may generalize badly and result in poor approximation of node marginal label distributions on test graphs."" -- Why might they generalize badly? Would an estimate of the full label distribution be expected to generalize better? Is the paper implying here that noisy estimates of the marginals would generalize better than a noisy estimate of the joint?
Sec. 3.3: ""Conditional random fields (CRFs) build graphical models for node classification."" CRFs are much broader than node classification. See my comment about GNNs above.
Sec. 3.3: ""intractable partition function"" -- The intractability depends on the graph, right? E.g., consider chains, trees, acyclic graphs..
Sec. 4.1: ""compute the a representation""
More details are needed about the DBLP dataset. What exactly is the structure of the ""citation graphs"" mentioned? Also, it appears that there is only a single graph in train, validation, and test -- is that correct? The test set contains papers from ""after 2010"" -- does that include 2010 or is it only from 2011 onward? Are the train/val/test graphs disjoint? The papers in the test set will mostly be citing papers from the training and validation sets, or are the latter removed from the test graph to avoid overlapping nodes among splits?
References:
Guosheng Lin, Chunhua Shen, Anton van dan Hengel, Ian Reid. Efficient piecewise training of deep structured models for semantic segmentation. CVPR 2016.
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun. Graph Neural Networks: A Review of Methods and Applications. 2021.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Neural Structured Prediction for Inductive Node Classification","-''-","The technical approach exploits, in an innovative way, an insight from graphical models on the relation between pseudomarginals and the joint distribution stated in the main reference (Wainwright and Jordan 2009), to devise a formulation of a a surrogate training objective (the proxy problem) which turns out to be relatively easy to solve, leading to high predictive performance. The problem under consideration is important, and current solutions have the limitations pointed out in the paper (limited expressive capacity of CRF vs only marginal node classification for GNN models).
The paper reads very well, and hardly has any errors or inconsistencies. Information is provided at the right point, is complete and accurate. The split between main paper and supplementary material is good. The narrative and exposition flow well.
The model and algorithm descriptions are excellent.
Baselines are strong, relevant, discussed well and evaluated fairly. The experimental methodology is appropriate, well implemented, described well. A large number of analytic experiments complete the main findings. The two evaluation metrics (node and graph-level accuracy) are adequate and it is an advantage that the proposed method can optimize for either at inference time. Experimental reporting is very good, with error bars. Experimental setups are presented accurately and consistently, at an adequate level of detail. The datasets are standard and easy to access, which contributes to reproducibility. (NB fig3 is missing a complete caption; colour coding is not explained; as a result sec5.5.7 doesn’t prove its point)","10: strong accept, should be highlighted at the conference","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Neural Structured Prediction for Inductive Node Classification","-''-","Strengths
The motivation of model design is clear and reasonable. Modeling label dependencies is a natural way to improve GNNs, which is also supported by experiments.
The model inherits advantages of CRF, such as describing the dependency of node labels and providing probabilistic interpretation. In the meantime, the model supports efficient learning and inference.
The model is technically sound.
Experiments cover several benchmarks. The model is tested on multiple datasets and shows very promising results.
The paper is well-written and easy to follow.
Weaknesses
The model is proposed for modeling discrete node labels on a graph. It is unclear how to extend the model to fit continuous node labels.
Insufficient baselines for comparison. For neural models that can describe node label dependencies, the paper compares with GMNN only. However, there are various recent works that are not compared, e.g.,
G3NN
(Ma et al., 2019), CopulaGNN (Ma et al., 2021), LCM (Wang et al., 2021) to cite a few.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"A New Perspective on ""How Graph Neural Networks Go Beyond Weisfeiler-Lehman?""","Keywords: Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman","The contribution of the work is definitely relevant and significant. The framework is solid, well-justified, and supported by proofs. The experimental results are quite convincing as they confirm the good performance of the method in classical datasets.
I would however encourage the authors to work a bit more on the presentation of the paper. While the paper is generally well-written, there are parts that are difficult to follow for someone who is familiar with GNNs, but not working exactly on aspects of expressivity. For example, the notions of subgraph-isomorphic, overall-isomorphic, and subtree-isomorphic are very technical, and hard to follow. I would appreciate it if the authors could make them more clear, by for example, explaining better Fig. 2.
Similarly, the proposed choice of \omega, in Eq. 4, should be better explained/motivated.
Some additional minor comments:
'0.05 level of significance': how do you define it? Please elaborate.
Any intuition on what the performance on the MUTAG dataset is not that great?
Please double check the References to make them complete and consistent (e.g., Waiss Azizian et al..., Cosro....'33'?,2020)","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"A New Perspective on ""How Graph Neural Networks Go Beyond Weisfeiler-Lehman?""","Keywords: Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman","This paper proposes an efficient method for message passing that can incorporate structural information (that of neighborhood subgraphs) that is provably more expressive than 1-WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the oft-quoted ""over smoothing"" problem. This is borne out in extensive experimental results and a satisfactory ablation.
In the recent literature on GNNs, there have emerged three directions of research that aim to construct procedures more expressive than 1 WL:
Higher-order WL-based methods, which often can get prohibitively expensive.
Using features generated from substructures (most of the papers taking this approach claim this information is available through domain knowledge, but often just use triangles and the likes).
Augmenting node features with identifying information to improve expressive power.
The central idea of the paper is grounded in the common observation that treating the neighborhood as a multiset of features ignores rich topological information, limiting the expressivity of message passing procedures that use such a representation. If the neighborhood is represented as a neighborhood subgraph, then 1-WL is only as powerful as distinguishing neighborhood subgraphs in terms of subtree structures. The question then becomes if structural information can be incorporated in a way that can go beyond neighborhood subtree isomorphism. Towards this end, the authors show that there exists a class of isomorphic graphs that lie in between neighborhood subgraph isomorphism and neighborhood subtree isomorphism, which they call overlap subgraph isomorphism. It is shown that by incorporating structural information that can solve overlap subgraph isomorphism, one gets a message-passing network that is more expressive than WL.
To be more specific, section 2 provides the hierarchy of local isomorphism mentioned above. Theorem 1 states that subgraph isomorphism implies overlap subgraph isomorphism but not vice-versa, and that overlap subgraph isomorphism implies subtree-isomorphism but not vice-versa. 1-WL can only distinguish those graphs that can solve for sub-tree isomorphism at each layer. In order to go beyond 1-WL, the authors focus on overlap-subgraph isomorphism and define a set of structural coefficients for each vertex based on overlap subgraphs. These coefficients depend on reasonable notions of closeness, density, and invariance. The exact form of such coefficients is shown in section 4, and it is also shown that incorporating such information can give a method strictly more powerful than 1 WL. Since it just admits the same message passing paradigm, the computational overhead is limited.
Extensive experiments on node classification (Cora, CIteseer, Pubmed, NELL, ogbn-arxiv), graph classification for the commonly used small graph datasets, and large graphs show across-the-board improvement compared to the competition (including the higher-order methods). The ablation shows the importance of the structural coefficients. Further, another set of experiments show that the proposed method is able to avoid the so-called oversmoothing problem (however, the results are presented without comment -- it would be beneficial to share some intuition on why this is the case).
In summary, I think the paper makes a solid contribution. It proposes a well-motivated method and validates it by extensive experimentation that leaves little doubt on its efficacy.
Minor comments:
The paper will benefit from sharpening the writing in the abstract and intro -- it feels a little bit wayward and has some convoluted sentences.
The paper title, and most of the paper itself, uses the spelling ""Lehman."" The actual spelling is Leman. Somewhere after the intro, when the original WL paper is cited (and in some following sentences), the correct spelling is used. I would suggest either using the correct spelling throughout, or sticking to Lehman throughout (since it is widely used, and Leman himself did not mind it https://www.iti.zcu.cz/wl2018/pdf/leman.pdf)
I don't believe that the citation of ""Provably Powerful Graph Networks' of Maron et al. is accurate. It is cited as a powerful (3 WL) method that is expensive, however, I think it is an efficient method. The authors might want to verify this claim.
Line 4 from the bottom of page 1: typo ""This solution enables GNNs to provably more expressive"": -> ""This solution enables GNNs to provably be more expressive""
Line 2 from the bottom of page 1: ""which require high computational overheads and are impractical"" -> ""which require high computational overhead and are impractical""
Line 1 page 2: ""(2), our method does not require any domain"" I am not sure it is accurate to say that these methods require domain knowledge. They can certainly benefit from it, and this claim is made in those papers, but they just stick to simple sub-structures such as triangles that are easy to treat. This repeats on page 3.
Line 3 of the second paragraph, page 2: ""capacity of a model"" --> ""capacity of the model""
Disclaimer: I have not verified the proofs for correctness. However, based on the statements of the theorems and the general idea, I can buy them to be true and base my review on that assumption (since I expect these statements to be true).","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"A New Perspective on ""How Graph Neural Networks Go Beyond Weisfeiler-Lehman?""","Keywords: Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman","Pros:
Overlap isomorphism is a nice point to find a balance between expressive power and computational efficiency. Based on that, it is persuasive to encorporate the three properties into structural coefficients.
The setting of
λ
is very interesting to me. It brings the coefficients some flexibility so that different
λ
's may capture inherent key information in different learning tasks, as shown in experiments.
GraphSNN shows impressive experiment results. On node classification, it improves the counterparts of traditional GNNs by considerable boost, but is also very easy to use. On graph classification, it outperforms baselines by an intriguingly large margin. The performance drops slowly when it stacks more layers.
Concerns:
In the beginning of page 2, the sentence ``compared with the methods of augmenting node identifiers ...'' needs more explanation. If I understand correctly, the latter claim is from the ablation study of
λ
in Section 5.3. But I do not see its relationship with node id/random feature models.
According to the definition of
A~vu
with normalization above (5), can the first summation over
u∈N(v)
be simplified as 1?
Since structural coefficients emphasize strongly connected neighborhood, I am wondering whether it would hurt the performance when there is a task requiring long-range information [1] and the path is, to some extent, adversarially going through a path with weaker connectivity.
With respect to oversmoothing on node classification, I am wondering whether the graph operator (can be derived from (5)) with different
λ,γ
has dominating subspace [3] that aligns well with the labels. A possible experiment is to send features through leading eigenvectors and then do pure MLP, following [2].
Expressive GNNs typically show better performance on graph regression tasks than graph classification. So it would be better to show comparision with expressive baselines on regression datasets such as QM9 and ZINC, instead of Table 4, if time permits.
Reference:
[1] On the bottleneck of graph neural networks and its practical implications. U Alon, E Yahav.
[2] Revisiting Graph Neural Networks: All We Have is Low-Pass Filters. NT Hoang, T Maehara.
[3] Graph Neural Networks Exponentially Lose Expressive Power for Node Classification. Kenta Oono, Taiji Suzuki.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"A New Perspective on ""How Graph Neural Networks Go Beyond Weisfeiler-Lehman?""","Keywords: Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman","Originality
The paper belongs to a body of recent works which aim to break the expressive power limit of MPNNs in a sub-quadratic memory complexity in the number of nodes. As far as I’m aware, the aggregation scheme suggested in the paper is novel, providing a constructive way to design more powerful architectures than the 1-WL test.
Experimental evaluation
Strengths - the paper demonstrates a boost in performance on node and graph classification.
Weaknesses -
Expressive power experiments - also an experimental evaluation showing what types of structures the proposed architecture can distinguish would be interesting to see (e.g., cycles, d-regular graphs).
Over-smoothing - the empirical results are not explained. Do the authors have a conjecture as to why their proposed method circumvents over smoothing?
Suggestions
Further discussion on generalization to unseen graphs (With larger / smaller average neighborhoods)? Since the aggregation coefficients suggested are normalized (edges/vertices) it might be that the suggested model is more robust to size differences in graphs. It would be interesting to see how it performs. For example, a simple experiment like the ones performed in [1].
Clarity - paper is well written; claims are well supported by theoretical proofs and guarantees.
[1] Yehudai, G., Fetaya, E., Meirom, E., Chechik, G., & Maron, H. (2020). On Size Generalization in Graph Neural Networks. ArXiv, abs/2010.08853.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond","Keywords: Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning","The majority of the manuscript is well-written and easy to understand. This paper is solely theoretical with promising guarantees. The authors showed that if
K≥c2κ
where
c2>0
and
κ=Lμ
, then their convergence rates are faster that the with-replacement counterpart. However, for the ill-conditioned case, the number of epochs (theoretically) will be huge to satisfy the mentioned assumptions.
It would be ideal if the authors could also provide some numerical results to demonstrate how well their proposed methods performs in practice.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond","Keywords: Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning","Strength. 1. The analyzed problems have significant importance in practice and lack a thorough theoretical understanding yet. 2. The technical contribution for analyzing gradient sampling with dependence is original, the technique for bounding gradient noise in local RR is involved and interesting. 3. The provided bounds are tight in some sense and variants with better performance (linear speedup) are analyzed. 4. The lower bound for mini batch RR in small-epoch regime is informative in that it concludes the weakness of mini batch RR for K \lesssim \kappa.
Some other points. 1. There exists a gap of \kappa^2 between upper bounds and lower bounds, which can be important given that \kappa itself is relevant in the considered regime. It may help to explain more on parameter dependence when claiming tightness. 2. The lower bounds and upper bounds are in different notions of convergence, it may help understanding if there are some explanations there. 3. In theorem 3, it might be useful to explain on the connections among c_1, c_2 and \kappa, does it affect the regime K \ge c_2 \kappa, if c_2 has dependence on \kappa given that c_1 has dependence on \kappa. 4. In theorem 4, it also assumes that K \ge MB/N, does it has influence on the followed discussions on the choice of B. It seems to me the cutoff between parameter regimes is not very clear, could you explain more on it, like, similarly, does c_4 has relationship with \kappa? 5. Although this is a theory paper, giving some experiments to validate the analysis especially for the discussions on different regimes would be appreciated.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond","Keywords: Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning","The paper presents a deep theoretical study of popular-in-practice methods that utilize shuffling of the data points. This paper has a good structure, all sections are well-written and fully described. All assumptions and statements of theorems are clear and understandable.
However, assumption 2 is questionable:
Assumption 2 (Intra-machine deviation). There exists
ν≥0
such that for all
m∈[M]
and
i∈[N]
,
|∇fim(x)−∇Fm(x)|≤ν, for all x∈Rd
Does this assumption hold in practice? Is it not too strict to have a uniform bound for this difference?
The theory is clear and it is easy to follow the proofs. The theoretical part is solid and the importance and novelty of obtained results are significant. Upper and lower bounds explain methods' behaviors in detail. These results should be interesting for the optimization community and the machine learning community in general. However, it might be useful to have a table with a comparison with rates of other methods for Federated Learning. Also, it is interesting to compare these results with results from https://arxiv.org/pdf/2102.06704.pdf. In this paper, local methods with shuffling are also considered.
This paper does not have any experimental results. I understand that this work is theoretical, but simple and small experiments on toy models are desirable. It can be useful for readers to have graphical illustrations of methods' behavior.
The synchronized shuffling method is not clearly described. The notation
σkm(i):=σ((i+NMπ(m))modN)
is quite confusing. Can you explain why you shift the permutation that way? Despite the main idea of this approach being understandable, some technical details are not easy to get. It might be useful to add some clarifications in this part.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions","Keywords: Neural networks, global optimization, convex optimization, convex analysis","Strengths. I find the results to be novel extensions of of Pilanci and Ergen from 2020 and important theoretically: characterization of the global and local optimum points of the objective function.
Weaknesses. The paper only considers a single-hidden later. Can this analysis method be extended to multiple layers?
Several typos and corrections I found:
The upper bound in the rightmost summation in equation (1) should be 'm' not 'd'.
Page 1, penultimate line, no definition for
Ci
for
i∈[p+1,2p]
is given.
Page 3, Section 1.3, first paragraph, third line:
Fix the mismatching brace.
I believe the
σ
should be dropped.
The
1(Xu≥0)=s
seems to be a slightly different definition of the cone corresponding to
s
than the original definition in page 1. Namely, the entries of
Xu
corresponding to
0
's in
s
are required to be negative in this definition and nonpositive in the definition in page 1.
In equation (5), B(u_j, \alpha_j) has vectors with
d+1
entries (the last entry comes from
R>0
or
R<0
from the definition of
Bi
) and
Ci
has vectors with
d
entries, so the containment under the summation doesn't make much sense here.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions","Keywords: Neural networks, global optimization, convex optimization, convex analysis","First of all I have to say this is a very interesting theoretical paper and is the best among all submissions I have reviewed in ICLR 2022. This paper reveals an interesting hidden convexity structure in two-layer ReLU neural networks by mapping minimal neural networks to a convex optimization problem and vice versa. The construction of the mapping is clean and elegant. The idea of connecting non-convex loss landscape with convex optimization can potentially be further explored to explain the success of deep learning. This paper is strong enough and I do not see much weaknesses. But I left to the authors few minor comments which can be seen as future considerations.
The first comment that comes to my mind is how the results obtained in the paper can be generalized to neural networks with other activation functions. It seems that the positive homogeneity of ReLU is the key to derive the convex programming problem. Can the authors comment on whether (and how) it is possible to establish similar results for smoother activation functios, e.g.
ReLUk
or Sigmoid?
Can the authors comment on whether and how the results can be extended to deep ReLU neural networks?
Some relevant references are missing.
Huiyuan Wang, Wei Lin, Harmless Overparametrization in Two-layer Neural Networks, arXiv:2106.04795.
Quynh Nguyen, On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths, arXiv:2101.09612.
Quynh Nguyen, Pierre Brechet, Marco Mondelli, When Are Solutions Connected in Deep Networks?, arXiv:2102.09671.","none","8: accept, good paper"
"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions","Keywords: Neural networks, global optimization, convex optimization, convex analysis","Strengths:
Interesting observations about and practical ways for constructing the optimal neural network set from the solution of the convex cone program.
The paper is mostly well-written and organized, and technically precise.
The analysis is mostly clean and easy to follow, which may be of interest for addressing related sets of nonconvex problems.
Weaknesses:
The (or similar) regularized convex formulation has been studied in previous works, see e.g., Pilanci & Ergen (2020), Ergen & Pilanci (2021) among several recent others by Pilanci et al. Though a more-in-dept analysis as well as theoretical observations are provided, how technically novel and practical useful of these results shall be further compared and elaborated.
Ergen, Tolga, and Pilanci, Mert. ""Convex geometry of two-layer ReLU networks"", 2021.
A number of related efforts dealing with learning two-layer ReLU networks by analyzing or modifying the landscape through introducing regularization terms are missing in the discussion; see e.g., Wang et al, 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. Moreocver, the contributions can be considerably enhanced by providing experimental validations across different tasks such as regression, or classification tasks.
One of the key difficulties in deep learning theory is that learning of deep neural networks is nonconvex due to their compositional structure. How would the results extend to deep neural networks?","NA","8: accept, good paper"
"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions","Keywords: Neural networks, global optimization, convex optimization, convex analysis","Strengths of the paper:
The authors provide a complete characterization of the global minima to the nonconvex 2-layer ReLU network training problem, in terms of the solutions of a convex program
The approach does not rely on duality and/or lifting perspectives of previous convex perspectives on neural network training
The approach provides an algorithm for testing optimality of a neural network in the studied context
The work provides significant extensions of the work upon which it is based
Weak points of the paper:
The method only applies to 2-layer neural networks with ReLU activation and weigh decay. It is a great contribution that may inspire further developments to weaken these assumptions.
Questions for authors:
One of the technical themes of the paper is the idea of minimal / nearly minimal networks. Several of the theorems are stated explicitly in terms of nearly minimal networks. Is it necessary to have minimality in the theorem statements? That is, could the primary claims be stated without reference to minimality (and where minimality would be a technical detail in the proofs)?
Additional feedback with the aim to improve the paper:
In the model there are no bias terms. I presume this is because they can be tucked into X with a row of 1's. Perhaps this is worth a comment in the paper.
Typo: Sec 1.3 ""neruons""","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics","Keywords: Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics","Strengths:
EX-BMDPs seem to capture several popular simulated distractors that are currently being used in applied RL papers, such as random video backgrounds in MuJoCo tasks. I suspect the fact that the exogenous dynamics are relatively unconstrained will also make them a good model for practical tasks. The requirement for near-determinism in the endogenous dynamics is unfortunate, particularly given that it applies to the initial state distribution as well (often you want to perform well over a distribution of similar-but-not-identical tasks). However, I think this is not a major concern for exploration methods: so long as the possible initial states are reachable from some common starting state within
H
steps, PPE will eventually explore them.
The fact that it is possible to obtain a sample complexity bound completely independent of the complexity of
Ξ
was surprising to me given the relatively complex form of exogenous noise in an EX-BMDP, and shows that the EX-BMDP class is at least tractable.
The clarity of writing was appreciated, particularly when stating assumptions and comparing against past algorithms.
Weaknesses:
I noticed that RND is missing from Section 3/Appendix A (shortcomings of existing methods), but does surprisingly well on the combination lock problem. Is it possible to give some intuition for where RND fails in general?
The visual gridworld experiments are a little confusing. The takeaway seems to be that PPE drives accuracy to ~100% eventually on a less-toy problem. However, there are no baselines, so it is hard to determine how significant this achievement is. It would be useful to see how the baselines from Figure 2 fare in this setting, particularly in terms of sample efficiency.","8: accept, good paper","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics","Keywords: Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics","STRENGTHS
(S1) The problem of efficiently learning in high-dimensional observation spaces is a good one, and the authors provided an excellent discussion of the technical components of this problem in the early parts of the paper. I thought the definition and presentation of the EX-BMDP was particularly good.
(S2) I appreciate the authors providing experimental results to show that PPE performs well in some regards.
WEAKNESSES
(W1) I'm afraid I found the discussion of PPE to be a bit difficult to decode. Even looking at Algorithm 1, I find myself with a number of important questions unanswered, chief among them being ""what about
ϕe∗
""? It strikes me as odd that nowhere in this algorithm are the inverse mappings from observation to exogenous/endogenous state represented or used. Why is this?
(W2) I'm a little concerned with the computational tractability of the algorithm. My understanding from the paper is that one seeks some sort of ""covering"" of the state space--in what sense is it practical to obtain and/or store this covering? If the problem setting consists of high-dimensional observations this seems especially challenging.
(W3) From my naive perspective, it seems that the ultimate goal here is to use PPE and then actually perform reinforcement learning. However, I didn't understand from the brief discussion at the end of Section 4 (or, frankly, from reading the Appendix) how exactly that would be accomplished. I feel this is important enough that it should appear in the main paper.
POST-DISCUSSION COMMENTS
During the discussion, I feel as though the authors adequately addressed each of the issues I raised above, and so I'm happy to raise my score to accept.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics","Keywords: Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics","Strengths:
The problem setting is important and seems to be relevant to many real-world problems.
While the idea of using inverse dynamics and even multi-step inverse dynamics [1] to learn representations in these types of environments to learn controllable representations is not novel, this paper provides a theoretical basis for this choice, which I think is valuable.
I found sections 1 through 5 to be very well written.
Weaknesses:
In the experiment section, it seems as though you are testing the planning/policy learning abilities with PPE in both of the environments. However, as far as I can tell, there is no indication of how you perform this planning in PPE in the main text. The paper simply switches from talking about learning policy covers to showing regret for environments with only a small mention of the fact that you could plan using the data collected with PPE. The way the experiments are run and the choice to test the planning performance / how the state representations are obtained in experiments should be explained in more detail in my opinion.
The paper discusses prior works with inverse dynamics seemingly only in the context of their applications in representation learning in two prior works on exploration. I think the application of multi-step inverse dynamics in goal-conditioned environments described in [1] is worth mentioning, since for a fixed goal this algorithm seems like it is doing something similar to PPL. Related is the lack of a ""related works"" section in the paper, which would be helpful for framing PPL in the context of prior works.
There is no conclusion section, which I feel would improve the paper.
My major questions are:
How is the policy learning done in the experiment section? How are representations found in the decoding accuracy plot?
How does this paper relate to prior works on inverse dynamics?
[1] Paster, Keiran, et al. Planning from Pixels Using Inverse Dynamics Models. 2020. openreview.net, https://openreview.net/forum?id=V6BjBgku7Ro.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics","Keywords: Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics","The paper is very well motivated. It has convinced me that being invariant to exogenous noise is important and that the EX-BMDP is useful abstraction for making this problem concrete. It is also very clearly presented. The argumentation is clear, and the flow of the paper is quite natural. Even the lengthy appendix is fairly easy to parse, with straightforward proofs and relevant details (e.g. specific claims about the unsuitability of alternative approaches).
One shortcoming is potential impact / applicability. I'm not convinced that environments with single starting states and near-deterministic dynamics is a very rich problem class. Or more specifically, I'm not convinced that restricting myself to this class would be preferable to working in the full space of MDPs with e.g. the possibility of aliasing a few states with a single-step inverse-dynamics approach. I am aware that impact is very hard to predict in advance, so I won't let this aspect unduly affect my scoring. And I'm convinced that even if PPE is never used in practice, this paper should still be accepted for its useful problem formulation and theoretical results. But a quick read would leave one with the impression that the alternatives (e.g. contrastive learning, inverse dynamics, bisimulation metrics) flaws outweigh their considerable advantages (e.g. applicability with stochastic dynamics) -- explicitly noting where alternative approaches are preferable would be appreciated.
I also noticed what I believe to be an omission in your related works: mutual information / empowerment based methods. ""Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"" seems particularly related in its search for predicable action sequences. And the follow-up work ""Variational Intrinsic Control"" appears to be a multi-step inverse dynamics analog that doesn't require open-loop policies. If you can show that these approaches are flawed in a way that PPE is not, then that would considerably raise my assessment of this work. And even if not, I believe this field (if not these specific works) should be acknowledged, as they similarly attempt to learn what in the environment is controllable.
While I agree that PPE is applicable in no-reward situations whereas bi-simulation metrics are not, your experiments all involve rewards, so I'm surprised a bi-simulation metric method was not included as a baseline.
While this isn't necessary for acceptance, it is worth noting that prior work has established much more challenging benchmarks for evaluating the representation of exogenous noise, and utilizing a pre-existing benchmark would make your empirical results considerably more impressive.
This is a small point, but it is not initially obvious why inverse dynamics fails on the combination lock problem. Does alliasing occur just when e.g. s_2a --> s_3a and s_2b-->s_3b have the same action?","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Bootstrapped Meta-Learning","Keywords: meta-learning, meta-gradients, meta-reinforcement learning","Originality: The method proposed in the paper is, to the best of my knowledge, novel. The related work section adequately connects the algorithm with existing work in similar directions.
Significance: The empirical results show considerable improvement w.r.t. well-performing baselines; moreover, the general idea behind the method could inspire future research.
Rigour: Both the theoretical results and the experimental protocols seem sound and solid to me.
Strengths
The method is based on a conceptually compelling and inspiring idea.
The empirical results are remarkable. The ablations and additional experiments (also from the Appendix) help in understanding what matters in the practical algorithm, as well as highlighting the important parts of the contribution.
In particular, being able to meta-learn hyperparameters for a behaviour policy can open up new avenues for exploration in reinforcement learning.
The theoretical results have a clear scope (although not so large) and provide some easy to understand local improvement guarantees.
The paper is well-written and generally easy to follow.
Minor Concerns / Questions
I believe the name matching function makes the presentation of the method a little bit harder to digest. Since the function is a pseudometric (i.e., the larger it is, the larger the distance from the target), it should really be called with a name that reminds the reader of this nature (e.g., mismatch function).
I enjoyed the theoretical results, but it is a pity that they only deal with targets of specific forms and, especially, with
L=1
only. Ideally, theoretical result with a dependency on
L
would shed some light on the benefits and limitations of longer bootstrapping horizons.
Can the authors elaborate on the connection between the way the bootstrapping target is formed in their method and traditional temporal difference learning? In particular, the grounding role of that subtracted gradient ""nudging the trajectory in a descent direction"" is the same as the one of the reward in temporal difference learning; but, while the reward is at the beginning of the trajectory, the grounding is here at the end of the optimization subtrajectory. Is there any mathematical connection beyond the general shared motivation?
As briefly touched upon in some passages of the paper, when the underlying function is highly nonlinear, there is the risk that the bootstrapping mechanism can lead the optimization process in worse areas of the landscape. For instance, if the function in Figure 1 had a bump/plateaux where
w~
is, the bootstrapping mechanism would cause more troubles than standard meta-gradients. Why is this not happening in practice?
After rebuttal: I am happy with the answer the authors provided and the update to the paper, which will help the readers understand the relationship between the proposed method and TD-learning. Overall, the improvements make me believe that this paper should be highlighted at the conference, to give other researchers working in the field the possibility to get inspired by this new idea. I am thus raising my score to 10.","10: strong accept, should be highlighted at the conference","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Bootstrapped Meta-Learning","Keywords: meta-learning, meta-gradients, meta-reinforcement learning","‌Originality
The paper is original and novel, proposing a new algorithm to overcome two shortcomings of standard meta-optimization algorithms: curvature mismatch and limited evaluation.
Quality
The paper is technically sound, and claims are backed by solid experiments in the reinforcement learning and multi-task meta-learning evaluation setting.
Clarity
The paper is clear, however, the algorithm description in section 3 is very abstract. I think the paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.
Significance
The work is significant and will benefit the reinforcement and meta-learning community by addressing some of the limitation of the current meta-learning algorithms.
Limitations
The theoretical analysis is limited to noiseless 1-step target updates.
The experimental evaluation in the multi-task meta-learning setting is limited to only compare with MAML on computer vision applications.
Questions to Authors
Some engineering / handcrafting is still required by the machine learning practitioner to select what ""target"" the meta-learner is going to optimize, as well as the proper ""metric"" for the meta-learner to optimize for. Could the authors comment a bit about what heuristics they used when making these decisions? and whether the automation for this process is possible or not?
What would it take to extend the analysis beyond 1-step noiseless target updates?
How does the performance of BMG compare to alternative meta-learning algorithms like R2D2, Meta-OPT-net and prototypical networks? Have the authors experimented with other meta-learning benchmarks beyond image classification?
Minor Typos
Abstract: ""show that metric"" -> ""show that a metric""","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Bootstrapped Meta-Learning","Keywords: meta-learning, meta-gradients, meta-reinforcement learning","Strengths
The proposed idea is simple.
The writing is clear.
To my knowledge, the proposed idea is novel yet concretely linked to many past works by virtue of generalizing them. The authors give the precise form to recover MG from BMG.
The proposed idea results in strong improvement of the STACX agent in Atari-57, resulting in a state-of-the-art result (caveat: for model-free agents; the gap to model-based agents remains large).
The authors demonstrate that the proposed idea is suitable in few-shot image classification, a popular application of meta-learning for few-shot learning.
The authors run informative ablation studies that support the intuition behind the benefit of BMG: resolving curvature and mitigating myopia.
Weaknesses
Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta-parameters), it would be nice to have some experiments on learned sequence model update rules (e.g. RNN). All current experiments use update rules with a fixed functional form.
I am not putting much weight on section 4 (""Performance Guarantees"") given the gap between its assumptions and results vs. what is actually implemented, and the restriction to local optimization.
Minor comments
p. 3, target bootstrap paragraph, 3rd line: are we missing a learning rate for the expression of the target?
p. 5, the actor-critic RL objective and Eq. 4: as-is, we are always minimizing policy entropy; is there a sign error?","10: strong accept, should be highlighted at the conference","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Bootstrapped Meta-Learning","Keywords: meta-learning, meta-gradients, meta-reinforcement learning","Strengths
The paper is generally clear and well written. It is content dense in and certain areas would benefit from more detailed discussions. However, the appendices are very detailed and provide answers to almost all of the questions that arise during first reading. In my suggestions below I’ve highlighted a few areas I feel this content would be especially beneficial (and one pare where the main text could be stripped-back as there is always a trade-off with fixed page limits).
The key ideas of BMG, bootstrapping a target to combat myopic MG updates and using matching functions to improve the meta-learning dynamics, are novel and well motivated. I strongly believe that these ideas will be of interest to the broader meta-learning community and will likely lead to multiple future research directions.
The empirical results are strong and thorough. The toy-model grid world in Sec 5.1 effectively highlights key properties of the proposed algorithm — such as the ability of BMG to exploit longer meta-learning horizons — and extensive testing on Atari environments (Sec 5.2) shows a significant improvement of STACX and leaves little doubt of the efficacy of BMG for self-tuning algorithms. Moreover, the demonstrated performance improvement of BMG over MAML (sec 6) suggests broad possible applications and further endorses the proposed algorithm.
BMG also has a couple of nice properties besides raw performance: (i) achieving less myopic updates without having to backpropagate through many update steps of the inner-loop parameters and (ii) allowing for the meta-learning of “behavioural parameters” that are not used in the learning rule (exploration epsilon is used as the example). Whilst (i) is clearly desirable in the pursuit of computational efficiency, (ii) is very intriguing and opens the door to new applications of meta-learning (indeed, whilst it is mentioned in the abstract and summaries, I believe this point is slightly undersold in the main text and that this experiment could be presented in more detail than the single paragraph at the end of Sec 5.1 it is given — however, given the page limit I can understand the authors predicament).
Weakenesses
My primary concern is not with the content of the paper, but that the it can be quite difficult to intuitively link the numerous experiments back to intuition or interpretation of the results. I believe this was largely because the exact methodologies are difficult to follow and, indeed, I found it was essential to refer to the appendices repeatedly to fully unpack the experiments and appreciate the results. Whilst I am sympathetic to space constraints, I believe the authors would be well served to provide more detailed descriptions in the main text or, ideally, an algorithm box.
I find the implementation and implications of the experiment on multi-task few-shot learning (Sec 6) unclear in the following regards:
I do not understand the intuition of why a “hot” expert transforms more information than a “cold” expert, and, moreover, why BMG is able to use this to improve performance. Could the authors clarify these points.
I feel this section in particular suffers from a lack of formal introduction of the task and methodology. For example, the adaptation seems to be defined as a single step, whereas the appendix (D.2) notes that 10 are used during meta-testing. Concretely, I think a formal description of the training procedure for BMG would be appropriate in the main text.
I do not find the analysis presented in Sec 4 (“Performance Guarantees”) especially insightful. My reading is that, whilst the MG update presented in Lemma 1 can guarantee local improvement, practical implementations of BMG do not. Empirically, however, grounding the bootstrapped target with a single final step on the meta-objective is sufficient for good performance. This conclusion is evident from the experiments themselves, and so I would have no issue if Sec 4 was in the appendix.
Errata
Sec 3, paragraph starting “To tackle myopia…”: the inline equation for
x~
is missing brackets around the step counters for
x(K+L−1)
.
Sec 5.2, second last paragraph: typo - “K is more sensitive curvature and the quality of data”.
Sec 6, sub-section BMG: typo - “raising the temperature in the expect allows”.
Sec 6, sub-section Setup: typo - “For 50 meta-updates and beyond..”: this should be 50k meta-updates I believe.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Coordination Among Neural Modules Through a Shared Global Workspace","Keywords: slot based recurrent architectures, attention, transformers, latent bottleneck.","I enjoyed reading this paper! I think the strengths fall into three main categories: good problem setup that took the reader from concept to formulation well, an interesting idea with good scoping, and an impressive experimentation suite. More details:
Good problem setup: I found the discussion of advantages of a shared workspace compelling. The proposed structure is intuitive and simple, but nonetheless convincing about its claimed advantages of higher-order communications, dynamic filtering, and complexity advantages. The qualitative parts of the writing are strong, effectively building a case for reading this paper.
Interesting idea with good scoping: I appreciate that this is one architectural change that considers all inputs and outputs, and shows its advantages over the right baselines. This work avoids the trap of making minor changes and then being unable to really ablate them. The paper does seem to be a unification of existing ideas, but I distinguish that from (and prefer it over) merely concatenating existing ideas.
Impressive experimentation suite: There are of course many experiments with thematic consistency, which is great - always better than papers with two testbeds and virtually the same experiment over and over. The first four testbeds make a nuanced point about sparse inputs, and the results on Atari are particularly exciting because they make a general claim and show significant improvement.
My critiques are as follows, in order of significance:
Content
Main critique: the goals of this paper seem to be to show that accuracy and performance improve overall, with a mechanism of higher order communications, dynamic filtering, and linear complexity that we will be convinced are attributes of the SW. This paper doesn't quite get there, though it gets part of the way there.
From my understanding, Triangles, CATER, and Sort-of-CLEVR all tell us that the higher-order communication and single channel will help identify relevant information earlier in the pairwise communication, leading to faster convergence - if information is sparse in the input. The MNIST generation experiment tells us something similar about independent regions in inputs. Figs 4 and 5 are convincing to this end. However, the main claim - higher-order communication and a sparser connection of graphs - help for inputs with sparse information in various locations - is less surprising than a claim of general improvement. Even if that's still interesting (it is), the intro and conclusion seem to make a more general claim. Physical reasoning and Atari show more general improvement (and the results on Atari are impressive), but they're also the weakest, most qualitative parts of Sec 4. The ideal solution would be more robust testing of general improvement in these two testbeds, but if not then I would at least appreciate a claim that acknowledges the common and specific nature of the first four testbeds.
This is the crux of the motivation behind my score: I worry that this paper overstates its claim and the real claim is elegant but not that surprising. It's made even murkier by a couple promising results that receive little attention.
Relatedly, the Atari section makes a claim about considerable improvement due specifically to more appropriate modularization. That would be cool and a real testament to this approach, but I would need more specific experimentation to demonstrate that the effect really was more appropriate modularization.
Presentation
The mathematical formulation is quite clear, but it's not presented clearly. I had to puzzle over the paragraph for a while, not because it was overly complicated but because it took a lot of treasure hunting to find all the parts and put them together. The issue is that the Notation and Step 1/2/3 sections write the math into large blocks of text without pause. It would be more helpful to present some end-to-end equations, then break those down in text. It can be unclear without doing it myself whether something is being element-wise updated, transformed, dotted, etc.
I really appreciate the slew of testbeds. However, that's how they come across: a laundry list of experiments, one after another, even though they support similar claims. That robustness can be shown by having a claims-driven structure for the results section rather than a testbed-driven structure, which to the reader is arbitrary and doesn't deliver the important information as well as it could. E.g. section titles like ""Performance advantage on sparse data"", then talk about the different results that show it. If one experiment shows multiple advantages, multiple sections can still point to the same figures.
The figures are confusing - they are cramped, small, and have lots of acronyms I have to hunt down in various parts of the paper. They're all helpful content-wise, but this could be shown better with something as simple as better aesthetics and tagging.
Nit: the text seems to switch between ""specialist"" and ""neural module"". Are these the same thing? Is ""specialist"" an abstraction/metaphor for ""neural module""? Is ""neural module"" an example of ""specialist""? All of those are great, just signpost and explain the switch.","6: marginally above the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Coordination Among Neural Modules Through a Shared Global Workspace","Keywords: slot based recurrent architectures, attention, transformers, latent bottleneck.","Strengths
Interestingness of the approach
The proposed method is appealing since it applies a theory from cognitive science (the global workspace theory) to information processing in networks. The implementation of the idea is quite simple - individual ‘specialist’ units interact with a shared memory layer instead of with each other directly, and the shared memory then broadcasts information back to each of the specialists. The exact means by which this interaction is carried out is by using key-value attention. This approach when applied to transformers leads to much higher computational efficiency (linear instead of quadratic in the sequence length). Further research in probing similar ideas for much more efficient information processing, backed by ideas from cognitive science, would be very beneficial for the field in my view and this paper would help the community in that regard.
Experimental Evaluation
The paper includes exhaustive experimental evaluation over 5-6 environments (including object tracking, and relational reasoning) where the solution requires considering a small portion of the input data, the authors demonstrate that adding the shared workspace model to attention based architectures like Transformers and RIMs leads to superior asymptotic performance, and faster learning. These environments include object tracking (CATER), relational reasoning (sort of CLEVR) and Atari games. The authors also include details about the experiments and algorithm implementation in the appendix, which will aid in reproducibility.
Weaknesses
Unclear if the approach will scale to larger, more unstructured datasets (where current attention based architectures have already been shown to thrive)
Most of the experiments considered involve problems that only require a small portion of the input to be solved (eg: just the patch with the points for the equilateral triangles, or just the target object for Cater object tracking). I am concerned whether the framework proposed here will also be effective in settings where this is not necessarily the case, such as general modeling of language and images. For example, adding shared workspace to transformers imposes a communication bottleneck between representations at different positions of the sequence. It is possible that for problems where the solution does not depend on only a small portion of the input, considering the pairwise relationships of representations at every point in the sequence is critical for good performance. To study this, the shared workspace model would have to be evaluated on larger, more unstructured datasets, where transformer based architectures have already been demonstrated to do well (for example the data on which GPT-2 was trained). Adoption of this approach would be much more widespread if the authors can demonstrate that on these larger datasets, training transformers with the shared workspace doesn’t lead to worse performance than training the regular transformer based models that are currently used.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Coordination Among Neural Modules Through a Shared Global Workspace","Keywords: slot based recurrent architectures, attention, transformers, latent bottleneck.","Strengths:
The paper is written very well. The introduction provides a good motivation and challenges and illustrates the use cases clearly using examples. The related work compares the proposed approach with other memory-based neural models. The description of methods is concise, but sufficiently detailed.
The proposed approach is original to the best of my knowledge. The key idea is very intuitive and motivated by insights from cognitive science literature.
The authors perform experiments on a wide variety of tasks including a toy task for detecting equilateral triangles, multi MNIST Generation, object tracking, relational reasoning, physical reasoning and Atari video games. I also like that the authors use different types of backbones (Transformers and RIMs) in different experiments which indicates the proposed Shared Workspace method is not specific to certain kind of backbones.
Weaknesses:
Although the authors perform experiments with wide variety of tasks, multi-agent tasks are missing where I believe coordination is more important. In all the tasks used in the paper, different neural modules process different parts of the input to make a common prediction. Multi-agent tasks are more challenging as each agent would get a different input but also predict a different action.
It is unclear how certain hyper parameters are chosen. For example why is the patch size 4x4 in equilateral triangles, 6x6 in object tracking? The number of memory slots, size of memory slots, etc. different across different experiments. How are these chosen?
The authors propose two versions of Shared Workspaces, soft and hard (SSW and HSW). Some experiments one contain a single version. Why are both SSW and HSW not evaluated in all the experiments?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Coordination Among Neural Modules Through a Shared Global Workspace","Keywords: slot based recurrent architectures, attention, transformers, latent bottleneck.","Strengths
Novel and significant contribution by proposing a new mechanism for inter-subnets information exchange/learning.
Of high relevance to the ICLR community. The generic approach cuts across different AI discipline as the experiments are performed on vision, language and reasoning tasks.
Experiments are comprehensive and well-explained.
Weaknesses Minor: Some technical details are not elaborated for readers who are less familiar with the respective topics. For example, the section on the key-query-value attention mechanism is quite brief. It's the key contribution on the success of the proposed approach.","No concerns.","10: strong accept, should be highlighted at the conference"
"Data-Efficient Graph Grammar Learning for Molecular Generation","Keywords: molecular generation, graph grammar, data efficient generative model","The proposed method is novel and opens potential for new line of research in graph grammar. I personally find the submission to be clear and well-written. Overall, I am positive and would like to give a ""accept"".
However, I have few more comments that I hope to help improve the paper. Several of them are around the modeling choices and some of them are to clarify my understanding of the practical usefulness of the proposed method.
The proposed method uses pretrained graph neural network to generate the sampling weights and claim that this can be replaced by any plug-and-play feature extractor. Can the authors provide data to support this claim? If this is actually true, I would suggest to use the simplest feature extractor to keep the proposed method clean.
Is the REINFORCE algorithm stable across random seeds? My hunch is that they are not. Can the authors provide error bars in for their data? Can the authors provide convergence curves to demonstrate the learning?
While the proposed method is efficient, I have the impression that it does not scale up well to more data (the authors have to subsample the training set for the experiments in Table 3). My first request is for the authors to report the actual computation time for the proposed method on the experiments they have done. Second, aside from the computational challenge, can the proposed algorithm benefit from more data? If so, can we estimate how much we can gain by including the whole training set of the large polymer dataset?","None","8: accept, good paper"
"Data-Efficient Graph Grammar Learning for Molecular Generation","Keywords: molecular generation, graph grammar, data efficient generative model","This paper proposes a generic solution for learning to generate graphs in an extreme few-shot manner where only dozens of examples are provided, which is pretty common in the polymer/monomer domain. The method is general and can have a large impact on the graph learning community. Although technically the proposed method is similar to some previous work like MHG[1], it cleverly preserves the class-specific properties by operating on subgraphs instead of individual atoms. Also, the idea of learning a grammar search algorithm using RL instead of directly learning the grammar is novel and interesting in the domain. The proposed method is well-supported by experiments on datasets of different scales.
The paper is well-structured overall but there is still room for improvement. The notation in section 4 is a bit unclear. In section 4.1 the subscripts are usually used for denoting the number of iteration, and the superscripts are used for different connected components. While in section 4.2 right before Equation (3) we see there is
et(j)
, where the meaning of superscript
(j)
is unclear. And
t
here is used both for denoting the current iteration and the total number of iterations. Also if I understand correctly,
X
is a binary matrix, and instead of
p(X)=∏t∏jϕ(etj;θ)
, it should be
p(X)=∏t∏jϕ(etj;θ)Xtj(1−ϕ(etj;θ))1−Xtj
?
For the experiment section, both Appendix A and Table (3) contain critical information for understanding the experiment setting and results, and so they should be put earlier in the main text.
Question:
The paper proposes a clean algorithm for learning grammar. But in practice, there must be a lot of hyperparameters to tune, such as the number of production rules, the maximum/minimum length of a single production rule, etc.? How do you decide these parameters? Do you do any pruning for the production rules?
The main text does not mention how to generate molecules using the learned grammar rules. Do you sample the production rules uniformly randomly during testing? If yes, do you plan to learn a policy for generating molecules? How to jointly learn the generation policy together with the rule search policy?
The space of grammar rules is combinatorial as mentioned in section 4.2. How does RL circumvent that and learn a reasonable policy?
Why do you only use diversity and RS as the optimization objectives, as mentioned in Appendix A?
[1] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In International Conference on Machine Learning, pp. 3183–3191. PMLR, 2019.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Data-Efficient Graph Grammar Learning for Molecular Generation","Keywords: molecular generation, graph grammar, data efficient generative model","This is a pretty solid contribution. Although the paper is written clearly, I admit I had trouble trying to understand the method initially, but once I got the big picture, everything lined up nicely. The methodology in itself (generative learning through graph grammars) is not new, and the proposed model presents some similarities to the HMG model of Kajino. However, it draws from previous methods in a clever way and addresses most of their limitations (for example, the production rules involve substructures instead of single atoms). This, in my opinion, is a plus. The main advantage of this model is its data efficiency, which the experiments corroborate nicely. Nonetheless, I have a few points I would like the authors to address before I give complete acceptance.
It is unclear to me what is the computational cost of constructing the grammar and training the model since generating the production rules involves graph matching (and although you affirm this isn't an issue). How does your training/grammar inference cost compares to other approaches?
""we also show that with more training data (0.3% of the whole dataset), our method can achieve better performance"". I might have missed it, but where is this shown?
Why are some results in the tables underlined? You should explain it in the captions.
This is more of a request for clarification. How is the final set of grammar rules chosen? If I understood correctly, the bottom-up construction process generates a lot of near-duplicates, as well as potentially useless production rules, especially during the initial iterations. Is there any pruning process after the optimization has been performed, or just everything is kept?
How many rules does the grammar construction process produce in the datasets you evaluated?","None","8: accept, good paper"
"Data-Efficient Graph Grammar Learning for Molecular Generation","Keywords: molecular generation, graph grammar, data efficient generative model","Strength: This method is very data efficient given grammar-based learning. The hypergraph contraction-based production rule learning process is very novel (at least from my perspective).
Weakness: It seems like the model can only work on a small set of molecules to generate the rules due to the computational complexity. The way this model learns the production rule is actually very related to a domain called task-based learning, where people tend to optimize their model towards some non-differentiable evaluation metrics. The authors should reference some of those works, e.g. ""Task-Based Learning via Task-Oriented Prediction Network with Applications in Finance"", ""Task-based End-to-end Model Learning in Stochastic Optimization"".","N/A","8: accept, good paper"
"Poisoning and Backdooring Contrastive Learning","Keywords: Contrastive Learning, Poisoning attack, Backdoor attack, CLIP","Strength
(1) Even though some progress is made in dataset security research, we still need to demonstrate the urgency and importance of addressing such data security issues. This paper did an excellent job, especially achieving a high attack success rate in a SOTA algorithm which requires lots of computational resources.
(2) The proposed method is simple but effective, and the experimental results are convincing.
Weakness
Major
(1) The title ""Poisoning and Backdooring Contrastive Learning"" might overclaim the contributions of this paper. Multimodal contrastive learning trains the vision model from natural language supervision as indicated in (Radford et al., 2021), which is similar to weakly supervised learning (Keeping in mind that we only poison the vision model). Meanwhile, the difficulties of the poisoning attack heavily rely on the type of supervision provided (the authors also discuss part of this in Section 3.2). Contrastive learning can be applied to supervised (Chechik et al., 2010), weakly supervised, and unsupervised data (Oord et al., 2018). As a result, the title fails to exactly convey the difficulties of the poisoning and the contributions of this study. Actually, I was astonished by the title since I even thought the authors have successfully poisoned (totally) unsupervised learning after reading the title.
In conclusion, I would encourage to use ""multimodal contrastive learning"" instead of ""contrastive learning"" in the title. It would also be better to modify some descriptions in the paper, such as
""AS we are the first to study poisoning and backdoor attacks on multimodal contrastive learning methods"" in Section 2.3.
(2) I'm concerned about this paper's lack of novelty. This study only proposes a simple poisoning and backdooring approach which is not technically novel, failing to match the ICLR's novelty standards.
(3) I am curious about the results in Figure 5 (left). Since we always use 30 epochs and a batch size 1024, it would take fewer iterations to train the model on a smaller dataset. I wonder whether the ASR plateau is due to insufficient convergence. If we have more iterations, will the model converge better and learn better representation, letting the backdoor Z-score increase as well?
Minor
(4) The writting should be polished further. Some discussions in experimental results are verbose from Section 5.1.1 to Section 5.1.3.
(5) typo:
(5-a) ""minimizes an inner product between the embeddings while maximizing ..."" in Section 2.2 -> ""maximize an inner product between the embeddings while minimizing ..."". This is because the larger the inner product becomes, the more similar the image embedding and text embedding are.
(5-b) ""however because the majority of labels are correct"" in Section 3.1. Is there any part missing?
(5-c) ""... one of the three cases above ..."" in Adversary Objective of Section 2.3: three cases -> two cases (feature extractor, zero-shot classifier) or four cases (feature extractor, zero-shot classifier) x (poisoning attack, backdoor attack)?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Poisoning and Backdooring Contrastive Learning","Keywords: Contrastive Learning, Poisoning attack, Backdoor attack, CLIP","Strengths:
Poisoning of an interesting setting
Much lower poisoning ratio requirement
Evaluation over an extremely demanding task
Weakness:
Minor methodological contribution over current literature
The paper is very well written and presents a convincing argument that poisoning and backdooring attacks are very effective against CLIP. Evaluation of how size and placement of the trigger changes performance of the attack is particularly interesting, especially the dip observed for larger triggers. Finally, attack performance as a function of model parameters in Figure 5 demonstrates that increase in number of parameters can help learn more generalisable features (point at 5), but from that point onwards, it only leads to increased vulnerability.
I only have a small number of clarification questions:
Why do you think there are dips in performance in Figure 2 with increasing number of samples?
A finding that location of the trigger dictates its performance depending on a number of samples is interesting, do you have any intuition as to why this is happening? Do you believe it’s a function of the trigger used?
Do you think zero-shot ImageNet performance was not affected because the base model had a large number of parameters?
Typos:
“However because the majority of labels are correct.” unfinished sentence In constructing the caption set.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Poisoning and Backdooring Contrastive Learning","Keywords: Contrastive Learning, Poisoning attack, Backdoor attack, CLIP","Contrastive learning is a widely used technique for self-supervised learning and it is common to apply it to data that is scraped from the internet without careful review by a human. The authors show that this use of ""uncurated"" data makes contrastive learning vulnerable to poisoning and backdoor attacks and they show that poisoning even a small number of instances can be very effective.
To my knowledge, this is the first paper that investigates the issue of a poisoning / backdoor attack for contrastive learning. I was surprised at how little data needs to be poisoned for this attack. I agree with the authors that their findings are especially alarming given the fact that contrastive learning is often used on uncurated data. The authors do a thorough job of including experiments that show how varying different aspects of the data or attack influence the success rate. I found the patch size experiments to be an interesting finding. Overall, the paper is well written and the experimental process is clearly described.
The main weakness of this paper is how well the findings generalize beyond the Conceptual Captions dataset as the authors only present results on a single (but very large) dataset. Are the threats to contrastive learning simply artifacts of that dataset or are they also present in other multi-modal datasets used for contrastive learning? Do these vulnerabilities also generalize to other types of datasets (besides multi-modal ones) used for contrastive learning?
In addition, while it is a contribution to identify these vulnerabilities, the paper would be much stronger if the authors could also present a solution that addresses these vulnerabilities.
Minor issues:
Section 3.1: the last sentence in the second to last pargraph has an incomplete sentence.
Caption on Figure 4: ""orage"" should be ""orange""","The authors show how to exploit contrastive learning via poisoning and backdoor attacks. It is useful to have this vulnerability explained and the authors point out in their Ethics Section that their goal is to highlight this weakness and spur the research community on to find solutions.","8: accept, good paper"
"Poisoning and Backdooring Contrastive Learning","Keywords: Contrastive Learning, Poisoning attack, Backdoor attack, CLIP","This paper focuses on the multi-modal classifications, especially on the recent multi-modal classifier CLIP. CLIP is trained on the extreme amount of internet data with text captions and has shown great generalization to other datasets. The proposed poisoning and backdooring attacks are able to inject a small amount of perturbed noise data into the training dataset and cause desired adversarial behavior. The fact that only injecting a few noise data can misbehave the underlying embedding function is interesting.
The paper is easy to follow and provides a very detailed study of this behavior. I have no further questions since the detailed experiments well support the effectiveness of both proposed attacks. This work can lead to new defense algorithms that focus on detecting those poisoned/backdoored data scraped online.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path","Keywords: neural collapse, deep learning theory, deep learning, inductive bias, equiangular tight frame, ETF, nearest class center, mean squared error loss, MSE loss, invariance, renormalization, gradient flow, dynamics, adversarial robustness","The strengths of the paper:
The phenomenon of Neural Collapse is certainly an interesting and thought-provoking and analyzing it further to understand its premises is certainly worthwhile of publication. In that sense, the contributions of this work are very relevant and welcoming.
The experiments seem to corroborate that the intuitions/derivations of the paper are on the right track. I particularly like the idea of decomposing the loss in two terms, one that is the 'optimal loss' and a secondary loss that accumulates the remaining errors.
I find the connection with the dynamics of learning a fascinating direction, and I think this is where research should focus on. It would be very interesting if indeed the dynamics of the learning, perhaps by examining the eigenvalues of the signal-to-noise ratio matrix, lead to Neural Collapse. However, I am not sure about this, because in all honesty it was hard to parse the respective subsection.
I would spend more time on the central path, which I find an interesting concept. What does it really mean? Are the features H supposed to be 'fixed' or at least 'fixed within infinitesimal time steps'? I believe this is the largest and most interesting contribution, and a good proof why using the MSE is relevant in this context.
The weaknesses of the paper:
A major weakness of the work is writing and structure. While very interesting ideas are in it, and it is clear that there is a worthwhile message, it is very hard to understand in precise detail the claims, so that a 'third' reader can derive their own insights. As one example, the intro is very technical, with lots of forward references, and extends till p 4. It creates a feeling of repetition and unclarity at the same time, eg, proposition 1 is basically defined twice. Lots of different concepts, terminologies, and ideas are mixed and the text often jumps from one place to another, even referencing later parts of the text that have not been read, assuming someone does read a paper sequentially. Another example is that not notation is not always clearly explained. For instance, what is the time t in equation 5? I suppose time steps during training. For the lack of writing clarity alone, I am not sure if the paper should be accepted, it would be a pity for the work itself.
It is not clear (at all) what are the 'assumptions' that are made for the sake of the analysis. -- For instance, equation 5 assumes that the features H (and thus X) are converged, that is the manifold of the identity-covariance features is fixed? Or, is it assumed that in infinitesimal training steps the features H (and the manifold H) remain roughly equal? In practice, what does it mean to have the loss L_{LS} for given features H, since the features H change during the training? How is this computed in the plots of figure 2? -- In the sentence 'As LNC1 decreases, individual activations would need to tend to their corresponding class-mean', how is this conclusion derived? Do you assume W_{LS} to be fixed? Otherwise, W_{LS} can also reduce the loss, no (in fact, that is the point of learning)? -- Although that is a point for the original NC paper, I think NC4 is self-evident.
Focusing on the figure 2, it shows that NC2/3 is much much smaller. Does this mean that the model basically distributes features 'uniformly' early on (thus NC2/3 goes to zero fast) and from thereon, it tries to group/cluster each class features as much as possible? What about overfitting? How well does the models generalize if keeping the training till zero loss? Doesn't this contradict standard practices, like 'early stopping'? To put otherwise, what would happen if having small training sets, say 50 or 100 examples per class.
Isn't the zero-global mean after bias b_LS an obvious conclusion, in that after subtracting the bias, the average is zero-mean? That is the point of the bias, no?
How do you obtain the A^{-1} in equation 8? Is this part of some definition, or Linear Algebra? More generally, is equation (8) suppose to derive a result or to state it? How do you go from W(AH) to W(H) A^-1? I think you mean to say that the W operator is linear (matrix multiplication), so the multiplier A can go out, but how do you derive the A^-1?
What is the intution behind the signal-to-noise ratio matrix in the off-diagonal elements? Class confusion so to speak?
Section 3.3 is very involved and I am not sure I understand how the NC1-4 are derived. It is stated what we are to conclude from it, but no guidance is provided on why this is the case. I am not asking for the proof, but for the interpretation of the results. For instance, in equation (11) we have the \omega_max in the denominator, while #1 in Corollary 1 tells that the eigevalues go to infinity. Does this imply that the SNR divided by inifinity goes to 0? Generally, I am not sure what am I to take from this section.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path","Keywords: neural collapse, deep learning theory, deep learning, inductive bias, equiangular tight frame, ETF, nearest class center, mean squared error loss, MSE loss, invariance, renormalization, gradient flow, dynamics, adversarial robustness","Strengths:
The paper empirically shows that NC also occurs when training deep networks for classification using the MSE loss on five canonical datasets and three backbone networks.
The theoretical part of the paper to justify the NC phenomenon for the unconstrained features model seems rigorous. The decomposition of the loss function and how it is helpful in understanding NC is interesting.
Weaknesses:
I think the paper contains too much information to be wised packed in a single 9-page conference paper. Specifically, for one of its main contribution, namely the empirical study of the NC with MSE loss, almost all the supportive experiments are deferred to the Appendix, while the main body of the paper only focuses on explaining the theoretical part. I have doubts on such practice (claim the contribution are two-fold while only mainly presenting one of them in the main paper). Note that the authors also admit in Section 1.3 that the empirical study is too long to be included in the paper.
Some statements need more clarification. In the legend of Figure 2, the term ""Lperp"" should be referred to as ""L^\perp"" in the caption. Also, in the caption it says ""early in the training, L^\perp becomes negligible compared to the dominant term LNC1""; however, I don't see this from the figure: for many of them, the L^\perp curve is on the above of LNC1 during the early phase of training. How to explain this?","6: marginally above the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path","Keywords: neural collapse, deep learning theory, deep learning, inductive bias, equiangular tight frame, ETF, nearest class center, mean squared error loss, MSE loss, invariance, renormalization, gradient flow, dynamics, adversarial robustness","Pro
+ The submission is well motivated and coherently structured. (Empirical observation -> Splitting the loss function in summands -> Examining the gradient flow from the predominant summand)
+ The theory is able to explain, some aspects, of the empirical observations by Papyan, Han and Donoho (2020) . It separates itself from the prior work, as it contains closed form solutions of the class means (of renormalized features).
+ The submission is relevant, as understanding why and how optimization of current neural networks works is crucial for making informed improvements in future algorithms.
Contra
- The main result (Section 3.3) is formalized in terms of the singular values of the SNR matrix. However, the interpretation is not obvious. I would advice to expand on the footnote 9 and move it to the main text as a brief discussion of the findings.
- There is an additional assumption which is required for Theorem 3, which is not mentioned in the statement and the subsequent discussion, but somewhere later: The within class covariance
ΣW
is full rank. While one expects the assumption to be fulfilled, this assumption should still be stated more prominently.
- There is a correctness issue, due to which I currently cannot give a higher score. The conclusions might not necessarily be wrong, but I am a bit wary. I hope, this can be resolved with the author response.
It is claimed that Corollary 1 and Eq. 11 imply neural collapse. So in particular, they imply (NC1):
ΣW→0
. However, these corollaries are concluded from the dynamics specified in Equation 5, where the representations are always such that
Σ=I
is constant.
The first limit in Corollary 2, i.e. the statement that the singular vectors of SNR stay constant with respect to
t
, is reasoned for by the fact that
L(w(t))
only depends on the singular values (and not the singular vectors) and so its gradient does not depend on the singular vectors. However, the dynamics in Eq 5 do not only depend on the gradient, but also on an projection operator, which might change the singular vectors.
Intuitively, the dynamics in Eq 5 correspond to gradient updates followed by a renormalization step, i.e. matrix multiplication such that
X
has identity covariance. This multiplication might possibly chance the singular vectors.
Questions and suggestions unrelated to score:
It might be a good idea to reduce the notation in section 2, by considering only the case of zero bias and move the general case to the appendix. In Section 3, the setting is reduced to the case of no bias term anyway.
Is symmetry really required for equation 8 to hold, or does invertibility suffice?
Could you explain the terminology central path (Equation 4)? I guess, central refers to the least-squares optimality, but why is it a path? It does not appear to be one dimensional.
In appendix D, there is a rather informal discussion on the feature space as a fiber bundle. Can the renormalized gradient flow be formalized as a connection on this bundle?
There is a typo in Corollary 1. The second limit should depend on
c3
instead of
c4
.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path","Keywords: neural collapse, deep learning theory, deep learning, inductive bias, equiangular tight frame, ETF, nearest class center, mean squared error loss, MSE loss, invariance, renormalization, gradient flow, dynamics, adversarial robustness","Pros:
--The authors provided extensive experiments to show that NC occurs in DNNs with MSE loss.
--The authors provided motivation and explained the required conditions of the theorems and the limitations of their results. Overall, the conditions of the theorems do not look too restrictive and unrealistic.
--The appendix has an extensive overview of related work.
--In the paper, a new theoretical construct, central path, is proposed and analysed.
Cons:
--Overall, the paper is well-written, but there are some places that are not easy to follow, for example, Section 1.1. I would recommend the authors to state the problem and all the necessary notions first and only after this to introduce what NC is. The first sentence of the introduction also does not sound good.
--The authors investigate the NC phenomenon from the point of view of the last classifier layer only and do not consider how DNNs generate the features that are passed as input to this classifier. It is still not clear how the NC phenomena are connected to generalization. While MSE delivers similar performance to cross-entropy loss (CE), it is less used in classification settings than CE.
Additional comments:
--In Section 1.3 I would recommend adding references to datasets and architectures.
--I would recommend adding an explanation into Section 1.2 about how classification performed when the model is trained with MSE loss.
--Could you please clarify for me whether NC4 is an independent condition or it follows from (NC1-NC3)?
--Could you please clarify how condition (A2) is related to empirical observations?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Weighted Training for Cross-Task Learning","Keywords: Cross-task learning, Natural language processing, Representation learning","The paper proposes a new weighted training algorithm, TAWT, to learn the task-aware weights on tasks for better using the cross-task signals. The author theoretically and empirically verified the effectiveness of TAWT. Strength:
The paper was clearly written and give a good formal definition for the problem of weighted training.
The paper can bring a new research interest for multi-task learning and transfer learning.
The experiment designs are very good,which show the affects on different training data size, impact of the ratio between source tasks training samples and target task training samples, etc.
Weakness:
The paper introduces a representation-based task distance, but this distance is neglected in the analysis of Eq.(3.8). I think it could not be negligible. Otherwise, the upper bound become task-agnostic.
The used four NLP tasks are closely related. It's better to add an irrelevant task, such as sentiment analysis, to show the effect of weighted training.
An illustration of weight vector should be provided.
Others:
It's interesting to extend this idea for incorporating some unsupervised tasks, such as Masked language model.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Weighted Training for Cross-Task Learning","Keywords: Cross-task learning, Natural language processing, Representation learning","I enjoyed this paper a great deal. The presentation is very clean and the underlying algorithm and theory is interesting. The results look quite promising for the method.
A few comments:
[1] Are there connections to work that learns data weighting using multi-arm bandits? Specifically:
Automated Curriculum Learning for Neural Networks Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, Koray Kavukcuoglu
It would be interesting to discuss connections to this work - the ideas are different but there does seem to be some connection.
[2] The choice of mirror descent is not critical, correct? For example projected gradient methods could be used instead? It would be useful to note this.
[3] Given Assumption B, it seems clear that \phi must be a vector, correct? This is a minor thing, but I think that was not specified earlier in the paper?
[4] Definition 3.2 (transferability) could benefit from much more explanation. I’m also not sure if it’s quite correct as written. Should there be a \forall quantifier, for example \forall \phi \in \Phi before Eq. 3.5? Also I’m not sure how this equation implies that the loss is “controlled by a polynomial…”. Finally, is there a reason that the expression (\sum …) is positive? And if it can become negative, how can we take a power to 1/p? Finally, I have very little intuition about what values p will take in practice, or what p intuitively corresponds to.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Weighted Training for Cross-Task Learning","Keywords: Cross-task learning, Natural language processing, Representation learning","Strengths
The proposed method is novel and well-motived.
The authors provide a theoretical guarantee of the proposed method.
Empirical results indicate that the proposed model greatly outperforms the vanilla methods in both pre-training and joint learning. The authors also open-source their implementation.
Weaknesses
Add a section that introduces ""Related Works"" would be very helpful for readers to understand the background and the novelty of this work. For example, what're the most popular/advanced methods in cross-task learning? How is this method related to and different from them? What's the relationship between cross-task learning, joint learning, and multi-task learning?
The empirical evidence would be much more convincing if authors compare their model with other methods besides the vanilla pre-training and joint training. Many existing works that utilize different strategies to calculate the weight of different task has been introduced in the area of multi-task learning. Although this work is designed for ""cross-task learning"" instead of ""multi-task learning"", the existing methods can be straightforwardly used to address the same problem. So it would be great if the authors can either compare the proposed methods with existing works or explain why other existing works cannot be directly adapted to this problem. There are many great survey papers that summarize the related works (e.g., A Comparison of Loss Weighting Strategies for Multi task Learning in Deep Neural Networks and A Survey on Multi-Task Learning. )","6: marginally above the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Weighted Training for Cross-Task Learning","Keywords: Cross-task learning, Natural language processing, Representation learning","Review note: As noted in the initial review assessment, I am less comfortable with the mathematics underlying section 3 and Appendix A. I will focus more on the empirical results and I hope other reviewers will help review those. This explains the lower confidence score.
Pros:
The method is well-motivated. It is clear what the method is aiming for and how it differs from existing approaches. The added theoretical guarantees are welcome.
The experimental setup is convincing and the results confirm the soundness of the approach. The settings are well described and I appreciated Section B.1. describing some of the rationale behind these choices.
The writing is very good.
The appendix is very rich in additional interesting experiments.
Cons:
Dynamic weights analysis not present in main text:
TAWT is more complex than prior methods. Seeing the importance of dynamic weights throughout training (Table 8) is very relevant as it shows that even a strong choice of initial weights for
ω
(e.g through hyperparam opt) may not be as good as TAWT. Currently, this feels like it is missing from the main body and deserves to be emphasized earlier.
Weight initialization for joint training More of a suggestion than a con: For joint training, I believe it would be better to use normalized joint training as the baseline. Indeed, initialization with task weights that depend on the sample size is a stronger baseline and more closely aligned with approaches used in practice. An added benefit is that table 3 could then be removed (since it’d be the later rows of Table 1 and Table 2). The non normalized results could still be kept in appendix for completeness. This would also allow Table 8 and its discussion to be moved into the main body.
Minor typos: The writing is good overall, but there are still some minor typos: e.g: p20 Pre-training first pre-trains and then fine-tunes","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data","Keywords: neuroscience, latent variable models, RNN, VAE, motor control, control theory, dynamical systems","Authors present a novel method for simultaneously learning latent dynamics and inferring unobserved control inputs. The method performs very well on several toy datasets (autonomous and input-driven linear dynamical system + Lorenz attractor), is on par with state of the art methods on benchmark datasets for neural data analysis methods, and allows for better reconstruction of hand kinematics for primate recordings in a continuous reaching task. The main novelty of the method lies in the utilization of iLQR with implicit differentiation. The paper also extensively discusses LFADS and provides very interesting insights on LFADS.
An important component of the method is the prior on input distribution. Authors show how to use a Gaussian prior or a multivariate Student prior (it allows for strong inputs when needed, represents the fact that inputs come as shared events and are spatially and temporally independent, and authors mention it might be one advantage over LFADS's autoregressive prior). This is a key idea of the paper and I would like to see more experiments highlighting the importance of the prior. I would recommend that the authors show (possibly in the appendix), the differences between the inferred inputs and performances when using different priors.
The derivation of the ELBO does not seem straightforward to me. To go from equation 7 to 8, don't you need to condition p(o|u) on z_0 as well? Maybe I am not correct, but I would appreciate if authors clarify this point (and correct the manuscript if needed)
About the computational complexity. The model is linear in T and cubic in n, which seems fast for low-dimensional latents only. Authors mention that iQLR-VAE enables fast learning of dynamics, but they also mention it as a limitation in the discussion. Could authors clarify this? Moreover, I believe the computational complexity comparison with LFADS in Figure 1 bottom left is not fair. Authors compare the number of training iteration without comparing the complexity of each iteration. I would like to see comparison for other datasets as well.
There are many performance metrics used in this paper to show that iQLR-VAE performs as well as other state of the art methods. However, there are no comparisons of the processes learnt by the different methods. Showing superimposed processes inferred by different methods (not only LFADS) for several experiments would allow to gain insight into the methods and how they compare (in term of uncertainty, smoothness, etc...). Similarly, for Figure 4, it would be interesting to see a low dimensional representation of neural activity (and comparisons of different methods). For this experiment, is it necessary to use a latent processes of dimension 50? Wouldn't it be sufficient and more efficient to use lower dimensional latent for this task?
Minor point :
In figure 4 caption, there is a typo and the last part should be panel D not C.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data","Keywords: neuroscience, latent variable models, RNN, VAE, motor control, control theory, dynamical systems","Post-rebuttal: Thanks for addressing my concerns. The authors have updated the manuscript and added experiments. I will not change my recommendation.
##########
Pros:
Implicit recognition model implied by the generative model. It incorporates the inferred forward dynamics.
Competitive performance to STOA method
Small parameter space for easy hyperparameter tuning
Concerns:
The method is extended and compared to LFADS. How about with other methods?
Unlike the bidirectional RNN in LFADS, the implicit recognition model should incorporate the dynamics from the trained generative model. This bias could be good or bad for the training. Can the authors address more?
There are other methods like VIND consider to incorporate the dynamics from the trained generative model. This better be compared or discussed.
Is this method fast or slow comparing to LFADS or other methods?
How well can the proposed method learn other typical types of dynamical systems such as fixed attractors, continuous attractors and etc.
Minor:
Typo: ""LFDADS"" in paragrah 2, page 2","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data","Keywords: neuroscience, latent variable models, RNN, VAE, motor control, control theory, dynamical systems","strengths
The problem of simultaneously learning dynamics and ongoing external inputs is a crucial one, especially in fields such as neuroscience where complex dynamical systems are often analyzed through incomplete measurements of the system (i.e. not every neuron is monitored). This paper presents a sound approach to tackling this problem, though I admit my understanding if the control field is limited.
The paper is clearly written and easy to follow.
I appreciate building up the complexity of the toy examples.
Comaprison to state-of-the-art methods is nice and clear.
weaknesses
Fig. 1: ""Indeed, iLQR is unable to find initial conditions that would explain the data well, resulting in a much higher initial loss."" While the paper implicitly points to this outcome as an advantage of the more flexible prior, it is concerning that iLQR cannot find the initial condition of this very simple example. Is this an issue with the iLQR algorithm itself? The chosen prior? It would be very useful to understand this behavior in the simple example before moving on to more complex examples.
The leap from example 1 (nonlinear autonomous dynamics) and example 2 (linear dynamics + sparse inputs) to the real data (presumably nonlinear dynamics + ongoing inputs) begs the question: how well does the model perform with (nonlinear dynamics + sparse inputs)? Showing good performance in this regime would increase my confidence in the model's ability to handle more complex, real-world data.
There is very little reference to related work, especially with regards to inferring control inputs in dynamical systems (linear or nonlinear). The paper would feel much more complete with this information (which could be provided as an additional appendix if space is short).
The general problem of decoupling ongoing dynamics from external inputs is underdetermined, and results rely heavily on the structure imposed on these aspects of the model. In this paper the prior on the inputs is a sparse one, but what happens if there is mismatch between this prior and the real data? How will that affect the results (and their interpretation)? The paper comments on this several times (and shows how an AR prior in LFADS performs when inputs are sparse, but not the reverse), but I'd appreciate a more thorough discussion of this issue in the Discussion.
Learning in the proposed model seems to have some strong parallels with the EM algorithm, except that maxima from iLQR replace expectations. Is this true? Could be interesting to briefly draw this connection when describing the inference strategy.
minor
Fig. 1 caption: first sentence, add ""system"" to end?
paragraph that starts ""At the beginning..."", second sentence, typo: ""learning consists in making""
same paragraph: ""We note that this regime is facilitated here by our choice of generator dynamics, which we initialised to be very weak initially and therefore easily controllable."" What does it mean for the generator dynamics to be ""initialised to be very weak""? This is potentially an interesting point that is unclear to me as written. What does ""weak initialisation"" mean in this context?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Extending the WILDS Benchmark for Unsupervised Adaptation","Keywords: distribution shifts, adaptation, unlabeled data","The problem is well-motivated, and the use of realistic problems to benchmark unsupervised adaptation methods presents a major gain over prior datasets which rely on different stylized images (e.g. PACS). The datasets used span a wild variety of modalities and domains, and are fairly robust with large sample sizes. The paper is easy to read and easy to follow, and the experimental evaluations are robust.
I would suggest the following improvements:
In order to present an informal upper bound for the performance of algorithms on the OOD domain, the authors should consider adding in two additional ""oracle"" baselines which have access to labelled OOD-domain data: 1) training an ERM model only on the OOD domain, 2) training an ERM model on all available data.
The authors should justify why the OOD domain was the one selected in each dataset. If possible, the authors could consider allowing for the ability to swap around the various training/validation/test domains, though it might be impractical if some domains do not have sufficient data.
The authors should clarify the model selection procedure for each of the algorithms. For example, how does each method make use of the Validation (ID) labelled examples versus the Validation (OOD) unlabelled examples for model selection?","It would be good to:
Verify that all of the datasets used have licenses that allow for public release, and that all datasets have been adequately anonymized/deidentified.
Flag any potential fairness concerns with using these datasets for the benchmarking and selection of developed algorithms, e.g. as has been found in ImageNet [1].
[1] https://arxiv.org/pdf/2010.15052.pdf","8: accept, good paper"
"Extending the WILDS Benchmark for Unsupervised Adaptation","Keywords: distribution shifts, adaptation, unlabeled data","Strengths:
The problem of unsupervised domain adaptation is well-grounded in a practical learning setting where labels are scarce, but observations are not. This makes the extension of the WILDS data set to this setting a very useful contribution that can facilitate impactful future work.
The empirical evaluation was fairly extensive in it's inclusion of a variety of unsupervised domain adaptation techniques. I think it is important to validate the trend established in prior work that these techniques typically perform poorly in more realistic domain adaptation problems. I also think the finding that data augmentation techniques work relatively well in image domains to be an interesting one that can motivate future work.
Perhaps a necessary consequence of this kind of work is that the paper serves as a nice survey of modern unsupervised domain adaptation, both in terms of data sets and methods. Someone interested in the topic could use this paper to begin a literature search.
Weaknesses:
Ultimately, the novelty of this work is low. The main novel contributions are the extension to the existing WILDS data set (using already established data sets used by the original WILDS authors) and some new empirical results using previously published results. However, I do not think for this kind of work that novelty is paramount.
The descriptions of the data sets are relatively shallow. Besides raw increase in instances, not much else is reported about the data sets. Is there anything interesting to say about the instances that were chosen to be labeled by the original WILDS authors versus the ones added by U-WILDS? Namely, is there anything unique about the new instances? Can you characterize and measure how different the new instances are from the original ones?
Data preparation was not discussed at all. The major practical concern I have about this endeavor is standardization across new and old instances. If the original WILDS data set was released with some effort to process the data before repackaging and release, the U-WILDS data set would need to undertake the same process. I would like to see some comments on whether effort was taken to ensure that the U-WILDS instances were preprocessed for consumption in the same way the WILDS data set were, or why that was not a necessary step.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Extending the WILDS Benchmark for Unsupervised Adaptation","Keywords: distribution shifts, adaptation, unlabeled data","Strengths
The dataset is a useful addition to the WILDS dataset, and the provided unlabeled data would be very useful to design many unsupervised generalization algorithms.
The paper is very well written, easy to understand and well organized, although the experimental evaluation could be explained in greater detail in main paper.
Required Clarifications
The models chosen for benchmarking the methods are not suitable. While domain adaptation methods like DANN and CORAL work strictly with an assumption of single source to target domains, semi-supervised and self-supervised works are well-known to work only for within-domain samples. In this respect, the observations found with respect to the benchmarking, although useful, aren't surprising.
self-supervised, semi-supervised and DA methods make different assumptions about the availability of target data. self-supervised requires target labels for fine-tuning, while DA methods only require unlabeled target data during training and nothing more. So, comparing both of them together using a common yard-stick is, in my opinion, not appropriate.
The main paper needs to have more experimental detail such as what are the exact labeled and unlabeled data from source and target for each of the experiments.
The DA methods considered are quite old and primitive. The authors are encouraged to consider more recent benchmarks like CDAN, CAN, AFN etc.
The authors are also encouraged to use multi-source/multi-domain adaptation benchmarks [1,2,3] which seem most appropriate for the dataset proposed. at least for the visual recognition datasets. While self-supervised and semi-supervised methods aren't particularly designed keeping in mind cross-domain transfer, UDA methods like DANN and CORAL aren't designed to handle multi-domains.
The authors are also encouraged to compare and contrast with [1] in terms of the datasets proposed in both papers (although these can be considered contemporary works).
The key takeaways from sec 7 are not that surprising, and things like lack of augmentation strategies for many modalities and model selection for DA are already well known to the community, while ineffectiveness of pre-training seems interesting.
Dubey, Abhimanyu, et al. ""Adaptive Methods for Real-World Domain Generalization."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
Li, Ya, et al. ""Deep domain generalization via conditional invariant adversarial networks."" Proceedings of the European Conference on Computer Vision (ECCV). 2018.
Li, Da, et al. ""Deeper, broader and artier domain generalization."" Proceedings of the IEEE international conference on computer vision. 2017.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Extending the WILDS Benchmark for Unsupervised Adaptation","Keywords: distribution shifts, adaptation, unlabeled data","Strength:
The work behind the data collection and baseline computation appears sizeable and very valuable
The authors implemented many of the top-performing algorithms meant to address domain shift with unlabelled data. They provide their implementation, as well as a unified, didactic, and detailed explanation.
The conclusions are both measured and essential: we lack a definitive answer to domain shift in the wild, and unlabelled only provides a very partial solution.
The writing is remarkably clear and to the point.
Weakness: I am struggling to fault the paper. The extensive appendix answered most of my questions.
The paper does not have a methodological contribution per se, but the answers provided from the meta-analysis are new - at least to me, even if the fact that current methods would not hold too well for in-the-wild data was suspected
I have some questions about missing details, detailed below
Questions:
""Models are trained on labeled data from the source domains, as well as unlabeled data from one or more of the other sources, depending on what is realistic for the application."" I couldn't understand which problems were allowed to use OOD unlabelled data for training and which ones were not. From my understanding, unlabelled data from all domains (source, val, target, extra) are merged and used for training; is this correct? And what would be the use of unlabelled data if not for helping the training?
As a follow-up, are unlabelled data from all domains (ID/OOD) used identically by all methods? DANN, for example, may work better if only using target domain unlabelled data, if available.
The justification to not use augmentation on GlobalWheat are weak: translating and rotating bounding boxes seems easy enough?
It seems that you did not try approaches that explicitly use the domain as weak labels or adversarially, such as ""Adversarial Multiple Source Domain Adaptation, Zhao et al. NeurIPS2018. I am not suggesting that you do (the comparison is already more than sufficient), but you should mention these approaches. Note that these would only work when the domains are not too numerous and meaningful.
Details: The domains of PovertyMap are missing in Fig2","N/A","8: accept, good paper"
"Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks","Keywords: out-of-distribution classification, symmetries, counterfactual invariances, geometric deep learning","The paper is well motivated, and examines an important problem of classifiers failing out-of-domain.
The paper's contributions could be significant to what is an emerging area of research. Other than [1, 2] and [3], I have not seen any other paper that offers a theoretical framing for the relationship between counterfactual invariance and OOD generalization and provides a way to achieve the same.
The paper flows smoothly and is easy to understand.
I wish the paper offered a greater discussion of prior work on counterfactual invariance as it relates to out-of-domain generalization. I hope the authors can address that in a future version. Additionally, while the authors have offered some discussion of [2] and [3], per my reading, [1] seems to offer a contradictory view (from this paper) of whether learning an invariant OOD classifier is solvable via interventional data augmentation or not. It would be great if the authors could share how one relates to the other. I'm not sure I found a convincing argument laid in this paper other than the example in Figure 1(c).
The experiments in the paper are on simulated tasks with limited number of variables. It is unclear how this method might work in high dimensional spaces, such as text. Though it's not necessarily a concern, I think it would be useful if the authors could comment on the same.
[1] Kaushik, D., Setlur, A., Hovy, E. H., & Lipton, Z. C. Explaining the Efficacy of Counterfactually Augmented Data. ICLR 2021. [2] Veitch et al. (2021) [3] Wang and Jordan (2021)
Typos and presentation:
Introduction Line 2: ""the task is requires"" -> ""the task requires""
Break the second sentence of the first paragraph of introduction.
Layer 3 description of Pearl's causal hierarchy: ""X\dagger describe an hidden variable"" -> ""X\dagger describe a hidden variable""
Your figures are blurry upon printing the paper. Please increase the font of the text in the figures.
Section 4.1: ""Our next results requires imposing"" -> ""Our next results require imposing""
Section 4.1 under Definition 2: ""Definition 2 holds for a equivalence relation"" -> ""Definition 2 holds for an equivalence relation""
Related Work should either be Section 6 or be Section 2 (right after introduction) and not be in between the theory and empirical results.
In justifying Assumption 1, can you provide a citation that is more recent considering the science in this area has evolved considerably since 1982?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks","Keywords: out-of-distribution classification, symmetries, counterfactual invariances, geometric deep learning","Update: After reading the rebuttal and the other reviews, I have decided to bump up my original score to an 8. Thank you to the authors for addressing our concerns in great detail.
Strengths:
The methodology has a strong theoretical basis, and is a result of combining insight from different fields. The idea behind this approach, is in my opinion, quite elegant.
The paper is well-written and the work is well-placed in the literature, the illustrations are well-thought-out and facilitate understanding the method.
Weaknesses:
The experimental section consists of only two simulated toy examples. While these showcase how the method is supposed to work, it remains unclear how well the method would work in a real-world, uncontrolled environment. How often are the assumptions made about the data generating process expected to hold 'in the wild'?
Other comments:
The figures and table appear out of place. Table 1 is referenced in the Results section, but is shown two sections before without context. Figure 1 appears a bit too early in the paper (first mentioned in ""Illustrative SCM example""), while Figure 2 appears much later (first mentioned before Figure 1, after that mentioned before Theorem 1).
The caption text for Figure 2a(iii) is clipped out. The subfigure is also way too small to be readable without zooming in.
Reference to ""Accounting for unobserved confounding in domain generalization"" appears to be duplicated.
page 2, Layer 1 - ""causal"" misspelled as ""casual""
page 3, Symmetry transformations - Sentence starting with ""Although"" appears incomplete. I would suggest conjoining this sentence and the one before: ""... equivalence classes, although ..."".
page 8, Causal structure discovery, third row from the bottom: ""between distribution"" -> ""between a distribution""
page 9, second row: add comma before ""under the assumption""","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks","Keywords: out-of-distribution classification, symmetries, counterfactual invariances, geometric deep learning","Strengths: This paper does a good job of describing an interesting and important problem, invariance to OOD symmetry transformations, and motivates why existing approaches may not work well. The approach novel as far as I'm aware, and clearly describes relevant concepts such as defining what we would want our OOD-transformation invariant representations to satisfy and how learning them is linked to causal structure discovery.
Weaknesses:
The method appears to identify which of some set of transformations one should be invariant to (and learn correspondingly invariant representations). But we must first have prepared a collection of all possible symmetry transformations, which seems like a limitation of the method. Additionally, is there a trade-off where considering more symmetry transformations makes the method slower?
The experiments and comparisons are relatively minimal and are only in a toy environment. From the description of the method I assume there are computational difficulties in practically implementing this method for more ""real-world"" problems. The papers would be strengthened by either having larger more realistic experimental comparisons, or barring that an explanation of the practical difficulties of applying the method to larger problems and future steps that could resolve them.","8: accept, good paper","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Comparing Distributions by Measuring Differences that Affect Decision Making","Keywords: probability divergence, two sample test, generative model","Strengths:
(a) The proposed class of divergences, H-divergences, includes well-known divergences such as Jensen Shannon divergence and squared Maximum Mean Discrepancy as special cases. The H-divergence also contains new classes of divergences, including the ones defined in Equations (1) and (2), which appear to be potentially useful in practice.
(b) Some results are given to prove the convergence of the empirical H-divergence to its theoretical one; see Theorem 2 and Corollary 1.
(c) The conditions on the non-negativity of the H-divergence are obtained; see Section 3.3. These theoretical results partly justify the proposed divergence as a reasonable discrepancy between two probability distributions.
(d) A sufficient number of experiments are given to demonstrate the usefulness of the presented methods; see Sections 4 and 5.
(e) The paper is clearly written. Appendix provides helpful information including detailed proofs of the theoretical results of the main article.
Weaknesses:
(f) Applications of divergences include not only two sample tests but also other methods such as robust estimation and independence tests. However the paper does not discuss possible applications other than two sample tests and plots in Figure 3. A brief discussion about other applications would be helpful for readers.
(g) The variance of the proposed estimator
D^ℓϕ(p^m||q^m)
is not discussed in the paper. Is it possible to discuss any result about the variance?
Minor Comments:
(h) p.1, Section 1, 1st paragraph, l.13: Add a full stop after 'point'.
(i) p.1, Section 1, 2nd paragraph, l.1, etc.: There are at least three different expressions, namely, H-divergence, H-divergence and H-Divergence, in the paper to denote the same divergence. The expression should be standardized throughout the paper.
(j) p.4, Section 3.3, l.6 up: This property allow us to ===> This property allows us to","I do not find any ethical issues with this paper.","8: accept, good paper"
"Comparing Distributions by Measuring Differences that Affect Decision Making","Keywords: probability divergence, two sample test, generative model","Strengths
Measuring the difference between two probability distributions is a fundamental problem in machine learning. Although many different types of divergencies have been proposed in the literature, they are typically decision independent. H-divergence seems a simple yet effective way of incorporating decision related domain knowledge into the discrepancy measure.
The theoretical results are non-trivial are provide insightful connections between H-divergence and other commonly used discrepancies.
The paper is well written. The theoretical results are solid and clearly explained. The three examples clearly illustrate the broad applicability of H-divergence.
Questions:
Theorem 2 assumes that the same number of data points are sampled from p and q, respectively. I wonder if the result can be generalized to the case when the number of samples is different for p and q.
Can H-divergence still be useful when the decision task is initially unknown (as in reward-free reinforcement learning) or uncertain (as in multi-task learning)?
In Section 4.2, only the results for alpha = 0.05 are given. I wonder if similar patterns hold for other values of alpha.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Comparing Distributions by Measuring Differences that Affect Decision Making","Keywords: probability divergence, two sample test, generative model","Strengths:
Proposes a new type of divergence that compares two probability distributions from the lens of optimal decision-making. By setting an action set and loss function, one can identify ""how far two distributions are"" in terms of leading to the same optimal action. Moreover, the paper clearly explains the connection of H-divergence with f-divergence and integral probability metrics. Overall, H-divergence can be seen as a valuable contribution to the family of divergences used in machine learning tasks.
Experiments are satisfactory. Especially, the flexibility achieved by using H-divergence by choosing application tailored actions and losses highlight the merit of H-divergence in performing two samples test.
Weaknesses:
The convergence properties of the empirical estimator of H-divergence deserve more discussion in the main paper. In particular, the role of Rademacher complexity in the bounds in Theorem 2 and Corollary 1 is not clearly explained. It will be good to add a discussion in line with proof of Corollary 1 (on how fast Rademacher complexity vanishes) into the main paper.
The paper does not give enough detail about computational aspects. This clearly depends on the structure of the action set and loss function. The paper would benefit from the characterization of a set of (general) sufficient conditions under which computation of H-divergence is feasible.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling","Keywords: Audio Synthesis, Generative Model, Hierarchical, DDSP, Music, Audio, Structured Models","This paper develops a parametric model of musical audio that leverages insight into the music domain to construct meaningful featurizations of music, together with models trained to invert these featurizations, to construct a hierarchical autoencoder with meaningful semantic features that can be used to control the music generation process.
The technical contributions of this paper consist of a semantic ""expression"" featurization of DDSP parameters (Section 3.2) and two inverse models: (1) a synthesis generator for converting the expression features back to DDSP synthesis parameters and (2) an expression generator for converting MIDI data back to expression features. The features and models are well-described at a high level in the main paper, and the appendix provides a thorough & precise description of the remaining details. The empirical study seems convincing, with comparisons to both academic prior work (MIDIParams) and commercial software (Ableton, FluidSynth). And the demos sound qualitatively good to my ear.
The paper thoroughly relates this work to related work in the field, making many thoughtful connections to both the symbolic & audio generative modeling literature. These connections both help situate this work in the broader literature, and also provide some welcome scaffolding for thinking more broadly about the field of generative music modeling. I also appreciate the connections to the signal processing literature.
One possible criticism of this paper is that, because its focus is on applying machine learning rather than in the development of machine learning techniques, perhaps it would be better suited for a music domain conference (e.g., ISMIR). I strongly support its publication at ICLR: the paper does a thorough job situating itself in the machine learning literature, making it accessible and appealing to the broader machine learning audience.
One question I have is about the choice of 3 stage modeling (notes -> expression -> synthesis -> audio) as opposed to the 2 stage process used by MIDI2Params (notes -> synthesis -> audio). I appreciate that injecting semantically meaningful performance features provides an additional level of control over the generation process. Does this additional stage also account for the performance improvement of MIDI-DDSP over MIDI2Params? Or is this improvement more attributable to better/bigger models?
Also, related to the previous point, I am a little confused by the MIDI2Params comparisons in Table 1. I am under the impression that the expression step is an innovation of this work (Section 3.2). How are you able to make these breakdown comparisons with MIDI2Params for these substages notes -> expression and expression -> synthesis. Have I misunderstood MIDI2Params?","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling","Keywords: Audio Synthesis, Generative Model, Hierarchical, DDSP, Music, Audio, Structured Models","The paper is well-written, so I only have few minor questions.
While reading the paper, I was wondering the three sub-modules are trained simultaneously. If not, can other dataset be used for training each sub-module to enhance robustness or the performance?
For synthesis generator module, I couldn't find some ablation test about the loss function.
The authors argue that the method can be easily extended when multi-instrument transcription model is ready. However, I see there exist many challenges (e.g. the model relies on CREPE in DDSP synthesis part, also the type of an instrument set between transcription part and MIDI-DDSP should be matched.), the tone can be lowered.
In page 3, ""Realistic Note Synthesis"", ""a"" is missing in a word 'concatenative'.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling","Keywords: Audio Synthesis, Generative Model, Hierarchical, DDSP, Music, Audio, Structured Models","This paper introduces a layer of controllable expression parameters for the audio rendering of MIDI files. The contribution is threefold:
The authors introduce heuristics to extract note-wise expression parameters from low-level synthesis parameters. The choice of the controls are relevant to the dataset the authors use (acoustic instruments).
They introduce the synthesis generator, allowing to predict the frame-level synthesis parameters, hence bypassing the DDSP decoder that used to predict them from F0 and loudness contours. The introduction of this model allows a better reconstruction error than the original DDSP. Also, it transfers the manipulation of the sound from the mentioned contours (which are frame-level) to note-level expression controls, more expressive and controllable.
The smart design of the expression controls allows them to be controlled by humans, hence allowing further manipulation of the synthesis. Each control is well defined, normalized between 0 and 1, and coarse enough (note-level vs. frame-level) so that a user can easily set them to their preferred value, enhancing the customization possibilities. Human manipulation of the controls seem to have the desired effect, as shown by the correlation study in Table 2 and further supported by the audio material. A few comments:
In paragraph 3.3, the authors write that the note expression controls are pooled over the duration of the corresponding note. To my understanding, the controls would rather be unpooled, repeated or upsampled, as there are more frames than notes (very clear in B.4).
The paragraph about the dataset doesn't mention that it also contains the MIDI ground truth. This could be made explicit.
The paragraph entitled ""Expression Generator"" in 4.2 features what looks like a residue of an unwanted sentence
In Appendix B.3, the definition for brightness seems to be lacking a sum over k, and I'm having a hard time trying to understand the multiplication by i rather than k.
Some of these expression controls naturally lie in [0;1] (e.g. Volume Peak Position). However, it is not clear how others are normalized (e.g. Vibrato). If normalizing constants are precomputed using feature extraction on the whole dataset, it should be written.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling","Keywords: Grammar Induction, Vision-Language Matching, Unsupervised Learning","Strengths
As far as I know, this is the first paper that proposes joint visual-linguistic grammar induction in a real-world setting (in contrast to synthetic settings; Hong et al., 2021).
The approach and the evaluation process are solid and make a lot of sense to me.
The visually grounded parsing results are quite impressive.
Weakness
My major concern is about the model selection process and the potential unfair comparisons to existing work.
Model selection: If I understood correctly, for text parsing, the best models are selected w.r.t. to the parsing performance on a 1000-example dev set (Appendix F).
This is an unrealistic setting (see https://aclanthology.org/2020.emnlp-main.614.pdf for discussions; in short, for any fancy unsupervised parsing model that uses a labeled development set, a supervised model trained on these development examples should be considered as a strong baseline) -- introducing unsupervised criteria for model selection is more important than our initial impression.
Unfair comparison: CLIORA, the model proposed in this paper, uses DIORA as initialization, which uses ELMo to initialize word embeddings and the PTB labeled development set for model selection. This means that CLIORA has seen far more text than other baselines (VG-NSL, C-PCFG, VC-PCFG, and so on) and human language learners. \
This issue also undermines the authors' arguments about potential links to how humans learn language. I expect either a CLIORA trained from scratch (without DIORA initialization) or weakened arguments about the relationship between the current CLIORA and human language learning.
There seem to be some confusion on basic linguistic concepts, e.g., nonterminal vs. terminal symbols, and a few typos that affects smooth understanding (please see also detailed comments below).
Other comments and questions
Introduction: ""These works, however, fail to consider a unified VL structure, nor have they demonstrated impact on visual understanding.""
I don't think I necessarily agree with this statement, especially regarding Hong et al. (2021). Despite that there is a clear gap between their dataset and the real world settings, they are aligning the ""visual grammars"" to language grammars, yielding an arguably unified VL structure.
Introduction: ""The non-terminal symbol of a conventional constituency structure is a category label from a limited set (e.g., the set of part-of-speech (POS) tags) (Hopcroft et al., 2001). ""
Do you mean terminal symbols here? We usually refer to POS tags (to clarify, phrase tags are not POS tags) by preterminal or terminal (depending on whether the phrase-structure grammar is lexicalized, i.e., whether it's considering real words or just POS tags), and refer to the phrase nodes by nonterminal nodes/symbols (e.g., NP, PP). It seems that this is not a typo -- I have the same questions for the following task definition section on page 3.
Task definition, evaluation metrics: if I understood correctly, CCRA requires some extra annotation of critical concepts -- how did you collect such annotations to determine which NPs are critical?
(Very minor) based on the full name, CCRA should really be CCRR -- what does A stand for here?
Section 3.2, feature extraction: the Yoon Kim et al. (2019b) paper is not relevant to image features at all -- did you mean Shi et al., (2019)?
Table 1: what is the dagger after VGNSL-HI?
Section 4.3: did you mean ""augments"" by ""arguments""?
Some more thoughts regarding motivation limitations: humans arguably learns how to parse concrete sentences first, and can then generalize to abstract domains that are not visually groundable. In this work, it seems that the model only works when both the text and image are available, as there is a need to infuse visual features into text spans. Do you have any thoughts on enabling a trained CLIORA model to parse pure text without grounding signals?
Missing Reference
Kojima et al. [1] has strengthened the VG-NSL model by simplifying the architecture, and argued that such visually grounded models are potentially biased towards concrete noun phrases. However, the paper neither cited it nor discussed the relevant issues.
[1] https://aclanthology.org/2020.acl-main.234.pdf
There have been a lot of relevant work earlier than 2019 on visual-semantic embeddings or structured visual understanding with text. To name a few,
Older work on structured image-text representations
[2] https://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.pdf
[3] https://openaccess.thecvf.com/content_cvpr_2018/papers/You_End-to-End_Convolutional_Semantic_CVPR_2018_paper.pdf
Contrastive loss for visual-semantic embeddings
[4] https://arxiv.org/pdf/1411.2539.pdf
Minor Editing Comments
I was confused about what CCRA is when reading the abstract -- would be good to include the full name and give an intuitive description of the metric.
Yoon et al.
→
Kim et al.
Shi et al. (2019) proposes
→
Shi et al. (2019) propose
In my opinion, putting Section 3.4 before 3.3 would better streamline the paper.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling","Keywords: Grammar Induction, Vision-Language Matching, Unsupervised Learning","The topic of grammar induction has been there for a very long time in NLP and is a very fundamental topic. The model was largely built based on an existing model for purely text-based grammar induction. The model essentially makes use of neural networks to learn good latent representations (using a reconstruction loss) where the latent representation is defined with neural networks which yield scores for constituents and vector representations of them. The approach adopts the classical inside-outside process for the computing of the scores.
The paper essentially investigates what might be the effective methods for integrating image information into text for improved grammar induction. The execution of the paper was quite good, and the results are convincing. However, I feel the overall model is essentially a way to use image information to regularize the grammar induction process. Little can be said about in what precise manner the image is actively contributing to the induction process. Indeed, the authors also acknowledged something along with what I thought in the final section. Nevertheless, I think it is an interesting piece that might inspire future research on multimodal processing (for image + language).","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling","Keywords: Grammar Induction, Vision-Language Matching, Unsupervised Learning","Strengths: The idea of jointly inducing structure in natural language and grounding the constituents with real-world images is intuitively correct. The ablation study also shows that both feature-level fusion and score-level fusion (including the contrastive loss, if I understand correctly) helps in improving the parsing results.
Weakness:
The image features are only used for computing the inside pass. The image feature should contain information that can help predict the missing word, such that it could be used in the outside pass too. Selecting the best image region for predicting the missing word is also an intuitively correct way to build the vision-language alignment.
The compute of sim(I, c_ij) includes a max operator, this could lead to a biased gradient.
As the author mentioned in the discussion section, the model doesn't consider the latent hierarchical structure of the image. For example, the sentence describes the entire image, while each phrase describes part of the image.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"PiCO: Contrastive Label Disambiguation for Partial Label Learning","Keywords: Partial Label Learning, Contrastive Learning, Prototype-based Disambiguation","Strengths
Incredibly strong PLL performance close to supervised learning and substantially stronger than baselines.
Helpful ablations and results on various datasets.
Provides theoretical connection of the proposed method to EM.
Weaknesses
More should be said about the closeness of PLL performance and supervised learning performance. This comment may seem counterintuitive, but I do not think this closeness can be so easily glossed over, especially when it is one of the main results.
(minor) Treatment of clustering is simplistic. K-means is a very specific type of clustering algorithm, and only one of multiple classic clustering techniques.
Questions and comments:
How is the “novel” prototype-based label disambiguation so different from using a softmax layer? A softmax layer will also have a prototype vector for each class.
I can see the convenience of momentum style updating of prototype vectors, but re-computing them every iteration (or every N iterations) is not so expensive and at most increases training by 2x.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"PiCO: Contrastive Label Disambiguation for Partial Label Learning","Keywords: Partial Label Learning, Contrastive Learning, Prototype-based Disambiguation","Strengths
The problem that the paper tackles, namely partial label learning, is important in the real life where labelling is difficult due to semantic ambiguity (e.g. husky vs malamute). The problem has several connections especially with weakly supervised learning.
The approach proposed in the paper is very well motivated. The idea of making contrastive learning and prototype learning is sound as they can work collaboratively in the EM fashion (proved in the paper).
The approach is backed by very impressive empirical results. The presented analyses support the motivation of the approach, e.g. cluster visualisation shows that the clusters are well formed with few classification errors.
The paper is well written with a clear structure. Reproduction doesn't seem difficult.
Weaknesses
As the approach is EM-like, it also inherits some cons from EM. This is however not presented in the paper. For instance, in which case the learning will get stuck in a bad optimum? What is the consequence? Is there any way to avoid that?
It is unclear how the models were fine-tuned. Did the authors use clean dev sets which don't have any false positive labels? If this is the case, would it be more realistic to fine-tune the models on noisy dev sets instead? One concern is that clean dev sets can provide lots of information for removing labelling ambiguity. For instance, one can achieve reasonably good prototype representations just from a few clean labelled instances.
It seems that the paper mentions too little about prototype learning literature.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"PiCO: Contrastive Label Disambiguation for Partial Label Learning","Keywords: Partial Label Learning, Contrastive Learning, Prototype-based Disambiguation","I have the following comments. Pros: 1. The structure of this paper is well-written and easy to follow. The motivation is clear, and the solution is simple but effective. 2. I think the theoretical justification of the proposed method from the expectation-maximization perspective is very interesting. It is a generic result and potentially help the community understand the property of contrastive learning. 3. The experimental results are quite strong and the ablation study also looks good. I especially appreciate the results on new fine-grained datasets to test the performance of PLL methods. Cons: 1. Experiments are mainly conducted on datasets with uniformly generated candidates, it is not clear how PiCO performs on non-uniform datasets, such as in the label-dependent setting where the probability of label flipping depends on its ground-truth label. 2. Why does the second equality in Eq. (9) hold? It would be appreciated if a clear explanation is provided. 3. There is a redundant space notation in Table 1 (the method CC with q=0.5 on CIFAR10).","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting","Keywords: sparse attention, pyramidal graph, Transformer, time series forecasting, long-range dependence, multiresolution","Strengths
The paper is well-written and well-motivated with sufficient technical details
The extensive empirical results demonstrate the effectiveness and efficiency of the proposed method
A proof is provided to guarantee the linear complexity of long sequence encoding
Comments
More datasets for multi-step forecasting would be helpful in evaluating the method.
The reason of using the second prediction module in multi-step forecasting need more justification, e.g. why take the concatenation as key/value in the second layer.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting","Keywords: sparse attention, pyramidal graph, Transformer, time series forecasting, long-range dependence, multiresolution","Strength:
The paper is well written and the proposed architecture is easy to understand. I believe the equations are correct, and the included model illustrations makes the model design very intuitive.
Extensive evaluation is provided, and the proposed model consistently outperforms baseline models.
A ablation study is provided to justify the effectiveness of each component in the proposed architecture.
Weakness:
Although I couldn't find any specific publication, I believe hierarchical structure illustrated in Figure 1 (d) has been explored before in the context of LSTM/RNN. In addition, another line of work trying to apply the residual connections from ResNet to RNNs (e.g., https://arxiv.org/pdf/1701.03360.pdf) also provides a means for long-range time-series modelling.
The paper states that Pyraformer simultaneously capture temporal dependencies of different ranges in a compact multi resolution fashion. This is intuitively understandable. However, it would be more convincing if the evaluation could also illustrate this perspective. To be more specific, how do we know from the current evaluation setting that the model indeed learns multi resolution temporal dependencies?
Connected to comment 2, it would be better if the authors could illustrate a bit more on why these datasets can be used to evaluate long-term dependency modelling. In my understanding, long-term dependency modelling could be a bit different from long-term prediction. The former focuses on extract meaningful information from long-term previous steps, and the latter focuses on accurate prediction into the far future. If the real-world dataset does not include the long-term dependency in it, some study on a synthetic dataset would help readers understand the benefit of the proposed model. In other words, inaccuracy in long-term prediction could be resulted from (1) accumulated noise in predictions along time, and (2) lack of long-term temporal dependency modelling. How does the authors distinguish the result from these two perspectives?
The proposed method uses a factor of 2 to construct the hierarchy. This number seems a bit arbitrary. I'm wondering if this is a design choice with some thoughts behind it, or could this number be treated as a hyper parameter? Basically this number will decide the number of hierarchies in the structure.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting","Keywords: sparse attention, pyramidal graph, Transformer, time series forecasting, long-range dependence, multiresolution","Strengths
The architecture is well-motivated, tackles the important problem of LSTF, and improves both forecasting performance and computational efficiency of state-of-the-art baselines.
Paper is well written and easy to follow – making motivations and contributions clear.
Weaknesses
However, some key architectural details can be clarified further for full reproducibility and analysis. Specifically: 1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed. 2. Can each node attend to its own lower-level representation? From equation 2, it seems to be that only neighbouring nodes are attended to, based on the description of N_l^(s). 3. Do the authors have any guidelines on how to select S/A/C (and consequently N) for a given receptive field L?
In addition, while the ablation analysis tests the impact of changing CSCM architectures, it would be good to evaluate the base performance without the PAM to determine the value added by attention. This would also provide a simple comparison vs dilated CNNs which have been used successfully in time series forecasting applications (e.g. WaveNet).
Finally, could I double check which dataset was used for the ablation analysis as well? I seem to be having some difficulty lining the numbers in Tables 4-6 up with Table 3.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting","Keywords: sparse attention, pyramidal graph, Transformer, time series forecasting, long-range dependence, multiresolution","Pros
Overall I like the idea conceptually. As mentioned in the summary it makes a lot of sense for time-series data as it has an implicit bias of summarizing the past at different resolutions.
This architecture as claimed, can handle long sequences with computational complexity O(L) where
L
is the number of elements in the sequence. It can also maintain short attention path distance between any node. This distance is O(1) w.r.t L.
The experiments are good enough to demonstrate the advantage of this network over other attention mechanisms.
The implementation has been done efficiently using TVM and when released the code base would be of value. I hope the paper is reproducible even though no code has been provided.
The empirical speed and memory consumption graphs are useful.
Cons
I have certain clarifying questions. i would be happy to raise my scores if these are answered. Further I have some comments that would hopefully make the paper better.
It would be better to formally define Signal Traversing Path and forward reference it from the introduction.
While reading the initial description in the introduction, it is not fully clear what the nodes at each resolution represent. This is only evident to me after reading section 3.2 which states that convolutional layers with specific strides are applied repeatedly to get the coarser levels. It would be better if one states upfront that some external mechanism is used to extract the initial states of the coarser nodes. Related comment: I think the clarity of section 3,2 can be improved.
I might be wrong but I believe N is not defined in section 3.1 before it is used. Please check this (ignore if I missed this somehow).What is the relation between N and S ? I would suggest defining A, L, C, N, S clearly in one place with some indentation in Section 3.1, for ease of reading.
Please be slightly more clear in the second paragraph of section 3. 3. I would like to verify my understanding of the multi-step forecasting modules. In the first module, we just have a fixed output layer which is a dense mapping to F outputs, where F is the future prediction length. In the second mapping, there is a decoder that sequentially generated the multi-step output and the decoder is equivalent to the decoder in the original transformer paper. Is my understanding correct?
In the experiments the information of the datasets and the experimental setup is not 100% clear to me even after reading the appendix. Please clarify or point me to the relevant places in the paper. In multi-step forecasting what is the length of the future prediction window. Also, is the task rolling prediction over the test set? These should be clearly defined.
is there a validation set? Typically I would prefer if A and C are tuned on a validation set per dataset and the best results are reported. the ordering would stay the same I guess, because fixed set of A, C are used in all experiments.
Is there a relation between A, C vs the granularity of the dataset. For example if the data is hourly, then one could build a resolution graph such that groups of 24 are constructed in the first level (24 hours in a day), then groups of 7 (7 days in a week), then 4 weeks in a month. Such a non-uniform pyramidal structure might be very useful. Can the authors comment on this or better yet try this? -- not absolutely essential but it would be interesting.
Generally, it would be better if we could rank pyramidal attention vs state of the art models on the well known benchmarks used in the DCRNN paper and newer papers like https://arxiv.org/abs/2103.07719. For instance one could just do the evaluation of pyramidal attention in the same tasks as the linked paper and report the numbers. This would reveal the rank of pyramidal attention in the SOTA table and would be a useful signal for the community. Again this is not required but a nice to have.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Expressiveness and Approximation Properties of Graph Neural Networks","Keywords: Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality","Strengths: the paper is very clearly written and makes a clear new connection between programming language and deep learning. Formalizing GNN as a TL is a new toolbox that allows the authors to get an unified theoretical analysis of the separating power of GNNs. In particular, they are able to get new results characterizing the expressive power of GNNs as a function of the number of layers. This alone is a very nice contribution.
Weakness: I think Section 6 on function approximation is less relevant. The main reason is that the authors consider the discrete topology on the set of graphs so that any function is continuous. But GNN are more restricted functions and are continuous as mapping acting on tensors. As a consequence, I think: 1- the results presented in section 6 are correct but could probably be proved with simpler arguments similar to the ones used in Appendix A of 'On the equivalence between graph isomorphism testing and function approximation with GNNs' by Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna. 2- the consequences of Theorem 6.1 and Corollary 6.2 are not clear for GNNs as GNNs cannot model any function taking a graph as argument. Indeed the importance of continuity is highlighted in section 3 of 'On the Limitations of Representing Functions on Sets' by Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, Michael Osborne. I think the authors should address this issue in Section 6.2 by explaining the possible limitation of their approach. Note that Azizian and Lelarge probably consider continuous functions of tensor representation of the graph for this reason.","10: strong accept, should be highlighted at the conference","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Expressiveness and Approximation Properties of Graph Neural Networks","Keywords: Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality","Post-rebuttal Comments (11/29/2021)
I appreciate the authors for their response. I am satisfied with their answers. So, I want to keep my score and vote for acceptance.
Initial Comments
【Strength】
[1] This paper gives a general procedure for deriving the upper bound of expressive power for any GNN as long as we can translate it into the tensor language.
[2] Theorem 6.1 and Corollary 6.2 allow us to give lower bounds on the expressive power of GNNs as well.
【Weakness】
[1] Compared to the upper bound, it is more difficult to show the lower bound of expressive power because the former needs individual proof for concrete GNNs.
[2] Those familiar with GNNs but not with the first-order logic may be hard to follow up the discussion of the paper.
【Correctness】
[1] As far as I checked, I could not find any inappropriate points in the proofs.
[2] What I am wondering is that in Theorem 4.1, the equivalence relation
ρ1(TLk+1(Ω))
is independent of the choice of
Ω
. Considering the case of
Ω=∅
, does this result imply that the separation power of the tensor language does not increase even if we add expressive functions such as non-linear activation functions or MLPs?
【Technical Novelty And Significance】
[1] As mentioned in this paper, several studies such as [Barcelo et al., 2020 ] have studied the expressive power of GNNs via first-order logic. However, while [Barcelo et al., 2020] related GNNs to the existing first-order logic (namely,
FOC2
), this paper defined a new grammar, the tensor language
TL
, and related GNNs to the first-order logic via this language. In this sense, the approach of this paper is novel.
[2] This approach allows us to analyze expressive power independently of particular GNNs. In addition, it is relatively easy to derive the expressive power of a GNN because it is sufficient to translate the GNN into the tensor language. Therefore, I think this approach is significant.
[3] There is no easy and general way to obtain the guarantees for the lower bound of the expressive power compared with the upper bound. We have to check the sufficient condition of Corollary 6.2 for each GNN, which needs proof tailored to the GNN. If we can find a general approach to obtain the guarantees, it would increase the paper's significance. (However, it is also notable that this paper obtained lower bounds systematically to some extent, as mentioned in Proposition E.3.)
[4] The paper gives positive answers to the unresolved issues raised by existing studies. In this sense, this paper is significant.
【Empirical Novelty And Significance】
[1] This paper does not have numerical experiments.
【Detailed Comments】
[1] P.3: I think it is better to write the definition of irreflexivity as it is not well-known.
[2] P.3: Initially, for a graph
G
and
v∈VGk
, ... → I am afraid this sentence is hard to understand, especially the part ""where, atp_k(G, v) is the atomic type..."". Could you reconsider the sentence?
[3] P.4: I think
S
is undefined.
[4] P.6: The definition of
Ck+1
does not appear in the main text (only available in Appendix).
[5] P.13: ⟦⟧⟦⟧
πσ⋆G⟦φ1,σ⋆ν⟧⋅π⋆G⟦φ2,σ⋆ν⟧
→
⋆G
should be
σ⋆G
.
[6] P.15: e..g., → e.g.,
[7] P.16: Here the unravelling is the (infinite tree ... → remove the parenthesis
[8] P.17: ⟦⟧
πH⟦φ1,μ[xi→v]⟧
→ ⟦⟧
πG⟦φ1,μ[xi→v]⟧","N.A.","8: accept, good paper"
"Expressiveness and Approximation Properties of Graph Neural Networks","Keywords: Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality","The unified framework provided by TL is interesting, and allows a more uniform study of GNN models, and a simpler means to derive expressiveness bounds for new GNN models. The study of TL is also very well-grounded in the literature, as it includes most of the key works on GNN expressive power, and additionally confirms their findings. Furthermore, TL establishes a new set of results, thereby addressing some open questions in the field. In particular, it produces interesting insights about the effect of more layers, connecting this explicitly to treewidth and thus adding a nice nuance to this discussion. The results also appear intuitive and sound, though I did not check these thoroughly. Hence, the paper seems to be a valuable addition to the literature on GNNs.
Nonetheless, I find that the novelty of the approach is limited, particular relative to other frameworks characterizing GNNs. In particular, I cannot see the novelty proposed by TL relative to the matrix query languages mentioned in the paper. Therefore, I strongly suggest a more through comparison with related work. Furthermore, it is not clear how much simpler writing a TL expression for GNNs is, particularly with respect to non-standard GNNs involving, e.g, sub-structure counting. Hence, a more detailed presentation of specific model TL expressions, including some from the supplementary material, should be added into the paper to more clearly present the TL translation process. On a more minor note, the current writing style and notation are hard to follow. I refer specifically to Page 3, and the explanation of cr, gwl,vwl, etc. This section required multiple reads to be properly understood, and is quite dense. This is also true of other parts in the paper, e.g., Page 7. Therefore, a more simplified presentation, supported by examples, would be beneficial. Finally, the authors can also consider studying their framework with respect to classes of functions (or universality), as in Barcelo et al. [1], rather than just function approximation relative to separating power, to provide a more holistic overview of the GNN landscape. Alternatively, illustrations of functions with separation power limits can also benefit the presentation in this part of the paper.
[1] Barcelo et al. The logical expressiveness of graph neural networks. ICLR 2020.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Expressiveness and Approximation Properties of Graph Neural Networks","Keywords: Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality","Strong Points
Evaluation of the expressive power of GNN is indeed not straightforward and needs a lot of architecture specific proofs. Indeed, we need model agnostic way to evaluate expressive power of any GNN. Thus the main motivation of the paper is appropriate and might have a significant effect on the literature.
Paper was written well, quite clear and understandable. It includes enough theoretical proofs and examples on how to find the expressive power of some well-known GNNs.
Clarifying the used WL test order
Clarifying the used WL test order would be great. In GNN papers, generally we call CR as 1-WL, then original 1-WL as 2-WL and so on and so forth. It seems that this paper follows (Cai et al 1992) notations. It means the GNN in (Maron et al.,2019b) , known to have 3-WL power in the original paper, has 2-WL power according to this paper. It would be better if this point is clearly mentioned.
Weak Points
The way of evaluating expressive power by tensor language is not new at all. According to my knowledge, this kind of evaluation was recently proposed in [1]. They introduced Geerts' matrix query language (MATLANG) for GNN world and showed how to use it in order to evaluate some well known GNN's expressive power by write down given GNN's forward calculations in matrix form. Then, they figured the expressive power out by determining which language can explain GNN's forward calculations. Thus, In Separation Power sections ""We do not have any general technique allowing us to expand these results for arbitrary GNNs"" and in Related Work section ""general matrix query languages are known,albeit not in the context of GNNs"" are basically not true. Also [1] falsified the claim that ""In summary, our paper draws new and interesting connections between tensor languages, GNN architectures and classic graph algorithms"". I strongly think the connection between the tensor language and WL-test order of GNN models is interesting and valuable, but not novel. Definitely the differences between similar recent works should be discussed.
Showing how many iterations of the WL-test equivalent of the given GNN is trivial. It is well-known that an additional layer in GNN is equivalent to WL-test's additional iteration (coloring rounds). So in expressive power analysis, we can assume that we always have enough layers as we assume the WL test continues till the stabilization of colors. Introducing that depth parameter as a contribution seems a little exaggerated. In my mind, just WL test order equivalence is enough.
Even though the main claim is that by this paper, anyone can determine the GNN's expressive power easily, is not that straight forward. One needs to write down the GNN layer in tensor form. It seems possible while using summation and/or weighted summation for neighborhood aggregation. How about other aggregations? Can PNA ( GNN that uses different aggregation such as max, min, std) be evaluated by this framework? I have seen that in Appendix, there is an example for GraphSage's version which uses sum aggregation. But the main version of GraphSage uses max. There is no analysis for max aggregator of GraphSage. Is it possible to do it? Also some GNN such as Chebnet can be more powerful than 1-WL in some certain cases. But in general it is not less powerful than 1-WL. Can the proposed method determine this special case? Can Chebnet expressive power analysis be at least on the appendix?
By Geerts' matrix query language MATLANG, we can determine the expressive power up to 3-WL (or 2-WL in Cai et al 1992 notation). This paper extends this matrix language in order to determine the expressive power of GNN which goes beyond the 3-WL. However, due to the memory and cpu complexity it is not that practical and I have never seen any GNN in practice whose expressive power is more than 3-WL. So the main contribution of the paper beside what MATLANG provides, seems not necessary in practice at least for now. I am suspicious if we really need more powerful than 3-WL GNN. Thus maybe these theoretical analyses will never be used.
One of the main practical advantages of this theoretical work would be to give us some insight on how to increase the expressive power of GNN. Even though it is mentioned in the abstract, I am not sure these insights are clear enough. Can we create a new GNN architecture to use these insights? I see that experimental work such that proposing a new GNN is not the main idea of the authors. But at least some clear examples on how to increase expressive power would be very valuable.
Reference
[1] Muhammet Balcilar, Pierre Heroux, Benoit Gauzere, Pascal Vasseur, Sebastien Adam, and Paul Honeine. Breaking the limits of message passing graph neural networks. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space","Abstract: Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.","Strengths:
The paper is well-written.
The proposed encoding-decoding framework and the corresponding two-stage learning process are reasonable.
the newly designed representation (i.e, high-dimensional features + 2D keypoints + coefficients) is shown to facilitate unsupervised learning from pixel space.
The paper provides good empirical results and extensive ablation studies for the effectiveness of each model component.
Weaknesses:
The authors may give more empirical analyses about the impact of using different types of interventions.
The proposed method is only compared with two existing approaches, including PhyDNet and a modified version of V-CDN. It would be better if the authors could include more existing approaches into the experimental comparison, for example, the keypoint-based model [Minderer et al., 2019], the VAE-based stochastic model [Ref1], and the ConvLSTM-based deterministic model [Ref2].
All experiments are conducted in a simulated environment based on CoPhy, which is somewhat insufficient compared with previous video prediction literature (though I understand the paper studies a new problem). Since the proposed method does not require the supervision of confounders, I wonder if it can be generalized to real-world datasets such as Human3.6M, KTH, or BAIR robot pushing.
[Ref1] Stochastic Video Generation with a Learned Prior. ICML 2018.
[Ref2] PredRNN: Recurrent Neural Networks for Predictive Learning Using Spatiotemporal LSTMs. NIPS 2017.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space","-''-","Strengths:
Filtered-CoPhy seems to be the next intuitive step from CoPhy from a modeling perspective. It is incremental work and a crucial one to get counterfactual predictions (for physics-based simulations) working in an unsupervised fashion.
The new benchmark proposal considering Identifiability and Counterfactuality constraints is systematic and alleviates the issues in the CoPhy benchmark.
I acknowledge the code released during the review period by the author(s). I have had a chance to briefly look over it and hope that the authors document the code (with a README and instructions on how to run the code) if the paper is accepted.
Weakness:
Keypoint based encoder/decoder seems to be a good choice. Have the author(s) tried/given a thought how other object-centric representations such as slot-attention would affect the counterfactual performance?
It would make the paper stronger if the CoPhy like baseline is reported i.e one using ground truth object positions. This would give at least an upper bound for the proposed benchmark.
It would be beneficial to add a quantitative metric based on the predicted and ground truth location of the center of mass (for both Table 1 and Table 3).
Also, since the keypoint points can be tracked, it would add more credibility if tracking metrics such as MOT are added as well. Here the author(s) can assume N=number of objects in the scene (otherwise this metric cannot be computed).
During the training stage, can the sequences AB be sent to the CoDy (i.e dynamics model) as well? Since
uk
should be the same for AB and CD, this acts as a consistency check.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space","-''-","Strengths
Introduces and justifies an important new benchmark task for video predictions involving inanimate objects
Introduces a sophisticated approach that is well-described and justified
Strong empirical evaluation that includes ablation studies and exploration of the learned representations
Thorough comparison with a previous approach that sheds light on reasons for the improvement
The counterfactual style of evaluation may be used in other domains of AI
Weaknesses
The paper would be even better if the diversity of the benchmark dataset could be improved.
Perhaps more could be said about the visible failures of the approach. The network does not seem to succeed at learning the structure of rigid 3d bodies, as we see from the videos where the cubes visibly distort and lose their edges over successive frames.
Other comments
Many incomplete sentences in the supplement. I would be happy to list them if this would be beneficial to the authors; I assume this was due to time limitations.","10: strong accept, should be highlighted at the conference","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"BEiT: BERT Pre-Training of Image Transformers","Keywords: self-supervised learning, pre-training, vision Transformer","(+) The paper is clear and easy to follow.
(+) There are proper ablations for most of the design decisions.
(-) The dependence on DALL-E to is a pretty hefty one (unsupervised pretraining on 250M images). This can potentially be lifted, but the authors made no attempt in this direction.
(+/-) To be fair with the above point, the authors did at least the ablation of predicting masked pixels directly (instead of visual tokens from DALL-E) in Table 4, and the performance drops by ""only"" ~1.7% top1 on ImageNet in the downstream task. This is still not great as this brings the accuracy below supervised training only, which is what is done as fine-tuning on this network in this case. This ablation should have been done in linear probing also.
(-) Regarding linear probes, the results (in Table 9) are somewhat disappointing. I wonder if this comes from the relatively small batch size (and/or lack of hyperparameters search) compared to other papers, or if there is a more fundamental reason for BEiT to be worse than contrastive methods there.
(=) No pretraining on larger than ImageNet-1K.
(+) The paper provides enough details for reproducing the results.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"BEiT: BERT Pre-Training of Image Transformers","Keywords: self-supervised learning, pre-training, vision Transformer","PROS:
The task is interesting per se, as it brings the concept of BERT into Vision Transformers
Well written paper and simple idea
Good analysis
CONS:
Some important details for practitioners are missing (e.g. choices on the masking)
experiments of different models are sometimes difficult to compare
In general, I liked the paper a lot. However, I have some issues that should be answered and addressed before publication. I'll number them for convenience
Authors says that pre-training is run for 800 epochs, which is fine for large ML groups but it might be very demanding for smaller groups. Moreover, it does not help the comparison with all other SoTA methods. For example, DiNO is trained for 300 epochs. Moco v3 is an experimental paper, but the main results are obtained for 300 epochs (although they report 600 epochs as well). Now, I wonder two things: 1) are pre-training results in Table 1 obtained with 800 epochs for all the models? I mean, are they comparable? 2) I'd have liked to see a comparison between models at 300 epochs (as standard) in Table 1 or in another additional table. Appendix A might be in that direction but I did not understand it well as it is not well explained; 3) practitioners would really like to see the training curve of BEiT, to understand what budget should they invest to have the desired accuracy.
Since BEiT is somewhat based on BERT, why do you replace 40% of tokens? Is there a motivation behind this number? then, BERT task was to replace the token 80% of the time with a [MASK] token, 10% of the time with a random token, and 10% of the time keeping it as it was. Did you consider this strategy? As far as I know, this is beneficial for fine-tuning tasks.
It would be good to see the standard (small) datasets for downstream tasks such as CIFAR-10, Oxford Flowers-102 and Oxford Pets or cars alongside CIFAR-100. This would have made the paper more comparable. Moreover, why 150 epochs? I believe that the standard is 100.
Why is DINO in Table 10 with 400 epochs? why not 300 as standard?
It's good to have the appendix, but authors should refer to them and comment on the (interesting!) results in the main paper. E.g. Appendix E, F.
Are authors going to release the code?
MINOR:
I would change the special token [S] to [CLS] as in BERT
I would cite this recent paper in the intro talking about how Vision Transformers are data-hungry https://arxiv.org/abs/2106.03746 and some tricks for small datasets
In Section 2.3 ""pixel-level auto-encoding"" might be confusing. I suggest rephrasing it and explaining it better (as it is in the intro).
If you have time, for the camera ready, I'd love to see BeIT applied to some convolution transformers such as Swin or CvT
I would like to see a comment on the throughput of DINO vs your paper in the main manuscript","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"BEiT: BERT Pre-Training of Image Transformers","Keywords: self-supervised learning, pre-training, vision Transformer","I think the MIM objective resembles the masked region modeling (MRM) objective, which is widely used in the vision-and-language pretraining (VLP) models.
VLP models often mask several visual regions and make their contextualized outputs predict their corresponding object class to guide visual inputs in tandem with the textual inputs' masked language modeling.
By removing textual inputs from VLP models and switching detection-based region features to patches, we have a BEiT-like structure.
Recently, ViLT[^1] proposed this type of VLP model but failed to make appropriate objectives for the patches, reporting that regressing masked pixels (masked patch prediction objective) deteriorate downstream vision-and-language tasks.
In my view, as MRM and MIM are similar enough to be noted, I recommend the authors add the relation with BEiT and VLP models in the related work section.
Using argmax-ed visual tokens is inevitable for DALL.E since they had to plug the discrete tokens into the decoder.
However, since BEiT only uses the discrete tokens as ingredients of the MIM objective, I think argmax-ing the visual tokens is unnecessary, and the token distribution can be immediately used for the objective (e.g., KL-divergence).
Actually, UNITER[^2], which used detection-based regional inferred class for the MRM objective to train VLP model, has tested three types of using the class information:
Use it as a one-hot label (as BEiT did).
Use KL-div.
Regress the features that are used to infer the class labels.
UNITER showed that the combination of (2) + (3) yields better performance than solely using (1).
I believe all three approaches are also available for BEiT and the discrete VAE; thus, I wonder whether they can further boost the performance of BEiT.
I believe using visual tokenizers such as discrete VAE can be a silver lining for the community and those seeking self-supervisable images' objectives, including the VLP community.
I think this paper showed rigorous and solid empirical results and well contributes to the community by providing valuable tools.
[^1] Kim, Wonjae et al. ""ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision."" ICML (2021).
[^2] Chen, Yen-Chun et al. ""UNITER: UNiversal Image-TExt Representation Learning."" ECCV (2020).","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"BEiT: BERT Pre-Training of Image Transformers","Keywords: self-supervised learning, pre-training, vision Transformer","This paper presents a self-supervised learning framework named BEIT, in which the input image can be masked in some regions, and the task is to recover the token of the masked region. Pre-trained on ImageNet, the model shows good performance on a series of downstream tasks.
I shall say that I very much like the idea of this paper. Self-supervised learning in computer vision seems far behind that in natural language processing, mostly because the proxy task is not good enough. This is highly related to the original data format -- masked language modeling is a perfect proxy for texts, but it seems very difficult to replicate it on images -- because image signals are mostly continuous in the space, and semantic signals are sparse, etc. This paper makes a good trial along this direction. I think the mask image modeling task is much more elegant than the existing contrastive learning (e.g. SimCLR & MoCo) or predictive learning (e.g. BYOL) counterparts, because those methods are mostly relying on data augmentation to provide the prior-to-be-learned, but data augmentation not strong enough. More importantly, data augmentation can bring conflicts, e.g. one needs to enlarge the intensity of data augmentation to improve the difficulty of learning, but strong data augmentation is risky and may generate duals with very different semantics (so that they are bad for SSL).
OK, I have expressed my opinions that this new direction is promising. Also, the experiments on ImageNet and ADE20K are good (though I expect to see more including ImageNet linear-tests, MS-COCO tests, etc.), showing the strong ability of the pre-trained networks. However, I have one major concern that avoids me from giving a higher score to this paper. That is, I am not sure if the proposed framework is fairly compared against the prior methods.
The key lies in the tokenizer, which delivers almost all priors in the BEIT algorithm. However, the tokenizers are borrowed from DALL-E, which means that a large number of image-text pairs have been used -- the authors did not specify which model has been used, but at least, the CC3M (if not JFT300M) dataset is included in the pre-training part. This is not considered self-supervised learning, as the texts can contain vast amount of semantics (according to the CLIP paper, after training on image-text pairs, the model can achieve good performance on zero-shot ImageNet classification). I do not believe that using this tokenizer as the ""teacher"" to ""distill"" the target ""student"" model is ""self-supervised learning"". In Appendix C, the authors claimed that a re-implemented tokenizer on ImageNet shows similar performance, but technical details are missing. Even in the unsupervised setting, if the tokenizer is trained for sufficiently long, it can offer powerful guidance to the target network, so that some statements (e.g. Section 2.5, The pre-training runs for about 500k steps (i.e., 800 epochs)) are not strictly meaningful.
I hope the authors can answer the following questions.
The detailed setting of the DALL-E tokenizer and ImageNet re-trained tokenizer. In particular, please specify what kind of external information has been used, and the cost of pre-training such tokenizers.
If indeed external information is used (e.g. ImageNet labels to the worst case), is it possible to re-implement a counterpart without any such information and re-test the performance? Also, I am very interested in the performance of a random tokenizer (i.e. without pre-training, just clustering the responses of the randomly initialized networks).
By any means, I think a discussion on the relationship between this approach and knowledge distillation is necessary. I shall re-evaluate this paper after seeing answers to the above questions.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution","Keywords: fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization","The strength of this paper is the extensive and detailed toy and benchmark experiments. The reasoning and intuition of why fine-tuning underperforms on OOD data are well explained and discussed throughout the paper. The suggested solution by combining linear probing and fine-tuning also has good performance.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution","Keywords: fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization","Pros:
the observation that fine-tuning performance worse on the OOD test is new and interesting
the proposed method LP-FT is simple yet effective
theoretically analysis is further provided to show why LP-FT works
Cons:
some important baselines are missing like [a-c], which should be discussed and compared
[a]. Guo, Yunhui, et al. ""Spottune: transfer learning through adaptive fine-tuning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
[b]. Zhang, Jeffrey O., et al. ""Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks."" Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16. Springer International Publishing, 2020.
[c]. Ge, Weifeng, and Yizhou Yu. ""Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
why different pre-trained models are utilized for different ID and OOD pairs? In fact, the authors could use different models for each ID and OOD pair.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution","Keywords: fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization","Strengths:
This paper explores two increasingly impactful research directions, fine-tuning pre-trained models and generalization under distribution shifts. I believe their results, both theoretical and empirical, would be of interest to many in the community
The theoretical results, despite studying a very simple setting that is unlikely to be used in any real experiments, offer intuitions that transferred well to practical results.
The paper is clear and well written.
Weaknesses:
Experiments could be more comprehensive. For instance, while this paper analyses several distribution shifts (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR->STL, CIFAR 10.1 and FMoW), it was surprising that distribution shifts like ImageNetV2, ImageNet-R, ImageNet-A, ObjectNet and ImageNet Sketch were not considered by this work. Moreover, only a single architecture, ResNet-50, is used for the experiments. As they stand, it is unclear whether the results from this paper would hold at larger scales.
This work overlooks simple baselines. For instance, given the finding that end-to-end fine-tuning significantly changes the weights of the backbone (which arguably hurts OOD performance), it would be natural to consider having a smaller learning rate for the backbone, and a higher one for the untrained final linear layer. It is not uncommon to do so in practice. Moreover, authors could have explored regularizing the difference of the weights of the backbone to the original weights, encouraging them to not be changed too much. Finally, the authors do not mention the fact that models like CLIP allow the possibility of starting with a good set of initial weights for the final layer, as they can be used in a zero-shot setting. If the intuitions presented in this paper hold, this prevent the ""distortion"" that happens when the last layer is far from the optimum.
The theoretical results are disconnected from realistic settings. Some clear examples are the assumption of a two-layer network, a squared error loss (while the de facto standard for classification is cross-entropy), and considering the worst case loss over distributions of bounded norm.
Apart from the theoretical results, there is not a lot of novelty introduced by this work. The two-stage fine-tuning strategy where end-to-end fine-tuning follows linear probing is commonplace and thought in introductory courses in deep learning, e.g. in the first lesson of https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb
Comparison to previous work is lacking. In particular, explicit comparisons of LP-FT with other recent robustness-oriented fine-tuning methods would greatly strengthen this work [1-4, among others].
References:
[1] Wortsman, Mitchell, et al. ""Robust fine-tuning of zero-shot models."" arXiv preprint arXiv:2109.01903 (2021).
[2] Aghajanyan, Armen, et al. ""Better fine-tuning by reducing representational collapse."" arXiv preprint arXiv:2008.03156 (2020).
[3] Jiang, Haoming, et al. ""Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization."" arXiv preprint arXiv:1911.03437 (2019).
[4] Zhu, Chen, et al. ""Freelb: Enhanced adversarial training for natural language understanding."" arXiv preprint arXiv:1909.11764 (2019).","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution","Keywords: fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization","This paper has many strengths:
It is not well known that FT often outperforms LP OOD -- the empirical and theoretical study of this phenomenon may be very important to the community.
LP-FT outperforms LP and FT on many distribution shifts.
In addition, the paper has some weaknesses with associated actionable items:
The main weakness of this paper was contextualizing LP-FT. In the abstract and introduction, it seems as though LP-FT is a method that is being introduced by this paper. For instance, the abstract states ""our analysis suggests the simple two step-strategy of linear probing then full fine-tuning"". It is not until the final page of the main paper that the related work section states ""LP-FT has sometimes been used as a fine-tuning heuristic"". Moreover, this is presented without any citations. I would very much appreciate clarity on this issue, is LP-FT something that has been used in the past? If so, by which papers and why? Does this reference that in fine-tuning, sometimes the hyperparameters in the final layer are decoupled from those used for the encoder [1] and performance improves? As shown by [2, 3], performance improvement ID lead to performance improvement OOD, and so this could be an alternative explanation for the OOD boost of LP-FT that is orthogonal to the authors theory. For instance, in the effective robustness framework of [2], do the solutions found by LP-FT exhibit more effective robustness than the FT solutions?
Section 4 is hard to follow in its current form as it is not clear exactly from where numbers are derived. For instance in the first line of the ""Results."" paragraph of 4.2, where is the 76.5 number from?
One of the networks studied by this paper is CLIP, for which a zero-shot final layer can be constructed (e.g., the final linear layer does not need to be constructed from scratch before fine-tuning). One possible addition to the paper could be discussion of this setting, and in particular if LP-FT is still required.
There are a few claims in the paper which could benefit from additional support. In particular, in the conclusion it is stated that the ""gap between FT and LP grows as the quality of pretrained features improve"". Are the authors referencing the single MOCO-v1 vs. MOCO-v2 experiment or is there additional support for this claim? Are the authors referring to absolute or relative difference between FT and LP? I am wondering as this claim seems a bit counterintuitive, if the features are already good shouldn't LP be sufficient?
The empirical verification of the theory (Section 4.3) is very interesting and could be more thorough. In particular, are there associated error bars for the 0.019 and 0.017 numbers as this seems quite close to conclude that one is larger. This euclidean distance experiment is very interesting, why is it only conducted for one distribution shift? Moreover, what happens when analyzing cosine distance instead of cosine distance?
[1] https://arxiv.org/pdf/2106.04560.pdf [2] https://arxiv.org/abs/2007.00644 [3] https://arxiv.org/abs/2107.04649","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"StyleAlign: Analysis and Applications of Aligned StyleGAN Models","Keywords: StyleGAN, transfer learning, fine tuning, model alignment, image-to-image translation, image morphing","##########################################################################
Pros:
The paper performs the first detailed exploration of model alignment based on StyleGANs. Considering the wide-range applications of StyleGAN, I think the analysis is timely, insightful, and interesting!
To analyze the model alignment during transfer learning from the source domain to similar/distant target domains, the paper uses weight resetting and quantitative channel alignment measurement. These techniques probably inspire others to analyze their GAN models.
The paper demonstrates impressive and promising experimental results on image translation and image morphing between different domains. Besides, it demonstrates the benefits of the shared latent space by zero-shot recognition/regression.
##########################################################################
Cons:
I’m confused about the example of “Age” in Figure 2. Why does the “Age” change the identity, background, and color largely? It looks like another totally different face.
Many details of the results are missing. Take Figure 2 as an example, does the semantic control apply in overlap channels (e.g., 15 for eyebrow in FFHQ2MetFace) or all the distinct channels of the child model (e.g., eyebrow 35)? Are they in the same layer? How do the authors transfer the pose control from face to church and bedrooms in Figure 14? Please check all the results and provide the necessary details.
Does the claim of “the hidden latent semantics” hold for the case of (parent FFHQ, child Church, grandchild FFHQ)?
I think most of the successful results can be attributed to the well-structured properties of the human face, animal face, and mega face. It would be helpful to provide further discussion on the “real distant domain”, e.g., face VS church. For example, is it helpful to transfer learning from face to church? Are there any other shared semantics between face and church except pose?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"StyleAlign: Analysis and Applications of Aligned StyleGAN Models","Keywords: StyleGAN, transfer learning, fine tuning, model alignment, image-to-image translation, image morphing","Strengths:
There are many works using aligned StyleGAN models but without deep analyses on the properties. This paper provides an in-depth study of the properties, which provides some insights on the aligned models and might inspire following more complex researches.
The organization of the paper is good and the analyses are comprehensive. Several observations are studied with quantitative/visual analyses to prove key properties, followed by corresponding applications utilizing the properties, making the paper easy to follow and concrete.
Weaknesses:
While the part of the property study is comprehensive and interesting and provides insights, the part of the application is less insightful. The application involves the image translation, image morphing and zero-shot vision tasks.
Image translation and image morphing are intuitive and well-studied in previous studies, as also pointed out in the paper, which might provide limited insights. Image morphing by interpolating aligned models has been studies in [R1].
The zero-shot vision tasks involve utilizing the label of the parent domain to edit/classify the child domain, which gives some novel ideas but alone are not very thorough to me.
I think it will be valuable to investigate the applications in terms of the specific properties. For example, only the conv parts of network change greatly for close domains, which supports translations. Can we just finetune the conv part and fix all other parts for better translations? (which is also discussed in [R3]) The paper discusses that the latent semantics are hidden rather than forgotten during finetune. It will be valuable to also discuss the potential application of this property.
Some small issues:
In Fig. 14, the authors demonstrate the semantic alignment between the human face and the church. However, only the church images are shown. The corresponding face images using the same latent z are suggested to be added to help the readers better find their correspondences.
In Sec. 4.2, the paper claims that the layer swapping performs the transition as a series of discrete steps, rather than continuously. In the implementation of Pinkney [R2], the layers can be smoothly swapped through the parameter blend_width instead of hard swap, which might be able to perform the transition continuously.
[R1] 2019 CVPR Deep Network Interpolation for Continuous Imagery Effect Transition
[R2] https://github.com/justinpinkney/toonify/blob/master/StyleGAN-blending-example.ipynb
[R3] 2021 TMM Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"StyleAlign: Analysis and Applications of Aligned StyleGAN Models","Keywords: StyleGAN, transfer learning, fine tuning, model alignment, image-to-image translation, image morphing","Pros:
The shared latent space for ""child"" and ""parent"" networks is interesting, and matches the learning representation goal.
The overall idea and the proposed fine-tuning method towards analyzing the shared latent space is reasonable. The StyleGAN-based architecture is a powerful structure, which consists of a mapping network, an affine translation, and a generator. This is exactly suitable for exploring feature attributes at different levels.
The paper provides many interesting visual results, such as in Figures 1, 2, 3, and 4, to show the effectiveness of shared information in different networks. These interesting conclusions may contribute to the generation community to design better generator works.
The wide applications are being met by subtly using the shared semantic information between different domains.
Cons:
While the impressive results are achieved, I believe some parts need more clarification (even after considering the supplementary material):
The key limitation of this work is the novelty. If I understand correctly, the authors just fine-tuning the different parts of the StyleGAN. This is very similar to the existing works AgileGAN and the shared attributes have also been mentioned in prior MUNIT. It takes me a hard time to buy the novelty of models, techniques, and theoretical insights.
The authors claimed the ""two W spaces are point-wise aligned"" on page 4. While they showed some shared attributes in Figure 2 and Table 1, and they also demonstrated some attributes are hidden, I do not believe these attributes are ""point-wise"" aligned. Is this conclusion correct? How to demonstrate it?
A quantitative and qualitative comparison with the latest CUT and F-LSeSim is given in Figure 5. While the improved results are provided, the original CUT and F-LSeSim work for aligned shapes. Why not provide some aligned examples, for example, horse2zebra, night2day, apple2orange, every two domains share a similar shape, but different appearances? This would be more robust to demonstrate the effectiveness of aligned style on both aligned examples (Figure 5) and unaligned examples (Figure 6).
In Figure 6, the StarGAN2 seems to be able to provide more consistent results to the source image, such as the smiling mouth in the first row, and the same head poses for the third row. Except for the FID score, it seems the StratGAN2 provides better translated results (better shared content, such as pose and expression).
The authors show abundant results in Figures 23, 24, 25, and 26. Similar results are also being met in the StyleGAN-based method. I do not fully understand the key challenge in such a situation.","6: marginally above the acceptance threshold","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"StyleAlign: Analysis and Applications of Aligned StyleGAN Models","Keywords: StyleGAN, transfer learning, fine tuning, model alignment, image-to-image translation, image morphing","Weaknesses
The main weakness for this paper is likely the limited novelty in the technical contributions, given that transfer learning is a fundamental, established technique. However, this paper is more of a study of this technique, applied to the particular domain of image GANs, and how their editability changes during the process.
The qualitative results do seem high quality, but it is sometimes quite difficult to see where the resemblance to the reference images are (especially with Figure 6, for the dogs dataset). Perhaps the translation is not focused strongly enough on the reference images. Despite more artefacts being present, I'd say OverLORD images are a better transfer, for both cats and dogs.
Strengths
The primary contribution of the paper is the extensive experimentation, around transfer learning in GANs, and they perform this very well. Their approach is very well documented, is very thorough, and convincing. The supplementary videos are helpful in further displaying their results.
Although there are aspects that are both better and worse, the samples from their models compared to other models, are mostly good. The benefits arise mostly from the editability of the output, given their findings that the channels maintain their editing effects from previous datasets with more labels.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion","Keywords: Black-box variational inference, missing values, evidence upper bound","(+) pros / (-) cons
(+) Predictive modelling over inputs with missing features is an important problem arising often in many application domains. This paper contributes this somewhat underexplored field.
(-) The rather low documented performance benefits over simpler baselines do not justify the use of the complex model (combining 5? networks) proposed here as opposed to simpler VAE or CVAE variants.
(+) The method and the various bounds introduced are mathematically intriguing, well motivated and potentially useful in follow-up research, however, ... (-) the paper is difficult to follow and at places the reader is left guessing what the authors meant. This should be improved. Concretely
last para of section 2 - the optimization of negative L ""is relatively difficult"". Why? What makes it difficult?
last para of section 2 - ""... have been no equivalent of VI ... "" What about CUBO and its variants pick up from in your work? These have some specific flaws for which they do not qualify here?
before equation 3 - ""exponential divergence"". You mean the Bregman exponential divergence? A citation to help the reader?
p(u,z|θ)
in equations (11) and (12) seem to use the same parameters
θ
though for (11)
u=(y,x~)
and for (12) it is
u=x~
. Is this in practice the same network with two outputs?
But then in equation (15) these use different
zθ
and
zψ
samples. How is this designed and trained in practice?
page 5 - clarify notation for and explain the gain function; what is the intuition / purpose for it?
Def 1 - effective parameters are those with gradient zero. "".. i.e. the set of parameters inducing tight variational approximation."" How zero gradient achieves this in a complex non-convex problem, i.e. can't this be a local non-tight extremum?
page 7 DVAE/ DVAE* - you say these are MNAR and MCAR model variants as in Collier at al. 2020. Can you clarify how these translate into your rather more complex model formulation and what specifically changes in the loss (especially the EUBO part)?
Further questions for clarification/discussion
Why do you condition
y
and
x
on
m
in equation (1). These are the complete
x
data so they should not depend on the masking so that
p(y,x|m)=p(y,x)
. Or is this not true? Or is it the
y
that depends on
m
? Or is it rather the
m
which depends on
x
? (As in some values being more likely to be masked?)
You introduce two latent variable models in equations (7) and (8). My understanding is that the latent
z
is shared as (8) is just a marginalization of (7) over
y
. You then formulate to approximate posteriors
q(z|y,x~)
and
q(z|x~)
the first learned through ELBO maximization, the 2nd through EUBO minimization. There is currently no link between the two (approximate) posteriors. Would it make sense to somehow link them? (Sorry, I don't know how and may not be possible, or not easily.)
You used the Bayes rule to decompose the predictive conditional log likelihood into two terms in equation (4) of which one you are bounding from the bottom (ELBO) and the oher from top (EUBO). What is the effect on the predictive conditional
p(y|x)
? Is it somehow sandwiched or not really due to splitting and modelling the two non-conditional log likelihoods separately?
Minor text problems / typos
This first proposition in in page 6 is numbered 2 (not 1) - confusing","5: marginally below the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion","Keywords: Black-box variational inference, missing values, evidence upper bound","The ab-initio generative model class proposed by the authors for handling predictions with missing features is convincing. It has the advantage that the involved distributions are simple (factorising), however at the price of introducing latent variables. To learn it discriminatively requires to maximise a difference of concave functions. The first term is lower bounded by ELBO as in VAEs. The second term requires a tractable upper bound. The authors develop a novel upper bound (starting from alpha-Rényi divergence) that admits a stochastic gradient estimator. They further introduce a data dependent surrogate reparametrisation in order to achieve an estimator with low variance. The technical part of the paper is concisely written and correct.
The authors prove that the transformation used for the reparametrisation preserves the effective parameter subset, i.e. the subset of parameter combinations for which the overall bound is tight. This is indeed a desirable property, but is in my view not sufficient. The reason is, that this effective subset can be very small and cover a subset of simple models only. Moreover, there is no guarantee that the respective gap will become small during learning.
The experimental section first analyses the learning properties of the method in an ablation study. The authors then show competitiveness of their method by comparing it with existing approaches on a subset of tasks taken from the UCI Machine Learning Repository. The description of the experiments is clear and reproducible. The experiments are however not fully convincing w.r.t. the scalability of the approach. All networks used for the model and bound construction have only one fully connected hidden layer. This seems to be sufficient for the considered tasks from the UCI repository. However, this would be not sufficient e.g. for image classification tasks where the involved networks are usually deep CNNs.
Further comments:
You mention earlier works (Ghahramani & Jordan, 1994; Smola et al., 2005), noting that their applicability is restricted to exponential families. Please explain whether these approaches are / are not applicable for the model class analysed in your work. As I understand it, the models p(y,x,z|m) considered by you are exponential families, but of course after marginalising over z, the resulting mixture model p(y,x|m) is not any more. It remains however unclear to me, whether a DCA (difference of convex functions algorithm) for learning p(y|x,z,m) can be somehow generalised for learning p(y|x,m).
I would suggest to drop the data instance superscript earlier in the text, e.g. starting from subsection 2.2. the latest. This would in my view improve readability and reduce clutter.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion","Keywords: Black-box variational inference, missing values, evidence upper bound","Strengths The paper is overall clearly written. The issue it tries to solve, discriminative tasks with missing input features, has great impact for a wide range of practical machine learning problems in real life. Technically, the paper has quite some novelty including the creation of a rigorous lower bound to the true objective using recent advances in the variational inference area, and designed an effective surrogate parameterization to stabilize the optimization.
Weaknesses and Questions
Sec. 3.1: More detailed explanation of the exponential divergence would be beneficial. Is there a reference for it? What role does
f(u;ξ)
play? If it can be any real-valued function, why was it chosen to be a Gaussian pdf, as shown in the appendix?
Sec. 3.2: Which standard automatic differentiation library was used? The submission mentioned both the reparameterization trick and the REINFORCE trick - which one was actually used in the experiments?
Sec. 3.3: I don't quite understand how this part works. All I can see that in Eq(17) the problematic ratio term is multiplied by
G
, which is always smaller than 1 and non-increasing based on Figure 3. But why
G
was defined in that math format? What does
∨
mean? Why does
G
represent the ratio before and after the transform (equation between proposition 2 and 3)?
Sec. 4.1: Missing value processes (MCAR and MNAR). What do they mean? Are they different ways to decide what values are missing in the feature, and thus leading to different versions of a dataset? If so, shouldn't we also add CVAE*, Simple* and MICE*? Could you give more explanation for the last sentence of Section 4 (saying DVAE* is more robust than DVAE)?
Size of the test datasets. Based on Table 3, the datasets are all quite small, ranging from 353 to 10k data points. And we further split these points into training and test, which makes the training sets even smaller. In the appendix it's said the minibatch size is 521 -- what if the entire training set is smaller than 512? How long did the algorithm take to run on YearPred? Is the algorithm able to be easily extended to larger datasets?
How does DIG works compared with other more recent imputation baselines such as MIWAE (Mattei & Frellsen, 2019) and GAIN (Yoon et al., 2018)?","6: marginally above the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion","Keywords: Black-box variational inference, missing values, evidence upper bound","Strengths:
The idea of learning missing data using discriminative learning together with generative modeling is interesting. As mentioned in the paper, performing such kind of learning will resulting a loss function as a subtraction on two integrals with respect to the latent variables, which makes it harder to derive a lower bound compared to the traditional variational inference cases. To solve the term being subtracted, the authors found a upper bound that could be estimated in an unbiased way with Monte Carlo methods.
The exponential function in the first version of CELBO will potentially has a bigger variance when estimated using Monte Carlo. To solve this issue, the authors propose adding a regularization to the loss function while remain the optimal solution unchanged under the zero gap case. This also helps a lot in making training more stable, as shown in the experiments.
Weakness:
The topic of this paper is to focus on missing data. However, this paper does not put enough efforts on learning various cases of data missing patterns. As in the experiments, the authors only test the case where the data are missing completely at random (MCAR), which may not be the most common case in reality. MNAR case might be a more interesting situation to study with. The proposed method is mostly focusing on solving the variational upper bound, while overlooks modeling the missing patterns. Suggest the authors could add some experiments with MNAR data. Also it would be better if the authors could add some modeling part on the missing pattern into the loss. For example, let the mask
m
to depends on
(x,y,z)
. This will make the proposed model more useful in practice.
From Proposition 2, we know that the surrogate parameterization can make the optimal solution of CELBO remains under the zero gap case. However, it is nearly impossible to reach the zero gap case in reality since it is unlikely to select variational distributions (i.e.
q(⋅)
's) to perfectly estimate the model posterior distributions. What about the ""sub-optimal"" cases? Is the CELBO-SP optimal solution close to the CELBO optimal solution in a small gap (but not zero gap) case? I understand this might not be easy, but it will be better if the authors could add some theoretical analysis on this.
The performance metrics in Table 1 is not showing the proposed methods can outperform the baselines with big gaps, meaning that the proposed methods is not much better compared to previous approaches empirically. However, I think there are many ways that the authors could try to improve the performances. For example, the authors could try a different divergence function, a better way to add the regularization, etc.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Efficiently Modeling Long Sequences with Structured State Spaces","Keywords: sequence models, state space, RNN, CNN, Long Range Arena","The paper seems well written both regards to clarity and citations. Contentwise, the theoretical and experimental parts are interesting and relevant. The novel contribution, which is efficient computation of the discretized convolution kernel, is highly technical with details given in the appendix, but the authors did a good job at summarizing the key ideas. On the LRA benchmark, which was originally introduced to benchmark scalable transformer variants, the model sets a new high score on all problems, outperforming transformers on their home turf. The model also outperforms competitors on a raw audio dataset, including a SoTA CNN variant specialized to audio. Overall, the performance is compelling on all considered tasks, in respect of both accuracy and efficiency.
I have just a few questions/remarks:
The authors mention in Section 2.2 that linear SSMs can perform poorly due to vanishing/exploding gradients. The given HiPPO matrix is given by the sum of a normal and a low-rank (i.e. NPLR) matrix, and in particular it is not unitary. I wonder then how it gets around the aforementioned problem? I expect the answer can be found by looking up the cited papers, but if there is a short answer to this, it might be good to include it in the discussion.
Related to the previous question, it turns out the authors only use the HiPPO matrix as an initialization (although as a very sensible one), but then the algorithm is free to learn any NPLR matrix. Is it true in general for NPLR matrices that they result in well-behaved SSMs in the previous sense, or does that only hold for a neighborhood around the HiPPO matrix? Additionally, I would be interested in what happens (e.g. how performance changes) when a) the HiPPO matrix
A
is fixed throughout the training, and only the parameters
B,C∈RN
are trainable, and b) if the model is initialized from a random NPLR matrix, but not the HiPPO?
Is the model able to handle multidimensional input signals
u(t)∈Rd
?
In Section 3.4, it is mentioned that a multidimensional feature map of size H is created by defining H independent copies of S3. I am wondering if it would be more efficient if these copies shared the input-to-state (
B
) and state-to-state (
A
) mappings, and only differed in the state-to-feature mapping (
C
)? Do the authors expect this would negatively affect the results? Somehow it seems wasteful to me for each feature-coordinate
yi(t)
to have its own separate
N
-dimensional state representation, instead of sharing a common one (perhaps with a larger state size
N′>>N
).
The S3 itself seems to be a linear model, which made me wonder where the nonlinearities are introduced into the deep model? Perhaps, is there an activation placed on the feature map after linearly mixing H independent copies of S3?
In Section 4.3 paragraph Irregularly sampled data, it seems what the authors really mean is a resolution change. As far as I know, irregular sampling means the data is sampled on a highly non-uniform time grid, which means that in eq.(3) the step size
Δt
would become time dependent. It looks like this might make the computation of the convolution kernel in eq.(5) a bit problematic (the discretized
A¯Δt
matrices might not commute with eachother).","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Efficiently Modeling Long Sequences with Structured State Spaces","Keywords: sequence models, state space, RNN, CNN, Long Range Arena","The paper presents a strong and clear theory for the proposed reparameterization. Its advantages over the prior work linear state-space layers (LSSL) are explained very well theoretically and presented empirically. The convolutional kernel defined by LSSL is a convolutional interpretation to unrolling SSMs over time, granting parallelization to SSMs during training. The proposed model S3 resolves the bottleneck in this formulation by reducing it to a Cauchy kernel, resulting in a significant improvement in the space and computation complexity. The model is also compared against the state-of-the-art models in a diverse set of benchmarks. Especially the performance in the Long Range Arena (LRA) benchmark is superior.
I have the following questions for the authors.
The base LSSL model is not included in most of the benchmarks, particularly in the LRA. Is it due to the scalability problem of LSSL?
Is it expected to get improved results over the LSSL (see Tab. 4 and 5)? Does the proposed reparameterization guarantee a “more” optimal state matrix A?
The diagonal matrix
Λ
and the vectors
p
and
q
are trainable parameters that construct the state matrix A which is initialized to be a HiPPO matrix (Section 3.4). Do these trainable parameters preserve the structure of the matrix A to be HiPPO during and after training? Is it necessary?
In autoregressive generation tasks (i.e., the model is fed with its own prediction), the sequence models (RNNs, TCNs, etc.) are likely to suffer from error accumulation problem as the prediction horizon increases. Could the authors comment on S3’s performance in a similar task? Can S3 alleviate this problem?
-- Post-rebuttal edit --
I thank the authors for their rebuttal. I read other reviews and the author responses. It is clear that this is both empirically and theoretically a strong paper. The improvements over the baselines are substantial. Looking forward to seeing the follow-up work.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Efficiently Modeling Long Sequences with Structured State Spaces","Keywords: sequence models, state space, RNN, CNN, Long Range Arena","Strengths:
The motivation for the approach is clear both from a high-level as well as mathematical viewpoint. The authors do a good job walking through the theoretical justification and solution of the approach and proposed algorithm.
The discussion and analysis is performed on a broad domain of tasks. As the authors discuss, many approaches today are narrowly focused around a single task/domain which limits our broader understanding of LRD modeling. By using such a varied set of experiments, the usefulness of this approach is well highlighted.
The introduction is very well written. The general reader may be less familiar with some of these past works and the authors do an excellent job highlighting the motivation and current status of efforts in this line of study. Similarly the background is clear and detailed without being unnecessarily complicated or verbose.
The performance is compelling from both an efficiency and accuracy standpoint across a number of tasks.
Weaknesses:
Limitations and next steps aren’t explicitly discussed. It would be helpful for the authors to include even a few sentences on this.
A known issue with approaches like transformers is the need for huge datasets (even by deep learning standards). How does the performance of this approach vary as a function of dataset size? Does this method work in a low-data regime?
The results section is more difficult to follow, especially compared to the rest of the text. Many of the results are included in the appendix (which is fine especially given their extensiveness). However, some of the table references are strange. Table 4 is given, but not directly referenced in the main text (should go with Raw Speech Classification on p.8). Table 5 is not referenced until after Table 6. The section ""Irregularly Sampled Data"" doesn't explicitly reference where those results can be found (i.e. Table 4). This makes the results section a little bit unclear as the reader has to the text-to-table mapping. Table 4 caption is a bit unclear (i.e. requires going to the text to interpret what things mean).
Figure 2 (feature visualization): the authors state that the visualization of the low-level features shows that context over a small area is being learned and the rest of the image is ignored. However, two of the activations seem to span the entire kernel. Similarly, one of the higher-level filters is over only a small area. This seems to be more of a general trend (low learns mostly, but not completely localized, high learns mostly, but not completely global), but not a hard and fast rule. Is there a hypothesis as to why only some, but not all filters in these layers follow these trends? With the current visualization, a number of the filters appear identical (just a few solid rows at the top)- are these degenerate or is this a limitation of the visualization? There is less “hierarchical” structural differences in the convolutional filters (Figure 5 appendix). Could the authors explain this?
Minor: Figure 1, I missed the green K several times (spent a while looking for the green in the figure). Perhaps altering the perceptual qualities of the font could help draw attention to these variables more.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Large Language Models Can Be Strong Differentially Private Learners","Keywords: language model, differential privacy, language generation, fine-tuning, NLP","Strength of the paper:
The paper is clearly written and easy to understand. It presents the DP training problem very well and demonstrates the challenge for large pretrained NLP models.
The proposed GhostClipping is effective in reduce memory consumption, at (not too big) cost of an extra backpropagation. The method is very clear and also simple to implement.
The experiments show strong results on both classification and text generation tasks, better than the prior DP methods in terms of memory consumption and accuracy/generation quality.
Weakness of the paper:
There are three aspects involved. GhostClipping, large batch/proper learning rate, fine-tuning with masked prediction. It seems the accuracy on text classification comes from the later two. Table 1, comparing full(RoBERTa-large) with RGP(RoBERTa-large) does not show improvement (please list the average score here). Full is actually worse. If RGP is augmented with larger batch/proper learning rate, and masked prediction for fine-tuning, it may be the best in terms of accuracy.
Figure 1 is a bit mis-leading. Please show and compare with RoBERTa (non-private) on 1(a) and non-private GPT-2 on 1(b). Those are the baselines to compare. Otherwise you are comparing private RoBERTa with non-private BERT or non-private T-GEN(non Transformer). Statement in the abstract and introduction are over-stated.
The organization of the paper could be improved. Ghost-Cliping is your main method, which could be moved earlier.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Large Language Models Can Be Strong Differentially Private Learners","Keywords: language model, differential privacy, language generation, fine-tuning, NLP","Overall, this is a fairly empirical paper on an important problem and shows good performance. The authors presented a set of thorough experiments investigating the impact of various hyper-parameters, which are widely known as sensitive and difficult to tune, providing a nice and informative guidelines for other researchers and practitioners who work in this area. The ghost clipping trick proposed in this paper for memory reducing in DP-SGD is simple yet effective, greatly reducing the memory cost when applying DP-SGD to large models, especially the popular large-scale pre-trained language models, potentially encouraging more research effort in the DP learning area.
The experiments presented in this paper are quite solid and clear, both well-designed and documented, and most of the claims made in the paper are reasonable and well-supported. My only complaint is that I wish to see more explanations and more principled approach to selecting the hyper-parameters in fine-tuning with DP-SGD, but I guess it is out of this paper's scope (the authors did provide some empirical discussions in the paper and the appendix, which I appreciate).","n/a","8: accept, good paper"
"Large Language Models Can Be Strong Differentially Private Learners","Keywords: language model, differential privacy, language generation, fine-tuning, NLP","Pros,
It achieves a remarkable performance of DP algorithms on NLP tasks with a satisfactory level of privacy protection.
There will be a number of potential applications since this paper verify the model on multiple NLU and NLG tasks.
It works on several pre-trained language models (e.g. GPT, BERT, Roberta).
The experiments part is quite solid and covers many necessary details.
Cons,
The experimental results in table 1~3 compared to the non-private seems amazing, however, the comparison with the baseline methods is also required, which would show the contribution of your proposed methods. There're several possible baselines: directly apply DP-SGD and an application of DP-SGD on language models (GPT-2) [1].
The choice of \epsilon makes sense because the \epsilon in a range of 0.1~5 provides meaningful protection. However, the choice of \delta in experiments may cause some problems in privacy leakage. The value of \delta on the order of 1/|D_train| is very dangerous according to [2]. In that level of \delta, they permit “preserving privacy” by publishing the complete records of a small number of database participants. It's better to choose the \delta as 1/( |D_train| *100), which may reduce the utility but ensure the privacy is well protected.
Will the ghost clipping increases the utility or privacy protection? Could you please provide some insights (or even quantitative analysis) about it?
It would be better to conduct some key experiments with a range of \espilon from 0.1 to 10 (e.g. 0.1, 0.5, 1, 2, 3, 5, 8). Many DP learning algorithms use a curve to show the variation tendency of utilities for the different \epsilon.
This paper did a good job on the empirical study, but the technical novelty is limited.
[1] Differentially Private Language Models Benefit from Public Pre-training. 2020
[2] The Algorithmic Foundations of Differential Privacy, 2014
After rebuttal: Thanks for your explanation! I believe it's a good paper and I'm happy to see its acceptance.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Large Language Models Can Be Strong Differentially Private Learners","Keywords: language model, differential privacy, language generation, fine-tuning, NLP","Strengths:
As a DP algorithm on a deep learning model, this algorithm achieves good performance on fine-tuning the language models and several real-world applications.
The experimental results are sufficient and solid.
The performance gap between the proposed method and the non-private method is quite small, which shows applying the proposed DP algorithm would not decrease the utility so much.
Some of the conclusions from the experiments seem to be adaptive to other applications. For example, the relations among parameter size (for fine-tuning), privacy leakage, and performance.
Weaknesses:
The \delta in (\epsilon, \detla)-DP is too large for training a DP model with a meaningful level of privacy protection. Normally, \detla should be much smaller than the inversed dataset size (cannot be in the same scale of the inversed dataset size), some researchers choose \detla = 1/(|D| * log(|D|)), where |D| stands for the dataset size.
It would be better to conduct an ablation study to verify the promotion of the proposed methods (ghost clipping technique, fine-tuning on only a part of the parameters, and DP-adam). It's better to make it clear how much does each strategy contribute.
In this paper, the authors introduce a lot about the selection of hyper-parameters, thus hyper-parameters play an important role in model training. However, tuning hyper-parameters requires additional information about the private information (accessing the validation set or testing set), which leads to private leakage. So, how to count the information leakages of users? How to avoid those private leakages.
Is the DP-Adam proposed by yourself? If not, the corresponding reference and explanation are needed. Does it come from [Wang 2019]? If it is proposed by yourself, I suggest the author make it clear as it's one of the contributions.
[Wang 2019] DP-LSSGD: An Optimization Method to Lift the Utility in Privacy-Preserving ERM.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation","Keywords: molecular conformation generation, deep generative models, diffusion probabilistic models","This paper brings together recent ideas and methods (e.g. diffusion, SE(3) equivariance) to the established task of molecular conformation generation with impressive empirical results. A few more experiments are necessary. The paper is well written. I am only tangentially in this area so I am not intimately familiar with the prior works and benchmarks, and may have missed something in the empirical evaluation.
Comments:
The authors state that roto-translation invariance is a critical inductive bias for this problem. But I am not convinced that it is critical for this setup and model class. It would be nice to include a baseline of the diffusion model without SE(3) invariance.
I am curious about the sensitivity of the performance of the model to the hyperparameters of the diffusion process. Can the authors describe how they reached the current settings given in Table 5. In particular, T=5000 steps seems quite high and I was curious about the performance with lower T say around 1000. As a minor note, there appears to be an inconsistency in the reported batch size (128 in the text vs. 32/64 in Table 5)
What is the generation time with 5000 diffusion steps? Does that become a bottleneck in conformation generation?
What happened to the other methods on the geom-drugs dataset for Table 3?
The authors should mention recent point cloud diffusion papers in their related works as it is addressing a very similar problem
What are the reverse process sigmas set to?
Minor comments:
Typo: translatoinal
Typo: sophisticate
I noticed a reference that seemed incorrect: Xu et al 2021a as the reference for molecular generation with VAEs in the second paragraph. The authors should double check their references","6: marginally above the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation","Keywords: molecular conformation generation, deep generative models, diffusion probabilistic models","Originality: The idea of the work is novel - GeoDiff is the first generative model for molecular conformation generation based on a diffusion framework.
Clarity and Quality: The authors clearly provide motivation and state the differences of the proposed model from other approaches. The architecture is described in an easy-to-follow and intuitive way. The paper provides good Related Work and Results & Discussion sections. Still, it would be great if authors add a clear description of contributions and additional figures of architecture and an aligned cloud of conformations to visually evaluate the diversity of conformations.
Significance: The contribution of the paper to the field is significant. The experiments show a significant metrics gap between the proposed method and baselines, still, it would be great if authors will add standard deviation of metrics to evaluate the stability of the method.
Drawbacks / questions:
In 3.1 Authors say, they are interested in generating stable conformations. Authors provide energy metrics for the generated ensemble on GEOM-QM9 split, which covers only 30 molecules. Still, they don't provide energy metrics for medium size molecules from GEOM-DRUGS dataset.
Despite the proposed model being compared against a variety of baselines and that the authors mentioned the reason why they are not compared with the GeoMol, it requires a more detailed explanation of GeoMol limitations, since the GeoMol is the current state of the art approach in conformation generation task.
Diffusion models suffer from high computational complexity. Based on the parameters provided by the authors in appendix B, it's logical to assume that 5000 iterations per molecule are very computationally expensive. The article lacks an estimate of the generation time for one molecule.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation","Keywords: molecular conformation generation, deep generative models, diffusion probabilistic models","Pros
GeoDiff is an end-to-end model for generating Cartesian coordinates from a molecular graph.
GeoDiff shows very good performance on standard test sets.
GeoDiff outperforms various state-of-the-art methods for generating molecular conformations.
Cons
GeoDiff's model seems to involve a large number of parameters.
Comments and Questions
What is the computation time for generating a structure with GeoDiff? How does this compare to other ML approaches or RDKit?
What is the accuracy of the structures at intermediate stages of the diffusion process? Perhaps this could provide some insight into whether
T=5000
diffusion steps are needed to reach the reported performance.
Have you also run tests with a smaller number of diffusion steps?
You show that GeoDiff's model and objective functions are invariant under rigid transformations. However, what if the training data are not invariant? Doesn't this still fix some arbitrary orientation in the trained model?
Which threshold
δ
did you use in the computation of the precision and recall measures?
What is the total number of model parameters that are trained?
How is GeoDiff +FF implemented? What is GeoDiff's performance in property prediction without FF?
Language
Page 3: ""It has been shown effective""
Page 9: ""can stay the superior diversity""
Page 9: ""yeild"" instead of ""yield""","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Frame Averaging for Invariant and Equivariant Network Design","Keywords: Invariant and equivariant neural network, expressive power","Strengths: - The paper proposes a very practical strategy of building equivariant nets - The universality proof helps convince the reader to use this method - The paper considers and experiments on three different instantiations of their method, showing wide applicability. - The experimental results show the method performs competitively.
Weaknesses: - I don’t understand what’s happening in theorem 4. It considers a subsample
μ^
of F(X) to be ‘good’ when the symmetrizer that uses the subsample is epsilon-close to the full F(X) symmetrizer. Then it says that the probability of one particular good subsample is bounded below. However, that bound seems vacuus, as plugging any reasonable number brings the bound quickly close to 0. Also, it’s counter-intuitive why the bound should become looser as epsilon grows or as k grows. What one would want instead is giving a lower bound of the probability that we get any
μ^
that is epsilon-close to the full F(X) symmetrizer. And we want this bound to get higher when epsilon or k increases. The line below theorem 4 draws a conclusion that would follow from a theorem as I propose it above, not from the theorem in the paper. As it is currently stated – and I’m not completely misunderstanding – theorem 4 can best be removed from the paper. - It is a bit unclear when the results apply to finite and infinite groups and frames F(X). Everywhere, a summation symbol is used, but in some places, F(X) is infinite. In the infinite cases, which measure should then be used? Can one always use some canonical Haar-like measure? In particular, in the proofs of theorem 1 and theorem 4 this should be discussed. - The writing of the paper can be improved. I don’t follow the choice of F(X) for the E(d) case. Which are the 2^d O(d) matrices? Perhaps the authors can elaborate in more detail one of the examples how to construct F(X) in the main paper, and then do the other two in the appendix. - I would like some more theoretical discussion about the choice of F(X). Does the choice of F(X) affect the output? If so, how? Is F(X) required to be continuous / does a continuous (non-trivial) F(X) always exist? What does the topology on 2^G look like? How does this affect the continuity of the symmetrized function? If it is discontinuous, does that affect the universality? When can F(X) be chosen to be finite?
Other comments: - Why does GA-MLP and GA-GIN+ID only get 50% score on EXP-classify? Are you there using a finite subsample of G or F(X)? And could you give any insight into why we’d expect then complete failure for a G subsample and complete success for a F(X) subsample? - In the proof of theorem 5, which norm is used for ||rho_2(g)|| ? It can’t be the max K-norm because K is a subset of the input of phi, not the output. Is it the operator norm? - I would like theorem 1 to be put in the main paper, as it shows why the key construction is correct. - I think a citation would be appropriate to Finzi et al 2020, “Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data”, as they also consider sampling from the group to build equivariant networks. - Why have the authors chosen the name “frame” for F(X)? I know frame as a set of vectors or in the context of a frame bundle.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Frame Averaging for Invariant and Equivariant Network Design","Keywords: Invariant and equivariant neural network, expressive power","Strengths:
FA is a very simple framework yet it is potentially very useful. Group equivariance is an important form of inductive bias in deep learning architectures, but designing architectures that has such equivariance is challenging. It would be very useful if we could adapt any back architecture to become invariant/equivariant to a certain group. However, the previously studied group averaging is computationally infeasible when the group is large, so this paper, which greatly reduces the computational cost of group averaging, could be potentially very useful. I really like this work, and will be looking forward to seeing future development of this work.
Technical statements are all sound and proved. Mathematical statements in this paper all look correct to me, and they have provided proofs. These statements are clear and rigorous.
Impressive results. Using the simple idea of frame averaging, the paper demonstrates state-of-the-art results on several tasks. The results are impressive, which suggests that the FA framework can be very useful in practice.
Weaknesses:
Lack of simple examples. While it is not hard to check the correctness of all these statements, it takes me some time to form an intuition of what is proposed in this paper. It would help me a lot if the authors could provide a simple example at the beginning to give readers some intuition. For example, it might be good to work through an example to make MLP translation equivariant (with the simplest possible construction of frames)?
Insufficient study and explanation of the proposed frames.
The construction of frames in Section 3.1 seems to come out of nowhere. While I could check they are indeed equivariant, I don’t think I understand the motivation or thinking process behind such designs.
Furthermore, can the frames be simplified? As an example, if we let the input
X
be a function on groups, i.e.
X=f(g),g∈G
. Can we let
F(f)=arg⁡maxg||f(g)||
? I might be wrong, but I think this simple construction is also equivariant?
Are the number of elements output by frames the smaller the better, or is there a balance between performance and computational efficiency?
Questions: How stable are the proposed frames? For the proposed frames, I would be interested to know how stable they are? That is to say, if I add noise to the inputs, will the output subset of groups be significantly different?","No ethics concerns","8: accept, good paper"
"Frame Averaging for Invariant and Equivariant Network Design","Keywords: Invariant and equivariant neural network, expressive power","Strengths This paper proposes a general method for transforming existing models into invariant/equivariant models using Reynolds operators. Combined with other methods, it provides state-of-the-art experimental results.
Weaknesses & Questions First of all, I would like to point out that the idea of using Reynolds operators to transform a model into an invariant model is found in Kicki et al. 2021, where the construction is exactly the same except for the use of frames. The authors should be cited for this paper. Also, the name ""frame"" is confusing with the concept of frame in differential geometry and should be given a different name.
Theorem 1: It is proved that if a frame is an equivariant function, then a partial sum over the frame gives a transformation to an invariant/equivariant function, but the fact that the frame is defined depending only on the input space is not appropriate. For example, if we have an invariant model F, the proposed method will sum over frames to convert it to an invariant function, which will increase the computational complexity. The transformation should be done without any transformation for the invariant model F. This leads to the problem of overlapping invariants when combined with other models in the experimental section.
Also, since calculating the specific frame itself involves mathematical difficulties, I believe that the evaluation should be done on the model for which the frame has been calculated, i.e., for which the experiment has been conducted.
Point cloud models: The
Sn×E(d)
or
SE(d)
-invariant model is constructed by computing a frame for
E(d)
or
SE(d)
and transforming the model of the Deep sets with that frame. The simple question is why don't you construct a frame for
Sn×E(d)
or
SE(d)
? If this method is good enough, the point permutation action should also be subject to the frame averaging model. I would like you to explain the rationale reason for not doing so.
Graph models: Two types of models have been proposed: MLP+FA and GNN+FA. The problem with MLP+FA is that it uses the adjacency matrix itself as input, so when the number of nodes is large, the input space is also large, and the number of parameters is significantly larger than, for example, the model in Maron et al. Is it possible to train MLP+FA with, say, 50 nodes? Also, in a real task, the number of nodes in a graph can take many different values, is it possible to train MLP+FA for such a case? In GNN+FA, the problem seems to be that invariance is calculated redundantly as described above, and the contribution of this method is not clear.
Reference
Zaheer et al. Deep sets Neural Information Processing Systems (NeurIPS) 2017.
Maron et al. Invariant and Equivariant Graph Networks International Conference on Learning Representations (ICLR) 2019
Kicky et al. A New Approach to Design Symmetry Invariant Neural Networks International Joint Conference on Neural Networks (IJCNN) 2021","There are no ethical issues in this paper.","8: accept, good paper"
"Frame Averaging for Invariant and Equivariant Network Design","Keywords: Invariant and equivariant neural network, expressive power","Strengths
The idea of replacing the averaging operator over the entire group by the FA is innovative. FA is simpler to compute and has less complexity in comparison with the averaging operator over the entire group.
The authors prove that the FA-based models preserve the universality property of their backbone architectures.
The FA framework is then applied to design several invariant and equivariant architectures for point clouds and graphs.
Weaknesses
My concern is mostly about the incompleteness of the framework. It may happen the case that for some
X
,
F(X)
is a small set, but for some other
X
,
F(X)
is large or even infinity. (For example the frame choice in Subsection 3.1 is in this case). In this case, a deeper analysis on how to separate these two cases and how to deal when
F(X)
is large or even infinity is necessary.
It is also not clear how the FA is adapted in the DA-Local-PointNet. A detailed explanation here would be helpful.
No comparison with the previous equivariant architectures are presented. Therefore, it is hard to estimate how novel and efficient the framework is in the world of current group equivariant architectures.
Some technical parts are quite compact and not easy to read. Maybe the reason is that the paper includes rich contents and the authors tried to fix all of them in 9 pages.","8: accept, good paper","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation","Keywords: tensor manipulations, tensor transformation, einops, einstein notation, einsum","Strength of the work. First of all, the paper provides a handful of case studies with insightful analysis on user experience on the existing design of operators, and then based on those studies it derives a series of notations for highly user friendly operator design. By providing various comparison between existing ones on real-world, the paper leads to a convincing proof of the strength of the proposed notation for deep learning researchers' daily usecases, in terms of expressiveness, elegance, usability and debuggability.
Second, the paper comes with solid engineering efforts that offload the computation to underlying libraries, including NumPy, TensorFlow, PyTorch, MXNet, Keras etc. Effectively, with the solid engineering effort, the library serves as a meta framework that can be reused across backends with close-to-zero engineering overhead. The paper also explores some techniques like caching to reduce the execution overload of runtime dispatching.
Third, by the ""stringly-typed"" abstraction of operators, the proposed notation enforces more explicit programming of the semantics of tensor axes, as well as more runtime checking to catch correctness issues. This is particularly helpful among deep learning practitioners who are already exposed to a set of conventions (e.g. ""NCHW"") to write the right code. The typed operators, instead of typed tensors (e.g. NamedTensor), provides the guarantee that users only need to type important part of their code as a drop-in replacement of the existing code fragments without having to worry about refactoring the entire codebase. This is again particularly helpful for deep learning practitioners
Weakness. With the proposed notation, compiler-based approaches, e.g. TVM, MLIR, haven't been explored to further boost performance, although such opportunities are real and tangible. For example in TVM, the Tensor Expression (TE) can be considered as a generalized form of EinOps, and it could bring extra performance gain to the users. Therefore, it would be desirable to integrate with those compiler backends to assist with potential performance.
The normal definition of the EinOps notation, while demonstrated in many examples, are not carried out formally. For example, brackets () in the notation may serve different purposes under rearange, reduce and repeat, on the left- and right-hand side of the notation. It may refer to tiling one dimension, or composition of new dimensions, or removing a dimension, etc. Another example that needs more clarity is stack/concatenate where there are multiple tensors involved, and might be desirable to state formally on the constraints implied in this particular case.
Last, while the notation is very flexible in expressing layout-related operations, it is yet under-explored on expressing the common operators that deep learning researchers may care about, namely, many variants of convolution.
Correctness. The reviewer is not aware of any correctness issue in all demos and figures in this paper.
Clarity. The paper is overall rich in detailed and insightful explanation. There is some minor clarity issues as pointed out in the weakness section.","The reviewer is not aware of any ethical concerns in this paper","8: accept, good paper"
"Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation","Keywords: tensor manipulations, tensor transformation, einops, einstein notation, einsum","There are numerous issues with the paper, which I will try to summarize below.
I should mention that, since no code was included in the submission, my assessment of this work is based entirely on the paper and the ideas expressed therein. I am not judging the work based on technical aspects related to the library except for those described in the paper. However, a complete assessment of the paper should also include a code review (see my first comment in the ""Weaknesses"" section).
Strenghts
I am a great fan of einsum notation for expressing tensor operations and I agree with the authors on the value of extending the notation as they did. I could see myself, as a practitioner, using the ""rearrange"" function (I thought that the example of reshaping an array of images into a 4x4 grid on page 6 was nice). As an engineering feature, the einops package could be useful.
Using the library seems to make the code less verbose in the examples provided in the paper. This gives a reasonable advantage in terms of the readability and maintainability of the code (although this is a purely subjective assessment, as I argue below).
The fact that the library supports multiple backends is a plus, although I have some concerns about some design choices discussed in the paper (see below).
Weaknesses
Unfortunately, I think that there are considerable flaws in the paper, the motivation behind it, and the described implementation of the library. I will try to summarize everything in the following points.
While I agree that much of machine learning is based on tensor manipulation, this paper is not suitable for publication at ICLR. Even if we were to focus only on tensor manipulation as a machine learning exclusive, better venues exist for this kind of paper, like the JMLR Open Source Software track. There, the paper could be reviewed along with the code (which was not included in this submission even if it is a crucial part of the authors' contributions). I realize that libraries like TensorFlow and PyTorch had a paper published at some top conferences like ICLR, but it's clear that their novelty and impact on the community was incomparably larger at the time of publication.
The motivation for the paper is weak. I will give some examples of claims made by the authors to motivate einops that I don't think hold up:
Users have to remember axes order: this is largely solved by einsum, not einops exclusively.
Control over data layout: einops does not solve the issue, it just gives a simpler interface to control it.
Einops is declarative and self-documenting: einsum too, einops simply extends the paradigm.
The contributions of the paper are not enough to consider a publication.
The paper should describe the notation in detail, but it doesn't. Instead, it is halfway between a description of the notation and a documentation for the package. The result is that it works badly as both. Some examples:
The notation is never formally described, for instance as a grammar, and one must look at examples to infer the form of a correct string.
Many features like anonymous axes and ellipsis indexing are essentially ignored, even if they are a key part of the notation.
A comparison with einsum is missing, which confuses the reader and does not highlight some features that are present in einsum but not einops (for example, computing the trace and the implicit notation).
Also, the authors claim that they ""align the interface with einsum to allow smooth simultaneous usage"" but this is misleading. Einops is clearly inspired from einsum and simply adds some features (while also removing others, as I said above).
In this regard, throughout the paper the authors seem to imply that einops solves problems (like that of rearranging) that were previously unsolvable (e.g., second paragraph of page 6). This is also misleading since einops is merely an interface/API to existing functions.
The example on page 4 concerning convolution is unclear. For example: when convolutions rely on axes order, the user is not ""expected"" at any time to use named axes for other operations. It is also not clear what kind of ""name checks"" would be prevented by assigning fixed names to the axes in convolution, and how einops overcomes this issue. I might have misunderstood this whole paragraph, so please correct me if I'm wrong.
In this regard, if the interaction with ""neural layers"" is a primary motivation for developing the library, the feature should be discussed more in depth (it is only briefly mentioned once, towards the end, with no explanation).
There are many non-rigorous/subjective claims. I have found at least five:
Footnote 4: ""New users frequently continue trying to imagine the tensors layout in memory."" -- On what grounds are the authors making this claim without any references? Have the authors conducted user studies?
Page 9: ""Caching plays an important role in einops high performance"" -- Since there are no performance benchmarks, how is performance quantified here? I assume that the performance of einops is equal to or worse than the backend, given the extra overhead. Is it faster than equivalent einsum operations? Can the authors quantify the ""important role"" of caching?
Page 9: ""einops was also referred once as a good intermediate solution"" -- By whom? How is this whole reported conversation meaningful to the reader?
Page 9: ""Design of such system is much harder than it sounds: previous ideas failed"" -- What does it mean that designing is hard and how can the authors be sure that it wasn't due to their own limitations? Whose previous ideas have failed and in what context?
I am obviously not implying anything about the programming and engineering abilities of the authors, I am merely pointing out that claims like this should not be made in a scientific paper. - Page 9: ""We observed that einops notation gets picked up for describing tensors with packed dimensions"" -- Again, did the authors perform a user study to make this claim? And, in any case, did einops notation get picked up more easily than einsum notation? If so, why?
Other minor issues
Figure 1: the example assumes that the reshape is done by hardcoding the dimensions and that the programmer will make a typo that ""luckily"" does not crash. First, we don't know how often this kind of bug happens and, second, this issue can be easily bypassed by accessing the actual shapes (like the authors do on page 6, line 2-3 of the code block at the bottom). I wouldn't use this as a primary motivation for the rearranging function.
Page 4, third bullet point: the authors imply that the code should crash, but in fact it is a perfectly well-formed expression that should not crash. Again, the authors should not assume the likelihood of a bug or what the users want to do.
Page 7: ""Einops alleviates necessity to introduce a function, as arguments describe input, output, and the transformation itself"". This is a claim in support of the readability of einops, although it holds for einsum already. While I partially agree, einsum/ops notation could also be seen as less beginner-friendly because it requires the users to know the syntax. For example arr.flatten() seems (subjectively) more descriptive than rearrange(""ab->()"", arr), especially if one doesn't already know einsum notation.
Comments on the implementation details
The choice of automatically detecting the backend from the input is arguable. Quoting from the famous Zen of Python by Tim Peters: ""Explicit is better than implicit"" and ""In the face of ambiguity, refuse the temptation to guess.""
For example, a situation could happen in which a Numpy array is given as input to a TensorFlow-based user-defined function.
Since TensorFlow knows to automatically convert arrays to tensors at runtime, this is standard and expected behaviour. Replacing the initial call of this imaginary function with an einops-based one would result in the first operation happening in Numpy and not TensorFlow.
I advise the authors to adopt a paradigm like that of multi-backend projects like Keras in which the users decide explicitly what backend to use (through a config file or by making the backend explicit at import time -- e.g., `from einops.torch import ...`).
Can the authors provide an example of how they improved the exception message for broadcasting w.r.t. the one from Numpy? How is it easier to understand?
Suggestions
After revising the paper, the authors should consider submitting it to an appropriate venue for open source contributions.
The revision should make it more clear that einops is an extension of einsum and should highlight the concrete additions and limitations of the proposed einops.
The authors should make the paper more rigorous: the notation should be formalized, the claims about usability should be removed unless backed up by user studies, and the motivation of the paper/library should be rethought. The paper could also focus less on examples (which are more suitable for the documentation).","3: reject, not good enough","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation","Keywords: tensor manipulations, tensor transformation, einops, einstein notation, einsum","Strengths: - The proposed toolbox allows for simple and easy to read code tensor manipulation operations and operates with most popular Python frameworks. - The usage of indices names is more flexible compared to already existing ones in the einsum operation in numpy and Pytorch, which are known to have some limitations, e.g. number of dimensions is limited by the number of letters. - It is very useful for using Tensor Networks where number of interacting tensors are usually big and can have a large number of dimensions.
Weaknesses: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation","Keywords: tensor manipulations, tensor transformation, einops, einstein notation, einsum","First, this paper studies efficient programming paradigms for multi-dimensional array operation, which is an important intermediate layer for modern ML systems, especially for deep learning systems. Personally I tend to buy these advantages of einops claimed by the author, including:
providing semantic check for tensor operations;
including high expressiveness and flexibility for tensor manipulating interfaces;
supporting multiple backend runtimes efficiently;
making tensor manipulating code more readable and reliable, and etc..
These claims sound very promising and valuable as a ML toolkit.
However, there are some fundamental concerns I have for the paper:
The writing is problematic as an academic paper. Comprehensively, this paper reads like a technical blog, which tries to introduce and advertise a Python library; some statements in this paper are casual, i.e., in the end of Section 7, the author says ""We intentionally omit discussion of user conveniences provided by einops package: anonymous axes, ellipsis, list inputs and neural layers."", although I would not doubt about it if reasonable amount of evidence (e.g., empirical study or analysis) was presented, this sentence sounds a little ungrounded. Further, the organization of the paper can be polished as well. To be specific, I do not get why Section 2 & Section 3 are split, it seems that both sections are talking about the issues and limitations of STOA systems.
There is a lack of reasonable amount of empirical study to justify the statements about the contribution of the work. For example, the author claims that einops ""significantly improves code readability and flexibility"" in the abstract, I would expect some user-study (which could invite a group of participants to accomplish some programming tasks about tenor manipulation with and without einops and measure some objective or subjective metrics for evaluation) to justify this claim---note that this is pretty standard in first-tier PL/SE volumes, it would be easy to find hundreds of such papers and adopt such methodology for evaluation. The current Section 5 can be considered as good motivating examples, but it is not sufficient to support this claim following the principle of scientific study. The efficiency evaluation of the implementation is also missing, e.g., it is important to learn how much overhead is introduced by imposing einops over the backend systems, e.g., what is the runtime gap between einops implementations and direct usage of the backend systems' interfaces.
Lastly, I feel lost about the discussion about the difference between einops and numpy einsum. As far as I learn, the interface looks similar (at least from coding examples). As so, I would expect some clear and concrete statements about the distinguish components of einops (different from einsum).","3: reject, not good enough","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"A Fine-Grained Analysis on Distribution Shift","Keywords: robustness, distribution shifts","Strengths
Firstly, this paper is a job well done! It was written very clearly and most parts were very easy to follow. The motivations of the paper are very clear. The major strength of the paper lies in the extensive experimental evaluation provided across the main paper and the supplementary, which may be used as a standardized reference for many future works.
The efforts made towards stringing various works aimed at generalization, as well as various natures of distribution shifts, on a common thread is a commendable effort.
The figures and the graphical illustrations are very well made, and effectively distill the major inferences of the experiments.
Required Clarifications
My major question to the authors of the paper is how general the proposed framework? You seem to select different datasets, but take only two attributes on each datasets which raises questions if the findings of the study hold with larger scale real world datasets with multiple factors of variation, often heavily entangled. I understand that these settings are chosen to keep the experiments tractable, but it would be helpful to provide insight into whether the framework holds for Imagenet scale datasets with unknown (and possibly unobservable) factors of variation [1]. For example, the inferences and observations in sec 4.1 and sec 5 are conditioned on the assumption that the factors of variation are known, which gives us a cue to which model might work (CycleGAN for low data-drift, pretraining for unseen data shift etc). But can this knowledge be extrapolated to judge in-the-wild datasets? This brings me to the next point.
I could not pinpoint a single take-home message from the paper. The empirical study results have high variance, and it can only be inferred that no single method works for all the cases. But can the authors, equipped with the knowledge of these experiments, draw any meta recommendation of mapping between (factors of variation, distribution shift, modeling) choices? Of course, I fully agree that the current findings are still incredibly useful, but I was expecting to see a more optimistic take home message for the readers : ) .
I fail to see that significance of the latent factorization model proposed in sec 2.1. While the formulation of the data, discrete attributes and the labels seem clear, I do not see the significance of introducing the latent factor z. The explanation seems equally effective by using only notation of (x,y). Also looking at the models used to achieve robustness, none of weighted resampling or data augmentation require any latent factorization. Similarly, Imagenet pretraining also does not necessarily relate to latent factors in the attributes. Beta-VAE however relates to latent factors but that's about it. Perhaps the whole section 2.1 can be better motivated in the context of the paper.
The domain adaptation works generally work with a covariate shift assumption [2]. How does that fit into the proposed framework? Also, DA works like DANN and CORAL generally make use of additional unlabeled data. How are the unlabeled data chosen for the experiments? While it is inferred (sec 4.2) that domain adaptation methods lead to limited improvements, the DA methods considered are quite old and primitive. Perhaps more recent DA methods might help?
The authors could also contrast the in-distribution vs. out-of-distribution performance of the models. It seems that arch. like ViT might do very well on training domain but lack generalization on OOD test data. Why is this so? Does this have to do with the strong prior that CNNs induce that help generalization?
Is Imagenet pretraining performing better only because it is trained on larger scale data compared to other methods? In Fig 7, are all models trained/pretrained using same amount of data?
Hendrycks, Dan, and Thomas Dietterich. ""Benchmarking neural network robustness to common corruptions and perturbations."" _(2019).
Ben-David, Shai, et al. ""A theory of learning from different domains."" Machine learning 79.1 (2010): 151-175.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"A Fine-Grained Analysis on Distribution Shift","Keywords: robustness, distribution shifts","“find that a model trained on one set of hospitals may not generalise to the imaging conditions of another.” -> Training on “one set of hospitals” is a weird formulation and it’s not clear what is meant; set of MRIs from multiple hospitals or similar? Please rewrite this sentence.
“These methods span the following 5 common approaches: architecture choice, data augmentation, domain generalization, adaptive algorithms, and representation learning.” The collection of approaches written in this way sounds a bit strange to me because the choices are neither orthogonal nor of the same kind. Architecture choice is a property of the model being trained (irrespective of the task), data augmentation heavily depends on the task, domain generalization IS a task, and representation learning is a whole area of machine learning. At this stage of the paper, I find this list very confusing and would suggest rewriting or clarifying this sentence.
“We assume that distribution shifts arise…” Distribution (or covariate) shift is usually defined using the definition in [1], section 2.1.1. Can you please comment on whether there is difference to your definition and what it means for the approaches one would use to tackle it? From my understanding, it is actually the same because in [1], covariate shift is defined as (a) P(X) != P’(X) and (b) P (Y |X) = P’(Y |X). Here, the authors also have the condition (b) and write that P(y_1:k) != p_train(y_1:k) != p_test(y_1:k) which will result in the condition (a) P(x) != P’(X). In any case, distribution shift is a well-defined phenomenon; thus, it would be good if the authors could cite some standard definition (e.g., [1]) and comment why their definition is different (if it is different).
Page 3: Data Augmentation. The authors write “alternatively” and I believe they refer to the previously proposed reweighting method. Writing “alternatively” means to me that they describe the methods separately first. Therefore, I am confused by the definition of p_aug since it still contains p_reweight. The authors should either (1) remove p_reweight from p_aug or (2) write “additionally” instead of “alternatively”. I would prefer (1).
Page 3: Data Augmentation. I don’t quite agree with how data augmentation is defined here. The authors write that one can synthesize artificial data with a generative model that aims to approximate the true generative model and use this data as data augmentation. I don’t think this understanding of using data augmentation is how researchers generally think about data augmentation. I think the main driving factor for using data augmentation is to artificially increase the dataset size and to make the classifier more smooth around the data points, i.e. its decision should not change if the data distribution changes slightly. For this, researchers generally do not try to learn the true generative model, but rather use simple augmentations such as e.g., Gaussian noise, crops and horizontal flips. Later, the authors write that they use AugMix, RandAugment and AutoAugment as augmentations. All these augmentations are simple parametric functions that neither depend on the latent z nor on x since they can be used irrespective of the data and/or task. Learning the generative model is more of a GAN-like approach, so I would maybe split the paragraph into something like (1) Standard/Simple/Parametric/Heuristic data augmentation and (2) Learned data augmentation. The authors actually split the two notions of data augmentation in section 3. I would suggest just using the same split here. The authors actually claim in Takeaway 3 that “Heuristic augmentation improves generalization if the augmentation describes an attribute.” Nevertheless, I think defining data augmentation in this way here is a bit preemptive.
Page 7: “We report the mean and standard deviation over the five seeds.” I absolutely appreciate the effort done here for reporting the error over 5 runs as this is unfortunately not standard over even common in most papers.
Page 4: “Test distribution…” please define A_i
Page 4: “Shift 1:” Should it be P_test on the right hand side of the definition?
Page 4: “Shift 2: Low-data drift” -> Maybe mentioning that this issue is being studied by the fairness community would be good.
Page 4: “Shift 3: Unseen data shift – Some attribute values are unseen under ptrain but are under ptest.” Suggest to add “present” as in “but are present under p_test”.
Shift 3: “which we make explicit due to its important real world applications” -> importance
Shift 3: I do not understand the right-most inequality in Eq.2. Please explain.
I really like the structure and content of section 2.2. The authors put a lot of effort into properly and carefully defining and disentangling different distribution shifts. I also appreciate the provided examples from DSprites to illustrate all the different shifts.
Figure 3: I find it weird that using more samples from the true distribution seems to hurt performance in many cases. Additionally, checking Figure 11 in the Appendix, it looks like higher N is correlated with higher accuracy though it is hard to judge only looking at the image. The authors should comment on this.
Figure 4: Does this Figure show the accuracy on the biased or non-biased datasets? Similar to Figure 11, in Figure 12, it again looks like a higher N results in higher accuracy. In Figure 12, some bars are missing.
Figures 10, 11 and 12 are generally hard to read because there are so many bars and the only information one can extract is that there is a monotonic increase over the different models which is pretty useless since the ordering has likely been chosen such that there is a monotonic increase. Judging how N changes the bar heights is not really possible. I would suggest to maybe plot these Figures as heat plots similar to Figures 3 and 4. With heat plots, the authors could also write the resulting accuracy numbers into the heat map squares. This will allow researchers to cite their numbers in the future and also make reproducibility easier since these numbers can be easily compared against.
It is annoying that the Figures 10-12 in the Appendix are not plotted together and also not close to the corresponding text. I would highly suggest restructuring the Appendix to make parsing it easier, especially, since it is very long. For example, the code for the framework can be put at the very end such that it does not break the flow when looking at the additional results.
Takeaway 3. I do not see how the presented evidence leads to the drawn conclusion that “Heuristic augmentation improves generalization if the augmentation describes an attribute.” In B.2, the authors merely show that “No augmentation always leads to a strong boost in performance.” But it is not discussed in what way the successful augmentations approximate the true generative model. This type of analysis has done on ImageNet-C and CIFAR10-C in ref. [2] which the authors should cite here. In ref. [2], the authors define a minimal sample distance between the expected distribution shift and the added data augmentation and find strong correlation between the two. Here, a similar analysis would need to be performed in order to be able to do such a claim. In this light, tip 1 is also not grounded on evidence (although it is pretty obvious).
I did not understand whether model selection is part of the framework code?
The tips in 4.2. are not really surprising (or novel). In my opinion, tip 1 is not grounded on the presented evidence since the authors did not show results that would support their takeaway 3. It is an obvious tip though and intuitively, it makes absolute sense, but the authors did not show results to support it. Considering tip 2, I think learning the perfect generative model is a hard task and I am not sure how feasible this task is for ImageNet scale datasets. Given how difficult and unstable GAN training can be, I am not sure how practical this tip is. The authors also did not show results on ImageNet, so it is hard to judge whether this tip would scale to ImageNet. Considering tip3, it is well known that pretraining on ImageNet is powerful as is has been shown in numerous previous works. As for tip4, I am not aware that DANN has been scaled to ImageNet. Additionally, DANN training (being a minimax optimization problem) can be unstable and depend on hyperparameters. Thus, I question the practicality of this tip.  Some of these tips are not novel and the authors (merely) provide additional evidence for them. It would be nice if the authors could cite some papers where these findings have also been reported.
References: [1] Bernhard Schölkopf et al. “On causal and anticausal learning”. [2] Eric Mintun et al. “On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness”","10: strong accept, should be highlighted at the conference","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"A Fine-Grained Analysis on Distribution Shift","Keywords: robustness, distribution shifts","=================== Strengths: In my opinion, the efforts to unify all the studies of distribution shifts into one general framework is the greatest contribution made by this paper. Recently, plenty of distribution shift settings become popular in various machine learning fields, such as long-tailed classification/detection, domain adaptation, out-of-distribution generalization, etc. However, they are all studied independently, despite the fact that a similar essence is shared by them under the surface. This framework puts them into the same testbed, which allows us to evaluate the scope and limitation of different methods in all kinds of settings and data types. Besides, the authors also provided a number of practical tips and useful conclusions based on massive experiments under diverse settings and datasets.
=================== Weaknesses:
However, some conclusions in this paper violate my own observations. For example, the heuristic data augmentation method, Rand Augmentation, is found to be very helpful in long-tailed classification (a specific type of low-data drift) and some label noise tasks. It can consistently improve 2-5 points of accuracy across different datasets and settings. Yet, Rand Augmentation looks to be the worst method in Figures 3 and 4. One possible explanation is that datasets I was using are all from real-world images, so the Rand Aug can reveal the underlying generative model p(x|y1:K), while the datasets used in this paper are mostly synthetic images or medical images, where Rand Aug didn't approximate the true underlying generative model p(x|y1:K). In order to prevent the conclusions in this paper from misleading future researchers, I suggest the authors summarize datasets into different types (like synthetic dataset, medical dataset, and normal dataset) and investigate them separately.
Another concern is that should the ImageNet pretraining be used as a valid method for the study of robustness to distribution shifts? It may violate the settings of SC, LDD, and UDS by introducing samples from unknown distributions. For example, unseen data distribution in the given task and dataset may be compensated by the ImageNet dataset. That's why pretraining is usually forbidden in the tasks like OOD image classification or long-tailed classification. As a result, the superior performance of pertaining is not just frustrating but also (could be) unfair.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Open-Set Recognition: A Good Closed-Set Classifier is All You Need","Keywords: open set recognition, image recognition, computer vision","The core technical contribution or claim of this paper is that closed set accuracy is important for openest detection. This is known for researchers working on openest detection, though not highlighted enough. For any representation one key challenge of open set detection is in distinguishing confusing instances (incorrect predictions) within closed set vs novel category instances along the decision boundary. So as the quality of closed set classification improves this overlap could decrease and hence would enable easier detection of open-set instances.
[1] makes similar claims that vision transformers result in better open-set detection without complex detection mechanisms. [3],[4] highlight the fact that good representation is all you need for meta learning or out of distribution generalization which though orthogonal to open-set detection, informs the research community that ‘quality of representation & performance on closed set’ is useful for auxiliary and relevant tasks of OOD, few-shot learning.
Similar to the observation of this paper of closed set accuracy, many works in ‘openworld’ competition [5] at CVPR has taken various strategies to improve closed set accuracy in pre-training phase to improve closed set accuracy and leverage it for open-set performance boost.
Another key contribution of this paper is proposing large scale ImageNet benchmark, [5] is a CVPR’21 competition and existing benchmark along the similar lines which unfortunately makes benchmarking contribution not significant enough.
Strengths:
This paper focuses on an important problem and highlights an observation and it’s simplicity is especially relevant for practical settings and safety-critical applications. This paper is well written and has great empirical evaluation methodology and demonstrate SOTA compared to other methods with additional components. This paper proposes additional benchmarks for openset detection leveraging fine-grained classification datasets, and this is adds a valuable dimension to openset benchmarks.
Weakness:
The core technical contribution(s) does not seem convincing to warranty a conference acceptance, especially given previous works making similar conclusions and contributions. This lacks sufficient commentary on why a good closed set accuracy would result in better open-set detection.
Suggestion to Authors: Visualization of representations (albeit noisy) with and without augmentations/modified training could be insightful. May be including few examples of how hard novel instances/samples map onto different representations. For completeness please consider including CIFAR100 (close set) vs CIFAR10(open set), as the number of closed set classes increase the task of openset detection becomes more challenging, this version of experiment is included in [2] which this paper includes as baseline and [1].
References:
[1] Exploring the Limits of Out-of-Distribution Detection. http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-001.pdf
[2] H. Zhang et al. Hybrid Models for Open Set Recognition https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480103.pdf
[3] Y. Tian et al. Rethinking Few-shot Image Classification: A Good Embedding is All You Need?
https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf
[4] J. Miller et al. Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. https://proceedings.mlr.press/v139/miller21b/miller21b.pdf
[5] Open World Vision CVPR’21 Competition and workshop https://www.cs.cmu.edu/~shuk/open-world-vision.html, https://eval.ai/web/challenges/challenge-page/1041/overview","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Open-Set Recognition: A Good Closed-Set Classifier is All You Need","Keywords: open set recognition, image recognition, computer vision","This paper is dealing with the open set recognition problem which is a challenging research problem. Based on some findings about the correlation of closed set recognizer and open set recognizer, this paper found a way to achieve a strong OSR through an enhanced close set recognizer. This is a valuable finding for future open set recognition research. The weakness of this paper is that there are not more in depth analysis the behind reason for these findings, thus the value of this paper is not that significant. So this is possibly a direction to make this paper even stronger.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Open-Set Recognition: A Good Closed-Set Classifier is All You Need","Keywords: open set recognition, image recognition, computer vision","Strengths:
The paper is well written, clear and straight to the point.
The appendix add more experiments and details to further understand the main paper statements.
Up to my knowledge the most important references in the field have been considered
The paper belongs to the category of very important papers that show that sometimes in research we consider very complex solutions, whereas a properly tuned standard approach can do the job as well.
The authors propose a clear distinction between open-set recognition (in which the none-of-the above class comes form well defined classes) and out-of-distribution (in which the none-of-the-above can be any kind of image). It is important to clarify the different tasks.
It is interesting to see that in new and not overfitted datasets, cross-entropy baseline seems to perform better than the best method for open-set scenario.
Weaknesses:
Authors compare with only ARPL and OpenHybrid approaches because they say that the other approaches perform lower on open-set benchmarks. I think it is still important to consider also the other methods especially because the authors propose new benchmarks and on these benchmarks the ranking between the methods changes. So it could be for the other methods. I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3).
In Fig.2 I would expect to see also results for OpenHybrid. Also, in the figure, for the simplest datasets (with performance close to 100) there is not much correlation between open-set can close-set performance as ARPL does not improve over cross-entropy on the close-set scenario. It is something that should be mentioned.
Results in Table 1 for the cross-entropy are presented with a ranking based on the classifier logits, which in my opinion makes more sense. However, as previous works use the softmax scores, it is important to compare the two ways. I see a comparison in Appendix B (Fig. 6c), but this is only for a model. I would like to see this comparison at least for the models in tab. 1.
Additional questions/comments:
Fig.1a presents ARPL in the legend, without really having yet introduced the method.
The authors propose a clear distinction between open-set recognition and out-of-distribution. However, is this distinction really necessary. Could not the two fields be unified? It would be more interesting to know if the same conclusions of this paper are valid also for out-of-distribution problems.
The ""more theoretical"" justification of the results at bottom of page 4 does not add much, but I do not have suggestions on how to improve it.
In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open-set scenario on ImageNet. Is it due to the fact that it has been trained on more data or it is the reduced inductive bias (no convolutions)? It would be interesting to see if with different sizes of ViT, the correlation between open and close-set scenario still holds.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Learning Strides in Convolutional Neural Networks","Keywords: Strides, Convolutional neural networks, Downsampling, Spectral representations, Fourier","The idea is interesting and makes sense to me. The writing is clear and easy to understand. However, from the experiment results (especially table3 and 4), it seems that the proposed method has marginal improvement compared to regular strided conv and spectral pooling baselines. It seems that the default setting can already achieve very good results on ImageNet and how it will affect the model performance when the model is large enough remains unclear. The benefit of learnable strides is not fully demonstrated. It would be great if the authors could implement one or more future works in Sec. 4 to showcase its capability further and show the proposed module's overheadein detail to let the readers better understand its limitation.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Learning Strides in Convolutional Neural Networks","Keywords: Strides, Convolutional neural networks, Downsampling, Spectral representations, Fourier","PROs
The paper is very well written and pretty much clear at a first read. All the introduced components are properly motivated intuitively, and figures and algorithms really help the reader in understanding.
The authors excel in framing their work within related art. Specifically, they discuss both standard alternatives such as pooling, as well as alternatives based on fractional stride. For all of them, issues are properly pointed out such that the motivation behind this paper is very clear.
The paper is, to the best of my effort, technically sound. In terms of novelty, one might argue it being a bit incremental over SpectralPool, as it substitutes the cropping hyperparameters with learnable values. However, i) this step is not trivial, as the cropping operation is non-differentiable, and the authors introduce a solution to that and ii) this modification enables a significant improvement in performances, registered across a number of datasets.
The authors support their proposal by carrying out experiments on multiple datasets. Specifically, 6 datasets are employed for audio recognition and 3 (CIFAR-10, CIFAR-100 and Imagenet) are used for image recognition. In my opinion, for a paper of this type, attacking a very fundamental and obiquitous operator in modern architectures, being able to showcase improvements on non-toy datasets such as Imagenet is remarkable and noteworthy.
For every experiment, the authors report the mean and standard deviation over multiple trials, strenghtening the reliability of the conclusions.
The authors properly discuss limitations of their work, by highlighting the computational cost, some failure cases on DenseNets and implementation challenges on specific types of hardware.
CONs
I think this paper is very solid, and I consider the following points as minor concerns.
In Sec 3.1, the authors claim that as ""DiffStride learns different strides for the time and frequency axes"", and as such ""this justifies using a different parameter for each dimension rather then sharing strides"". However, this validation is flawed. Just because the model takes advantage of this flexibility, it doesn't necessarily mean that it is beneficial in the end. This might be the case if we completely trusted gradient descent to reach global optimums of the cost function, which is not the case. Indeed, the final stride configuration might depend much on the initialization, as also suggested by the experiment in Fig. 3. Therefore, to validate such a claim the authors should simply include a baseline model where DiffStride is applied with shared striding parameters for the time and frequency axes.
Fig. 4 would be much clearer if on the x axis MACs or FLOPs were represented, instead of the regularization term. This would help quantify how much computation DiffStride can save without losing significant performances. As it is, the value of the regularization term dos not tell much about actual computational cost.
To my understanding, within experiments the authors substitute a few downsampling layers (not even all of them) in a network with DiffStride. As a reader, I would have expected that every convolutional layer would be equipped with its own learnable striding. I grasp that this may not be practical but I don't have a clear view of the reason. The authors should consider adding some motivation behind this choice.
Another point that may be worth discussing as an extension/future work. May the DiffStride technique be employed within a conditional computation framework, by predicting the striding parameters conditioned on the current example? I think that this strategy might succeed especially when aiming at optimizing the computational cost of a model. I do not expect such an extention to be implemented and tested within the rebuttal period. I would just like the authors' opinion about that (potential, concerns etc).
In Tab. 1, the Speaker Id dataset provides no information whatsoever, as all methods score a perfect accuracy of
100.0±0.0
. I suggest the authors to remove this dataset, as it is apparently beyond trivial, and including it downgrades the quality of the experiment rather than increasing it.
On CIFAR-100, the authors show that SpectralPool is significantly better than strided convs even with the same downsampling hyperparameters. Can the authors provide an intuition for this behavior?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Learning Strides in Convolutional Neural Networks","Keywords: Strides, Convolutional neural networks, Downsampling, Spectral representations, Fourier","Positives
Overall I feel that the paper presents a really neat idea well. Besides a few minor issues (see below) the paper was enjoyable to read and describes the ideas well. There is thorough experimentation on a range of tasks and model architectures that demonstrate the power of the proposed approach.
Concerns
Really, the only major concern I have is around the rather limited discussion of the limitations. In particular, I'd at least like some discussion (or thoughts) on why it doesn't improve densenet performance, and I'd like to see experimental results presented on the impact of the use of DiffStride during training and inference, both in terms of measurable memory usage and impact on training & inference wall time. Obviously such results can be caveated with a statement that the implementation could be improved as per the existing discussion in the limitations section.
Minor points
Please check and fix algorithm 1 - I believe line 4 should be using the output from the filtering with the mask, rather than the raw FFT result. It would also be better not to reuse the
y~
symbol on lines 4 and 5, and it would also be helpful if the symbols (the variants of the intermediate computations
y
,
y~
}) were clearly labelled on Fig 1.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Learning Strides in Convolutional Neural Networks","Keywords: Strides, Convolutional neural networks, Downsampling, Spectral representations, Fourier","The paper is well-written and the proposed formulation is sound.
Also extensive experiments are provided on several datasets to demonstrate the benefits of the proposed method.
Authors demonstrate that the proposed method can recover from different initial strides and learn the optimal one. However, my most important concern is the lack of appropriate comparisons with neural architecture search approaches, since - in my view - these are the most direct competitors of the proposed method. Therefore, I would expect experiments and appropriate discussions between the cost and benefits between the proposed method and neural architecture search approaches.
The proposed method seems to target only residual architectures. Discussion on how the proposed method could be used with other architectures would be beneficial
Also, providing experiments with different architectures (apart from resnet) on the same dataset would also further improve the confidence on the obtained results.
What is the actual overhead of the using the proposed method in real applications? How much does the training time and memory usage increases?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Understanding over-squashing and bottlenecks on graphs via curvature","Keywords: Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature","Overall the paper is well written, however, the paper improve further to have better readability. It would be helpful to have more intuitive clarity on the use of hyperbolic spaces since many readers may not be familiar with the topic. Throughout the paper, homophilic and hetherophilic nature of graphs are mentioned, however, the exact definitions of them are not clearly defined. Further, in experiments “low-homophily” is mentioned, but there is not measure for low or high homophily. I suggest that the authors provide a proper definition for homophilic/heherophilic as [1,2] and provide numerical measures for datasets used in experiments as in [1]
The main weak point of the paper is experiments. First of all, is it fully-supervised node classification of semi-supervised node classification? The experiments are not substantial in terms of dataset selections, comparisons with baseline methods and obtained accuracy.
a) The selected baseline methods are limited, can the authors use further baselines methods? Can the proposed graph rewiring method be compared with the +FA method in [1]?
b) To my knowledge Cora is not a homophilic dataset [1]. It would be helpful if more datasets from hetherophilic graphs (Citeseer, Pubmed) are also considered in experiments, which would allow us to have a better understanding of the over-squashing phenomenon.
c) Cornell, Texas and Wisconsin are small scale datasets. I suggest to experiments with Chameleon or/and Squirrel dataset, which have large number of nodes and dense graphs.
d) Why are accuracies of Cornell, Texas, and Wisconsin low? There are methods that have shown greater accuracy for these datasets [1]. I wonder whether the low accuracy is due to the use of GCN and if a different GNN method is used in with the proposed rewiring method the accuracy would increase. Do authors have any experience with other models?
References [1] Eli Chien and Jianhao Peng and Pan Li and Olgica Milenkovic, Adaptive Universal Generalized PageRank Graph Neural Network, International Conference on Learning Representations (2021)
[2] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. International Conference on Learning Representations (2019)","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Understanding over-squashing and bottlenecks on graphs via curvature","Keywords: Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature","Detailed comments:
A critical aspect of rewiring is its effect on the structure and topological properties of the underlying graph. This is not captured by just analyzing the number of added/ removed edges across the graph. I think that it would be more useful to analyze graph characteristics (such as the node degree distribution) or to measure the distance of the original and the rewired graph globally, e.g., with a transportation distance.
Can you comment on the cost of curvature-based rewiring vs. random walk-based rewiring? If curvature-based rewiring is less efficient than random walk-based rewiring, is this (in your experiments) mitigated by a reduced cost in the downstream task (due to the smaller number of edges).
In section 4 you briefly remark that curvature-based rewiring may reduce over-smoothing. I think it could be interesting to expand on that.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Understanding over-squashing and bottlenecks on graphs via curvature","Keywords: Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature","Strengths
The paper provides good intuitions to over-squashing. This intuition is visualized in Figure 1 that gives a good intuition of over-squashing and of the proposed SDRF solution.
Further, the paper provides a good connection to graph curvatures that allows to measure and analyze over-squashing.
The paper provides a thorough analysis using the proposed notations and connects it to further geometric theoretical and classical ideas.
The paper provides a good conceptual comparison to random-walk-based rewiring and whether this can address the GNN bottleneck, and also an interesting discussion about the likelihood of GNN bottlenecks in homophilic and heterophilic datasets.
The evaluation shows that the proposed SDRF method outperforms the DIGL baseline without a significant increase in the amount of computation compared to the original graph structure, while the DIGL baseline adds an order of magnitude more edges, which increases the computation cost.
Weaknesses
Generality - I am not sure how do these results generalize beyond Graph Convolutional Networks (GCN). The use of GCN's augmented normalized adjacency matrix is assumed at the beginning of the paper, and this assumption is never questioned. Are Lemma 1 and Corollary 2 correct only for GCN, where it is common to use the augmented normalized adjacency matrix? Specifically, this matrix has
D−1
which decays messages over paths. What happens if nodes are summed (as in GIN)? How does the signal then decay with the number of layers? Intuitively, I can understand that summing an exponentially growing number of nodes in a single vector cannot compress all the information after some compression steps (that is, GNN layers). But how is this expressed in the curvature theory? If the analysis in the paper refers only to GCN and GNNs which perform averaging, please mention this explicitly.
Writing - as is, the paper can be perfectly understood only by audiences that are very familiar with all related work. The paper can be significantly improved by giving more background, explaining equations more intuitively, and avoiding unneeded citations which confuse the reader. For example, the paper uses the names such as ""Ricci"", ""Poincare"", ""Ollivier"", and ""Forman"" frequently. These references can confuse audiences who are not familiar with all terms and papers. Specifically, calling the main method ""Balanced Forman"", gives the reader the feeling that they would not really understand the proposed method without understanding the original Forman first. I believe that this is not the case, and that the paper could be standalone. While the authors show an impressive understanding and familiarity with the related and classical work, I advise the authors to avoid this ""name-dropping"", in favor of better readability and accessibility to a broader audience. I even think that some parts of the paper can be moved to the appendix, in favor of the readability of the remaining parts. For example, I am not sure that Corollary 2 is new (see below), and I'm not sure whether this is directly related to the point the paper tries to make, or whether is it a side-result. See more specific details below.
The Evaluation could also be strengthened by addressing more datasets and more GNN types. See more details below.
Reproducibility - please provide more training details, hyperparameters tuned and ranges, and the final selected hyperparameter values for the experiments in Section 5. A table of dataset statistics (in the appendix) can also help, before and after applying every type of preprocessing method (similarly to Table 3, possibly with absolute numbers as well).
Writing
Section 2.1: what is ""
(i,j)∈E
if
i∼j
""? What does
i∼j
mean? Does it simply mean that there is an edge between them? If it only means that there is an edge between them, why is this a different notation than
(i,j)∈E
?
What is
a^
in Equation (1)? I cannot find its definition. Is it
A^
? It is unclear from the text.
Section 2.1:
""
dG
is the standard minimum-walk (geodesic) distance on the graph""
Is it simply the shortest-path? If so, I suggest using this term which is much more common to the broader audience. If not, the term ""minimum-walk (geodesic) distance"" should be better explained.
Section 2.1:
""the node features and representations are assumed to be scalar from now on""
that is,
p0=1
and all
pl
equal to 1?
Corollary 2 - I think that the ""Jumping Knowledge"" paper (Xu et al., 2018) had already recognized this role of self-loops. What is the difference between this corollary and Theorem 1 in Xu et al. (2018)? Also, as the role of self-loops is not directly connected to the main contributions of this paper, I suggest considering moving it to the appendix, or removing it if it was already recognized by Xu et al, in favor of more space for explanations of other parts.
Section 3, definition of
λmax
- what does ""traversing the same node"" mean? the same
k
node from the previous definition of 4-cycles? A formal definition of
λmax
will be helpful.
Section 3, the example describing Figure 3: I cannot understand the examples because in (ii),
♯◻i
is defined for a specific edge
♯◻i(i,j)
, and here it appears as
♯◻0
and
♯◻1
Also, why is
♯◻1=5
, and not also the node 0 is included, as it also creates a 4-cycle (1-0-3-5)?
Corollary 4: what is ""the volume of the geodesic balls"", and what does growing ""polynomially"" exactly mean? How is the volume measured, and what does it grow over? (what is the ""x axis""?)
Theorem 5 - it would help if the authors could clarify this theorem, explain its meaning in words and intuitively. I am not sure I understand why every detail there is necessary, and what are its implications.
Cheeger constant (Equations (5)+(6)) - I did not really understand the notations in Eq. (5), what is the meaning of Equation (6), what is its significance?
Evaluation:
It would really improve the evaluation if the authors could experiment with more GNN types and more benchmarks. Specifically, if the authors could take the datasets and exact settings used in Alon & Yahav (2020) and show how the proposed SDRF method improves the results there. The reason that this would be helpful is that Alon & Yahav showed that these datasets already suffer from over-squashing, by taking existing source code of other papers and improving their results. It would be interesting to compare SDRF and the (computationally expensive) solution of Alon & Yahav in terms of the tradeoff between accuracy and computation cost (the overall number of added edges).
DIGL is compared to SDRF as the main baseline. However, I couldn't find what exactly does DIGL do. The paragraph about random-walk-based rewiring on page 7 does explain the general idea, but it is unclear whether this is explaining directly about DIGL, or in general about PageRank-style approaches?
Additional questions to authors
According to this analysis, in the authors' opinion, are over-squashing and over-smoothing the same thing or not? What is the difference between them? (The answer to this question will not affect the rating negatively, as it is obviously out of the scope of this paper) If the authors have an insightful explanation, I recommend including it in the body of the paper.
Other minor comments:
The paragraph about Discrete curvatures on graphs on page 4 is completely unclear for the audience who are unfamiliar with the described curvatures. I suggest delaying this to later or even moving to an appendix in favor of more text that will help clarify the rest of the paper.
Theorem 5(i) writes
ℓ∈[0,L−1]
. For clarity, I suggest being consistent which previous notations, for example as Lemma 1 which denotes
0≤ℓ≤...
.","10: strong accept, should be highlighted at the conference","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Understanding over-squashing and bottlenecks on graphs via curvature","Keywords: Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature","The paper is well-written and well-organized. Formalizing the concept of over-squashing is interesting and timely, and is of interest to the field. This paper, as is often the case when one delves into novel grounds, makes somewhat arbitrary choices in order to move forward: the definition of over-squashing is somewhat arbitrary, the choice of working with that particular definition of Ricci curvature could be more motivated. Efforts are however made in an interesting direction, and the several discussions around the Cheeger constant gives depth to the paper, and its main messages.
minor: Appendices F and G are announced at the beginnning of the Appendix section but I did not have them in my pdf document","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme","Keywords: speech, voice conversion, diffusion models, stochastic differential equations","Post rebuttal
My initial review was largely positive, but for a few issues with the motivation
what does the MLE solver give over the usual Euler-Mayurama integration setup?
some checks on the formulation and intuitions behind proof.
The authors have sufficiently answered my concerns:
The MLE solver gives an optimized trajectory for data, even though the overall scheme is still first order, as shown through a simple example. The authors argue that while second (or higher) order schemes are very much possible, they would need more function evaluations and therefore not necessarily faster (this point could use another demo).
The formulation as far as I can see is correct, as shown in the author rebuttal. Intuitions behind proof are also summarized.
I think a lot more work could be done in exploring diffusion probability models on the lines of this paper, in regards to numerical integrators and formulation setup.
I am raising my score.
Initial Review
I reviewed this paper with great interest. It is rather remarkable how much progress has been made in the last few years in speech modeling. It is also interesting that the newer models (and this is one of them) compute the input-output alignment differently from the attention mechanism used in works such as Tacotron. Likewise, it is inspiring that generative modeling research (e.g. flow based, and DPMs) finds application in speech models quite rapidly, and with telling practical use (e.g. WaveGlow vocoder).
Strengths
Builds upon current work in DPMs (Grad-TTS, Wave-Grad) to come up with a setup for voice conversion.
Uses a sensible scheme involving computing an 'average voice' into which speaker information is added and output voice is decoded.
Paper contains a good review of existing work, and it is possible to piece together developments in score based modeling and adaptations in speech.
Maximum Likelihood solver formulation for reverse SDE is well motivated and gives good results. It produces a first order scheme - not very different from Euler-Mayurama - with three additional hyperparameters. Supposedly, this results in an improvement in sample quality through optimized likelihood.
Weaknesses
I found the mathematical derivations not very clearly explained. The development is generally easy enough to follow if we work out the slightly tedious derivations. However, I think it would benefit from a few lines of 'intuition'. What is the intuition behind the derivations, why does it work and in what types of SDEs does such a procedure work?
There seems to be an inexactitude (or mistake) in the derivations (equation 40). I think we get
kt,h∗=1+k^t,h
(i.e. the original derivation needs an extra 1 on the right hand side).
The motivations behind the MLE derivations aren't very clear (at least to me). Is it that since we maximize likelihood, the setup is more optimized and therefore produces better samples, or are we talking about being able to take 'larger' step sizes (smaller N)?
As such, it seems to me that the scheme is O(h), and should not work well when timesteps are large. It would be nice to see some explanation or analysis of why the setup converges in such a small number of iterations. I would expect a second order scheme to converge faster. Likewise, a predictor-corrector scheme (as in the stem SDE works by Yang Song) improves performance.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme","Keywords: speech, voice conversion, diffusion models, stochastic differential equations","Strengths: (1) A diffusion probabilistic model-based voice conversion method has been proposed for the one-shot voice conversion scenario.
(2) The proposed method can generate high-quality converted speech compared to state-of-the-art approaches. (3) To solve the disentanglement problem, a new approach in which the encoder predicts the ""average voice"" is proposed. This idea is interesting and novel. (4) For the decoder part, the diffusion-based method is adopted which reconstructs the converted voice from the ""average voice"". (5) To solve the problem of slow inference of the diffusion probabilistic model, a novel inference scheme is developed which significantly reduces the number of iterations. (6) A novel stochastic differential equations solver, named maximum likelihood SDE solver, is proposed with theoretical analysis and empirical studies. (7) A lot of experiments have been conducted to validate the effectiveness of the proposed method. (8) Besides the experiments on VC, the maximum likelihood sampling scheme is also validated by the CIFAR-10 image generation task.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme","Keywords: speech, voice conversion, diffusion models, stochastic differential equations","This paper is well-written and remarkable on many aspects:
A novel SDE solver than can be of interest for anyone interested in Diffusion models is proposed. This solver is provably better than Euler-Maruyama discretization in the setting of a small number of diffusion steps,
The idea to condition the diffusion model on the ""average voice"" is novel and fully exploited by the proposed architecture,
There are extensive experiments that demonstrate the relevance of this approach,
A website showcasing many voice conversions is available and the examples are convincing,
The relevant literature is properly addressed,
Code will be available.
Small remarks:
p. 2: Chen et al. 2021a also reported results (for vocoding) using only as few as 6 diffusion steps.
""most of them lack theoretical grounds"" may be a bit harsh.","Same potential harmful applications as any voice conversion method.","10: strong accept, should be highlighted at the conference"
"Resolving Training Biases via Influence-based Data Relabeling","Keywords: Training bias, influence functions, data relabeling","I believe the paper is very well-written and structured. I appreciate that the authors had taken time and consideration into writing an abstract and introduction that clearly motivates the problem in hand, gives enough background into the problem, and that clearly explains the solutions and the experimental results.
The paper seems to be solidly based in theoretical proofs of their methods, together with experimental results comparing it to some baselines plus the state-of-the-art approach. I liked that the limitations are clearly explained in the appendix.
All in all I think is a good paper.
Regarding weaknesses of the paper that I believe could help at improve the paper if addressed.
First, is not until the reader reaches section three that the reader realizes that the authors use influence analysis on the validation set rather than on the test set. I think that the authors should improve the consistency across the paper of using the term ""test set"". Throughout the introduction, abstract, background, and part of the methodology, the validation set is referred as the test set. Which then is clarified to be the validation set, since the test set is only used for a proper test evaluation (without altering the training set). I would suggest to either clarify that at the beginning, or simply use validation set instead of test set.
Algorithm for RDIA such as done for RDIA-LS (Algorithm 1) in the Appendix E. I believe this will help portray more details for further reproducibility.
Regarding reproducibility, the authors mention that the code is available in the Appendix. Do the authors mean the programming code? If so, I couldn't find a link there. A link with the source code would be beneficial for the same reasons as the point above.
My last and probably the most relevant concern is the following. Imagine we have an imbalanced dataset, and that the algorithm is not able to classify well that portion of the dataset with fewer examples. The problem in this case is that we don't have enough data of some particular group of instances, needing some sort of solution like data augmentation (for example). However, if we would apply the authors solution (or the other authors solution for that matter) we would be treating these examples as incorrect. In the authors case the positive aspect is that the examples won't be removed (as with some of the others solutions). But, how would relabeling those examples help the model learn about this specific category of instances? I am thinking on the lines of a dataset where we have sensitive attributes e.g race, gender. Where some of the people are less represented. From my perspective this solution might have a negative impact in the generalization aspect since the model might not really learn those people that might look different from the rest. Or it could be the case where it helps at mitigating this bias. This comment is more of an opening for a discussion with the authors, rather than ""something to fix"" . It would be great if the authors could just comment what they think on this matter during the discussion period. Since I am sure they have thought about this aspect, and I wouldn't want to have missed or misinterpreted some part of the paper. Just to clarify, my scoring hasn't been influenced by this last comment.","Not sure if data resampling can have a negative impact and be a fairness concern if we are dealing with imbalanced datasets where the datasets represent people where some minorities are less represented in such datasets. I have posted this as part of a possible discussion with the authors, to see what they think.","8: accept, good paper"
"Resolving Training Biases via Influence-based Data Relabeling","Keywords: Training bias, influence functions, data relabeling","The manuscript is well-written and clear. The proposed solution is simple and clean with a sound theoretical justification.
The practical utility of the approach is unfortunately limited given the cost of estimating influence functions on non-linear models. This is a well-known problem, especially when applying influence function to deep architectures. The authors address the problem by proposing to replace influence functions with training loss while retaining their relabeling scheme. I wonder how this simple strategy works in comparison to their RDIA approach (the appendix does not report comparisons in terms of accuracy), and under which conditions it could fail (e.g. distribution shift rather than noise). Otherwise one has the (probably false?) impression that all you need is training loss + relabeling.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Resolving Training Biases via Influence-based Data Relabeling","Keywords: Training bias, influence functions, data relabeling","The paper uses influence functions from robust statistics to first identify harmful instances and then relabel them based on a novel relabeling function using influence approximation. Influence estimations have been widely used to identify harmful instances or understand the impact of training samples. This paper goes one step further and uses this analysis to relabel the instances in order to achieve better generalization and lower test error. While the technical novelty is limited as the proposed formulations are extensions of existing influence functions and application is interesting. The authors use their relabeling strategy on multiple datasets and models (including deep models).
Pros:
(1) The paper does a focused job of using influence function for identifying harmful examples and fixing them by relabeling. While in the recent times there are papers on influence functions to identify harmful instances, this paper does a very good comprehensive and focused study compared to others.
(2) The paper is well-written and easy to follow. Related works is well laid out and well covered.
(3) Experimental section is complete with experiments on deep models and the proposed RDIA-LS. The authors acknowledge the limitations of RDIA on deep models due to erroneous influence estimates for deep models and provide a workaround for it in the Appendix. This is an improvement from the earlier versions of the paper (from a previous conference). I would definitely like to highlight this section in the main paper as it’s important for all practical purposes.
Cons:
(1) The technique of using influence function for identifying harmful instances is not new and well known and applied in the recent times. Hence I feel the technical novelty is not solid and limited in some aspects. However saying that, applications of existing influence functions are not straightforward and hence that’s one point to be noted.
In general, the paper is well-organized, a good study on influence based relabeling and has sufficient empirical studies to backup it’s proposed formulation.","N/A","6: marginally above the acceptance threshold"
"Resolving Training Biases via Influence-based Data Relabeling","Keywords: Training bias, influence functions, data relabeling","Strength
The authors empirically demonstrated that the label correction is much more effective than the standard sampling/data reweighting-based approach for model improvement. This result is coherent with the one previously reported in [Ref1] where a very similar data relabeling approach was studied. The results on DNNs (on MNIST and CIFAR10) are new where [Ref1] considered only kernel-based models.
[Ref1] Training Set Debugging Using Trusted Items, AAAI 2018.
I am not the author of [Ref1].
Weakness
The proposed method is very similar to the one proposed in [Ref1], while the current paper misses this important prior work. [Ref1] formulated the data relabeling problem as the bi-level optimization, and proposed an algorithm to optimize the amount of relabeling using gradient descent. An important point here is that the gradient of the amount of relabeling considered in [Ref1] is essentially identical to the relabeling criteria proposed in (10) in this paper (see below for the detail). In addition, once we interpret the proposed label correction criteria as the gradient descent update, Lemma1 and Theorem1 confirming the decrease of the validation loss are more or less trivial (unless the amount of correction is too large). In summary, although the ways the amount of relabeling is optimized are different (using gradient descent or by one step update), I believe the close connection between [Ref1] and the proposed method needs to be described appropriately in the paper, e.g., through detailed discussions and experimental comparisons.
Close connection to [Ref1]
As the special case of the formulation of [Ref1], we can obtain the following bi-level optimization (here, I removed some terms from the original formulation in [Ref1] for simplicity, and used the notation of the current paper).
minδ∈RnL(Q;θ^δ):=1M∑j=1Mljc(θ^δ), s.t. θ^δ=argminθ∈Rp1N∑i=1Nli(ziδi,θ),
where
δ
is the amount of relabeling. This is the optimization of the amount of relabeling so that the the validation loss to be minimized. Following [Ref1], we can derive the gradient of the validation loss
L(Q;θ^δ)
with respect to
δi
as
dL(Q;θ^δ)dδi=∇θL(Q;θ^δ)dθ^δdδi=−1N∇θL(Q;θ^δ)⊤Hθ^δ−1∂li(ziδi,θ^δ)∂δi.
When there is no label correction,
δ=0
and we have
θ^0
.
Then, if we consider relabeling the
i
-th training instance with the gradient step size
ϵ
, we have the update
δi←0−ϵdL(Q;θ^δ)dδi|δi=0
and the decrease of the validation loss induced by this update is
L(Q;θ^δ)−L(Q;θ^0)≈ϵdL(Q;θ^δ)dδi|δi=0
=ϵN∇θL(Q;θ^0)⊤Hθ^0−1∂li(ziδi,θ0)∂δi
≈1N∇θL(Q;θ^0)⊤Hθ^0−1(li(ziϵ,θ)−li(θ)).
The last line is nothing but the criteria (10) proposed in this paper.
After Discussion with Authors
I conducted an experiment on breast cancer dataset by myself (see below). There, I confirmed that RDIA does better than the One-Step GD update of [Ref1]. I therefore decided to increase my score with a strong expectation to the authors for referring [Ref1] appropriately in the main body of the paper. As I mentioned in my original review, this is not the first study considering relabeling based on the influence function-like technique. Recalling that influence function is one specific example of implicit gradient, [Ref1] is the first work in this direction to my knowledge (even if [Ref1] does not describe the fact explicitly). As I demonstrated in my code, the simplest version of [Ref1] with only one-step update (and without any human intervention) does good job. I strongly expect the authors to pay certain respect to [Ref1] and do not underestimate their technical contribution. For example, the current writing in Appendix H seems to be not appropriate. Even without human intervention, DUTI [Ref1] can do the same (i.e., relabel the training bias towards better model performance) as the proposed approach (but with slightly inferior performance).
(2) The learning task of DUTI is to debug the training instances which may contain the wrong labels and predict the true labels. While the target of our approach is to relabel the training bias towards better model performance. In this way, only our approach could relabel the biased training samples with correct labels towards better model performance.
The results on breast cancer dataset
Test Accuracy
ERM One-Step GD RDIA
0 0.959 0.96 0.961
0.1 0.96 0.968 0.962
0.2 0.953 0.951 0.959
0.3 0.931 0.944 0.957
0.4 0.842 0.897 0.958
0.5 0.563 0.762 0.958
0.6 0.178 0.782 0.953
0.7 0.075 0.875 0.953
0.8 0.05 0.931 0.949
0.9 0.037 0.95 0.94
Test Loss
ERM One-Step GD RDIA
0 0.141025 0.161033 0.13819
0.1 0.237151 0.24697 0.146115
0.2 0.345921 0.347718 0.157343
0.3 0.453018 0.451272 0.171787
0.4 0.56437 0.557865 0.185989
0.5 0.698697 0.63033 0.197167
0.6 0.876071 0.590184 0.203124
0.7 1.08732 0.492453 0.196512
0.8 1.44151 0.362804 0.197389
0.9 2.0108 0.265325 0.241273
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from IPython.display import display
def sigmoid(u):
return 1.0 / (1.0 + np.exp(-u))
def logloss(w, b, x, y):
p = sigmoid(x.dot(w) + b)
return - (y.dot(np.log(p)) + (1 - y).dot(np.log(1 - p))) / y.size
def logreg(x, y, lam=0.1, lr=0.01, max_itr=100):
num, dim = x.shape
xo = np.concatenate([x, np.ones((num, 1))], axis=1)
w = np.zeros(dim+1)
for itr in range(max_itr):
p = sigmoid(xo.dot(w))
g = (p - y).dot(xo) / num
g[:-1] = g[:-1] + lam * w[:-1]
w = w - lr * g
return w[:-1], w[-1]
# experiment
lam = 1e-2
acc, loss = [], []
nr = np.linspace(0, 0.9, 10)
for noise_rate in nr:
acc_n, loss_n = [], []
for seed in range(10):
# breast cancer data
# train / val / test = 300 / 169 / 100
x, y = datasets.load_breast_cancer(return_X_y=True, as_frame=False)
x, xte, y, yte = train_test_split(x, y, test_size=269, random_state=seed)
xval, xte, yval, yte = train_test_split(xte, yte, test_size=100, random_state=seed)
# normalize features
scaler = StandardScaler().fit(x)
x = scaler.transform(x)
xval = scaler.transform(xval)
xte = scaler.transform(xte)
# noisy label in training
np.random.seed(seed)
flip = np.random.rand(y.size) < noise_rate
y = np.logical_xor(y, flip).astype(int)
# fit logreg
w, b = logreg(x, y, max_itr=500, lam=lam)
# test accuracy / loss
zte = (xte.dot(w) + b > 0).astype(int)
acc_te1 = np.mean(yte == zte)
loss_te1 = logloss(w, b, xte, yte)
# influence function
num, dim = x.shape
xo = np.concatenate([x, np.ones((num, 1))], axis=1)
p = sigmoid(x.dot(w) + b)
H = xo.T.dot((p * (1 - p))[:, np.newaxis] * xo) / num
H = 0.5 * (H + H.T) + np.diag([lam] * dim + [0]) # Hessian
g = ((p - y)[:, np.newaxis] * xo) / num # gradient
f = - np.linalg.solve(H, g.T).T # influence function
# label gradient over validation loss
xv = np.concatenate([xval, np.ones((xval.shape[0], 1))], axis=1)
q = sigmoid(xval.dot(w) + b)
gv = (q - yval).dot(xv) / xval.shape[0] # gradient of validation loss
gy = f.dot(gv) * (np.log(p) - np.log(1 - p)) # label gradient
# label correction by one-step gradient descent
ynew = y - 1e+3 * gy # step size of GD = 1e+3
ynew = np.minimum(1, np.maximum(0, ynew)) # clip label
wnew, bnew = logreg(x, ynew, max_itr=500, lam=lam)
# test accuracy / loss after label correction
zte = (xte.dot(wnew) + bnew > 0).astype(int)
acc_te2 = np.mean(yte == zte)
loss_te2 = logloss(wnew, bnew, xte, yte)
# label correction by RDIA
ynew = y.copy()
#ynew[f.dot(gv) > 0] = 1 - ynew[f.dot(gv) > 0] # flip label
ynew[f.dot(gv) > 1e-3] = 1 - ynew[f.dot(gv) > 1e-3] # flip label
wnew, bnew = logreg(x, ynew, max_itr=500, lam=lam)
# test accuracy / loss after label correction
zte = (xte.dot(wnew) + bnew > 0).astype(int)
acc_te3 = np.mean(yte == zte)
loss_te3 = logloss(wnew, bnew, xte, yte)
acc_n.append((acc_te1, acc_te2, acc_te3))
loss_n.append((loss_te1, loss_te2, loss_te3))
acc.append(acc_n)
loss.append(loss_n)
acc = np.array(acc)
loss = np.array(loss)
acc = pd.DataFrame(np.mean(acc, axis=1))
acc.columns = ['ERM', 'One-Step GD', 'RDIA']
acc.index = nr
loss = pd.DataFrame(np.mean(loss, axis=1))
loss.columns = ['ERM', 'One-Step GD', 'RDIA']
loss.index = nr
print('Test Accuracy')
display(acc)
print('Test Loss')
display(loss)","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Representational Continuity for Unsupervised Continual Learning","Keywords: Continual Learning, Representational Learning, Deep Learning","Strengths
The submission is well written and easy to follow. The proposed concept is well motivated with various quantitative and qualitative justifications.
While many unsupervised/self-supervised training approaches require pre-training on massive unlabeled data, the proposed method here works well with the help from Mixup and does not require additional pre-training set, largely making it more applicable to real-world use cases.
I especially enjoy reading Sec 5.3 regarding the analyses on feature similarity between different learning approaches, visualization of feature space, and loss landscape visualization. This section provides additional justification besides the absolute accuracy improvement over the supervised continual learning counterparts.
Weaknesses
I think the main limitation of using the proposed pipeline in practice is runtime and memory constraints. To run K-NN in a lifelong learning setting is challenging due to ever-growing storage requirements for storing samples coming from different stages. During inference, the algorithm also needs to compute distances between the query and many stored data points. Can the authors shed some light about the runtime and memory comparison between the proposed method and supervised counterparts?
It would be great to also compare with other recent supervised continual learning approaches as well such as [A1, A2]
[A1] Zhao et al. Maintaining discrimination and fairness in class incremental learning. CVPR 2020.
[A2] Liu et al. Mnemonics training: Multi-class incremental learning without forgetting. CVPR 2020.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Representational Continuity for Unsupervised Continual Learning","Keywords: Continual Learning, Representational Learning, Deep Learning","Strong Points
The paper takes one of the most import issues in CL: learning robust representation in unsupervised setting. For me, the problem itself is real and practical.
The paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of UCL and the proposed LUMP algorithm over SCL methods.
Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty, the method has been well motivated by pointing out the limitations in SOTA methods.
The authors provide code for reproducing the results in the paper.
Weak Points
The proposed LUMP algorithm is adopted from supervised mixup technique(Zhang et al, 2018). So the novelty is limited.
The authors conducted extensive experiments in task-incremental setting. It would be interesting to see how UCL and the proposed method perform in class-incremental and task-agnostic CL settings.
Fig 3: In general, higher layers have lower feature similarity than lower layers, and similarity between UCL models are higher than that of SCLs. However, there is an exception in Layer 4 of DER method -- the similarity of SCL is higher than that of UCL. It is worth some discussion on this exception.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Representational Continuity for Unsupervised Continual Learning","Keywords: Continual Learning, Representational Learning, Deep Learning","Strengths:
First the paper is well written and easy to read.
The experiments are rich and well-chosen to better understand the superiority of unsupervised representations in the context of CL. I especially appreciated the experiments in Fig 2 that investigate the impact of the size of the training dataset.
The conclusions are enlightening and will be very helpful to design new supervised or unsupervised CL methods.
The code is publicly available and looks clean and easy to use.
Weaknesses:
Some details are unclear:
Sec 5.1: Knn classifier: which set is used for NN? the replay buffer? validation set?
The Knn is used both for supervised and unsupervised experiments, right?
Sec 5.3: more explanations about CKA are required for the reader that is not familiar with this measure
I recommend changing the title. ""Rethinking the Representational Continuity"" is much too strong. The conclusions of the paper are great but it does not provoke a real rethinking of the problem.
I am not really convinced but the visualization in Fig4. It seems that LUMP has sparser activations. The shapes of the objects are more clearly visible in its feature map. Does it simply mean that it learns lower-level features (similar to edge detector)? Maybe a TSNE visualization would help to see how the features of old tasks are affected when learning a new task.
In Fig.5, we can notice that the range of value gets smaller in T19 (from [4.6,5.6] for T0 to [4.4,4.6]). Any idea why?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Representational Continuity for Unsupervised Continual Learning","Keywords: Continual Learning, Representational Learning, Deep Learning","Strengths
Rethinking continual learning with unsupervised representation learning is interesting, and empirical results indicate that most supervised continual learning methods can be improved by the proposed approach.
A bunch of experiments have been conducted to demonstrate the effectiveness of the proposed approach in various settings. And several visualizations have also been included for a better understanding of the learned features.
Weaknesses
[Missed Comparison with CURL]
Though the authors criticised that Continual Unsupervised Representation Learning framework (CURL) to be limited by digit-based gray-scale datasets, no direct comparison with CURL is done by following their evaluation protocol. Adding this result could better reveal the difference between the proposed method and CURL in terms of effectiveness.
Beside, the evaluation of cluster quality used in CURL seems to be an important evaluation metric in unsupervised continual learning, which has not been used in the paper.
[Degraded Performance of DER and MULTITASK] \ In the Table 1, we see that the proposed unsupervised continual learning can improve all baseline methods except DER and MULTITASK. A clear explanation about this performance drop should be added.
[Qualitative Analysis] \ Why the visualization of feature maps stops at
T13
, while the loss landscape visualization continues to
T19
? And in Figure 5, the difference between SCL-DER and LUMP is hard to interpret.
[Why Not Directly Use Unsupervised Learned Presentations]
As an important purpose of the unsupervised representation learning is to learn a powerful embedding space that can be quickly fine-tuned for latter down-stream tasks. Why don't we consider a baseline where the feature backbone is initialized with SimSiam or Barlow Twins, and directly fine-tune them on a sequence of tasks. This is probably not considered in the standard continual learning, but the results of this baseline could be informative to the community of both domains.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Vision-Based Manipulators Need to Also See from Their Hands","Keywords: reinforcement learning, observation space, out-of-distribution generalization, visuomotor control, robotics, manipulation","Strengths
Well motivated idea
Compelling results
Overall clearly written
Good ablations and the idea is shown for multiple algorithms and in a wide variety of settings
Interesting approach to generalize the hand camera to settings with a higher degree of partial observability
Weakness
zshift
is only explained in the appendix, it should be explained in the main paper.
The information bottleneck technique harms performance initially
Suggestions for improvement
In concurrent work, Szot et al (NuerIPS 2021) also used a hand/arm camera to learn manipulation policies and found similar trends. It may be worth citing as additional support for these finding.
How useful is the data-aug in DrQ for the hand camera? Part of the argument for DrQ is to reduce overfitting of the Q function during training. Perhaps with the hand camera you no longer need aug?
What about recurrent policies instead of the third person camera?
Szot et al: https://arxiv.org/abs/2106.14405","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Vision-Based Manipulators Need to Also See from Their Hands","Keywords: reinforcement learning, observation space, out-of-distribution generalization, visuomotor control, robotics, manipulation","Specifics about the training
84x84 RGB input images.
outputs 3D end-effector position relative to the robot base, 1D gripper width, and a boolean contact flag for each of two gripper fingers. No rotation.
Three learning algorithms: Dagger, DrQ, DAC.
The experimental set up to show that hand perspective is better than third perspective involves picking up a cube (which is somewhat trivial task and naturally favours hand perspective). My first impression was that it's certainly helpful to have hand perspective as it always gets a close up / zoomed-in view of the object free of any occlusions etc. so it makes sense that the hand perspective performs better. However, if you have a more complicated manipulator e.g. hands the self occlusions from fingers and as the hand starts to cage the object, you'd need a third person view to get a better view of the object. Secondly, I feel adding rotation in the end-effector target (which the current output parameterisation doesn't include) could make things even for third perspective too. Is there a reason why the network only outputs just 3D end-effector position? For this particular task I believe you don't need rotation but any general task you'd need to also parameterise rotations as rotations will bring occlusions for hand perspective too.
In the 6 meta-world tasks, they show that combining a third perspective together with information bottleneck regularisation leads to better generalisation. Though most (if not all) of these tasks involve decision making that has more to do with the object they are reaching to and less about doing any long horizon interaction with scene or other objects in the scene after interaction with the object they are reaching to.
The paper seems to suggest that a zoomed-in view of the object by using a hand perspective almost always helps which I agree with. It seems to highlight the design choices that we regularly make when doing manipulation with raw observations need to be carefully looked into. Although this is not the first paper to show that e.g. https://arxiv.org/pdf/2012.07975.pdf have also shown that adding hand perspective helps. This paper was also not cited.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Vision-Based Manipulators Need to Also See from Their Hands","Keywords: reinforcement learning, observation space, out-of-distribution generalization, visuomotor control, robotics, manipulation","The paper is well motivated, clearly written and coherently structured. The exposition of the main ideas is linear and easy to follow. The contribution is clear, well presented and well motivated. The notation and the formulation of the proposed method are clearly presented. Claims are supported by thorough experimental results. Figures and tables are presented in a nice and easily readable way, and help grasping the contribution of the study. Videos are also helpful in understanding the experimental setup and the results in a qualitative way. You mention tactile signals as a good example in robot manipulation of local streams of information. What is the applicability of your study to the tactile sensor modality? It may be that some of the tasks could be solved using proprioceptive information only - do you have results using proprioception only as your observation space? Comparing the results presented in the paper with results obtained with proprioception only would be instrumental to understand the effect/contribution of vision (especially in the case of O_{h+p})","N/A","8: accept, good paper"
"Meta-Learning with Fewer Tasks through Task Interpolation","Keywords: meta-learning, task interpolation, meta-regularization","Pros
The paper is well written and easy to follow.
The idea of interpolating tasks in a meta-learning setting is novel, intuitive and simple. Although previous work exists that augment the number of tasks, this is the first approach that augments across tasks (rather than within task).
The authors shows good result on different datasets, settings and backbones.
Cons
I feel like results in more “traditional” (and larger) FSL datasets are missing. For example, it would be nice to see results in tieredImagerNet or metaDataset.
I also feel that the authors introduce the method as being a general meta-learning approach, but only show results on image. classification/regression. It would be nice to see results in other domains such as RL/NLP/etc tasks.
I find the theoretical analysis difficult to follow and potentially not very informative to the rest of the paper (that been said, I am not an expert on generalization theory/Rademacher complexity and cannot properly validate it).","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Meta-Learning with Fewer Tasks through Task Interpolation","Keywords: meta-learning, task interpolation, meta-regularization","[Strengths] The work provides extensive theoretical analysis to provide theoretical guarantees as to how the proposed MLTI task interpolation method achieves better generalization. In contrast, previous methods that have employed common augmentation methods (e.g., label noise, CutMix, MixUp) without theoretical guarantees. The work introduces scenarios that are more challenging than standard benchmarks by limiting the number of meta-training tasks. The work provides extensive experiments across various datasets under such challenging scenarios and demonstrates better performance than previous methods, providing empirical support for the effectiveness of the proposed task interpolation method.
[Weaknesses] I believe the work has minor technical novelty compared to the related work by Ni et al [1]. In particular, Ni et al. [1] performs several augmentations for meta-learning, one of which is MixUp for tasks. Using MixUp between any given pair of classes, Ni et al. [1] also creates new tasks.
[Comments] In Related Work section, the work states that compared to work by Ni et al. [1], the proposed method directly desnifies the task distribution. But, doesn’t [1] effectively densify the task distribution, where a new task can consist of new classes that are constructed by using MixUp on pairs of classes? As such, I believe more discussions on this issue should help better differentiate the work from the related work. Why does the proposed method randomly sample a location where features are to be interpolated? Is there an ablation study on the sampled location? I wonder if this technique is what makes the proposed method perform better than other works. I’m curious as to whether the proposed method, without this technique, still performs better than other works. The ablation study on this would be helpful for better understanding of differences from other works. Also, how does it compare with related works on standard benchmarks, such as miniImageNet. I think that the proposed method should still work with a larger number of tasks and believe that these experimental comparisons can strengthen the contributions of the proposed method.
[1] Ni et al. Data Augmentation for Meta-Learning.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Meta-Learning with Fewer Tasks through Task Interpolation","Keywords: meta-learning, task interpolation, meta-regularization","strengths
Although nifty, the idea of pair-wise task interpolation is an incremental change over the existing data augmentation approaches. The theoretical results, highlighting the relationship between task interpolation and the Rademacher complexity, are non-trivial extensions of the Zhang et al. ICLR 2021 and Yao et al. ICML2021 to account for pair-wise task interpolation. I view this as the primary contribution of the paper.
The comparison against existing data-augmentation baselines for both metric- and gradient-based meta-learning approaches is quite exhaustive. Furthermore, MLTI is tested on a wide variety of datasets. While the improvement on each dataset is only marginal, the consistent improvement in all datasets and across all approaches strengthens the paper’s contribution.
The paper is well-written and easy to follow.
Concerns
Current approaches in meta-learning rely on heavier backbones such as ResNet. As the goal of all the meta-learning methods is to improve the model’s generalizability, I think it is fair to evaluate the effectiveness of MLTI with heavier feature extraction backbones. Such a comparison is relevant as the proposed task interpolation is conducted on the features extracted from some intermediate layer of the network.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Meta-Learning with Fewer Tasks through Task Interpolation","Keywords: meta-learning, task interpolation, meta-regularization","Strength 1. This paper proposes a novel task-augmentation method, which is affected by Manifold Mixup, which can be applied to many existing few-shot learning tasks. 2. The theoretical analysis shows that the proposed MLTI augmentation has a regularization effect and leads the meta-learner to have a better generalization capability. 3. Extensive simulation results on variety of few-shot learning datasets and two representative few-shot learning methods show that the proposed MLTI is highly effective for meta-learning with fewer data.
Weakness 1. Comparison with the prior methods in large dataset is missing. For example, in Table 3, the comparison results are provided only for small datasets or reduced version of large datasets. However, the proposed method is not restricted to small dataset. The ablation experimental result in Figure 2 shows that proposed MLTI is still effective when the full miniImageNet/DermNet dataset is used, although the performance gain becomes small when the full dataset is used. I suggest the authors to include the comparison of MLTI and prior methods with full size of miniImageNet and DermNet.
Question 1. In Section 3, the authors mentioned that it is intractable to calculate prototypes with mixed labels. However, in prior work on semi-supervised few-shot learning [1], the prototypes are computed using soft-labels. What happens if we compute prototypes using soft-labels as done in [1]? 2. Some additional studies on the interpolation layer would be helpful for understanding the proposed method. In Algorithm 1 and 3, the interpolation layer
l
is randomly chosen in step 7. What happens if we fix
l
instead of randomly sampling
l
for every iteration? In that case, how are interpolating at lower layer and interpolating higher layer different?
Typo: In last line of page 4, there is a typo (regularizaiton -> regularization)
[1] Ren, Mengye, et al. ""Meta-Learning for Semi-Supervised Few-Shot Classification."" International Conference on Learning Representations. 2018.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Meta-Learning with Fewer Tasks through Task Interpolation","Keywords: meta-learning, task interpolation, meta-regularization","I found the approach to be simple and relatively well explained, including ablations studies on large-point questions I had while reading, including its behavior and effectiveness for different sizes and number of classes in the original base dataset, as well as effects of inter- and intra- task interpolations.
The key difference between this work and MetaMix (Yao et al 2020) is incremental but important: MetaMix will run the inner loop on the unmodified support set only, and use a mix of support+query in outer loop comparison optimization, whereas this work interpolates support set in inner loop as well. This difference enables between-task interpolation which adds additional augmentation particularly in settings where few tasks can be drawn from the base data.
I didn't follow much of the theoretical sections in detail, and had to look at the appendix proofs to even understand some of the notation in the main text. In my somewhat limited understanding they seem reasonable. These claim to show a theoretical generalization improvement in simplified settings (binary classification of single layer model, linear protonet feautres).
Additional questions:
NLS: In addition to a single set of correspondence pairs, the input examples for each class can be mixed with all-pairwise-combinations. How many combinations are used? That is, for two sets of k examples {xs_i} and {xq_j} (i,j in 1..k), one can form k^2 interpolated examples {a xs_i + (1-a) xq_j} using each i,j combination. Are all of these combinations formed or just a single set of k pairings? If using more than k pairings, this would change the task from k-shot to k^2-shot; but the l-layer features for each of the k^2 combinations could be computed, and then up to (k^2 choose k) tasks could be selected from these and used in the upper layer loss comparisons. For k=5 that would increase interpolated pairs from 5 to 25, but potentially get up to 53130 upper layer loss comparisons from each task pair sample -- would this get even more benefit from this task augmentation technique?
eq 5: what does the name of the subscript ""cr"" mean (does it stand for something)?
It could be useful to have a more explicit explanation of differences with MetaMix. MetaMix will run the inner loop on the unmodified support set only, and use a mix of support+query in outer loop comparison optimization, whereas this work interpolates support set in inner loop as well. This is already mentioned at a high level (fig 1 caption and sec 5 last paragraph), but I think it could be even clearer by pointing out the difference in the discussion around eq 5, that support set H^s,Y^s in the inner loop is mixed between tasks, whereas in MetaMix, only the H^q,Y^q are replaced by mixing.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS","Keywords: representation bottleneck, representation ability, interaction, explanation","Strengths
The paper is well-written and easy to follow. In particular, Figure 1 and 2 support the concept of representation bottleneck reasonably well.
Two questions the authors raise about DNNs are rational, and prove convincingly that this phenomenon is a common problem of DNNs.
The idea of describing the representation of the DNN using multi-order interaction utility is good and easy to understand.
Weaknesses
The authors propose two losses to encourage/penalize DNNs to learn interactions of specific orders and show the results in Figure 4. This loss can stimulate interactions of specific orders well. High-order DNN seems to have been fully explained and experimented, but not on middle-order and low-order interaction. I wonder why the authors didn't explain it in detail.
The authors conduct several experiments, but these show similar results, making it difficult to gain new insights. And it is difficult to understand what Table 1 means and shows.
Although this paper shows good enough insight to others, it would be good to present directions for solving the problem of representation bottleneck or providing a clear problem definition for future works.
I know it’s due to the limitations of the amount, but putting too many formulas and terms into the sentence. Instead of including all the formulas in the sentence, it would be better to reorganize it and make it easier to legible.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS","Keywords: representation bottleneck, representation ability, interaction, explanation","[Strength]
This paper discovers a very interesting tendency (called representation bottleneck) in existing DNNs, i.e., a DNN is usually more likely to encode low-order and high-order interactions, but difficult to encode middle-order interactions.
This paper analyzes theoretically the reason behind the bottleneck. Moreover, the simulated results based on Theorem 1 well match the real cases, which validates the theoretical reason.
From the perspective of the bottleneck, the authors explore the difference between DNN learning and human learning, which provides the key idea of the proposed two losses.
To relieve the bottleneck phenomenon, the authors propose two novel losses to enable DNNs to learn interactions of any specific orders.
Experimental results demonstrate the effectiveness of the two losses, and show that middle-order interactions can provide useful information for classification.
[Weakness]
The explanation for the representation bottleneck is not intuitive enough.
There is a lack of discussion why the trained DNNs do not achieve a significant improvement than a normally trained DNN.
The experiments on adversarial robustness can not well support the conclusions, since they were only conducted on tabular datasets.
[Suggestions]
I would suggest the authors give an additional illustration of the intuition behind the proof for the bottleneck.
It’s expected to give guidance on how we can use the two losses to boost the classification performance.
It’s not clear why the output change in Eq.(5) can represent interactions of specific orders.
The authors are suggested to report results in terms of adversarial accuracy on more datasets such as image data.
The authors are suggested to clarify whether the proposed two losses will increase the time complexity.
After carefully reading the response, other reviews, and the revised version of the manuscript.
The added proof skeleton makes the theory more clear and intuitive.
New experimental results show a strong connection between representation capacity and interaction orders, which may inspire follow-up studies.
Overall, I think this work is insightful and may make a great impact on both training and explaining of deep neural networks. I have modified the score accordingly.","10: strong accept, should be highlighted at the conference","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS","Keywords: representation bottleneck, representation ability, interaction, explanation","The research topic of this paper is significant in understanding the representation power of deep neural networks, and is interesting to the entire community of deep learning. The writing of this paper is clear enough, and the main contents are easy to follow.
Strengths:
The definition of the relative interaction strength is elegant.
The theoretical and numerical analysis in Section 3.2 is novel and interesting.
The loss functions to encourage or penalize interactions of certain orders are also novel and very natural.
There are experiments showing the effects of these loss functions and connecting them to properties such as accuracy and robustness.
However, there are several concerns of this paper. The following are about theory.
Regarding Thm 1, how reasonable is the assumption that
∂∂WΔv(i,j,S)
is a Gaussian? At least some numerical validations should be made here.
Regarding the simulation in Fig 3, there is still a tiny gap between the blue and orange curves, which does not look like a random pattern. Can you explain where this gap comes from?
Regarding equation 5, there are two issues. First, it is confusing to reload the notation
Δv
. I would recommend using another symbol. Second, are you missing an expectation in this equation?
In equation 7, it is clearer to write
L+(r1,r2)
rather than
L+
. Simimlar in equation 8.
Regarding equation 9 and simulations in Fig 4, it is possible to use multiple
L+
or
L−
with different
(r1,r2)
pairs? If so, why are you not using it? I feel this can provide more flexibility and is therefore potentially more useful.
There are also concerns regarding experiments. In summary, the experiments are not extensive and cannot well support the findings.
Regarding accuracy, what can we read from Table 1 (left)? The numbers seem random.
Regarding structural representations, Fig 5 & 6 make some sense, but what about other masking methods?
Regarding robustness, Table 1 (right) tells us that penalizing low-order and boosting high-order interactions harm robustness. What about doing the opposite, i.e. penalizing high-order and boosting low-order interactions? Does it improve robustness?
In general, the experiments are not extensive. In order to better understand the effects of the proposed loss functions, there should be experiments carefully adjusting the
λ
's and
(r1,r2)
's so that we can extract general patterns of the effects.
In addition, I do not see any significant improvement by using the proposed loss functions. Are there scenarios where using the proposed loss functions outperforms standard training in any of the metric (accuracy, robustness)?
Finally, figures and tables are a bit small so hard to read.
Scores can be modified based on authors' feedback and additional experiments.
After rebuttal: I have carefully read the response and added experiments.
The revised theory with milder assumption is great.
The additional experiments are excellent and provide a much more complete picture of the proposed theory.
I have modified the score accordingly.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS","Keywords: representation bottleneck, representation ability, interaction, explanation","Strengths:
The novelty of the paper lies in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs.
The major strength of the paper lies in the extensive study of different neural networks and datasets to back the claim. Also, the effect of the proposed loss functions
L+
and
L−
on deeper networks like AlexNet, VGG-16, and ResNet-18/20 is quite interesting.
Suggestions:
The number of sets
S
s.t.
|S|=m
is given by
(nm)
. This function increases from
m=1…n2
and decreases thereafter till
n
. Thus, for achieving large
I(m)(i,j)
at
m=n2
, the model needs to capture dependence between the variables
i
and
j
for significantly higher number of contexts, compared to the cases when
m=3
(local context) or
m=n−2
(global context). One may not need the dependence to be large for all possible contexts in the intermediate context length case. Hence, can the authors give an example to explain why the increasing number of contexts in the intermediate contextual interactions doesn't necessarily lead to a small
I(m)(i,j)
always?
Since, the authors consider
v(S|x)=log⁡1−Pr[y^=y⋆]Pr[y^=y⋆
for computing
J(m)
(which depends on the probability scores of a data example), I believe the magnitudes of
v(S|x)
aren't necessarily comparable for different examples. Then, isn't the following definition of
J(m)
a proper relative interaction strength to measure?
𝕜
J(m)=Ex∈ΩEI(m)(i,j∣x)EkE|I(k)(i,j|x)|
The above interaction measure measures the strength of the contextual utility for each example.
In the adversarial robustness experiments, are the low-order and intermediate-order interactions more robust to adversarial attacks? It will be great to have some experiments for intermediate-order interactions, to possibly showcase the fact that neural networks are susceptible to adversarial attacks because of the cognition gap.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Sparse Communication via Mixed Distributions","Abstract: Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call ""mixed random variables.'' Our starting point is a new ""direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (""sample-and-project'’) and an intrinsic one (based on face stratification). We experiment with both approaches on an emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables.","This paper was a pleasure to read. The idea is simple and intuitive, and it addresses a recurring issue with commonly-used simplex-valued distributions, allowing to better model sparcity while avoiding diverging likelihoods in the presence of 0s. The paper is clearly written, and the mathematical exposition is formal and well presented. I also believe the idea presented in this paper will be easily used to propose new simplex-valued distributions and be valuable to the community: while the authors propose several instances of mixed distributions, one can easily think of potential alternatives.
My only complaint about the paper is that the experiments focus mostly on sparcity, and not on avoiding ill-defined log-likelihoods, which I actually believe is another benefit of the proposed distributions. For example, the distribution proposed in [1], which is discussed in the paper, addresses learning
p(y|x)
when
y
is simplex-valued in the presence of 0s in the data; which can be handled in a more principled way through mixed distributions. Finally, the experiments are carried out against the Gumbel-Softmax, but not more recent improvements upon it, e.g. [2,3,4]. Nevertheless, I believe the contributions of the paper are enough to warrant publication.
Minor things:
-Section 3.1, when defining a face, I believe ""A face P is any intersection of P with a halfspace such that none of the interior points of P lie on the boundary of the halfspace"" should be replaced by ""A face P is any intersection of P with a closed halfspace such that none of the interior points of P lie on the boundary of the halfspace"" for added clarity.
[1] The continuous categorial: A novel simplex-valued exponential family, Gordon-Rodriguez et al., ICML 2020
[2] Estimating Gradients for Discrete Random Variables by Sampling Without Replacement, Kool et al., ICLR 2020
[3] Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax, Potapczyski et al., NeurIPS 2020
[4] Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator, Paulus et al., ICLR 2021
========================================================================================================
UPDATE 1 AFTER REBUTTAL
========================================================================================================
I have read the author's rebuttal as well as the other reviews, and my opinion on the paper remains that it should be accepted. I particularly appreciate the authors adding the suggested additional experiment to an already strong paper.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Sparse Communication via Mixed Distributions","-''-","Strengths:
The technique in this paper is very solid. I believe the theoretical foundations built in this paper will be useful in the future research on mixed random variables.
The idea of this paper makes sense, and this paper is well written and easy to read.
The experimental results demonstrate the usefulness of the proposed framework.
Weaknesses:
My only concern is that whether ICLR is a proper conference to publish this paper, since it involves some concepts about measure theory and probability theory.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Sparse Communication via Mixed Distributions","-''-","Reasons to accept:
The paper is well-written and appears relevant to recent developments.
The motivation of the paper in relevance to bridging the continuous representations computed by neural networks and other machine learning models with the discrete representations that characterize humans is interesting. Table 1 nicely summarizes the contributions of this work.
The mathematical coverage also seems reasonable.
Extensive experiments on three different tasks, including an emergent communication benchmark. Supplementary material also provides qualitative examples.
Suggested improvements:
Mixed random variables are a standard topic in probability theory. Definitions 1,2,3 appear to be natural in this theoretical context and the same applies to Propositions 1,2. More references in the mathematical and probability literature would be helpful due to the coverage of mixed random variables and associated theory in this literature.
Update after rebuttal: I have read the reviews and the author's responses. This is solid well-written work, my review remains unchanged.","None","8: accept, good paper"
"Sparse Communication via Mixed Distributions","-''-","The authors are encouraged for their good work. Though they do try to introduce ""mixed variables"" (ie random variables consisted by both categorical and continuous data) there are a few unclear points: for example, they consider a (ie one) categorical variable with alphabet K. What is the case when there are more than one categorical points, with different alphabet (eg ordinal data with alphabet X) ? Also, what is the algorithmic complexity with respect to the number of observations, the number/rate of continuous and discrete outcomes and the number of variables (in the case of multivariate mixed variable? Finally, Table 1 is quite unclear to me.","6: marginally above the acceptance threshold","2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Finetuned Language Models are Zero-Shot Learners","Keywords: natural language processing, zero-shot learning, language models","Overall well-written with compelling results, this paper describes a new language model (FLAN) and shows how it improves upon the zero-shot task performance of previous language models such as GPT-3. While the paper is lacking some additional analysis, I am hesitant to recommend extremely compute-intensive ablations due the large size of the model (137B parameters).
Strengths:
Considers a reasonably wide set of 62 datasets; although the inherent arbitrariness in dataset clustering was listed as a limitation, the clusters look quite reasonable to me, and the removal of overlapping datasets (e.g., ""Reading Comprehension w/ Commonsense"") seems appropriate.
Results are better than a strong Base LM baseline, as well as existing state-of-the-art models (GPT-3)
Overall the approach is intuitive and conceptually compelling
Highly relevant to ongoing work on language modeling, prompt tuning, and zero-shot learning
Weaknesses:
From these experiments, it is unclear whether models are actually ""learning to follow instructions"" or just learning a very large space of tasks from the fine-tuning procedure. In other words, even though prompt variance is reported at inference time, the models could potentially perform just as well with nonsense or missing prompts during fine-tuning. As far as I can tell, no experiments that rule out this possibility exist.
Although qualitatively useful, the analysis in 4.1 does not conclusively show that the number of instruction tuning clusters aids performance, or that this trend is likely to continue with more clusters. Most of the gain could be acquired by tasks which are most difficult, or most similar to the heldout task, and this analysis cannot disprove such an interpretation. A proper analysis would consider more heldout tasks and permutations of training data, but presumably this is prohibitively expensive.
The paper is missing important details about hardware usage and training time
Some possible issues which might be resolved by the additional questions below
Additional Questions:
""For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset."" Do you have unique prompts for each dataset or only for each dataset cluster? Based on a cursory look at the supplementary material, I would assume the latter.
I didn't fully understand the justification for the OPTIONS token. Are the fine-tuned models successfully putting (almost) all of their probability mass on the corresponding options? How is the Base LM evaluated (if it's not fine-tuned, presumably it doesn't learn how to handle these options)?
Figure 6A: why does the untuned model see worse performance with more parameters?
Nits:
Figure 1 (Bottom) is possibly misleading, since AFAICT zero-shot FLAN underperforms few-shot GPT-3 on the majority of tasks
Not clear what ""turning the task around"" means for some tasks, or why this is a useful type of prompt diversity","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Finetuned Language Models are Zero-Shot Learners","Keywords: natural language processing, zero-shot learning, language models","Pros:
The problem addressed has high practical value: it tries to make large pre-trained language model more accessible to a range of NLP tasks. The ""instruction tuning"" idea will significantly reduce the cost for task-specific fine tuning, labeled data and prompt engineering compared to other approaches.
The method is simple and easy to implement. Authors carefully design the experiment to minimize the leakage between the fine-tuning and inference data. Given that, it still shows superior performance on different types of NLP tasks. The result on specific task can be further improved when adapting with ""prompt tuning"" on labeled data, which shows that the instruction-tuning process does not drop much task-specific knowledge from the original pretrained model.
The analysis presented in the main paper and the appendix is thorough enough. Authors also discussed about the limitation of model when downstream tasks are more similar to language modeling tasks.
Cons: There are still a few questions that can be addressed to make the analysis comprehensive.
Have authors try to use the FLAN prompts on GPT3 or BaseLM and how does the performance look like?
Since instruction tuning will adjust all the parameters in the original pre-trained language model, there is a question what about what is the potential impact of this tuning process? Will it drops any knowledge of any tasks, which will be a disadvantage when the task's labeled data is available? In the Analysis C in the appendix, it will be good to have results for tasks other than classification such as summarization or question answering; and also to have a baseline where the BaseLM model is fine-tuned directly with the task labeled data (without prompt/soft-prompt).","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Finetuned Language Models are Zero-Shot Learners","Keywords: natural language processing, zero-shot learning, language models","Detailed comments:
""For each dataset, we manually compose ten unique templates"": Why not have templates per task cluster instead of per dataset? it is likely a relatively minor effect given the results from Appendix B but it seems like it could slightly prevent overfitting
The ablation in 4.1 was great (number of clusters). Nit: I would have tried to move the (datasets per cluster/templates per dataset) ablation to the main body as well and shortened Section 3
The 4.2 (scaling laws) ablation is perhaps the most interesting of all.
In figure 6A, why was performance not increasing for untuned models w.r.t model size? This seems to contradict findings from Brown et al where larger models did better on essentially all tasks. Were there perhaps some poor datasets that happened to be in the held-in split (since the held-out tasks don't seem to have the same trend)?
Appendix:
I liked the section B ablations (as implied above).
That more templates per dataset didn't help is particularly interesting and suggests some questions. You hypothesize that more templates doesn't help because ""models at such scale do not easily overfit to a finetuning single task"" - but my intuition is for an opposite explanation -- that the models at such scale easily memorize a small number of templates! One may even wonder if the instruction nature of the templates is helping at all.
From what I can tell, Appendix C on prompt tuning (which is very interesting) is maybe the primary evidence the instructions are important. I think more could be done here, some ideas, probably there are better ways to test:
Have templates that leave out ""instructions"": I would guess it wouldn't affect held-in task performance much, but would affect held-out tasks.
Consider HellaSwag/PiQA/etc, where FLAN underperformed few-shot and even zero-shot. One might hypothesize that if using a (subotimal) template that is less natural for language modeling, that zero-shot performance would suffer, but that FLAN performance wouldn't
One might hypothesize that the ""turn the task around"" templates help more than the other more straightforward templates that don't swap information between the prompt and response.
Easy but probably not great thing to try: held-out tasks with wrong/useless templates
A final thought: It's not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset-specific spurious correlations. This could be another area to investigate
Misc:
UnifiedQA seems potentially worth citing as prior work","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Finetuned Language Models are Zero-Shot Learners","Keywords: natural language processing, zero-shot learning, language models","While the method is a simple and straightforward scaling up of concepts and ideas from prior works (e.g. Zhong et al, Adapting ...; Mishra et al, cross-task generalization ...), the empirical results are thorough and impressive (outperforming GPT-3 with a slightly smaller model). The analyses also helps us understand when this method would work and inform us about future research directions.
Below are my concrete questions and comments:
Additional Tasks Results (3.4)
In Appendix A.1, the paper mainly draws conclusions based on comparisons between GPT-3 and FLAN, which I do not think are fair: GPT-3 and FLAN differ in model size and pre-training data distribution. Instead, I think Base LM vs. FLAN might be a better comparison between “Off the shelf LM” and “instruction-tuned” model (though it won’t change the conclusion).
It is also worth pointing out in the main paper that for most of the additional tasks, even though it does not lead to higher accuracy, the performance of FLAN is still at least comparable (e.g. <1% worse and difference generally negligible) to Base-LM 0-shot. The only outlier seems to be ReCorD where the performance drops significantly after instruction-tuning, and this probably deserves some discussion.
Also I might have missed it - for the Base LM 137B zero-shot result, is it on the average template, or the best template?
Number of Task Clusters (section 4.1)
For Figure 5 can you add the untuned model to the curve with the x-axis=0 (0-task cluster)? This can help us understand how much even 1 cluster (e.g. summarization) may help.
Explanation for scaling (section 4.2)
It is an insightful empirical result that instruction tuning only works when model size reaches 68B. However, I am not entirely sure about the potential explanation of “model capacity”. There might be two potential explanations to this phenomena: 1) “model capacity”: as the paper has mentioned, smaller pre-trained models do not have enough model capacity and underfit the instruction tuning data, and 2) “better OOD generalization”: better quality pre-trained models have higher OOD generalization ability (and OOD accuracy) and they are less likely to “overfit” to in-distribution data.
I personally find the second explanation more convincing. For example, Sahn et al. (https://arxiv.org/abs/2110.08207) finds that even models with only 11B parameters can generalize to unseen tasks, using T5 (MLM) and a larger set of prompts. The use of MLM objectives (might) improve the pre-training quality, while more prompts reduce the “overfitting to in-domain data” issue.
I appreciate the fact that the author explicitly states the model capacity hypothesis more as a conjecture rather than a solid explanation. It’d be great if the authors can support the explanation further with more empirical evidence. On the other hand, however, since the results from Sanh et al came out only 2 weeks ago, I would not change the score based on the response to this question.
In-context Few-shot vs. Fine-tuned Few-shot (Section 4.3)
Can the authors compare “fine-tuning/prefix-tuning an instruction tuned model with 16 examples” (appendix C but with only 16 examples) with “in-context prompting” (in 4.3 of the main paper), similar to Chen et al. (https://arxiv.org/abs/2110.07814 )? This would further inform us how we should use the few-shot learning examples for larger language models: put it in-context, or fine-tune? Again, since the comparison of Chen et al. came out only 2 weeks ago and the paper limit is 9 pages, I would not change the score based on the response to this question.
Others
Results of Appendix C are interesting and potentially impactful - this might imply that instruction-tuned models will become the new “base” model for the pretraining-finetuning paradigm. Is it possible to briefly mention it in the main paper as well (and redirect the readers to the appendix to see the full results)?
It might be too late to change the name, but “Finetuned LAnguage Net” (FLAN) is uninformative, since it does not capture any unique aspect of this method. What does “LAnguage” mean here, ""natural language instruction"" or ""language model""? If it is the former, then directly including the word “instruction” might be better; and hopefully it’s not the latter, since even fine-tuned BERT on SST-2 counts as a fine-tuned language model ...
Typo Intro: Instruction tuning is “a” simple method that, as ….
Conclusion: Moreover, our work supercedes recent work such “as”","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization","Keywords: Neural Network Quantization, Fixed-Point Arithmetic","Pro: This is a very strong paper and I have not seen any paper of such quality in this space for some time. The analysis of relative error and normal distributions gives a strong empirical and theoretical basis for the approach used in this work and provides deep insight into what the optimal fixed-point data type in each layer would look like. This analysis alone will inform future research in this area. Usually, such data types would be learned. It is very creative to abandon this approach and instead start from the analysis. Later this approach is merged with learned quantization through unification with PACT.
Overall, the results are outstanding and appear to be reliable given the extensive analysis of the approach.
Con: Some details might not be accessible to readers that do not have an extensive background, especially pre-estimation of the fraction length and the sharing of the clipping thresholds for residual connections. It might be beneficial to trim the conclusion and add more context for these sections.
==============================UPDATE===================================
All the other reviewers take a very hardware-oriented perspective. I feel like they are not seeing the broader picture of this work which goes much beyond its initial applications. Many insights of this work are directly applicable for mixed-precision hardware where most, but not all operations are done in some 8-bit data type. From my own experience, I know that most 8-bit training methods fail when scaled up to models with billions of parameters. For that area, quantization precision is the most important unsolved problem. This is a very important problem since transformer models keep getting larger and larger and accelerating training through 8-bit methods can make it more feasible to train these models. This work is one of the only ones in this space that makes some headway by its excellent analysis and impressive results. Many of the insights of the paperw are immediately applicable to my research.
Overall, I disagree with the perspective of the other reviewers and I still believe this is an outstanding paper. As such, I keep my score of 10.","10: strong accept, should be highlighted at the conference","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization","Keywords: Neural Network Quantization, Fixed-Point Arithmetic","Strengths: 1- The proposed numerical format is evaluated in variant of benchmarks which mean the proposed approach can be deployed and generalized for different models 2- Combining the PACT and fixed point is interesting approach.
Weaknesses: 1- The correlation between standard division and fraction bit-width is interesting but it is obvious and in my opinion it is not required. The fraction length can be selected based on dynamic range and I think the reason behind the constant in the equation 1 is the correlation between standard division and dynamic range (DR=3ST). If we closely look at the figure 3, we find most of these standard divisions ( more than 1 ) is not useful for your case study, since dynamic range of most parameters is less than one for most of the layers. This small dynamic range of parameters explains why most of the layers parameters are using 6 to 8 bit fractions in figure 2. This brings a doubt on the motivation of why using the fixed point instead of INT8 approach? 2- The state of approach like dyadic quantization [1] and linear quantization with bit shift [2] performs DNN inference with 8-bit quantization without 32-bit INT multiplication. How is your approach different compared to these studies? 3- The author claims the performance of his approach is better than state of the art. However, the 32-bit baseline is different and the degradation accuracy should be reported in Tables 1 and 2. For instance, in ResNet18 result the degradation of accuracy in HAWQ-V3 is less than the proposed approach. 4- The new approach needs to compare with pervious work for 4-bit quantization. It is difficult to understand the advantages of this approach in compared to pervious work in 8-bit? [1] Yao, Zhewei, et al. ""Hawq-v3: Dyadic neural network quantization."" International Conference on Machine Learning. PMLR, 2021. [2] Langroudi, Hamed F., et al. ""Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge."" arXiv preprint arXiv:1908.02386 (2019).","5: marginally below the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization","Keywords: Neural Network Quantization, Fixed-Point Arithmetic","strengths:
The authors consider a practical issue from the perspective of DNN hardware implementation. The problem is well-defined.
The paper is easy to understand. The solution is presented clearly.
Weakness:
I am not convinced that 8-bit fix-point format is cheaper to implement than the other numeric formats. Quantization has been studied very extensively by the ML community, and numerous numeric formats have been applied to facilitate the implementation of the DNN. (e.g., binary quantized DNN, Logarithm quantization [a1], block floating point [a2], etc). The authors should better motivate this fix-point format by providing some hardware evidence. Without this, it is hard to convince the reviewer about the usefulness of this work.
Equation 1 is based on the empirical approximation with Gaussian distribution, not the real DNN trace. Given the importance of this equation, authors should better justify the correctness of this empirical approximation with real DNN trace.
This work assumes the ReLU is used for nonlinear operation. How to handle other types of nonlinear operations (e.g., leaky ReLU)?
In the paper, the author mentioned that they use previously stored activation fractional length for pre-estimating the fractional length for the following layer. What does the previously stored activation mean? Is it the activation from the last training batches?
Observation 1 and 2 is obvious. It is nice to have some real empirical results to demonstrate this, but I think they should be described very briefly.
[a1] Oh, Sangyun, et al. ""Automated Log-Scale Quantization for Low-Cost Deep Neural Networks."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
[a2] Darvish Rouhani, Bita, et al. ""Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point."" Advances in Neural Information Processing Systems 33 (2020).","5: marginally below the acceptance threshold","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization","Keywords: Neural Network Quantization, Fixed-Point Arithmetic","Strengths:
The paper is well written and easy to understand. Technical details are clear.
The idea seems practical and straightforward to implement in real systems
Weaknesses:
The paper claims that the quantized training flow uses only 8-bit fixed-point multiplications. However, calculating the fractional length requires the standard deviation, which needs to be done in float? The two-pass method for batch norm also uses float in the first pass. It's not clear if the technique is suitable on a specialized accelerator that performs only fixed-point multiplication. Instead the technique seems to target quantization-aware training (QAT) on GPUs for producing a quantized model. If this is the case, the results seem weak - the improvement over SOTA is very small, and the models tested are all small scale. The results are not enough to convince me that this is a significant contribution to QAT literature.
The novelty of the formulas to compute the fractional length seems weak. It's common in QAT to scale the tensor by dividing by the max value or dividing by some clip value. The scaled tensor then has range [-1, 1] and maps to a fixed-point representation with no integer portion. It's not clear what is gained by using stddev to estimate the fractional length instead. I would like to see some small experiments specifically comparing these ways to handle the dynamic range of a float tensor. In my mind they should achieve similar accuracy results.
It's not clear to me what combining PACT with quantization achieves. PACT is already meant to be used with quantization, and it's not clear what we gain by combining the two instead of doing PACT, followed by quantization. Again, some small experiments comparing the two would be useful.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design","Keywords: Agent Design, Morphology Optimization, Reinforcement Learning","Strengths:
The resulting morphologies of the algorithm seem interesting and effective.
The idea of breaking the training into multiple stages where policy first designs the agent and then controls it is novel.
Paper is in general well written.
Weaknesses/Questions:
Not sure how much control a user has over the design space. For example one may want the design to mimic certain animals, or be symmetrical.
The training for the first two stages would involve a delayed sparse reward signal. Is there any intuition why PPO handles this okay in the presented case? Does the horizon of the first two stages, where no reward is given, impact the learning effectiveness?
It’d be interested to analyze individual components of the proposed algorithm. E.g. comparing to [1] or [2] assuming a fixed morphology. This will help compare the condition policy approach to a bi-level optimization approach.
[1]. Reinforcement Learning for Improving Agent Design. Ha. [2]. Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning. Schaff et al.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design","Keywords: Agent Design, Morphology Optimization, Reinforcement Learning","Strengths:
Formulating design and control co-optimization as one sequential decision-making problem is novel.
The paper's ideas all make sense (transform-and-control policy, skeleton and attribute transform, JSMLP).
The empirical results look stronger than existing baselines.
The paper is written to be easy to understand.
Weaknesses:
Experiments are only conducted on four custom environments. Why not use existing environments from NGE or [1] (see references below)? Also, three random seeds are far below the standard.
Little analysis on the empirical results, given no theoretical justification of the algorithm. The analysis can be further enhanced from several aspects: 1) Discussing comparison with NGE (probably the strongest baseline) about similarities and differences and how those differences lead to a huge performance increase; 2) Enriching ablation studies by training with skeleton transformation disabled and attribute transformation disabled respectively.
Concerns:
I can't entirely agree with the argument that this approach enables first-order optimization of agent design. Technically, both this approach and ES-based methods do not have access to the ground-truth gradient and estimate first-order gradients based on the gathered experiences. So, this approach is still zeroth-order optimization, and it's not appropriate to claim that the sample efficiency comes from the first-order nature of the method.
While I agree that ES-based methods have a high-dimensional search space for design, your approach does not essentially reduce that search space. Instead, the search space for policy is much larger in your formulation, i.e., the dimension of MDP becomes much higher.
Could you provide more intuition on the comparison against NGE? By looking at the performance curves, it outperforms NGE by a large margin even without all JSMLPs. Is it because of the new formulation of the design optimization, or the attribute transform is optimized (unlike sampling from uniform distributions in NGE), or other reasons?
I'm skeptical about the reported performance of NGE because of several reasons: 1) In the original NGE's paper, it outperforms RGS by a large margin, while in Figure 3 of this paper, the improvement is marginal; 2) I believe NGE should be much better than RGS is because NGE also uses GNN policies that allow experience sharing across different designs; 3) If experience sharing via GNN is not effective in NGE, why this is effective in your approach as you claimed? A good way to address my concerns is to probably run NGE's original implementation besides your own implementation and report the performance.
Other suggestions:
Section 4 can be shortened to include more analysis in Section 5.
In many practical use cases, probably we already have a decent hand-designed agent at the beginning. If your approach can also effectively improve upon that and is better than other baseline methods, the results will be more solid.
The result will look even stronger if you can show your method even beat the previous works on continuous design optimization [2,3] (probably with attribute transform only and no skeleton transform).
Typos:
JSMPLs -> JSMLPs in the caption of Figure 4.
References:
[1] Gupta, Agrim, et al. ""Embodied Intelligence via Learning and Evolution."" arXiv preprint arXiv:2102.02202 (2021).
[2] Luck, Kevin Sebastian, Heni Ben Amor, and Roberto Calandra. ""Data-efficient co-adaptation of morphology and behaviour with deep reinforcement learning."" Conference on Robot Learning. PMLR, 2020.
[3] Schaff, Charles, et al. ""Jointly learning to construct and control agents using deep reinforcement learning."" 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.
--------- Post Rebuttal Update ----------
The authors adequately addressed my main concerns through extra experiments and analysis.","8: accept, good paper","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design","Keywords: Agent Design, Morphology Optimization, Reinforcement Learning","Strengths: clear idea, described well, convincing experiments
Weaknesses:
Formulations of some claims could be made more neutral. E.g., ""improves sample efficiency tremendously"" -> substantially, considerably, significantly
The paper refers to policy gradients (PG) as ""first-order"" optimization methods. This may not be exactly technically correct as these methods still only use zero order information about the objective function. The authors are encouraged to consult the recent literature on the analysis of PG methods, e.g., [1].
Table 1 in Appendix E shows that N_s=5 skeleton transforms and N_z=1 attribute transform were selected using hyperparameter search. What qualitative results does one obtain if there are more than 1 attribute transform? Why allowing more skeleton transforms performs worse? In principle, it should provide more flexibility. Adding a discussion explaining the effects of these numbers to the main body of the paper would be illuminating.
Table 1 in Appendix E shows values -2.3 and 0.0 are selected for diagonal values of covariance matrices Sigma^z and Sigma^e. This seems to be a mistake, because diagonal values should be greater than zero.
[1] Agarwal, A., Henaff, M., Kakade, S., & Sun, W. (2020). Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design","Keywords: Agent Design, Morphology Optimization, Reinforcement Learning","POSITIVES
To the reviewers knowledge (which is admittedly limited), this is the first attempt to learn the morphology of a robot this straight forward with RL. The method is easy to understand and improvements over evolutionary baselines appear very significant (even for 3 seeds).
NEGATIVES
The reviewer has 4 major complaints:
(1) The reward of the tested environments is not scale invariant. When the agent is allowed to construct a robot with twice the dimensions and torque, the robot will move (approximately) twice as fast and therefore get twice the reward. The reviewer assumes that there is a build in penalty for using large torques, but this is a delicate balancing act that the authors need to at least discuss. An additional figure that shows how the robots' size changes during training would reveal a systematic growth of the robot and put this reviewers mind at ease if that is not the case.
(2) The results are based on 3 random seeds. While this is (sadly) all to common in RL, it severely limits the conclusions one can draw with any certainty from the results. The performance gap to the baselines appears large enough to be significant, and the same holds for the ""ours w/o GNNs"" ablation, but the ablations without JSMLP have barely non-overlapping standard deviations, which is an insufficient measure of statistical significance for 3 seeds anyway. To make the presented claims about the ablations, the authors need at least (!) twice the number of seeds (if not more).
(3) The JSMLP method is complicated and poorly explained. It is also not clear to the reviewer why asymmetry is inherently favorable for robotics design. Even if one would like to include the possibility, the presented JSMLP method seems very ad-hoc and might have some unintended side effects. For example, the decision to remove one node can change the node index (e.g. from 31 to 21 in Figure 6) without changing anything about the node. The authors are encouraged to look into Relational GCN as an alternative to represent asymmetric morphologies.
(4) It is unclear whether GNN actually generalize well (in particular out-of-distribution) over different robot morphologies. Kurin et al. (2021) have shown some very convincing counter-examples and suggest to use attention instead of GNN layers. The authors must discuss this alternative and are encouraged to evaluate their experiments with attention layers to see whether this yields any improvement.
DETAILED COMMENTS
rt
must be defined
clarify that in the definition of J, H is a variable (due to episodic environment)
when you introduce the design D, doesn't it also affect the state and action space?
you do not explicitly state in the main paper which variant of GNN you use, e.g. which aggregation function
(eq.7) looks as if
ae
and
ad
can be executed simultaneously instead of alternatively. Better select one
a
(over the union of the action spaces).
it is not clear why you allow the removal of joints, as it could lead to cycles
it would be good to have a ""stop changing the morphology"" action instead of always running for
Ns
and
Nz
actions
the ""Policy Update"" line in Algorithm 1 must be in the first while loop, as PPO first collects the data set M and then uses it to update the model (currently it is called after every episode)
you should mention before page 7 that your graph has a root node
it would be nice to run in Figure 3 PPO on some standard morphology in each environment, for example HalfCheetah, Ant, Swimmer and Hopper, to see that your algorithm is a ""super-human"" designer. In particular Ant would make for an interesting comparison
REFERENCES
Kurin, V.; Igl, M.; Rocktaeschel, T.; Boehmer, W., and Whiteson, S. My body is a cage: the role of morphology in graph-based incompatible control. In International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=N3zUDGN5lO.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics","Keywords: inverse kinematics, deep learning, pose modeling","Strength
Impressive demo and usable system: this work focuses on a practical and important problem: inverse kinematics based on a sparse set of user-specified constraints, and builds a usable system in Unity that shows impressive results in generating realistic and natural-looking human poses based on fast-changing inputs. The proposed system looks complete and deployable for graphics and animation use-cases.
Proposed network: the use of prototypical networks in the task of human pose generation or pose infilling, to the best of my knowledge, is novel, though it would be beneficial if the authors can provide more in-depth motivation and in-depth analysis of why such network design is more suitable in the proposed task. What inductive bias is prototypical networks leverage in this specific task?
Comprehensive ablation: the ablation studies provided in the paper and supplement substantiate the benefits of the proposed learning procedure and loss weighting schemes.
Weakness
Missing simple baselines: based on the problem setup, one simple baseline is to use nearest neighbors to retrieve the closest pose from the dataset that satisfies the given input. Since MoCap data is continuous, it is highly possible that some realistic pose already exists in a large enough human motion database (such as AMASS [3], around 3 million frames). If no exact match exists, another naive approach could be finding close samples and interpolating between them either through simple linear interpolation or learned embeddings such as VPoser [4].
Missing motivation and analysis for the proposed ""ProtoRes"" architecture: as mentioned in the strength section, it is interesting the prototypical networks are used for this task. However, it is not well-motivated and not fully studied why the proposed network architecture is chosen for this task. Given the recent progress in Transformers, it is possible that the Transformer requires more data (which a dataset like AMASS can provide) but can learn a better embedding space for the pose synthesis/infilling task.
Missing generative aspect: for such an under-constrained problem, it is natural to assume that generative models such as normalizing flow [5] or VAE [6] are a more suitable model for generating diverse pose that conforms to the same user input.
Minor Issues
Multi-task learning: I am not sure if multi-task learning is the right term for describing the loss terms: the look-at, rotation, position loss for joints are largely the same objective expressed in different modalities. Since all loss is calculated in a root-relative coordinate system, the rotation term should contain all information available to reconstruct 3D positions through forwarding kinematics. Using both rotation and positions more likely served as a more explicit representation of the objective. This objective has also been utilized in some recent works in pose estimation [1][2].
Variable input length: while variable input length is mentioned as a feature of the proposed system, it is not demonstrated via demo or experiments. Since there is only a fixed number of joints, I presume the system can use a fixed number of inputs (all the joints) and use a mask to indicate whether there is an input for that specific joint?
==================================================
After author response
After reading the author's response and other reviews, most of my concerns are resolved. Thus, I would like to raise my score to 8 for this work for its novel use of prototypical networks in human pose modeling and the practical use case of the results.
[1] Li, Jiefeng, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang and Cewu Lu. “HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation.” CVPR 2021
[2] Rempe, Davis, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar and Leonidas J. Guibas. “HuMoR: 3D Human Motion Model for Robust Pose Estimation.” ICCV 2021
[3] Mahmood, Naureen, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll and Michael J. Black. “AMASS: Archive of Motion Capture As Surface Shapes.” CVPR 2019
[4] Pavlakos, Georgios, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman,
Dimitrios Tzionas and Michael J. Black. “Expressive Body Capture: 3D Hands, Face, and Body From a Single Image.” CVPR 2019
[5] Henter, Gustav Eje, Simon Alexanderson and Jonas Beskow. “MoGlow: Probabilistic and controllable motion synthesis using normalising flows.” ACM TOG, 2020
[6] Yuan, Ye and Kris Kitani. “Diverse Trajectory Forecasting with Determinantal Point Processes.” ICLR 2019","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics","Keywords: inverse kinematics, deep learning, pose modeling","Pros:
(1) This paper focuses on an interesting and important task in human animation. The proposed method and animation tool may alleviate the workload of animation by the artists.
(2) Several modifications about the model architectures are clearly described, and most of them make sense to me.
(3) This paper provides comprehensive materials, including the experiments, appendix, and demo videos. The effectiveness of most of the proposed modules is proved by the experiments.
Cons:
(1) The proposed method is a new way for human animation, so the comparison with FinalIK should be more detailed. The speed of both methods should be clearly shown in Table 1.
As mentioned in the paper, the proposed method satisfies the constraints approximately (P.9 The limitation of the current work). The construction results for both methods with full constraints may help to clarify the limitation.
The generalization ability of both methods should also be investigated. How about performing both methods on non-human skeletons with similar structures.
(2) Although this paper has provided very comprehensive results, some comparisons and analyses should be further shown in the experiment part.
In Table 1, the inference speed & parameter numbers should be provided for machine learning-based methods.
Quantitative evaluation for the look-at-loss should be presented to prove the effectiveness.
Table 2 shows the ablation study for GPD, but how about only using GPD (without IKD) for the final result.
(3) The dataset is one of the most important contributions in this paper. It is better to show more details about the dataset, especially the 'miniAnonymous' dataset.
How many instances are included in the dataset?
What kinds of actions for each instance are captured?
Does the motion style have a big difference with the 'miniMixamo' dataset?
(4) Some descriptions of the proposed unity tool should be presented. Does it just show the results of the neural network? Or some animation functions in the unity is also used as post-processings?
Detailed questions:
(1) What is the minimal number of the input constraints?
(2) What kind of results will be output if a sequence of the ambiguous input constraints is given?
(3) There are many interesting comparisons in the demo videos, so it is better to show some of them in the main paper.
(4) In Table 5 and Table 6, there are some distinct results, i.e. 1.27 e-6. Are those typoes?","I have reviewed this paper and I think there are no ethical concerns.","8: accept, good paper"
"ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics","Keywords: inverse kinematics, deep learning, pose modeling","Pros:
The paper solves an interesting and long-standing inverse-kinematics problem. To solve the problem, the paper novelly proposes a neural-network-based method to automatically handle the problem.
In general, the paper is technically sound with experiments to support its argument. Additionally, the paper proposes two datasets to train and validate the proposed methods.
In general, the paper is easy to follow.
Cons:
The organization of the paper could be further improved. For example, the definition of 6D rotations is only explained in Sec 1.2. It would be better to mention it the first time this term appeared, otherwise, it is slightly confusing. Besides, it is suggested to merge the five contributions into three or four. e.g. merge the fourth and fifth contributions.
Some important details are missing. a). How are the metrics L^{det}{gpd}, L^{det}{ikd}, L^{det}_{loc} defined and what are the units of them? b). How are the two datasets split into train and evaluation?
It would be great if the paper could show more qualitative results. For example, the qualitative comparison between the proposed method and the previous methods.
It would be great if the paper could show more analysis on the relation between inputs and model performances. For example, how is the performance influenced by the number of input joints and input types (joint positions or joint rotations which are better)?
I have a concern about the overall framework. The paper resolves an ill-posed problem, i.e. there could be multiple feasible outputs given the one input. However, the paper uses the encoder-decoder framework which is only able to generate one output given the input. Therefore, I am wondering would it be better to change the current AE architecture to VAE so that multiple outputs could be produced? I would like to hear about the authors' options.","6: marginally above the acceptance threshold","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Hyperparameter Tuning with Renyi Differential Privacy","Keywords: differential privacy, hyperparameter tuning","The problem studied here is a really important one, since it is directly related to actual deployment of DP models in real life settings. The proposed method seems solid and the experimental results look promising. I do have a few clarification questions:
How would this method be extended to be used with adaptive search methods? Grid search can be too time consuming/inefficient, especially when used with DP-SGD (as DP-SGD needs per-example gradients that are expensive to obtain) and there are some reinforcement learning based methods that can yield optimal hyper parameters much faster than grid search. Can this method be modified, maybe with a higher privacy expenditure, to be used in those cases?
What kind of real-life attack scenario would you envision that could actually use the best models obtained by hyper parameter tuning to extract information. Basically, what type/how much information about a model do the set of optimal hyper parameters leak?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Hyperparameter Tuning with Renyi Differential Privacy","Keywords: differential privacy, hyperparameter tuning","This paper considers an interesting and important problem: how hyperparameter tuning on private dataset can leak information, where it provides an intuitive SVM example. The paper then considers how to reduce the leakage. The problem is formalized in the following way: we pick a total number of runs
K
from some distribution. Then, for each run
k=1,2,…,K
, we pick an index
jk∈[m]
uniformly at random and run
Mjk
. Then, at the end, we return the best of the
K
outcomes. The paper proposes theoretical guarantees when
K
comes from truncated negative binomial distribution, or Poission distribution, which strictly generalizes the previous results. Furthermore, it also proposes a new method of computing privacy leakage when
K
comes from a general distribution, which should be of independent interest. Finally, the paper conducts empirical experiments to show the improvement if the new method.
My only concern for this paper is the problem formalization. First, for the current problem formulation, it is possible that the same parameter will be tried multiple times, which is definitely a waste of privacy. Not sure whether people will do it in practice. Second, the paper assumes the following scheme satisfies DP: randomly choose
j∈[m]
, and run
Mj
. Note that this is not equivalent with assuming each
Mj
is DP, where the latter is more realistic. For example, in the hyperparameter tuning of DP-SGD, it easily holds that each run (with different clipping norm) satisfies DP. However, it is not clear to me whether randomly selecting one clipping norm, and then running DP-SGD is differentially private. The authors need to justify the relationship between these two assumptions. Naively speaking, the second can not lead to the first assumption. Please correct me if I miss something.","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Hyperparameter Tuning with Renyi Differential Privacy","Keywords: differential privacy, hyperparameter tuning","The paper is very well written and the analysis seems solid (though I did not check every line). The problem is important and the paper improves the state-of-the-art so its merits are clear.
My only critique:
As emhpasised in Sec. 3.2, the ‘strawman approach’ of fixing m does not work for pure eps-DP. However, I find this a bit misleading since you are not observing (eps,0)-DP of hyperparameter tuning but (eps,delta) (or RDP). So the delta might actually play a crucial role here. Allowing bit of delta in the DP bound, the randomness in choosing the number of repetition might perhaps be not that crucial. Or vice versa, in (eps,0), you would not get great gains from that randomness.
I suspect that this fact that you have to have the randomness in the number of repetitions actually is a requirement of the RDP analysis that you carry out. As far as I see, in Proposition 17 you claim it is not, but I do not fully see why that would be the case. The result of Proposition 17, i.e., that the RDP of k repetitions is not RDP for less than eps’(lambda), does not really give a ‘counter-example’ in the same way as that result for pure epsilon. For example, suppose the underlying mechanism is eps=0.5 - DP. Then, you choose lambda = 1.9483. Then, eps - (log(1+exp(-eps))/(lambda-1)) ~ 1e-4 in which case the bound does not really say anything even for quite large numbers of k. And you can of course make that bound arbitrarily small by choosing lambda appropriately. I.e., if you let delta > 0, I think there is room for tighter analysis also for fixed k.
Could you comment on this tradeoff between lambda and eps in the bound of Proposition 17?
Could you give more intuition on why the randomness in number of repetitions k would be crucial or some other example that illustrates this?
Other:
There is something strange at the bottom of p. 22: equation going over the line.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Hyperparameter Tuning with Renyi Differential Privacy","Keywords: differential privacy, hyperparameter tuning","DP-focused machine learning systems would benefit from this work, as most of them require hyperparameter tuning. The algorithms provided in this work help us avoid a large cost of composition and perform tuning at much smaller privacy loss. In addition, the generic bound allows us to be flexible on the distribution of the number of runs (
K
), though it requires some effort to translate the logarithmic term into something usable. I have checked all the proofs and there are no critical errors.
The authors have sufficiently compared their method with previous approaches. Specifically, using the main results, the authors show that their method extends and improves upon the work of Liu and Talwar (2019). The authors also compare their method with the exponential mechanism-based method (Gupta et al., 2010; Theorem 10.2) and show in Section D.4 that both methods have the same lower bounds of the utility guarantees up to constants.
Nonetheless, the authors might want to compare their method with the noise perturbation method proposed by Chaudhuri et al. (2013). This method requires the “stability” condition on the training procedure, which might not be tractable for neural networks. But for linear classifiers, the algorithm (Algorithm 1) only adds noises of size
O(1/nε)
to the scores, which is quite attractive for training on large datasets.
The paper is mostly written with DP-SGD in mind, but it does not consider when the model's evaluation on a hold-out validation set is incorporated in the base algorithm
Q
, which is a common practice in training an ML model. From an application point of view, the authors might want to discuss a bit how the model’s evaluation (in classification or regression) can be made DP or RDP as a part of
Q
.
I see that distributions with finite support (e.g. the truncated binomial) are not considered in this study. But in practice, one might want to limit the number of hyperparameter searches (e.g. due to computational constraints), so such distributions might come into play. Have the authors performed some experiments on these against the truncated negative binomial and Poisson distribution? I wonder if the privacy-utility tradeoff is better when restricting to finite support.
Could the authors comment on how to determine an appropriate size of the candidate set (
m
)? A simple heuristic is
m=E[K]
but the authors might have something better in mind.
Specific comments
Page 2: In Eq (2), the special case where
λ=1
should be mentioned here, as it is also in the range of
λ^
in the main results.
Page 6: When I tried to derive (6) from (5), I got an extra
−ρη
term from rewriting
ϵ+ρ(λ−1)+(1+η)(1−1λ^)ϵ^=ρλ+ρ(1+η)(λ^−1)=ρ(λ−1)+ρλ^(1+η)−ρη,
combining this with the rest of the terms, and then choosing
λ
and
λ′
so that the equality holds in the following inequality:
(ρ(λ−1)+log⁡E[K]λ−1)+(ρλ^(1+η)+(1+η)log⁡(1/γ)λ^)−ρη ≥2ρlog⁡E[K]+2(1+η)ρlog⁡(1/γ)−ρη.
To clarify this, I think the authors should provide the proof of Corollary 4 somewhere in the Appendix, as I find the bound to be non-trivial.
Page 7: In Lemma 7, “
q
and
q′
are arbitrary probabilities” is misleading. When I saw the statement for the first time, I read this as “
q
and
q′
can be anything in
[0,1]
”, but the proof indicates that they take specific forms in order for the Lemma to hold true. One way to resolve this issue is by directly stating the definition of
q
and
q′
after (7).
Line 1-2 in Page 8:
Vaguely,
f′
being smooth corresponds to the distribution
K
being spread out (i.e. far from being a point mass).
This is not true; if
X
is a point mass at
1
i.e.
Pr[X=1]=1
, then the PGF of
X
is
f(x)=x
, and so
f′(x)=1
which is smoother than any polynomial.
Looking at the definition of PGF, the smoothness of
f′
should depend on the right-tail heaviness of the distribution of
K
. Specifically, a heavier right tail corresponds to a faster growth rate of
f′
, which in turn leads to a larger RHS of Eq. (7) when
q>q′
. This observation is in line with the privacy-utility tradeoff: more probabilities of sampling a large
K
(i.e. more utility) lead to a larger privacy loss.
Page 18: when
p=r=∞
, then the value of
r/p
is unclear. What is the convention for this case?
Page 23: It is mentioned below the definition of
Q,Q′,A,A′
that “the total ordering prefers the first option (corresponding to the first coordinate probability)” which should refer to the ones with the probabilities
1−b−c
and
1−b′−c′
. In other words, we assume that
1−b−c>b
and
1−b′−c′>b′
. However, the approximations below suggest that
b>c>1−b−c
and
b′>c′>1−b′−c′
.
Page 25: The proof of Lemma 20 is missing. Does it appear in Bun & Steinke (2016)?
Minor corrections
Page 6: In Remark 5: “
(λ2,ε)
-RDP implies
(λ2,ε)
-RDP”
→
“
(λ2,ε)
-RDP implies
(λ1,ε)
-RDP” and “ann”
→
“any”.
Page 14: In the footnote, “experssion”
→
“expression”
Page 16: In the definition of
g(y)
,
t
should be
t∗
.
Page 21: In the third display equation, the equal sign should be replaced with
>
.
Page 23: First line in Section D.4: “hyperparamter”
→
“hyperparameter”
References
Bun, M., & Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. TCC'16.
Chaudhuri, K., & Vinterbo, S.A. (2013). A Stability-based Validation Procedure for Differentially Private Machine Learning. NIPS'13.
Gupta, A., Ligett, K., McSherry, F., Roth, A., & Talwar, K. (2010). Differentially private combinatorial optimization. SODA '10.
Liu, J., & Talwar, K. (2019). Private selection from private candidates. STOC'19.","10: strong accept, should be highlighted at the conference","5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
"Real-Time Neural Voice Camouflage","Keywords: automatic speech recognition, predictive models, privacy","Strengths:
the paper is well structured and easy to follow
clearly explains how the proposed method works
evaluation framework is well designed and straight forward
experiments are solid and the results support the proposed method is working
in-depth analyses on the results: I really enjoyed the analysis shown in Figure 7
Weaknesses:
some arguments are not validated: In Section 4.4, the authors provide in-depth analyses and discussions on the attack characteristics. The first was whether the proposed model attacks vocal timbres, and concludes it does and attacks are speaker-dependent by stating that the attack performance drops - i.e., both WER and CER drop - when swapping attacks for speakers. However, it may not be true unless the experiments were carefully conducted with speech samples where different speakers say the same content. It would be interesting to see the results of voice-converted samples.
some observations are not scientifically grounded: In Figure 3 and 4, the authors repeatedly state that the attacks resemble speech ""formants"" but I don't see any formant-like structures in the spectrograms. If the authors are referring to the wave-like frequency components, I'm quite confident they are not formants. If time in seconds is denoted in the x-axis and the corresponding text is aligned and overlaid, it will be easier to determine.
Some minor comments:
supplementary video was helpful to experience how the attack sounds like, but it was pretty disturbing. And the white noise was inaudible so I couldn't make comparison. If such attacks are practically to be used, it would be good to perform a user study for perceptual evaluation of different attack methods.
is multiplier m in Figure 6 the same as Power of Noise in Table 1? And what is the unit of WER in Figure 6? Why such big discrepancy in WER?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Real-Time Neural Voice Camouflage","Keywords: automatic speech recognition, predictive models, privacy","Strengths
The motivation for the problem, the formalization, and the experiments are clearly written and well-organized.
The proposed method is tested in various situations that reflect real-world scenarios, and successfully deployed the method in a real room environment.
The authors also show that the attack is specific to the speaker; which partially implies that the NN-based approach is crucial for a given problem.
Limitations
Though practical ASR applications use an additional LM (language model) to correct the output, such cases are not examined. By looking at some examples of attacked transcription and the ground truth labels, I'm pretty sure that the accuracy of the model will increase if aided with LM. This is the main reason I'm giving a score of 5, and I'm willing to adjust my judgment if authors succeed in examining and discussing the effect of LM.
One of the limitations of this work is that it was only tested with a single specific ASR model; I think this should be also mentioned in the ""Ethical considerations"" section.
Questions
What DeepSpeech model are the authors referring to? Please be specific about the model information and cite the literature if necessary; the authors might want to add a section in the appendix for this.
In Figure 6, I wonder if the authors forgot to multiply 100 on WERs. It'll be clearer if ""(%)"" is added to every WER/CER in plots.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Real-Time Neural Voice Camouflage","Keywords: automatic speech recognition, predictive models, privacy","I think that the idea of training a model to learn predictive attacks is a contributive and effective way for an NVC model to be used in a real-world scenario. Also, the various experiments in this paper seem that the authors have considered a lot about the real-world scenario and can give many insights to the future works.
However, there are several concerns about this paper.
Personally, this paper was difficult to understand especially due to the way of indexing. For example,
αt+δ+r
means a noise to be added to the speech up until
t+δ+r
?
I think the
δ
is rather a room for computation time than an exact computation time. Is it right?
I think it would be better if there is a comparison with a previously proposed real-time NVC method even if it works only in a certain frequency region. This is because it seems more plausible online NVC model compared to the online PGD.
The paragraph, ""How robust is the attack to temporal shifts?"" is a little difficult to understand. When I read it, I think it is not an ablation study about varying
δ
, but I think the paragraph is saying about it (e.g. the sentence, the larger the delay
δ
, the further into the future our model needs to predict.""). Plus, I think there should be an ablation study about varying
δ
for training the NVC model.
When it comes to the real-world scenario, I think it is also a very important condition where we do not know about the ASR model. Therefore, I think it would be better to conduct experiments showing the performance drop when using an ASR model which is different from the ASR model used in training.
Personal Opinion
When I read a paragraph ""Real-time Machine Learning"" in Section 2 at first, I felt it could not have been written because this paper is not about speeding up the inference speed. Therefore, I think it would be better if it explains how the delay enables this method to be operated in real-time.
I think there should be a reference about the numbers appearing when it explains the relationship between the high sampling rate and instantaneous computation.
There should be a reference about DeepSpeech, and in Section 3.4 (or in the appendix section), I think the architecture about DeepSpeech and the Language Mdeol should be written.
When I read this paper, I really wondered about the audio samples (generated perturbation only / speech + perturbation). However, I cannot see it and I cannot open the video in the supplementary material.
How the WER can be larger than 100%? (off-line PGD)
I'm a little confused about using 0.5s delay, considering the summation of computation and playback time? (0.014s + 0.5s)","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"CycleMLP: A MLP-like Architecture for Dense Prediction","Keywords: MLP, Dense Prediction","The method modifies the existing 1x1 convolution (channel FC in paper), to sample points in a cyclical manner inside neighbourhood window along the channel dimension. It has the same weight shape as 1x1 convolution, ie: C_in x C_out. Stepsize is used to control the receptive field and can be thought of as analogous to kernel size in convolution. Overall Cycle MLP mimics the effect of sparse convolution kernel where each element along channel dimension is sampled at a different spatial position in a cyclical manner.
Strengths:
Simple idea with promising results.
Readily adaptable to existing architectures with vanilla MLP.
Increases the receptive field without increasing the number of parameters
Weakness/suggested improvements:
There are no ablations or explanations on how/why the induced sparsity is helping in performance.
It is not evident that why cyclical sampling is better than random sampling or what is the significance of cyclical sampling. Some discussion and experiments around this point would be useful.
A minor typo: MHSA is written as MSHA below eqn3","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"CycleMLP: A MLP-like Architecture for Dense Prediction","Keywords: MLP, Dense Prediction","The paper is strong in the sense that it proposes a simple yet effective idea to extend previous MLP work for image tasks. In a sense the basic idea of a sparse sampling across spatial and channel space is a natural variation to the other extremes in previous work such as channel and spatial FC. Thus the methodology follows the basic pattern as previous work so doesn't require much explanation in terms of details. In any case, the primary contribution here is the extensive sense of experimental results. Figure 2 and Table 2 already provide decent support for CycleMLP, and the results showing FLOPs and parameter counts further strengthens the results. Also as CycleMLP should be generic in terms of tasks the additional results on detection and segmentation make the idea more convincing. Overall the basic contribution of improving MLP models to have linear complexity and applicability to varying image sizes is solid.
Although the main work here is experimental in proving out the idea, it would be useful to have a deeper analysis of why the method works well. In particular, the strided sampling of CycleMLP seems a bit motivated by ease and computational complexity, but seems it doesn't quite have the basic property of CNNs in being adapted to local correlations in natural images. Presumably the patch embedding part is critical here as it already explicitly enforces spatial locality before CycleMLP is applied? Do the previous MLP methods use patch embedding, and if they do, would they also have similar performance? In other words, how much of the desired properties of linear computational complexity and applicability to varying image sizes comes from the patch embedding itself?","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"CycleMLP: A MLP-like Architecture for Dense Prediction","Keywords: MLP, Dense Prediction","Pros: (1) This paper designs a new MLP-like architecture that can cope with various image sizes and achieves linear computational complexity. (2) The authors contribute amounts of experiments to show the comparable performance even higher performance in semantic segmentation.
Cons: (1) It is a little difficult to understand Figure 1. In Figure 1(c), you implement cycle operation in the HW dimension, but in Figure 1(d), it seems that you only implement cycle operation in the H dimension. It needs to be clarified. (2) In Table 4 and Table 5, the authors conduct ablation studies about stepsize. It is implemented when stepsize is 1x7 and 7x1, what if the stepsize is 2x7 or 7x2? Does this situation exist? Another, can the stepsize be an even number? (3) Besides, ViP uses the 3-branch structure and AS-MLP seems to be the concurrent work. What are the specific differences between CycleMLP and these architectures? (4) About Figure 4, what means the effective receptive field? And what does the higher color value mean? From this figure, Swin has the sharp ERF and CycleMLP has smoother ERF. Can you explain it?","6: marginally above the acceptance threshold","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"CycleMLP: A MLP-like Architecture for Dense Prediction","Keywords: MLP, Dense Prediction","Strengths: 1. The Cycle FC is capable of dealing with various image scales, and has linear computational complexity the same as channel FC and a larger receptive field than Channel FC. 2. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation.
Weaknesses: 1. The Cycle FC align features at different spatial locations to the same channel, but analysis is slightly insufficient. There could be many different designs of it. For example, the experiments or analysis with different sampling intervals and sample size. 2. CycleMLP is slightly insufficient in discussion of the design and ablation studies.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models","Keywords: diffusion probabilistic models, generative models","novelty, significance
The result on optimal variance is new, to the best of my knowledge. The authors also do a reasonable job in convincing me that a Monte Carlo estimate of it is useful for inference. Given the recent progress and interest on DPMs, I think this work will also have reasonable significance and influence.
presentation
Authors do a reasonable job on the writing and presenting experimental results. Past works are adequately and appropriately cited, to the best of my knowledge.
Technical quality and correctness
One thing I'll add here is that once the Monte Carlo estimate of
σt2
is plugged into the bound computation, it seems we end up with a stochastic lower bound of the ELBO (assuming the loss is concave in
σt2
). The important thing here to note perhaps is that bias is introduced. To put it more concretely, say the quantity being estimated is
E[f(σt2)]
, where I've written the bound as the expectation of the loss
f
as a function of
σt2
. The estimator
f(σ^t2)
is now a stochastic lower bound on the original quantity by Jensen's, since
E[f(σ^t2)]≤E[f(E[σ^t2])]=E[f(σt2)].
I think there should be some discussion about this. I'm assuming
f
is concave in
σt2
, mostly reasoning from past bounds, but authors should perhaps make parts of the discussion more precise.
Experimental results
Authors do a reasonable job in evaluating their method. One particular point I didn't get is how
M
(number of samples for estimating the expected score) is chosen. The are a couple of potential issues here.
Selecting
M
requires additional hyperparameter tuning, potentially; the tuning procedure should be reported.
How results depend on
M
isn't entirely clear just from reading the main text (maybe there's some discussion in the appendix, but I didn't have time to read all content in the appendix). Ideally, some discussions should appear in the main text.
Large
M
incurs more compute cost during inference -- while this seems less an issue when inference is run on GPUs (since most scenarios, I'd guess, there's enough cores to parallelize the Monte Carlo samples), this could be an issue for CPU inference. How does the run-time in practice compare in this case? Note while practical systems don't tend to run training on CPUs, inference on CPUs is still quite common.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models","Keywords: diffusion probabilistic models, generative models","Strengths
Strong theoretical motivation and strong empirical results to support it.
Nicely written, does a great job putting prior work in the context of the new insights on optimal reverse mean and variance.
Despite the abundance of theory / derivations, the paper still remains accessible.
Weaknesses
I am convinced by the paper in its current form. But if I had to list something:
It would be great to see applications of this method to other data modalities, such as for example speech / sound generation.
The performance of Analytical-DPMs on methods that learn forward process variance schedules (e.g. VDMs) would also be great to see.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models","Keywords: diffusion probabilistic models, generative models","Strength:
the paper derives the optimal reverse variance for diffusion probabilistic models as well as its lower and upper bound. This leads to more efficient DPMs with better performance compared to existing DPM variants. It also leads to new insights into DPMs such as why the way the reversed variance is chosen by existing work is not ideal.
The paper is clearly presented. The technical results reported in the paper are non-trivially obtained. The relationship between the paper and existing works is well discussed and well-motivated.
Experimental results compared to other variants of DPM are well discussed. It also demonstrates the advantage of the proposed method compared to existing DPM variants.
Weakness:
Although proposition 1 establishes the relationship between data covariance and score function, it is unclear to me what is the practical implication of proposition 1.
In the experiment, it is unclear to me whether timestep is a good metric to measure efficiency in Table 3. Does each method spend roughly the same time at each timestep?
While the authors compare the proposed method with other existing variants of DPMs. Is there any reason why the comparison between the proposed method and other classes of generative models such as GAN should be conducted or not?","Performant generative models can be misused in situations such as deep fake.","8: accept, good paper"
"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models","Keywords: diffusion probabilistic models, generative models","Strengths:
The paper is clearly written and well organized. Contents are easy to follow. There are many technical details for readers to understand the results, such as the derivation of Theorem 1.
The analytic results are interesting and novel. According to the introduction and the related work sections, the optimal forms of the reverse process of DPM didn't appear in the previous DPM work and I believe it will help people in the field of DPM better understand this type of models. The bias analysis and the bound of the variance are helpful to understand the estimate and improve the estimate performance. The authors also make detailed discussions on these results, which are very helpful.
The experiment results are strong. The authors not only validate their analytic forms but also present the outperformance of their methods. The experiment results show significant improvements in their methods.
Weaknesses:
The DPM is a Gaussian-distribution-based simple model and similar analytic results can exist in similar models. The Gaussian model considered in this paper has a simple Markov property, which as a result has decomposable probability. The optimization is via forward KL divergence. Therefore, for example, the classical result on the expectation propagation algorithm with the Gaussian process can simplify the derivation: the decomposable form of the probability guarantees the forward KL decomposable, and minimizing the forward KL is equivalent to moment matching. So, I am concerned that the contribution is not as much as what was introduced in the paper. I also believe that such kind of connection will be helpful to figure out the possible further directions along this line and also save energy to get new results. I suggest authors add some discussions on the connection to other Gaussian models and their results.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models","Keywords: diffusion probabilistic models, generative models","Strengths:
Paper has single well-executed idea (optimal variance can be computed as a function of score)
Improvements over generally fair base lines are achieved by only post-processing; no additional training needed
Paper is generally well-written and straightforward to read
I did not check all Lemmas (appendix) in detail, however, it seems that their proofs are generally very rigorous and detailed.
The bounds in Theorem 2 are quite nice. Given that DPMs are often used for images, the specific bound for the data distributions supported in the
d
-dimensional cube are quite applicable.
Weaknesses:
For
K<100
, FID scores of the proposed method greatly rely on a trick of clipping the variance of the step
n=2
(can be seen in appendix G.4). This seems to be a crucial element and should be discussed more in the main paper. In particular, I would like to see how this clipping compares to the bounds (and the clipping) of the optimal score wrt Theorem 2.
The post-processing method can be quite expensive. For example, on ImageNet the best values are achieved using
MK=400000
additional (
M=100,K=N=4000
) function evaluations. For even trajectories (ET), the obvious solution would be to simply use
K≪N
(results for this are shown in paper), however, if I am not mistaken, for the optimal trajectory (OT), computed using the DP algorithm from [4],
MN
functions have to be evaluated for any
K
(see appendix B and eq (14)). Therefore, in my opinion, the comparison of DDPM and Analytic=DDPM in the OT setting of Table 1 is unfair.
Suggestions:
It would be helpful to understand how often the estimator is actually clipped. I suggest to compute
R
(maybe
R=100
) estimators for, say
M=[1,10,50,100,500,1000]
, and plot he number of the ratio of estimators that was clipped over
M
. This could be done for a few different instantiations of
n
(I guess there will be more clipping for
n
being small).
the estimator
J
in (14) is biased even when the correct score is known (
J
is the log of an unbiased (Monte Carlo) estimator); it might be nice to mention this fact.
It would be nice to see even lower
M
in the ablations in G.2. Instead of only indicating variance by plotting the estimator in Figure 3, it would be nice to see the estimated variance of the sampler directly.
Please state the number of Monte Carlo samples used for results in Tables 1,2,3.
To me it seems that the optimal variance could also be used for training, using for example only
M=1
. I would be curious to see how this performs compared to using the optimal variance only as a guide for post-processing. I greatly encourage the authors to try this (in case there are no major problems I am missing)","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation","Keywords: differentiable rendering, differentiable simulation, system identification","Strengths
S1 This paper tackles the challenging (inverse) problem of directly recovering object properties (system identification) or control parameters (visuomotor control) from image/video observations. The current best approach to this (
∇
Sim) proposes to compose differentiable physics simulation and differentiable rendering to result in a computation graph where ground-truth physical parameters are no longer required. However, this approach has a crucial shortcoming: it works well only when the reference and target trajectories are drawn from the same distribution (i.e., identical physics and rendering engines used across both). This work proposes RISP to bridge that gap (bridging this gap is crucial to enable several downstream, including real-world, applications).
S2 The paper is very well-presented. The core ideas are easy to follow, and the rationale for incorporating the rendering gradients into a regularization term is well laid-out. Of the three contributions, I believe this to be the more significant one (as also corroborated by Fig. in section 4.6)
S3 This approach (RISP) demonstrates strong performance over current art -- this is over multiple environment settings (rigid, articulated, deformable object identification; control).
In general, this is well-executed work and I would as such recommend acceptance. However, there are a few issues I hope to see discusses/addressed over the rebuttal phase.
Weaknesses
W1 Clarifying the impact of differentiable physics simulation vs rendering: Are the train sets generated using a different differentiable physics engine? (the manuscript mentions they're generated using a different rendering engine -- but, arguably, generating them using a different physics engine as well would create a wider domain gap) This also raises several interesting questions such as: will RISP generalize to scenarios where the underlying dynamics change too? If not, are there other domain randomization / regularization methods that can assist?
W2 Baseline choices: Do all baselines use all frames in the reference trajectory to compute the loss. Just as dense pixelwise loss baselines are ablated upon by bringing in other baselines that treat the image as a whole in loss computation; a similar analogy may be drawn at the sequence level (i.e., computing the full pixelwise loss across all frames in a sequence, vs. using pixelwise loss across select frames in a sequence)
W3 Disentanglement/Compositionality of the learned RISP: The current training strategies do not seem to explicity focus on disentangling state representations or improving compositionality of the learned representation. (This seems clearly out of scope for the current work, but I'd like to bring this discusison point here to perhaps prompt addition of clarifying statements in the manuscript). Does this have an imact on e.g., where the object may lie within an image, and perhaps impact the extension of RISP to simulations with multiple objects?
Minor comments
These comments are nitpicks/typos and are easily addressed in a minor revision. The authors needn’t respond to these
“Articulate-body” -> articulated body
“Simulates” -> simulate","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation","Keywords: differentiable rendering, differentiable simulation, system identification","Strengths:
The paper is very well written and contributions are clear.
The idea of using a rendering-invariant state-prediction as a pre-step before the gradsim-like framework is novel and sensible.
The idea of the using the gradient with respect to the rendering parameters as a regularization is novel and interesting. It could additionally be very used in many other fields where training on synthetic data is used with random rendering parameters.
The Combination of weak/strong/oracle baselines chosen in the experiments are very appropriate for comparison.
The paper performs a large range of experiments using several environment and tasks to study the effectiveness of the proposed method compared to baselines.
The ablation study shows the impact of the gradient loss on the state prediction model performance.
Weaknesses:
It is not clear if the training and testing rendering environments are the same. This means that the parameter distribution in both training (for the RISP) and testing cases come from the same distribution. In reality, often the test parameter distribution comes from a different distribution. It would be interesting to see results when the train and test rendering parameter distributions are different.
It would be interesting to see how the various methods would perform on real videos.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation","Keywords: differentiable rendering, differentiable simulation, system identification","This work addressed a crutial problem. The proposed method is novel and neat. The paper is well-written. I like the main idea and technically novel components proposed, such as the rendering invariant regularization term and efficiently calculating the second-order gradient. The results have demonstrated the importance of these components and the improvements over previous methods. For the weakness of this paper, it is not clear how this method works on real datasets. I suggest that the proposed method be evaluated on more datasets, especially real datasets.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"The Information Geometry of Unsupervised Reinforcement Learning","Keywords: unsupervised skill learning, reward-free RL, mutual information, DIAYN","Strengths:
This paper attempts to answer the fundamental question of why RL practitioners should care about unsupervised skill discovery. Providing a satisfying answer to this question should attract more attention to learning skills in this setting, as well as test and clarify the fundamentals on which the problem is based.
The high level results this paper presents also seem interesting.
Adapting the value function polytope to a polytope over state visitation frequencies and analyzing the problem from a geometric perspective is also a novel approach that deserves to be studied further.
The reframing of the unsupervised skill discovery problem as a vertex discovery problem is very interesting.
Weaknesses:
I am not completely convinced of the correctness of all the results. The paper introduces a lot of tools for its analysis. Specifically, the state marginal distribution polytope is an entirely new concept and its use in the analysis in this paper was nontrivial to follow. My questions in this regard are below.
The adaptation procedure that this paper refers to when it asserts that the mutual information maximizing skills are optimal initializations ignores the dynamics of the environment. As such, I am not sure if this adaptation procedure should even be considered feasible. The paper should reframe its statement to make the strong assumption on this adaptation clear, rather than merely stating that the initialization is optimal ""under some assumption about how the adaptation is performed"".
Detailed Comments and Questions:
The problem setup specifies
γ∈[0,1]
. Doesn't
γ=1
not work for the state marginal distribution and the subsequent RL objective?
The value function polytope is a closely related to the value function polytope approach (Dadashi et al., 2019). This related work seems almost hidden in section 2, and the contrast between the two approaches beyond the fact that one looks at value functions and the other at state marginals is not clarified. I felt like a paragraph clarifying the difference so that a reader familiar with that work might be able to grasp the state visitation polytope better would be useful.
In the 3-state MDP example considered for this paper, the start state distribution and the dynamics of the environment are not illustrated sufficiently. Does the agent start at all three states with equal probability?
When analysing this polytope in terms of state-only reward functions, does the origin indicate reward functions that do not prefer any particular state over the other? Are the rewards adjusted to be positive, so that they lie in this quadrant that is being analyzed?
In general, section 5 seems very condensed. I understand that the limits of the paper might be forcing the authors to be succinct, but I come away from this section feeling like I haven't completely understood the geometric view the paper is utilizing.
In Section 6, the notion of policies being closer or further in terms of the KL divergence is mentioned when analyzing the skills learned via mutual information maximization. I'm having a hard time wrapping my head around this concept, and perhaps it stems from not grasping the polytope in the previous section completely. Do points further away from each other on the polytope have higher a higher value of KL divergence? But if it was possible for two policies to spend all their time at individual vertices of the outer triangle (if the dynamics of the MDP allowed it), wouldn't the KL divergence be infinite? The notion of treating distance in this polytope in terms of KL divergence is something that I am not convinced of, and leads to me being unconvinced of the rest of the results of this paper.
In Lemma 6.2, does the probability of sampling a skill (
p(z)
) have to be uniform? I am unsure how skills with different probabilities could lead to state marginals that have the same KL divergence from the average. Do the skills with lower probability of being sampled travel farther?
For theorem 6.4, if the agent is able to learn
|S|
skills that each maximize the time spent at individual states, then one of these skills should maximize the return for any state based reward function. What am I missing here? Why do we need to consider all
|A||S|
policies?
On page 7, after the proof of Lemma 6.3, the paper mentions a regularity assumption. What is this assumption?
ICLR allows 10 pages of content this year. Given the density of Section 5, I feel like this paper wasted some space that could have been used for some clarifying exposition on the geometric view that the paper proposes.
Other Comments:
After Equation (3): ""... can be written is the average state ..."", is should be as.
Section 6.3: ""Maximizing mutual information pushes skills away from one another, not as finding a skill or policy that is close to every other policy"". I'm not sure what this sentence is trying to say. It is hard to parse.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"The Information Geometry of Unsupervised Reinforcement Learning","Keywords: unsupervised skill learning, reward-free RL, mutual information, DIAYN","STRENGTHS
(S1) The paper provides a very nice analysis for a growing class of methods that seek to solve a problem of growing interest in the community. The results are interesting--they say what unsupervised skill discovery methods can and can not do--and the discussion is timely as more and more researchers start to work in the space of unsupervised reinforcement learning.
(S2) The geometric perspective presented is intuitive and does a nice job of communicating the main results of the paper.
WEAKNESSES
(W1) The paper lacks a good empirical confirmation of the claims made here. For example, if unsupervised skill discovery indeed hits the theoretical upper bound that the authors claim, shouldn't there be even a very small toy domain on which the authors could do some experiments to validate that claim? That said, I understand the contribution of the paper here is mostly theoretical in nature.","8: accept, good paper","3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
"The Information Geometry of Unsupervised Reinforcement Learning","Keywords: unsupervised skill learning, reward-free RL, mutual information, DIAYN","Overall, the paper presents an insightful analysis of some of the challenges of unsupervised RL. In general, I believe that the primary impact of this paper would be through its geometric approach of analyzing the space of learnable policies. The concrete theorems proven by the paper, while interesting, appear to have a relatively large set of conditions. It would be interesting to see how the analysis can be applied to more general settings.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Language modeling via stochastic processes","Keywords: contrastive learning, language modelling, stochastic processes","Pros
The paper is well structured and easy to follow, the idea of modeling sentences to a Brownian bridge latent space is neat and generic enough to (1) allow for noise given its stochasticity (2) doesn't require explicit domain knowledge for planning.
Well Structured Experiments sections with 4 RQs and results that confirm each of the hypotheses
Reproducibility and Transparency in reporting of experiments in terms of available source code, dataset information, details about human evaluation, generation examples.
Areas of Enhancement & Questions to authors
The information about each of the ablations (ID, BM) could be explained better. namely the section ""ablations"".
There's a clear Inconsistency in the best TC method between different latent dimensions (8,6,32), in most of the experiments there's at least one of the 3 that is performing drastically worse than the other baselines, while there's overall no clear winner. I wonder if you have thoughts about this.
Table 5 the VAE(32) method performs the best overall in ""Wiki section"" although the TC (16) method has been highlighted as the best. Is there a reason behind this?
During the training of the decoder how do you make sure that the decoder uses the information given by the latent plan?
Overall the paper would have benefited from an intrinsic visualization of the latent space, to make sure for example that there's no Information collapse of the embeddings when dealing with long sentences. This could be done by visualizing the planning trajectory difference between coherent and incoherent text.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Language modeling via stochastic processes","Keywords: contrastive learning, language modelling, stochastic processes","This paper proposes a generation from a language model not only from an initial state, but also using a goal state. Instead of Brownian motion, the authors employ a draw from Brownian bridge by designating initial and end states, called Time Control. Experimental results show the proposed generation from Brownian bridge is more natural and coherent for text-infilling task, and also preserves text structures both by automatic evaluation and human evaluation.
Using Brownian bridge is a very simple and effective idea for text generation. My only concern is the range of its applicability: while it is far more natural than a simple random walk, Time Control only allows designating the first and last states for generation. However, in the actual situation, it is not always the case for the first (and sometimes, last) sentence should have a designated state. First few sentences might constitute just an ice-break, and the actual content might start after that. More generally, it is more desirable that we can condition the generation at arbitrary time. In fact, I think that this can be done by a conditional draw from a Gaussian process. Since Brownian motion corresponds to using an exponential kernel of GP, sentence generation from conditional GP would be the way for the future extention of this work. Anyway, this work will surely pave the way for such principled generations.
Minor
Some tables are located within the main text. Tables and Figures should be placed top or bottom of the paper for readability: please use \begin{figure}[t] for something like that.
Numerical results in Tables can be rendered in a smaller font (i.e. \small). Also I recommend to condense line spacing for Tables for readability, using \usepackage{setspace} and begin{spacing}{0.9} ... end{spacing}, for example.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Language modeling via stochastic processes","Keywords: contrastive learning, language modelling, stochastic processes","This paper has an interesting approach and tackles an important problem of streamlining sequence generation from autoregressive models.
The experiments show the value of learning a manifold over the latents that have a linear relationship with some stochastic perturbation. They provide evidence that learning in such a manner is promising in order to maintain coherence over long text generation.
However, the setting is fairly limited because this approach requires two contextual endpoints, the start and finish. This is especially underwhelming given that the introduction states that this approach aims to perform \emph{controllable goal-oriented} generation. In my view, the setting described and experimented with doesn't reflect this goal. For example, there are limited experiments with regard to controllable generation, or goal-oriented generation tasks.
Secondly, the assumption that autoregressive generation follows a Brownian motion is strong and I would like to see some empirical evidence or theoretical argument supporting this. One simple experiment could be to actually try to fit a Brownian motion model to a bunch of sequences generated from GPT-2, and show that this fitted model is not suitable for naturally occurring text.
Experiment wise, my biggest concern is the VAE baseline. The point of this baseline is to show that for the same setup of Brownian bridge process, contrastive learning is better than the VAE objective, but I feel that the VAE implementation as described in the appendix does not make the comparison fair. Due to lack of details in the paper, I am assuming that the priors p(z0) and p(zT) are standard gaussians. If this is not true, then a clarification would ease this concern of mine. But assuming this is true, the loss basically tries to match the encoder distributions q(z0) and q(zT) obtained by f_{\theta}(x0) and f_{\theta}(xT) to the standard Gaussian. What this means is that there is a pressure to make the 0 and T embeddings similar which is not at all what we want from this bridge process model. A more careful instantiation of prior for VAE, or even learning a time sensitive prior would be a better implementation of the VAE baseline.
Table 1 is another concern. This experiment basically trains a linear classifier over the encodings to identify if they are in-or-out of order. The proposed approach is naturally suited for this metric/classifier because the encodings at different times are more or less linearly related with some stochasticity. However, this is not true for the other baselines, so I am not sure what is the takeaway message from this experiment.
Also, more exposition on the Brownian motion baseline would be helpful. The current description is not enough to get an idea about what exactly was done for generation and other experiments with this baseline. On a related point, I don't get why BM for Table 2 would be the same as the brownian bridge. Isn't it the case that Brownian motion baseline doesn't get to see \emph{both} the endpoints? If I am mistaken about this, then more exposition is required here because I checked both the paper and the appendix carefully for this.
Table 5 shows mixed results. More discussion and analysis here would be helpful.
For clarification: please make explicit whether the triplets have a notion of distance or not i.e. it is sensitive to different value of t depending on which sentence in the middle was sampled. From the context, I am assuming this is the case but clarification would be helpful. Also, notation in equation 2's denominator is confusing. Are you summing over all the negative x_{t'}?","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"Language modeling via stochastic processes","Keywords: contrastive learning, language modelling, stochastic processes","I really like the main modelling contribution of this paper. It is this reviewer's personal opinion that to do long-form text generation, it is not enough to generate token-by-token, but that some high-level planning is required, and the Brownian bridge process model (Time Control; TC) the authors propose is definitely a good candidate to model the latent drift of discourse (indeed, papers like [1] already used random walk-style models to explain properties of word vectors). There are some prior works on using structured probabilistic models, such as switching latent dynamical systems, for text generation [2], which should also be cited.
The motivation of the model present is clear, and the description of how the model is trained is generally clear enough to reimplement. It wasn't immediately clear that training the model on triples only is enough to guarantee general Brownian bridge dynamics for the entire text trajectory, I feel a note should be added to clarify this. My other quibble here is with how the model is presented: although the general probabilistic model is written down in Equation 1, the likelihood function (i.e. the functional form of p(z_t | x_0, x_t, x_T)) is not explicitly written down anywhere, which leads to confusing things like the variance of the process \sigma^2 being used in Equation 3 without prior introduction. I feel like explicitly writing down the likelihood would make the equations in the paper flow much better.
I feel the major weakness of this paper is with the experimental sections. For various reasons, I have objections to each of the experiments, which I will go through below:
The first experiment attempts to show that TC is a better model of local discourse coherence. The authors take two sentences from a document k steps apart, embeds them and them attempts to predict the sentence ordering from the embeddings. They say that for k=1 all models considered perform at chance level on all datasets, and only show results for k=5 and k=10. However, models trained using the k=1 objective (such as ALBERT [3] and StructBERT[4]) seem to be able to perform the task better than chance, so theoretically this should be possible. Therefore, I think the baselines should at least include an ALBERT model to show the performance upper bound on this problem. Further, k=5 (or even 10) starts meaning the sentences start becoming very far apart (10 dialogue turns is more than enough to complete some of the simple dialogue agent tasks!), so it's questionable whether the model is really modelling 'local' dynamics at this point.
The second experiment looks at text infilling on the ROCStories dataset, and use BLEU and BLEURT to automatically evaluate their models (although the BLEURT results do not appear to be anywhere in the paper). The reported BLEU results are really low, to the extent that it's unclear whether an improvement from 2 to 5 BLEU is really meaningful. Part of the issue is that BLEU measures precision, which penalises text generation where there are a variety of possible outputs; for this reason, [2] report ROUGE results on ROCStories, which are much better. The missing BLEURT results would help contextualise model performance here. The human evaluation shows the model performs about as well as the ILM baseline from [5], which is ok I guess?
In addition, the table ordering is incredibly confusing. Table 6, which shows the human evaluation for experiment 2, appears much later in the text, after tables for the later experiments. It took me a long time to find it. Can you group the tables a bit better, in thematic order?
The third experiment attempts to measure 'global text dynamics' by measuring length mismatch per section on Wikisections. It's unclear what notion of 'global text dynamics' the authors are referring to - there are many theories on discourse coherence of long text, and none of them easily map onto a simple measure of section length. If the authors simply mean whether the model has learnt a notion of document structure, I think it would be better to be more explicit about this: showing that fine-tuned GPT2 can't even replicate the structure of a homogenous document corpus is an interesting negative result.
The fourth experiment forces models to generate beyond the expected document length by suppressing generation of the EOD token. I'm really not a fan of this experiment, because I don't even expect TC to perform well on it. Do the authors just keep on conditioning the decoder on z_T, and force the model to generate from this? At this point, the model is just a standard autoregressive model, so the modelling contribution should have no effect. Alternatively, do the authors resample z_{t+1} each time the model finishes generating a sentence? In which case, how do the authors preserve the Brownian bridge dynamics, conditioning on hitting a target state z_T? There are a few methodological issues with this experiment. A better experiment to run would be to simply ask the human annotators to score texts freely generated from GPT2 and TC for coherence, as a measure of how well TC can generate coherent text.
Overall, while the experimental section is weak, I really believe the core idea of directed Brownian dynamics for planning is a cool one, and deserves to be shared more widely. This is why I recommend acceptance.
References:
[1]: RAND-WALK: A latent variable model approach to word embeddings, Sanjeev Arora et al. 2015
[2]: Generating Narrative Text in a Switching Dynamical System, Noah Weber et al. 2020
[3]: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Zhenzhong Lan et al. 2021
[4]: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding, Wei Wang et al. 2021
[5]: Enabling Language Models to Fill in the Blanks, Chris Donahue et al. 2020
==================
Post author response:
Nonetheless, we think these observations fit well with the intuition our work proposes: Neighboring sentences are close to each other and act like Brownian motion where ordering is difficult to infer, and goal-orientedness / discourse structure emerges on longer stretches of sentences in a document.
I like this framing - currently it's implicit in the paper, but maybe it can be made more explicit that we expect the larger k results to be better, and that this verifies the Brownian bridge approach towards modelling text dynamics.
Nonetheless, the end arbiter of this task is a human (how coherent do the generations sound to a human?) and we care about at least matching ILM, a method developed specifically for text-infilling. So, it’s promising that our method performs better and/or competitively with ILM on human-based metrics (BLEURT and Human evaluations in Table 6).
I think it should be made explicit then that ILM is in effect an upper bound for model performance, as it is a model trained specifically to do the task, and that matching the performance of ILM is actually a strong result for the TC model.
So, to directly answer the reviewer’s question: we do not condition the decoder on z_T and do not resample new latents during generation.
The model is thus primed to generate much longer text than it was typically exposed to? Thank you for the clarification.
We in fact do already ask human annotators to score the generation (rf. Table 7). In this setup, we remove the middle section of the generated output as the text is extremely long. See Figures 3-6 for examples of the full forced long text generation results.
I believe the stronger (and more realistic) human evaluation is to not just evaluate the tail coherence on forced long text generation, but instead directly sample from the model naturalistically and evaluate that output using human annotators. If TC better captures global coherence, this should be visible even in this setting.
Overall, I would like to thank the authors for their response. Many of my concerns have been addressed, and I am happy to increase my score.","8: accept, good paper","4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."