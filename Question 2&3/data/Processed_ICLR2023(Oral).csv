,Title,Keywords,Strength_Weakness,Review_Rating,Review_Confidence
0,3D generation on ImageNet,"Keywords: 3d-generation, gans, generative adversarial networks, knowledge distillation, nerf, stylegan, radiance fields, volume rendering","Strength
The paper is well motivated as existing methods do not work well on non-aligned data.
Using an off-the-shelf depth estimation as a generic prior is intuitive, and experiments show that such prior helps improve the generated quality.
I find this paper easy and enjoyable to read and follow.
The visual generation quality is significantly higher than compared methods.
The learnable Ball-in-Sphere camera model does have a higher than a standard with 2 dof and works for objects that are not center-aligned.
The experiments are carefully and extensively conducted. I appreciate that failed attempts are also mentioned in the appendix, which helps the audience and following researchers better understand this work.
** Weaknesses**
The usage of an off-the-shelf depth estimator may make the comparison with other depth-estimator-free methods unfair. Plus, how do the performance and generalization of the monocular depth estimator affect the performance? Will the performance improve if trained with ground truth depth (for example if it is trained on a depth estimation dataset or a synthetic 3D dataset)?
The gradient penalty is widely used and studied in GAN-based approaches, the authors simply extend that to the parameters of the camera model. How good is the method if multiple objects are in the scene? The authors imply that their model works well on ""a scene consisting of multiple objects"", but I cannot find any experiments verifying this claim.
The so-called knowledge distillation for discriminator is more like an engineering trick and does not provide much technical insight to me. Moreover, the authors mention that ""it can work with arbitrary architectures of the discriminator,"" but it can only work on a discriminator that produces features with the same dimension as ResNet50. I am also wondering if this L2 KD loss works better than the mostly used KL divergence loss.
This paper should compare with a 3D photo baseline [a]. Combining a vanilla 2D image generator with such 3D photo generation methods could also produce realistic 3D image synthesis results while it seems to generalize on a larger range of images. I wonder how the performance differs for these two different generation paradigms. Will the proposed method have a higher multi-view consistency? Will the proposed fail while the 3D photo methods work better in certain situations? Thus, this line of work should also be discussed.
[a] 3D Photography using Context-aware Layered Depth Inpainting. CVPR 2020. | ** Strength **
The use of depth priors is an interesting idea to guide 3D-aware generative model.
** Weakness **
The use of the off-the-shelf depth estimator could, however, result in unfair comparisons as the depth estimator relies on another datasets for training. To be fair, the depth estimator has to be trained on the same dataset for the generative model, but this might not be possible on ImageNet. The effect of these additional data should be properly discussed.
a) Geometry prediction of the original depth estimator should be provided.
b) Compare a) to the geometry generated by the proposed method.
c) Compare the geometry generated by different depth estimators, and depth estimators trained on different datasets. Again, this point is not convincing to me still because the use of the additional data.
More results on ImageNet should be presented as claimed in the paper title. For example, out of the 1000 classes on ImageNet, how many categories have the authors experimented? To evaluate the geometry quality, the authors might want to refer to real object datasets such as ScanObjectNN, and compare the reconstructed geometry with these objects in point cloud format with FID evaluation.
[a] Uy et al., Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data, ICCV 2019. | Strengths
This paper is an empirical work, and the empirical results are relatively strong.
There is no reliable quantitative measure of generated geometry, so the paper introduces Non-Flatness Score. NFS is a very simple heuristic, but it does seem to provide at least some insight into the flatness of generated geometry.
The NFS values (Table 2) show that for unaligned datasets, previous work (e.g. EG3D) does not learn reasonable depth values, whereas 3DGP does learn reasonable depth values.
The proposed camera regularization strategy is intuitive. Figure 7 is an illustrative figure in this regard.
The inclusion of Appendix D (Failed Experiments) is very nice to see and much appreciated. The field would benefit substantially if sections like these were included in all papers.
Weaknesses
The novelty of this paper may or may not be viewed as a weakness. On the one hand, there are no groundbreaking ideas; the paper uses components that have been used before and combines them to obtain strong results. On the other hand, the exact method has never been used before; nobody else has combined these components in the precise manner that is being proposed here.
For the ImageNet experiments, the method uses more time/compute than competing methods (28.2 days vs 18.7 days or 15.9 days; Table 2). Training for a similar amount of time as prior work would allow for a fairer direct comparison.
When discussing GANs with external knowledge (at the top of page 4), the paper claims that “A similar technique is not suitable in our case as pre-training a generic RGB-D network on a large scale RGB-D dataset is problematic due to the lack of data.” However, it should be possible to train a generic (2D) RGB-D network on existing 2D image datasets by converting the 2D RGB images into RGBD images using monocular depth estimation; the paper effectively uses the same.
Minor Weaknesses
The method appears to be quite sensitive to hyperparameters (e.g. those on page 7). This is somewhat expected, as other GAN-based methods also tend to be quite hyperparameter-sensitive.
Questions:
Existing 3D monocular depth estimation models are far from perfect. Are the failure cases from the monocular depth estimation network learned and reproduced by the generative network? This is mentioned briefly on Page 6 as motivation for the depth adaptor; it would be interesting to provide some more detail on this point.
Do symmetries (e.g. those with respect to the position of the camera on the sphere) cause any issues with learning the camera distribution?
With regard to the camera distribution, it seems strange to have the same camera distance for diverse ImageNet categories such as a goldfinch and a church. One can of course partially account for this by predicting different focal lengths, but have you tried learning a more flexible distribution of camera such as the inside of a sphere or a 3D annulus? Additionally, how does the distribution of learned focal lengths for different ImageNet classes look like?
Given that the model learns a triplane representation, it should be possible to rotate all the way around the scene. However, all of the examples shown are front-facing views with small-to-medium camera movements. Does the model learn reasonable backsides for objects? For example, the backside of the horses shown at the bottom of Figure 5. It is not critical for the paper to reconstruct the backside of objects.",5.0,3.6666666666666665
1,A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification,"Keywords: failure detection, out-of-distribution detection, predictive uncertainty quantification, selective classification, robustness, method evaluation","The key strength of the paper is the thorough discussion of evaluation of model classification failures, which is an important and often overlooked part of ML research. The key conclusion is concisely stated in the conclusion: a need for greater introspection and possibly less novelty.
The key weakness is that the paper is difficult to follow. It is not clear how the pitfalls discussed in Section 2 are evaluated or studied in the remainder of the paper. A number of relevant method definitions are found in the appendix, rather than the main paper. In addition, definitions of techniques is often found after the fact. The paper would benefit from greater organization and extraction of key findings and results that support these conclusions. In Table 2, a number of results are reported, however, it is difficult to parse the metrics, color code, and main results from the paper. One of key findings is that MSR outperforms other techniques, but (a) this is not obvious from the results as its not clear what trends to look for in the metrics (a simple up/down arrow would help) (b) no analysis and discussion of other techniques is provided. | (strengths)
The paper evidently springs from a substantial understanding of and experience with the topic.
The proposed criteria are thoughtful and offer, agree or disagree, a good point of departure for discussion.
The ""step back and look in the mirror"" approach is welcome.
The findings section is super interesting.
(weaknesses)
Sometimes the paper covers ground too fast for me (too condensed). | Strengths
I strongly encourage the intention stated by the authors: to articulate a call to various interrelated yet isolated communities to acknowledge the shortcomings of current practices and adapt to a more reasonable practice that not only better fit their alleged purpose but also help bring the isolated fields together.
Section 2 is a deliberate and eye-catching section. The authors discussed three pitfalls of the existing methods and derived the corresponding point-by-point requirements for a desired evaluation protocol for failure detection. Among them, I particularly like (1) the second pitfall where the authors drew out attention towards different sources of classification failure and pointed out that existing methods often cover a subset of them (also illustrated in Figure 1). (2) the third pitfall where most relevant research claim failure detection as their purpose but in fact are evaluated on outlier detection, causing problems due to the mismatch (also illustrated in Figure 2).
I also like that the authors pointed out an easily neglected issue: rounding errors during softmax operation may largely affect the resulting evaluation metrics.
Weaknesses
Personally I believe reorganizing section 4.2 would be beneficial. For example, consider organizing the CSFs to be compared by their subfields (MisD, SC, PUQ, OoD-D) if possible?
Table 1 is a bit difficult for me to quickly process — I understand what is displayed but my brain is not well trained to instantly make sense of this representation. Is this just me or is it a common inconvenience? If it’s that latter, I would suggest adding a column for each dataset showing the “sum of rank order for the current method across all evaluations in this dataset”.
Minor Things
Typo? Section 2, Pitfall 2, last sentence. “…it is not realistic to exclusively assume classification failures from label-altering shifts an no failures caused by label-preserving shifts.”
Typo? Section 4.3, Under heading “Different types of uncertainty are empirically not distinguishable.“ 3rd line. “…we are interested in the extend to which such relations can be confirmed…”
Is there any reason
yf,i
in Eq. 24 is highlighted?",8.666666666666666,2.6666666666666665
2,A Kernel Perspective of Skip Connections in Convolutional Networks,-''-,"Strength:
An asymptotic bound on the eigenvalues of the kernels of ResNets is given for the first time, which generalized the existing similar bound for that of convolutional networks.
A new lower bound for the eigenvalues of the kernels of ResNets is given, which implies that the receptive field of ResCNTK is smaller than that of CNTK (without skip connections), thus the ResNet is supposed to be more local biased.
The author justifies why ResNets converges faster than the normal CNN by utilizing the NTK technology.
The overall structure and logistics of the paper are good. The formulation and writing of the theorem statements and proofs are well structured and organized.
The results of experiments match the conclusions derived from theoretical analysis exactly.
Weakness or Questions:
In Theorem 5.1, the author states that the bound matches the counterpart of the case without skip connections up to constants, and the experiments show that the eigenvalues of ResCGPK are quite close to that of CGPK for the same
k
. The given comparison is between CGPK and ResCGPK, I wonder whether there is any comparison between the eigenvalues of CNTK and ResCNTK ? Also, this result is verified for some specific function, will the phenomenon that the ratio constants are close to 1 holds true for general functions?
Theorem 5.2 provides a lower bound of the eigenvalues of ResCNTK. But it seems that there is no strict argument for
λk
of ResCNTK being larger than that of CNTK because there is no upper bound for
λk
of CNTK. I wonder that which factors would affect the order relationship between them. The author only posted one figure of experiments to qualitatively demonstrate this fact instead of comprehensive quantitive comparison either in numerical or theoretical way. | Understanding skip connections in CNNs is an important question in the foundations of deep learning, and the present paper provides a comprehensive picture of what role they may play in the context of kernels. This is thus a significant contribution, and I support acceptance.
There are nonetheless a few questions which could deserve more discussion:
Some of the bounds derived on eigenvalues are not tight, e.g., only lower bounds in Thm 5.2. While the experiments seem to confirm the findings, and suggest the bounds are relevant, is there a way to show upper bounds that also include the
ck
quantities?
In practice, residual networks often involve pooling/downsampling operations in intermediate layers as well, which seem important for good performance (this is also true in convolutional kernels, e.g. in these papers). Do you have a sense of how these would affect the locality bias in the present paper? Should I interpret your analysis as focusing on a fixed intermediate residual block at a fixed resolution?
Regarding condition numbers in Section 6.2, this seems to suggest that eigenvalues decay more slowly in the residual case, at least non-asymptotically. Do you have a sense of what could be causing this, and which eigenfunctions might be dominating the spectrum in the residual vs non-residual case? These questions seem important to discuss, since they would give insight on the kinds of target functions for which we can hope that ""smaller condition number"" => ""faster learning"" holds. | Strengths
The effect of skip connection is rigorously analyzed in the language of kernels.
The eigenvalues of the kernels are evaluated.
Weaknesses
There's no clear outcome for practitioners. It's not straightforward to utilize the theoretical results for new methods/algorithms/architectures.",7.0,3.0
3,Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation,"Keywords: Domain adaptation, parameter choice, model selection, aggregation, importance weighting","Strengths:
1: This paper proposes a simple but effective method that extends weighted least squares to deep neural networks for hyperparameter selection in unsupervised domain adaptation.
2: Experiments on different tasks are conducted to verify the superiority of the proposed algorithm.
3: This paper is well-written and easy to follow. The background knowledge is presented well. The readers can easily understand the paper.
Weaknesses:
The baselines are few. Do the baselines include important and all the SOTA UDA methods?
In theory, the authors show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. Such a bound cannot provide guarantees of better performance of the proposed method over others. | Strenth:
The proposed aggregation approch is general without strong assumptions.
The paper gives detailed theoretical analysis.
Weaknesses:
For experiments, though the paper’s benchmarks are large and diverse, but still some popular UDA benchmarks, such as VisDA-2017, DomainNet etc are missing. Besides, the paper mainly compares with conventional methods, while certain more recent works are missing.
The novelty of the paper is limited besides the combination of IWV with linear aggregation. | Strengths
Simple method: The proposed method is based on importance weighting, which is a well-understood yet pretty simple method. In spite of its popularity, importance weighting has not been well studied in the context of linear aggregation for unsupervised domain adaptation. Hence, it does not undermine the novelty or contribution of this paper.
Effective empirical performance: The empirical performances through massive experiments demonstrate the practical effectiveness of the proposed method. It is remarkable that the proposed aggregation approach outperforms the existing methods to select a single best model.
Weaknesses
Theory does not elucidate the effect of the model size. Although the experiments demonstrate that the aggregation of multiple model candidates outperforms a single best model, it is not evident from the claim of Theorem 1. I tried to figure out from its proof but failed because the model size
ℓ
is hidden in absolute constants of concentration bounds. Is it possible to comment on what goes on with the target error when we increase
ℓ
?",6.666666666666667,3.0
4,Agree to Disagree: Diversity through Disagreement for Better Transferability,"Keywords: OOD generalization, Diversity, Ensemble","Strengths
S1: The paper is interesting, and overall well-written. The presentation is logical S2: The proposed method is intuitive S3: The experiments show a big improvement in performance on the Camelyon17 dataset compared to strong baselines S4: Experiments on uncertainty estimation show nice results.
Weaknesses
W1: The theory is somewhat confusing. W2: The improvements over the ERM are quite small on Waterbirds and Office-Home. W3: The Camelyon17 experiments appear to use a somewhat different setup compared to the baselines (although I don't think it provides a significant advantage to D-BAT). W4: Would be nice to have a direct comparison with [1] in the experiments.
I expand on the weaknesses below. | Strengths: The authors tackle an important problem, and present a nice approach with a relatively simple intuition. I think their proxy for identifying OOD items is clever. The mathematical derivation is thorough (though I have not checked its details) Their evaluation is convincing
Weaknesses: I'd like to see an explicit evaluation of their proxy for identifying OOD items It would be nice to see more of a qualitative evaluation of which type of items their approach improved performance over, and which it didn't. Is the improvement randomly distributed over all OOD items, or are there qualitative properties of some items that dictated how well it would work? | Pros :
The paper is very well written and clear.
The claims are supported by both theoretical arguments and empirical evidence
The methods successfully avoid spurious correlations
Cons :
Some parts of the paper are a too quick on some choices (agreement loss term, number of learners in the ensemble, weights of the ensemble ..)",8.0,3.6666666666666665
5,Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,"Keywords: Computer Vision, Primate Vision, Adversarial Robustness, Behavioral Alignment, Inferior Temporal Cortex","Strengths:
(a) The understanding of the abilities of biological neural networks in a language that can directly be incorporated in the imporvement of artificial networks is a fundamental topic for the ICLR audience.
(b) The work uses original experimental results from IT (that I guess will be shared with the community) to improve nets previously fitted with general-purpose BrainScore, making them more suited to classification. And they succesfully show that the nets are not only closer to IT (as expected from the loss function), but they also improve the explanation of certain behavioral behavior and the classification performance.
(c) This works represents an advance in the literature that deals with the comparison between artificial and biological networks.
Weaknesses:
(a) The authors make a really interesting claim that I dont see clearly supported by the results (or I'd like to see it better stressed). In the last paragraph of the introduction and in the first paragraph of the discussion they say that increasing robustness to adversarial attacks does not necessarily imply being more close to IT. This is very interesting from the neuroscience point of view because solving the eventual problems of artificial networks in a specific task (such as classification) does not necessarily make them more human. This implies that eventhough the classification goal may be functionally sensible, the architecture or the actual strategies used by the artificial nets to get the goal are so different from the biological mechanisms that improving the performance of the artificial nets does not do them ""more biological"", and hence they provide little insight on what the brain may actually be doing. I see that Figs. 4 and 5 show that improved similarity with IT leads to better robustness in classification (blue points or blue lines, respectively), but in Fig. 5 I dont see how similarity with IT is measured in the orange curve. And in Fig. 4 it is not clear to me how the orange dots were trained: were they trained enforcing robustness to adversarial attacs but not imposing alignments with IT? (is this what ""random IT"" means?).
(b) Reproduction of behavior is limited to a single experiment. This is fine for a conference paper but the authors should acknowledge that a single experiment does not summarize the rich aspects of visual psychophysics [Bowers et al. BioArxiv. 21]. In this regard, there is literature that proposes that biologically sensible alternatives to the current artificial neurons in conventional artificial architectures improve robustness to adversarial attacks [Bertalmio et al. Sci. Rep. 20] (in line to what is proposed in this work), but also reproduce a range of classical psychophysical (behavioral) results on brightness and texture perception. Similarly, Gomez-Villa et al. Vis.Res. 20 and Li et al. J.Vision 22, show that simpler (more human) architectures better reproduce classical color illusions and the Contrast Sensitivity Functions. The need for more exhaustive connections between physiology/architecture and behavior (wider range of psychophysics) should be acknowledged in the introduction or discussion.
(c) I think readers would appreciate a mathematical expression for the ""CKA"" loss function rather than the verbal descrioption given in section 2.4
References:
[Bowers et al. 22] Deep Problems with Neural Network Models of Human Vision https://psyarxiv.com/5zf4s/
[Bertalmio et al. 20] Evidence for the intrinsically nonlinear nature of receptive fields in vision. Sci.Rep. https://www.nature.com/articles/s41598-020-73113-0
[Gomez-Villa et al. 20] Color illusions also deceive CNNs for low-level vision tasks: Analysis and implications. Vision Research https://www.sciencedirect.com/science/article/pii/S0042698920301243
[Li et al. 22] Contrast sensitivity functions in autoencoders. Journal of Vision https://jov.arvojournals.org/article.aspx?articleid=2778843 | Strengths
Simple idea well executed
Improved alignment of CNNs with human perception
Improved adversarial robustness
Paper is well written and easy to read
Weaknesses
Unclear to what extent this result generalizes to other network architectures
Improvements are relatively small | Strengths
The paper contains a wealth of experimental data, including human psychophysics and monkey electrophysiology. The number of monkeys is large for this type of study (6), which allows to test similarity metrics on held-out animals.
The evidence supporting the results is generally convincing.
The paper clearly underscores one limitation of the current approach/set of results, by pointing out how the increase in behavioral match between humans and network with increasing representational alignment does not hold for object categories not included in the training set. Some possible ideas for a way forward on this issue are proposed in the discussion.
Weaknesses
I could not find a link to the experimental data collected for this work, or a promise to publish such data upon acceptance.
Section 3.2 states that ""the label information during training helps on the behavioral task, but is not required for the trend [of better representation alignment improving behavioral match between humans and deep nets] to hold"". This is correct, but in my opinion this underplays the fact that label information is still used for the pre-training of the network. Perhaps this passage could be rephrased to remind the reader that label information is nevertheless still available from the pre-training, even when it's not included in the fine-tuning process.
If I have understood correctly, the monkey data was recorded specifically for this work. However, the phrasing around this fact in the supplement is somewhat confusing. Please clarify, stating explicitly that the data previously recorded from these monkeys for various research objective is different data that was not used in this work.
I urge the authors to consider raising their compensation for human subjects in future experiments. Psychophysics subjects were paid 4 USD an hour in this work (Supplementary Material). This seems very low considering that the experimenters are based in Massachusetts (based on the information in the paper), and the minimum hourly wage is 7.25 USD federally and 14.25 USD in MA.",8.0,4.0
6,AutoGT: Automated Graph Transformer Architecture Search,-''-,"Strength
Well-explained basic transformer and graph encoding strategy, which show excellent intuition of the search space design.
The proposed framework is compatible with more existing graph encoding strategies, which allows a broader comparison between graph transformer designs.
Tab5 shows an ablation study using a single supernet vs. additional supernets. The ablation study shows the improvement of performance by splitting a supernet. Such a design brings extra performance without extra parameters.
Weakness:
Section 3 explained the proposed AutoGT well in text. However, neither Fig 1 nor Fig 2 are self-explanatory. I could not understand their relations with the proposed method without reading the main paragraphs. The small fonts in the figures made it more challenging.
Figure 2 was never mentioned in the text, which makes it tricky to understand which section it belongs to.
Table 3 tries to show the importance of the proposed evolution search by comparing it to simple random selection and existing hand-crafted options. The results have large deviations. How can we ensure the proposed method results in consistently high-quality architecture designs?
The proposed method aims to automatically provide graph transformer designs. However, AutoGT introduces extra hyper-parameters such as the number of supernet sizes and the search space's design. | Strengths:
The proposal of automatically searching the optimal graph transformer architecture is important in the community of machine learning.
Constructing a unified graph Transformer formulation by combining several typical transformer architectures and graph encoding strategies seems to be a novel contribution.
Taking graph-specific properties into the automated graph transformer architecture search process is helpful and can distinguish this paper from other approaches such as autoformer.
Experiments are conducted over several widely used and benchmark datasets.
Weaknesses:
The major focus in this work is the complex relations between transformer architectures and the graph encodings in transformer layer. The authors are suggested to further discuss why this problem is important and challenging, and how the authors address the problem specifically.",8.0,4.0
7,Betty: An Automatic Differentiation Library for Multilevel Optimization,"Keywords: Multilevel Optimization, Automatic Differentiation, Bilevel Optimization, Meta Learning, Software Library","Strength:
The efficient automatic differentiation method is novel and effective.
The proposed system BETTY is thoughtfully designed and clearly presented. It is a very meaningful contribution to facilitating efficient implementations of MLO programs. The authors also showcase the scalability of the proposed system to models with hundreds of millions of parameters by performing MLO on the BERT-base model.
The system will be released open source.
The background knowledge and math foundations are clearly presented.
This is very impressive work to me. I do not see a major limitation of the proposed system. | Strengths
Principled expansion of bilevel optimization (which enjoy increasing popularity) to a DAG of sub-problems, well described
Understandable and usable formalism to define such problems in practice in a framework
Several informative examples, demonstrating the range of capabilities of the framework
Comparison with (and exploration around) SOTA, demonstrating similar (or improved) results and efficiency.
Extensive appendix with details
Opensource code
Weaknesses
Clarity could be improved in some places, as it can be a difficult problem to grasp (see below) | important problem
useful library that achieves good results",8.666666666666666,3.3333333333333335
8,Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only,-''-,"Pros:
This paper proposes a new angle for backdoor attacks where the attackers poison the image labeling process. Given the fact that a lot of data labeling work is being out-sourced, I think the threat model is realistic.
This proposed attack shows a high attack success rate while has high clean performance.
Cons:
The limitation of triggers. This paper uses some classes combination of the existing training images. I think there are many class combinations that are not covered in the coco training data. Is it possible for the attacker to use external images or synthesized images to create any trigger (any class combinations) that he wants?
All the evaluated attacks are done on object detection. Is it possible to attack another machine learning task? Only being able to attack object detection makes this attack a bit constrained. | Strengths:
The authors consider various scenarios of attack evaluation. However, it is still a lack of justification that the problem setting is significant enough.
Weaknesses:
In supervised learning, labels are critical, which makes the author's goal trivial. Hence, the novelty of this work is limited. The clean label setting will be more astonishing. The authors also mention that malicious behaviors are hard to be distinguished from common labeling mistakes.
Under the same threat model (third-party service provider), NeuRIPS 2021 ""Manipulating SGD with Data Ordering Attacks"" provide less capability to attackers (only batch reorder, no modification of label or image). This work needs to cite and compared.
Questions:
Why is this attack a targeted attack? Given an arbitrary image, how can the attacker specify the model's behavior during inference? | Strength:
Generally speaking, this work uses a new direction to generate backdoor attacks. It is different from previous works. The multi-label learning task is also a new task for the poisoning attack.
The results are good. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses
It also proposes three target label selection strategies to achieve different goals.
Weaknesses:
The background seems to be too simple. What are the differences between the poisoning attack and the backdoor attack？
Maybe you should highlight your contributions in the introduction. Besides, please introduce your weaknesses in the paper.
Compared with previous works, maybe changing the labels is easier to be noticed. | I generally like the idea of this paper, and the execution is decent.
Can this really be called a backdoor attack? Conventional backdoor attacks focus on triggers selected by the attacker that can be placed on an image or on a physical object (e.g. a sticker on a stop sign or a pair of sunglasses). What is called a trigger in this paper is not what is typically considered a trigger. As is, this paper is essentially the same as standard backdoor attacks except they just consider an object already in the image to be the trigger, which in some ways makes the paper more akin to other data poisoning attacks (where there are other attacks that only modify labels).
Along the same lines, this paper cites some related work as backdoor attacks which are not actually about backdoor attacks. For example, the authors cite Shafahi et al. as a backdoor attack which it is not. Please correct this issue.
Existing backdoor literature focuses largely on classification tasks, but some of these backdoor attack methods might be directly applicable to the poisoned labels only setting as well. For example, Sleeper Agent backdoor attack is a gradient-based backdoor attack, and one could simply compute gradients with respect to the annotations.
It would be good to mention label flipping data poisoning attacks in the literature review. They are not competing with the proposed method, so they are not required as baselines. You can find some of them in the survey paper, “Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses”.",6.0,3.5
9,Compressing multidimensional weather and climate data into neural networks,"Keywords: Weather and climate data, Compression","Strengths:
compression ratio (300 to 3000)
spherical transformation, which is indeed very important for earth data
usefulness of the method: while at first I was skeptical on the usefulness of such an approach for climate data, as clearly ERA5 will never be only stored in a degraded version, I was convinced by the usage of machine learning applications (and probably also others), where these amount of data are usually untractable for a local storage+usage.
comparison with state-of-the-art and on different settings
current limits of the method clearly analyzed
Some clarifications would be valuable for the paper, and in particular the following points:
I think that in order to be used, this kind of approach would need to have an uncertainty quantification, at least based on the frequency of the signal to recover, or based on the type of data, etc. in order for the user to be able to know when it can trust it and when in can't. How would you do it?
The compression consists in training a neural network. Do you think this is robust enough to work with all size, input field, etc.? Do you think this could be used by a non-machine learning expert?
In which type of applications would your error be negligible? It is good to have a very high compression, but the final quality needs to be satisfactory for targeted applications at least (since the hurricanes are for example not the case).
The compressed data representation are the neural network weight values directly. Could your compression be used directly to feed a machine learning model, or would users have to decompress the data before using it?
Would your method, and in particular the first part about the coordinate transformation, work in the case of non-worldwide data, i.e. only a part of the globe? | On the plus side,
Weather data is an interesting application. The need for compression is also well motivated in the paper.
I like the that the authors explain the specificities of weather data and how it impact their model.
Compression is evaluated both in terms of reconstruction error and accuracy of a neural network trained on decompressed data. Results are good.
On the negative side,
Experiments are a bit limited in scope. Only four data sets and one weather variable are considered. Unless I am mistaken, this means only four encoding were done. I appreciate that the data sets of different characteristics and I am aware encoding time is long. The largest time frame considered is 24 hours. The results are good, but they did not convince me the proposed approach could extended to years of historical data, an application mentioned by the authors. I think discussing this point would be interesting.
Another application mentioned by the author is to compress simulation data. It would be interesting to compare compression time with simulation time.
The use of GELU is not really motivated and not included in the ablation study.
Update I had misunderstood the scope of the experiments. The authors pointed this out and were kind enough to add additional experiments. They also answered the other points I raised. I believe this work is worth publishing.",8.0,3.5
10,Conditional Antibody Design as 3D Equivariant Graph Translation,"Keywords: conditional antibody generation, equivariant, multi-channel attention","Strengths
Progressive fullshot decoding is more scalable and less prone to accumulated errors compared to autoregressive models that generate amino acids one at a time.
Light chain and antigen information can provide additional context and can generate CDRs with high binding affinity.
A novel equivariant attention mechanism was developed to better capture interactions | Strengths: (+) This paper gives a novel formulation of the antibody design problem by including the target antigen and the light chain of the antibody as conditions and designs a novel network architecture for this problem. The novelty contribution is significant and enlightening. (+) Comprehensive empirical studies and ablation studies are conducted to demonstrate the advantages of the proposed method over prior methods. The experimental design and results are solid. (+) The writing of the paper is very clear and well-organized.
Weaknesses: (-) (minor) In the section 3.3, it is better to provide more explanations or details about Huber loss and its advantages. (-) (minor) In the end of section 3.1, the antibody design is formulated as a prediction problem, i.e., the output and input are one-to-one. In practice, for a specific antigen, is there only one antibody with a unique stable structure that can bind to it? I will appreciate if the authors can explain why the task is considered a prediction problem rather than a generation problem. | The model architecture the authors develop is very thoughtfully constructed and the gains are demonstrated in the state-of-the-art performance. Their ablation study is an important addition, as it shows which aspect of the method contribute most to its performance gains. The use of Progressive Full-shot decoding was the biggest contributor to performance while also having major run-time gains. This suggests that it could have promise for utilization in other methods as well.
The authors did not give much explanation as to why a message propagation paradigm was used with the internal encoder and a graph attention scheme used in the external encoder. The ablation study also demonstrated that the attention scheme did not contribute hugely to the performance. Some more explication of the modelling decisions would be beneficial.
The “Affinity Optimization” experiment is a great section of the paper, and moves towards real-world usage of these methods. The authors show their method is able to generate a CDR with the biggest decrease in delta-delta-G (a proxy for binding affinity) compared to benchmarks. To make the result more biologically interpretable it would have been useful to have an idea of how much a delta-delta-G decrease contributes to a decrease in binding affinity (as measured by Kd for example). This could have been done using the Shan et al model and comparing to test performance on the SKEMPI set. While this correlation would be different for different protein interactions, it could have been a useful discussion point. Using a general delta-delta-G predictor over an oracle model trained on experimental data is a clever extension ofhe Iterative Target Augmentation algorithm for this use-case. However, while the authors do mention that they only benchmark on antibody complexes within the training set of the Shan et al model to ensure generalizability, this points to how the efficacy of this pipeline for any candidate antibody is limited by the generalizability of the predictor. While this is not a fault of their proposed model, if this pipeline is a major point of utility of this paper, it is a point worth discussing.
While the authors do cite a major paper in the field for sequence-structure co-design (Jin et al 2021), they do not mention the new paper from that group (Jin et al 2022) which makes similar developments on its prior model as this paper performs (eg. modelling the antigen structure in the CDR generation). Furthermore, in terms of benchmarks, while the authors’ model did show state-of-the-art performance when compared to any cited paper, there are some discrepancies in RefineGNN’s RAbD benchmark performance between the original Jin 2021 paper and this paper’s reproduction of it. Lastly, it seems that the authors criticize RefineGNN for not being invariant in its loss calculation, however this is not the case.
A high level thought : An overall weakness of this work is common to much of the published work in this field; i.e. the work does not really address the ""real world"" problem of antibody design; - the 'real-world' question should be given an antigen target, design an antibody that will bind with high affinity and specificity. However good a method is at predicting antibody binding to a specific epitope on a target antigen, and however well the model can generate sequences and structures that bind that epitope - the 'real-world' question should be able to find new epitopes, given an antigen target, design an antibody that will bind with high affinity and specificity. Nevertheless , these authors do not claim to do this so thats fine - but the ML community should be aware that these evaluations and target problems only address a very narrow need- e.g. I have one epitope on an antigen of know. structure - can I make a 'better' antibody to that specific target surface? | Strengths:
Multiple novel contributions.
Modeling all components of the antibody/antigen complex
joint sequence/structure design
iterative prediction of model outputs.
Convincing and strong results on multiple downstream benchmarks. Dataset splits are constructed well and baselines (both neural and non-neural) are compared to appropriately.
Ablations of model are included to verify claims as to the utility of the different additions.
Well written paper. Model and experimental results are well explained and an appropriate level of detail is provided to understand the data split strategy and baselines.
Weaknesses: To be honest, I see no major weaknesses in the paper that must be addressed before publication. Below, I've listed some questions for future work and other minor comments.
Questions / Future Work:
It is interesting that the graph is constructed without full knowledge of the location of all residues - a linear imputation is used for missing residues. It would be nice to analyze this a bit further - how much do residues move from this initialization? How ""accurate"" is the initial graph structure - are there edges missing/present in the initial graph vs. in the ""true"" graph and how much does the graph change over iterations of the iterative refinement?
Related to the point above, it seems this constructions works in part because much of the antibody structure is already known. Would it be possible to apply the method to general sequence design, as opposed to loop re-design (e.g. Dauparas et al. 2022, science.org/doi/10.1126/science.add2187)? This would be a breakthrough result if so.
Ultimately, in-silico evaluations for protein design only go so far. It would be great to see the designs verified experimentally.
The authors note the current model cannot model sidechains. I do think this is a limitation that future work should explore, as it seems this would be important for the design of binding.
Minor Comments:
It was slightly hard for me to associate different tables with experiments / datasets at a glance (e.g. it took me a little bit to figure out what exactly is the difference between Table 1 and Table 2 (left)). Perhaps the experiments section could include a bulleted list of datasets used, or this could be more clearly highlighted in the table captions.",8.0,3.5
11,Confidence-Conditioned Value Functions for Offline Reinforcement Learning,"Keywords: reinforcement learning, offline reinforcement learning, ensembles, adaptation","This paper has the idea to learn confidence-conditioned value fundction for adaptive policy optimization. The idea is the strength of the paper. However, there are flaws in the experimental design and experiment discussion, which make it impossible to thoroughly access the new idea.
Problems (ordered by appearance in the paper)
Two important terms, epistemic and aleatoric uncertainty, are unexplained in Section 2. It would be better for the paper to include the definition especially when the paper tries to emphasize its work on epistemic uncertainty.
1.1 The last sentence of Section 2 gives a false impression that studying epistemic uncertainty is better than aleatoric uncertainty. Focusing purely on epistemic is no better or worse than focusing purely on aleatoric uncertainty. These are two different areas. The paper should address the difference of epistemic (parametric) vs. aleatoric (intrinsic) uncertainties.
1.2 Could the paper also include a comparison between uncertainty-based work and confidence-conditioned work (this paper)?
Last sentence (""existing Markovian policies that can only act according to a fixed level of pessimism"") of Section 4.2 is a false claim, which shows the paper does not cover a thorough literature review. One NeurIPS paper (Tactical Optimism and Pessimism for Deep Reinforcement Learning, https://arxiv.org/abs/2102.03765) uses the idea of dynamically apply the level of conservatism (optimism/pessimism).
The Gridworld example in Section 7.1 is unconvincing. The idea of the setup is similar to the Cliff Walking example in Rich Sutton's RL book, but the reward dynamics is weird: entering a lava state (a dangerous state) only receives 0 reward for the remaining trajectory and without further penalty.
3.1 Should the paper use a large negative reward for dangerous states instead of 0?
3.2 The paper only compares the optimal path between CQL and CCVL. However, a shorter path does not always mean a good path (like the SARSA vs. Q-learning performance in the Cliff Walking experiment). It would be more convincing if the paper includes a comparison plot of CQL and CCVL on cumulative reward.
Missing study on weight alpha. The paper changes from tuning conservatism ""delta"" to tuning weight ""alpha"" from my understanding. I believe the alpha is important in CCVL. As in Section 4.1, why tuning hyperparameter \alpha in CCVL is easier (or better in any aspects) than tuning an ""opaque"" hyperparameter (degree of conservatism)? Could the paper clearly explain or elaborate its statements?
4.1 Also missing alpha in Table 3: Hyperparameters setup
As in Table 4 - 6 in Appendix B., the CCVL shows a level of minor to moderate improvement in the majority of the selected Atari games. However, the variance of CCVL is way larger than CQL in all games in Table 4 settings, and way larger than CQL in some games in Table 5-6 settings. Given the level of improvement of CCVL, I believe the introduction of large variance is unacceptable.
5.1 The paper should include an explanation of the large variance.
Minors:
repeated article ""a"" in the first paragraph of Section 1
""delta"" -> ""\delta"" in the first paragraph of Section 4
mixed use of symbols of ""Pr"" vs. ""\mathbb{P}"" in Section 4
References order is confusing, making it hard to follow. It's neither alphabetically ordered nor first-appearance ordered. May I ask what is the reference setting?
Questions:
For Theorem 6.1, what is the drawback of only showing lower-bound on states than state-action pairs?
In Appendix A.2, could the paper include more detailed steps of solving inner-minimization over Q while proofing Theorem 6.2? | Strength:
The topic of offline RL is essential to the field. The quantile estimation and confidence adaptation proposed in this paper are novel to me and perform well empirically.
Weaknesses and Questions:
There are a couple of unclear points to me.
Q1:
The authors may want to specify the randomness source in equation (3), as
Q∗(s,a)
is itself deterministic, so the probability that
Q∗(s,a)>q
is either
0
or
1
. I assume that the probability bound means the bound on some estimated
Q∗
given the dataset, or is there a typo in (3)?
Q2:
Which objective ((4) or (5)) does the authors adopt in the empirical comparisons? Is the policy in (9) adopted in empirical comparisons? Does the uniformly sampled
δ
,
δ1
, and
δ2
affect the lower bound estimation in Algorithm 1, as the right-hand side does not align with (4)?
Q3:
In addition, the theorem feels insufficient to me. What is the benefit of utilizing different confidence levels in the target estimation of (4)? Can the authors justify their benefit compared with standard pessimistic methods in (1)?
Q4:
In the grid world experiments, how does CCVL learns the optimal trajectory given that there seems to be no coverage of such trajectory in the dataset (illustrated in the leftmost figure of Figure 1)? Is the performance of CCVL desirable, as offline RL is supposed to rely more on confident regions from the dataset? | Strength:
The idea of confidence-conditioned value function learning is a very novel way of encoding confidence levels into offline reinforcement learning algorithms. This is different from all previous papers using explicit or implicit confidence-based penalty in Q learning, as CCVL realize a more ambitious learning goal of learning Q at all confidence level.
At the test time, CCVL executes a non-Markovian policy which adaptively changes the confidence level \delta. This gives an additional capacity for test time learning/tuning for the policy. It is also a novel contribution of this paper.
Weakness:
Additional ablation study and more description of methods in existing ablation studies.
The implementation use Eq (5) instead of Eq (4), what will be the performance of CCVL with the update rule from Eq (4)?
A more detailed description of how AEVL and Fixed-CCVL are implemented should be included in the appendix.
The proposed algorithm requires additional sampling or computation. It would be better to conduct an analysis of how much these parts cost additional computation compared with baselines.
Eq (5) need to hold for all \delta. So the CCVL needs to perform the learning over all \delta or sample \delta enough times (as in Alg 1) for convergence.
Confidence adaptive policy requires more computation at the test time as well.
Minor comments:
In section 5, it is unclear why n(s) is approximated by phi(s)^T phi(s). According to the argument in O’Donoghue et al. (2018), ignoring the difference between actions, it should still be phi(s)^T \Lambda^{-1} phi(s), where \Lambda is the empirical feature covariance matrix.
Eq (4) and Eq(5) have ambiguity around the operator max_{\delta_1, \delta_2}. There should be a parenthesis to clarify the scope.
\hat{Q} is not well defined without showing the limit exists or contraction, as the Bellman operator is modified. | Strengths:
Problem is well-motivated, mathematical descriptions and derivations are detailed.
I really like the toy gridworld problem, which is illustrative of the expected effect of using the proposed algorithm.
Weaknesses:
Some theoretical details need clarification:
Could you clarify the proof for Lemma 6.1, I don't think I follow why
δ1,δ2≤δ′
""implies
Q(s,a,δ)≤Q(s,a,δ′)
, as desired"".
How does setting
δ1=δ
in Eqn (4) reduce it to Eqn (1), the numerator of the fraction under the sqrt is
log⁡(1/δ)
instead of 1
Is
α
still a hyperparameter, how should it be chosen, and what is its interpretation now that
δ
""confidence"" is introduced. Since in CQL
α
is used to implicitly control the confidence level in the pessimistic Q-values.
It's best to make clearer how much of the observed performance gains in experiments is from the proposed learning confidence-conditioned values, vs from some of the empirical/engineering decisions in Sec 5 practical algorithms (IQN, approx inverse visitation). Perhaps if there's a simpler (maybe tabular?) problem that doesn't have these deep RL complexities that would strengthen the results and help the reader understand the contribution better - and help to show what the learned Q-values are actually doing.
There are some typos throughout, ""hyperparamters"" and ""suprenum""",7.0,3.25
12,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Keywords: Fairness, Audit, Confidentiality, Zero-Knowledge Proof","Strengths: 1. The proposed method is built upon standard protocols. It is efficient and secure. 2. The proposed method can be easily extended to various fairness metrics. Weaknesses: 1. The data or the model are not revealed to the auditors. It would be better if the authors could discuss or demonstrate through experiments about the confidentiality from an adversary perspective. | Strength:
The idea of considering confidential proof of fairness for training is interesting.
They design and implement the zero knowledge proof protocol for verifying the fairness and efficiency.
They conduct experiments for verifying the effectiveness and efficiency of proposed methods. Weakness:
The paper proposed a method for ZK proof of decision tree training process. Also the authors mention that ""Our objective in this work is to provide a company with an approach that can be used to prove to a third party the fairness of a machine learning model trained and deployed by that company"". It seems that the proposed method does not meet the proposed goal. | I think this is in a great application of zero-knowledge proofs.",8.0,2.3333333333333335
13,Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting,"Keywords: Transformer, multivariate time series forecasting, deep learning","This manuscript is well written and organized very well. Also the studied topic is very interesting as it is usually the case that for MTS the cross dimension dependencies is overlooked.
The authors argue that as “For MTS, a single value at a step alone provides little information”, embedding in a way that “nearby points over time form a segment” could be beneficial. This argument seems interesting and valid as it is used and illustrated nicely (In Figure 1) by the authors.
In my opinion this is an interesting study. I think you can complete your great work by comparing your method with “Long-Range Transformers for Dynamic Spatiotemporal Forecasting” which you can find in “https://arxiv.org/pdf/2109.12218.pdf” and also include the very simple baseline that is mentioned in this study “https://arxiv.org/pdf/2205.13504.pdf”. | Strength: Dimension information is an interesting topic in the time series. This paper proposes Crossformer, which performs cross-time and cross-dimension dependency. The experiment results are comprehensive.
Weaknesses: (i) Efficiency is my main concern about this paper. Since 2-D attention is performed in Crossformer, the complexity is much more than in the previous 1-D attention method. Can authors provide some metric of efficiency (i.e., FLOPs, number of parameters, time) in Table 1? (ii) Interestingly, there is another paper (https://openreview.net/forum?id=GpW327gxLTF) that investigates the dependency between dimensions. But their conclusion may be different from yours. For example, the dependency between ILI is significant compared with other data sets. However, in Table 1, Crossformer is not the best compared with other baselines on ILI. Can you explain that? (iii) The extension from 1-D to 2-D is somehow trivial in time series fields. Thus, the novelty of the paper is weak in my view. | [+] The idea for utilizing cross-dimension attention in multivariate time-series data seems novel and interesting. The reduced time complexity is also an advantage.
[+] Its effectiveness is validated through extensive experiments across six datasets. Sensitivity analysis on hyperparameter setting and ablation study is also provided.
[-] In computational efficiency analysis (Section 4.5), it would be better if the authors can also provide a comparison of actual execution time.
[-] In the ablation study (Section 4.3), the MSE of the TSA layer with and without the routers should be compared to validate the proposed router mechanism.
[-] The authors mention that HED decreases the performance when the prediction length is short, and the possible reason is the information on different scales is helpful to long-term prediction. However, when it is combined with TSA, short-length prediction shows good performance. The explanation of how TSA overcomes the shortcoming of HED should be addressed.",7.0,3.6666666666666665
14,DaxBench: Benchmarking Deformable Object Manipulation with Differentiable Physics,"Keywords: deformable object manipulation, differentiable physics, benchmark","Strength:
As far as I know, DaXBench is the first deformable object manipulation benchmark platform based on JAX.
This manuscript compared a large set of methods on their proposed method, which provides the readers with a good sense of the performance of their simulator and the compared methods.
The provided code represents the visible reproducibility of the work.
Weaknesses:
I wonder how the mixture is handled in these demos: are they two-way coupled, one-way coupled, or any other coupling was used?
I personally did not follow the Lazy Dynamic Update part: MPM will require a grid for simulation. Does it mean the overall grid is 32632, or is the grid actually 128128128, but only a small region of it is activated? My personal feeling is this setting only applies to a small fraction of the proposed environments, is it?
My main reservation is the comparison at the framework level. JAX has a very similar position to taichi and Nvidia warp, which are more famous for physical simulation. And I would expect they have similar performance to JAX-implemented environments, is it? Is DaXBench distinguishable from them because of user-friendliness? Or better interface to deep learning modules? Or speed? | Strength
In my view, the experiments are the highlights of this paper. The authors implement the recently proposed differentiable simulation papers in their benchmark DaX, and compare them with gradient-free methods. The experiments include various robotic learning algorithms. This systematic evaluation indicates that the differentiable-physics-based method can indeed have better performance in some cases, but they also suffer from limitations as many gradient-based methods do, i.e. sensitive to the optimization landscape and initialization. And it also reiterates the importance of exploration in those differentiable-physics-based methods. Moreover, the authors perform real-world experiments, making the benchmark more convincing.
Weaknesses
Since the research of differentiable physics is progressing fast, the authors might better illustrate and clarify its difference from related literature. For example, PlasticineLab is another platform for differentiable soft body manipulation. Since both DaX (fluid, elastoplastic, and mixture) and PlasticineLab are using MPM as the governing dynamics, it is not apparent to me why PlasticineLab cannot support liquid and mixture (in Table 1) while DaX can. Moreover, it is unclear to me how the solid ingredients are represented in Pour-Soup. Are they elastoplastic objects or purely rigid bodies? Is there a coupling issue between the fluid particles and the ingredients? Furthermore, the Saving Memory strategy in Section 3.1 is the same as the checkpoint scheme in [1] (Section 4.2), where the back-propagation trades time for memory usage by recomputing the intermediate variables in each step.
[1] Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. ""Efficient differentiable simulation of articulated bodies."" In International Conference on Machine Learning, pp. 8661-8671. PMLR, 2021. | Strength:
A very respectable implementation efforts was put into this work. I think this simulator will be of interest to many in the community.
A large range of learning algorithms are studied. Offering some interesting insights.
Weakness:
Some of the more recent DOM works are not referenced to, all of these works offer insights / simulator for DOM:
ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation, Gan et al, Neurips 2021
ACID: Action-Conditional Implicit Visual Dynamics for Deformable Object Manipulation Bokui Shen, Zhenyu Jiang, Christopher Choy, Leonidas J. Guibas, Silvio Savarese, Anima Anandkumar, Yuke Zhu, RSS, 2022
VIRDO: Visio-tactile Implicit Representations of Deformable Objects., Youngsun Wi, Pete Florence, Andy Zeng, Nima Fazeli. ICRA 2022.
I'm not confident about the sim2real part. Saying the simulator has sim2real ability is a pretty big claim. However, only a demo video is provided without any systematic evaluation.",8.0,3.3333333333333335
15,Dichotomy of Control: Separating What You Can Control from What You Cannot,"Keywords: Offline reinforcement learning, return-conditioned supervised learning, stochastic environments, decision transformer","Strengths
The paper identifies an important problem and solution for RCSL with inconsistent conditioning, especially in stochastic environments.
The paper is clearly written with all the necessary details and flow to understand everything.
A good set of simple environments are chosen, including a didactic Bernoulli Bandit environment and MuJoCo benchmarks.
The paper has theoretical results on the consistency of their proposed method.
Weaknesses
Missing crucial comparisons
There are two key differences of DoC from prior approaches (i.e., VAE) using latent embedding of future to mitigate inconsistency: (a) Mutual Information (MI) constraint and (b) Inference from a learned prior and value function. The importance of these two components must be ablated on all the environments. Specifically, the following comparisons can be added:
VAE + Inference with learned prior (a separate copy with stopgrad) and value function
DoC w/o MI constraint (= DoC - a)
DoC + conditioning on highest return (= DoC - b)
Comparison against VAE
Since future-VAE also regularizes the z to the learnable prior conditioned on the past, this ensures that the latent z is incentivized not to use future information. Therefore, DoC's key benefit must come from utilizing the controllable part of the future in z while ignoring the environment transitions. Why is it expected that encoding the controllable information in z will improve the performance? What are example environments with this property?
The difference between the ideology of VAE and DoC being seemingly small is reinforced by the results on the mujoco environments, where the empirical performance of VAE and DoC are pretty similar. Even on reacher, where KL beta is not tuned well, VAE first reachers almost the optimal performance before falling. Therefore, it is not clear that DoC is necessarily better than VAE.
Is more stochasticity in mujoco environments expected to increase the difference in performance between DoC and VAE?
Is any other experiment (quantitative or qualitative) possible to show that the importance of encoding controllable future information in z is helpful? | Strength:
The paper has clearly pointed out the problem sets they are trying to improve on. And the way they came to their choices on mutual information sounds natural.
Most of the notations and the derivations are clear to follow
The experiment setup and resulting figures are clear
Weakness:
Some part of the derivation mentioned in the next section is confusing to me
Some of the phrasings are very abstract and can be more rigorous, such as ""By only capturing the controllable factors in the latent variable, DoC can maximize over each action step without also attempting to maximize environment transitions as shown in Figure 1"". | Strength
Empirical performance on environments with high stochasticity is strong compared to other future-conditioned supervised learning baselines.
The paper presents various examples and experiments to backup their claims.
Weaknesses
DoC inference requires sampling latents from the learned prior and choosing the latent with the highest (estimated) value. I worry this process adds randomness to the policy's performance and thus is unstable. Also, if there exists only few expert quality trajectories on the dataset and other trajectories are low quality, much more sampling will be required to include a latent with good returns. In addition, the paper does not discuss the protocol for setting K (number of samples hyperparameter).
The authors did not provide the source code, which makes the credibility of the paper questionable.
The paper lacks results on hyperparameter sensitivity. For example, how does the return curve change if we change \beta and K?
Questions
On Gym MuJoCo, why is time-correlated noise used instead of simple Gaussian noise?
Can the authors provide results on applying the proposed regularizer to other future-conditioned supervised learning baselines, for example RvS [1]?
[1] Emmons et al., RvS: What is Essential for Offline RL via Supervised Learning?, ICLR 2022.
######## Post-rebuttal comment ########
The updated manuscript addresses my concerns mentioned above. Thus, I am raising my score. | Pros
The paper is written very well and good to follow
The paper is focused on a relevant question in the field
Concepts are well introduced and explained
Experiments make sense and are executed nicely: testing in stochastic scenarios of different scale -- visualization, experiment repetition etc.
Cons: My main critique is around the point of universality: I feel a bit unsure about how universal the approach is for different forms of stochasticity in the environment and how well it would perform if no stochasticity would be present:
I feel a bit unsure about Eq. (7) (especially in continuous cases): to calculate the mutual information between the reward r_t and z (and s_{t+1}) you say you ""we set ρ to be the marginal distribution of rewards in the dataset"" and that is has to be a ""fixed sampling distribution of rewards"" But this sounds like you will have to make simplifying assumptions here about the stochasticity of the s_t and r_t:
are you limited to Gaussian and empirical (histogram) distributions? What happens if the environment is multi-modal or long-tail in its state or reward transitions?
do you learn the parameters of the conditional distribution \omega?
what happens if the environment would be stochastic in some regions and deterministic in others?
Looking at the experiment, I noticed the algorithm is only tested in situations where randomness is either Bernoulli (6.1, 6.2) or Gaussian (6.3).
The authors evaluate their method solely on stochastic tasks (or deterministic tasks, like MuJoCo but made stochastic). I do not know how well the method performs under standard (deterministic) settings. E.g. perhaps it will behave a bit more conservative because it assumes some form of stochastic behavior",7.0,3.75
16,Do We Really Need Complicated Model Architectures For Temporal Networks?,"Keywords: temporal graph, link prediction","Strengths:
Proposes a novel architecture that achieves performance competitive with other networks without memory or attention mechanisms.
Strong analysis and ablation of the different design choices made.
Weaknesses:
The model takes significantly longer to train and there is not significant analysis of the inference cost. This lack of discussion in the main paper also makes some conclusions about faster convergence somewhat misleading.
Questions: Why are the train average precisions for the competing methods in Figure 3 so much lower than the final results in Table 1. For example, DySAT training AP is around 60-70% while the final eval is 96.71. Are all the results from reimplementations or from taken from other papers.
It would likely be helpful to bring appendix C.3 into the paper and analyze and discuss more about the model efficiency since the model does take significantly longer to train than some baselines like TGN. If some of this is due to preprocessing, it would be helpful to separate the time required for each and/or analyze the inference FLOPs for your model compared to the others. | Pros: Focused on the temporal graph learning, the authors propose a conceptually and technically simple architecture GraphMixer for temporal link prediction. GraphMixer not only outperforms all baselines but also enjoys a faster convergence speed and better generalization ability. The author conduct empirical study to identify three key factors that contribute to the success of GraphMixer and highlights the importance of simpler neural architecture and input data structure.
Cons: There is not obvious weakness in the draft. | First of all, I'm not the expert in the graph learning area and my research focus is computer vision. From my side, I would to say: A simple method along with state-of-the-art performances satisfies me.
When I read this paper, I'm satisfied by the simple method, the detailed experiments. Thus I would like to give an accept rating.
However, there is still a little bit concerns:
I think the major novelty lies in the time-encoding part, also the proposed GraphMixer performs extremely better on LastFM dataset (c.f., Table 2). Could the author give explanations of why GraphMixer work well on LastFM? Since I only observe marginal improvements on other datasets.",7.333333333333333,3.6666666666666665
17,Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness,-''-,"Strengths
Developing robust text-to-SQL models is important and this benchmark can serve an important role to test that
The paper covers more comprehensive perturbation for all components of text-to-SQL tasks. Moreover the use of LLMs to generate perturbation makes their process scalable without making the task artificial (e.g. same fluency scores of questions)
The paper is clearly written and easy to understand
Weaknesses
I would have liked to see the robustness performance of large LMs such as GPT-3. It would be interesting to see how much incontext learning with large LMs can be robust to the proposed perturbations | Strengths
The paper is well-written and easy to follow.
It is important that SQL benchmarks test for linguistic variations and challenge systems to do more than lexical matching between the NLQs and the SQL queries
The experiments are convincing and thorough.
Figure 1 and Table 1 are great at helping readers follow along with the particularities of Dr. Spider.
Weaknesses
Did not see any. | The paper is clearly written, systematically laying out the kind of data variation they are targeting and how they aim to achieve it. The experimental results show significant problems or gaps with existing models against these variations, but the error analysis also points to potential directions for improvement. The paper seems to reflect deep domain knowledge on this problem area and good contextualization with respect to related works.
I found the paraphrase categories (Table 1) very helpful.
The Appendix is extensive, containing fairly fine-grained details about the implementation and execution of the experiments, as well as even more details on the results themselves. To be honest I think the Prompt Prefix tables (Appendix C) could be put in the code repo instead but it is a minor suggestion.
The error analysis and diagnostic insights seem very promising for future work. It's interesting that PICARD got confused by ""8 youngest winners"" but I guess queries in datasets probably often ask for Top 3 vs Top 8.
Fleiss Kappa of 0.61 is reasonable but not perfect agreement - were there interesting patterns of disagreement among annotators?
There may be weaknesses that a deeper expert on this area may have better context on, but I saw no red or yellow flags. | Strength:
A clear and useful goal. The paper addresses a specific and useful problem of evaluating the robustness of text-to-SQL systems, where existing datasets either cannot do, or have gaps in coverage.
Detailed and useful evaluation. Most of STOA algorithms are evaluated, with detailed break-downs and also examples to help reader understand.
Clear writing. The paper is well written, The Reviewer had no trouble following the main idea and technical details.
Helpful to the community. The dataset will be released, which would help the improvement of the robustness of related methods.
Weakness:
The necessity and complexity of using PLMs are not clear, as 1) the total amount of data generated is small, in the range of hundreds. 2) significant human review is still needed up and down the pipeline.
The size of DR SPIDER (The Reviewer's impression is 1k) is much smaller than SPIDER (10K questions).
Detailed suggestions:
Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics?
Section 3.2 paragraph 2: better explain what is ""naturally occuring tables and columns"".
Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting?
Section 4. Upper-case of common model names, ""Bert-large"" -> ""BERT-large"" etc.",8.0,3.25
18,"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",-''-,"While using LLMs for auto-formalization seems novel enough, this is the first work where LLM is applied to generate the whole proof. The idea of guiding formal proof generation with informal proof sketches is novel. The experimental section justifies that sketching indeed helps generate more correct proofs. The study in this paper demonstrates that the model makes use of the alignment between formal and informal proofs. This is the main contribution of the paper.
The paper also contains some analysis of how the prover's performance depends on the correctness of informal proofs. It turns out that the prover can fix some simple errors in informal proof and even ignore the informal proof if it is incorrect. This observation is interesting and suggests that the model can perform some kind of formal mathematical reasoning.
The paper is generally well written, experimental section demonstrates state-of-the-art results on the MiniF2F dataset, ablation study justifies the usefulness of the guidance by the informal proofs and the alignment between informal and formal proofs. The approach is built on top of existing pre-trained models and seems easy to implement if the models are available.
Cons:
The examples play an important role and should be carefully constructed. In the paper, the authors try to use appropriate examples: first of all, all of them are from the MiniF2F dataset, secondly, whenever the problem type is known (from the problem name, e.g. it contains ""number theory"") they use the corresponding examples. So, the approach cannot be easily applied to other datasets.
More analysis of the model should be conducted. Figure 3, ""Human informal proof drafts"" implies that it is also important to have good examples as different proof attempts differ only in provided examples. How close are the examples of successful proofs to the statement compared to failed proofs? It is also interesting to separate the effect of chosen examples from the effect of generated informal proofs. For example, we can use the same 3 examples for all proof attempts and only change the informal proofs.
The method requires access to the LLM which is not widely available and limits the reproducibility of the approach. | This paper studies a promising direction of ATP, to guide theorem provers using informal proofs. The use of proof sketches is reasonable. The steps of prompting LLMs are technically sound. The proposed approach requires no additional training on ground-truth proofs but achieves impressive results.
One question is that what if replace with the Sledgehammer + heuristics prover as the Thor prover? How much improvement can we get from Thor? | Strength:
The paper is well written and easy to follow. I enjoyed reading it.
The approach is technically sound, conceptually simple (which is good) and empirically working. Even better, the design of entire process is quite natural: first coming up with a proof in natural language and then turning it into a formal proof, organized in a way that subgoals can be easily automated, which is exactly what a human ITP expert would do.
The paper nicely assembles existing approaches from subareas of ML for theorem proving (i.e., theorem proving in natural language, autoformalization, and learning-based automation for ITP) to make a complete procedure that provides high-level automation for interactive theorem proving.
Weakness:
The good performance is probably not that surprising. Both Minerva (used for drafting) and Codex (used for sketching) might have seen the problems somewhere on the internet, formal or informal version in different forms (e.g., comments or code from different provers, or math overflow etc). After all, the competetion problems are famous.
Minors:
It is reported that some problems in MiniF2F are not well formalized and some are wrong. The performance should perhaps be interpreted carefully. But apparently this isn't the authors fault.
In related work the authors wrote: ""... perform search over the generated subgoals using powerful search methods such as MCTS"". Recently there have also been approaches (e.g., [1][2]) proposed where learnable search could be useful for formal reasoning. Citations are missing.
[1] Wu et al. 2021, TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning
[2] Jonathan Laurent, André Platzer 2022, Learning to Find Proofs and Theorems by Learning to Refine Search Strategies: The Case of Loop Invariant Synthesis | Strengths See above
Weaknesses
I would like to see existing SOTA results.
if you could fit it, a plot of success v the number of steps in Appendix C would be nice.
Next steps I would like to see in future work
it would be particularly nice if DSP found new or 'better' proofs (akin to Newell, Shaw & Simon's Logic Theory Machine on Russell & Whitehead).
I would like to see consideration of multiple proofs of the same result: Nipkow's 2009 JAR article formalized two of Geanakoplos' three proofs of Arrow's impossibility theorem, to explore how easy different proofs mode could be formalized. (I would love to see style transfer eventually: prove this result in the style of von Neumann, or Erdős.)
Can the structure of proofs be better exploited? For example, Hilbert-type proofs correspond to trees. Isabelle-type proofs are more complex due to local assumptions. (Was this the concluding reference to HyperTree?)
Thomas Hales' had a 'formal abstracts' project a few years ago, in which he sought to formalize abstracts of mathematical papers, easing searches across notation and terminology. Could DSP help?",7.5,3.5
19,DreamFusion: Text-to-3D using 2D Diffusion,"Keywords: diffusion models, score-based generative models, NeRF, neural rendering, 3d synthesis","Strength: The method is pretty novel and effective. The analysis of the instability of the loss function is technically sound, and the solution makes the model effective in optimizing the high-quality 3D content based on the text prompt.
Weakness: (1) the proposed text-to-3D generation is a bit tricky to evaluate. Since the model is per-scene optimization with a pretrained DM and text prompt, both the quality and text input will affect the synthesized output. Also, as it is difficult to have real 3D ground truth to compare the generation quality (in comparison to 2D generation) (2) while the generated results correspond to the text input well and sometimes generalize to very novel scenes, I think it is only thanks to the original power 2D diffusion model. The generated 3D, however, seems to have severe mode collapse issues compared to 2D models (blurry, dark, and unnatural high-contrast colors). Is it because the objective function encourages that? What might be the possible way to avoid such a generation further? How can we generate multiple 3D scenes with the same text? (3) the model seems to require a lot of computational resources to optimize for a single scene. Will the same approach be generalized to synthesize the radiance field without optimization directly? | Strength:
The method allows flexible control of the resulting 3D model of the given text, e.g. it can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment.
The approach requires no 3D training data and no modifications to the image diffusion model.
The 3D rendering process is well-considered such that many local minimums can be avoided.
The paper is well written, and one can quickly grasp the main idea and many technical details.
The paper presents a novel way to sample parameter space instead of pixel space.
Weakness:
On P4, the paper says that it minimizes the diffusion training loss:
LDiff
. As shown in Eq. 1, it is
ϵϕ
that is used to compare with
ϵ
. However, in Eq. 2, the variable becomes
ϵ^ϕ
. Please explain the discrepancy here.
On P6, above Eq. 5, the paper says that it performs alpha-composition from the back of the ray to the camera. This is quite unusual, as in NeRF we usually go the opposite way, i.e. from near camera to far, such that objects near the camera are first added, and the further objects can be occluded.
There seems no specular term in Eq. 7. However, some specular effects can be seen in Figure 4, e.g. on the armor. I wonder where those effects come from.
On P6, the description of the second MLP for scene generation is too brief. How important is it for the final synthesis performance? Why does this MLP only take ray direction as input? Why does it only produce RGB color instead of 3D geometry as well?
Some important results are missing from the ablation study: 1) instead of adding components one by one, the study should consider removing each component separately, such that it is easier to understand which component is the most relevant; 2) the comparison between the new SDS loss and the previous CLIP loss in DreamField is missing.
Minor issue: In Table 1, it is better to add a dash in the CLIP names, for example, CLIP-B/32. | ++ The proposed method should be useful for 3D asset generation, and the results are very promising.
++ The loss from Score Distillation Sampling opens a new way to use the 2D diffusion model as prior for NerF model optimization.
++ The diffusion model is pretrained and can be directly used in the text-to-3D task.
There are also several concerns. -- In Fig. 1, some images are without a symbol indicating the prefix, and there is no ""wide angle zoomed out...""
-- CLIP model is not explained and cited in Sec. 1 while Sec. 4 shows the reference of CLIP.
-- The ground truth noise
ϵ
in Eq. 1 is not defined or explained before use. What does
ϵ
look like?
-- It is better to define the KL operation in the main text near Eq. 4, letting the reader have a smoother understanding.
-- Why use Imagen in the proposed method? How is the performance if using other diffusion models? | Strengths:
The visual quality is obviously phenomenal in generative modeling research, although the formulation is not generative modeling. Under the umbrella of creative content synthesis, it becomes interesting whether do we really need a real-time generative model, or simply a faster (1.5 hours in this work is still too long) inversion method.
Weaknesses:
This is personal, but I do not like the writing strategy of the paper. Many of the core concepts in this paper are borrowed from Dream Fields, but the abstract and introduction are written in a way as if the concept is first proposed in this paper. And the differentiation between DreamFusion and Dream Fields is written in obscure sentences. I would suggest the authors take a deeper look into how to present the differences.
In Figure 5., the re-implementation version of Dream Fields is visually much worse than the original version, despite the quantitative scores seeming to be fine. I would suggest the authors adding more visual comparisons between these two methods in the Appendix.
Questions:
Current 3D generative models mostly rely on a canonical coordinate to align objects, and therefore fail on categories without aligned canonical coordinates (such as trees or scenes). Does Dream Fusion have a similar limitation?",6.75,5.0
20,Efficient Attention via Control Variates,"Keywords: attention mechanism, transformers, random features, control variates, importance sampling","Strength
The idea is easy to follow, and the paper is well-written.
The mathematic analysis is given to explain the dissecting RFA with control variates.
The EVA attention with simplification achieving higher accuracy than softmax in image classification is impressive.
Weakness
Overall, I would say the paper is in good shape in terms of detailed mathematic analysis and solid experiments on multiple vision and natural language tasks. My main concern is about the usage in the practical. Though the EVA achieves higher accuracy (very subtle) than softmax in table 1, the softmax is still a better choice in terms of accuracy, #params, and FLOPs. According to numbers reported in all tables, softmax still dominates the performance and has comparable #param and FLOPs to EVA.
The not-well-designed approximation in computing control variate could impact the EVA's performance. As no code is provided, it is hard to trace the author's implementation. | Strength:
The paper develops an efficient attention mechanism EVA via control variates. This novel attention mechanism is efficient and can bridge the gap between RFA and exact softmax attention. The EVA attains a good trade-off between modeling quality and efficiency.
Weakness
The author should compare more state-of-the-art linear attention like Flowformer or FLASH. | Strengths
The proposed EVA approach is motivated quite well and the paper is quite clear.
The theoretical results (Propositions 1, 2 and ) in the paper are impressive and appear to be quite sound. (I didn't check the extensive appendix though.)
The experimental validation is quite convincing on a variety of problems.
Weaknesses
Certain choices are not well-motivated. For example, rather than learning the partitioning scheme via a K-means-like algorithm for the keys receiving clustered coefficients (instead of optimized control coefficients), the clusters (partitions) are chosen as contiguous chunks. This is called out in the final section though.
It's not clear how hyper-parameters such as E and C in EVA were tuned. | A nice analysis of RFA that provides some insight into what the method is doing.
Interesting method building on the analysis.
Some weak empirical results suggest that this method may not scale to larger models (this is common for RFA-based efficient attention mechanisms).
A couple missing baselines.",7.5,3.5
21,Efficient Conditionally Invariant Representation Learning,"Keywords: conditional independence, kernel methods","Learning conditionally invariant representation is a very important problem that has the potential to solve a wide of issues in machine learning, such as distribution shift, spurious correlation, racial bias from datasets, etc. The authors consider a quite general setup where one seeks to convert the hard-to-measure conditional independence into a manageable form.
The major strength of this paper is a new measure that can be efficiently computed from data. This measure involves marginal independence between
X
and
(Y,Z)
under square-integrable functions, thus cleverly separating the computation of (potentially high-dimensional) inputs
X
from
(Y,Z)
.
Also, the empirical evaluation seems to favor this CIRCE method when compared with existing methods. This suggests that the proposed approach has good potential impact.
A weakness of this paper, partly due to the limit of the paper length, is a lack of analysis of the statistical efficiency (or power in statistical tests). | Pros:
a new CI meansure with finite sample estimate with convergence guarantee
the method is shown useful in some experiments
writing is good (expect for several places where notations are not defined in the main text)
Cons:
except for avoiding minibatch computing of
Z
and
Y
, it is not clear what other benefits the proposed statistic could provide.
the experiments are somewhat toy and the results are mixed | Strengths
The authors point out that Eq. (4) has a nice property that is useful for mini-batch training.
They provide a clever way of estimating it using RHKSes.
They prove the correctness of the proposed method.
The experiments are interesting and demonstrating the effectiveness of the proposed method.
Weakness
I suppose that the paper claims the proposed method is computationally efficient, but I could not find any analysis or empirical results supporting it. In fact, Theorem 2.7 suggests that we want M to be large enough to compensate the slow rate, but calculating the kernel matrices may not be scalable in
M
.",7.333333333333333,3.0
22,Efficiently Computing Nash Equilibria in Adversarial Team Markov Games,"Keywords: multiagent-reinforcement-learning.marl, rl, reinforcement-learning, learning-in-games, optimization, game-theory, policy-gradient","Strength:
The problem is well-motivated and the paper is written well overall. The proposed approach to computing an approximate Nash equilibrium looks non-trivial and brings in many interesting concepts. The authors also provide a good summary about the related work.
Weakness:
Section 3.3 could have been improved to make the main idea clearer. It is unclear to me what is the main advantage of the proposed methods? In particular, why not just solve Q-NLP without the regularizer in the objective function, which gives a Nash equilibrium directly and seems much more manageable than the current formulation?
The approach relies on an oracle to tackle a computational obstacle, which may be crucial. This further deepens the question of how meaningful the proposed methods are compared with solving Q-NLP without the regularizer --- now that there's an oracle to use, so supposedly it also simplifies the problem of solving Q-NLP without the regularizer. | Strengths
The paper introduces the first poly-time algorithm to compute a Nash equilibrium in the setting of adversarial team Markov games. Computing a Nash equilibrium is an important practical problem, but unfortunately the general-case is difficult. Therefore, it is useful and relevant to consider special cases such as the coalition considered here.
The algorithm has good scaling with the action sets of each agent; that is, it scales with their sum rather than the product.
The techniques used to obtain this result are well described at an overview level and compared to the most relevant related work which is to compute a Nash equilibrium in the analogous normal-form setting. The challenges in generalizing the normal-form setting to Markov game include: nonlinear program with a set of nonconvex constraints, which requires the Arrow-Hurwiz-Uzawa constraint qualification technique.
Weaknesses
The type of game considered is very special. It is only a modest generalization of two-player zero-sum games. More practical would be algorithms that have exponential worst-case but still run on practical examples of n-player Markov games. Still, any generalizations from fully cooperative or competitive two-player games are welcome.
The practical scalability of the algorithm is not evaluated. Although it is polynomial, the number of iterations looks very large from inspection of the pseudocode. Still, this is a theoretical work, so this isn't too significant of a weakness. | Strength
Looking for Nash Equilibrias in Markov Game is an important task, and reducing the time complexity from exponential to polynomial is also a significant contribution.
Weaknesses
There are many parts not very clear in the IPGMAX algorithm (for details please see the questions below)
My Questions
Proposition 3.1 shows that there must exists such a
t∗
, and Corollary C.1 shows that with high probability, there is one such
t∗
in the randomly chosen set. However, it is still not very clear to me that how you can find this
t∗
is this set. Do you mean that it is easy (with low computation cost) to check whether
xt
is good or not? How much is the computation cost here?
In line 6 of Algorithm 1, we need to look for the best response of the adversary. What is the computation cost here?
In line 7 of Algorithm 1, we need to compute the gradient of
Vρ
. What is the computation cost here?
It is mentioned that the IPGMAX algorithm is a decentralized one (which may avoid communications between players), but I am wondering whether line 6 and line 7 in Algorithm 1 could be done without any communications? Do you mean that these steps could be done even if all the players (e.g. the team players) do not know others' policies in the last time step?",7.0,2.6666666666666665
23,Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement,"Keywords: low-light image enhancement, high-resolution image processing, Fourier transform, benchmark","Strength
A reasonably sized, high-resolution new dataset for joint luminance enhancement and denoising is provided.
The proposed network architecture cleverly mixes and matches the amplitude and phase of the low light input and normal light GT to constrain the denoising network.
Weakness
The importance of the Fourier branch (FB) compared to a more conventional spatial branch (SB) in FourSpa is less than expected and should be further validated. Swapping FB with SB brings about a
0.6 dB
difference in PSNR, as ablation studies #1 and #2 show, which made me wonder if 2xSB (i.e. increase the size of the SB) would have similar performance as FB+SB in FourSpa. Moreover, for the adj. block, it's necessary to show the performance of SB only and SB increase to the comparable size of SB+AM+PG. Lastly, there should be a Fourier-free architecture where there are only SBs (of the equivalent size of SB + FB + AM + PG + SB, e.g. #9) in both FourSpa and Adj. Block. The author should consider adding those in the final version.
What are the computational and time costs of FFT/IFFT-related operations? It will be useful if the author can add this to the ablation study table.
Even for consumer audio/video market, ""HDR"" and ""4K"" are oftentimes mentioned together. Moreover, HDR is common for low-light scenes as light sources can often be seen in night-time photography. However, HDR scenes are not considered in this paper. I suggest the author mention such drawbacks of the proposed dataset in discussion and/or indication of the dynamic range of the proposed datasets (i.e. the specs of the cameras used for the data capture).
Also, since an important aspect of this paper is luminance enhancement, it's natural to discuss HDR tone-mapping. A simple search on joint HDR and deoising leads to many recent papers on the topics. I suggest the authors consider including such papers in the related work section.
Denoising research is at a stage where one has to ""pixel peep"" to tell the difference. Please provide zoom-in insets of the denoising performance in the result figures.
The proposed UHDFour method is only tested on one external dataset, LOL. It's reasonable to include at least another state-of-the-art dataset
For the dataset comparison table, the authors should add SID datasets. The argument that SID only works for particular Bayer pattern is not valid. SID data's Bayer or Fuji pattern raw can be demosaiced to RGB raw, meanwhile All the authors' data also undergoes some demosaicing too. | Strength:
This paper is well written and easy to understand.
The performance is great.
The novelty is enough for ICLR. The feature modeling in frequency domain and the dataset.
Weakness.
I'm little confused about table 2. Why the performance of proposed method is not shown in Table 2?
I suggest authors also show the Amplitude and Phase in the network. People can see how the network improve the Amplitude and Phase. | Strong points.
A new method is proposed for ultra high-resolution low-light noisy image enhancement with notable performance advantages against existing methods.
The network design has a clear motivation with some interesting experiments to justify the observations.
A new ultra high-resolution dataset of 2150 low-noise/normal-clear paired 4K images is constructed.
The paper is written well and easy to follow.
Code and dataset will be released.
Weak points.
The first observation is justified via visual illustrations and quantitative experiments while the second one is illustrated by visual comparisons. It would be better if the authors can provide some interpretations. Can authors explain from the FFT why these phenomena happen? The second observation only mentions that HR images and their corresponding LR images share similar amplitude patterns, but how about the phases? Do they have similar patterns? If not, would the noise removal in LR version affect that in HR images?
In the 3rd paragraph, Introduction, there are some vague descriptions when talking about existing methods, e.g., ""existing methods are mainly..."" and ""some studies..."". I suggest adding references here so that readers can have direct references.
The LRNet/FouSpa block designs seem not to be aligned with the observations. For the FouSpa block, it is mentioned that ""we observe that luminance and noise can be decomposed in the Fourier domain"", but the design (Fig. 4) shows that the Fourier branch only separately processes the A and P, then combines them together using IFFT. There is still a Spatial branch that works with the Fourier branch. It is not clear why there should still have a spatial branch and how the Fourier branch enhances luminance in A and removes noise in P. Regarding the LRNet design (Fig. 3), it is not clear why FFT and Conv layers are paralleled. Further, do A_r and P_r have any supervision signals? Would the authors show some intermediate visualizations of A_r and P_r?
Regarding the dataset acquisition, it seems that images are taken in normal-light scenes, and low-light images are captured with under-exposures. In this case, it seems the noise would not be very severe (e.g., in Figure 5 it is hard to see noise). This makes this paper more related to under-exposure image enhancement but not low-light image enhancement. It would be better if the authors could provide more justifications for this issue. The dataset split is 2000 for training and 115 pairs for test (The train/test ratio is 20:1), which I think is extreme. It would be better if the authors could justify this.
A minor issue is that ""Table 5"" on page 8 should be ""Table 4"".
In Section A of the Appendix, I think it would be better if the authors can discuss how FFT is used in other low-level vision tasks, e.g., image deraining, denoising, and deblurring, and highlight the differences to the proposed method.",7.333333333333333,4.333333333333333
24,Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,"Keywords: world representation, GPT","Strength
The experiments are well motivated and well designed. The claims are well supported and convincing. The findings are significant.
The paper is extremely well written and crystal clear. I personally find it enjoyable to read.
Weakness
The training is a little strange to me. Why only predict y_T but not each y_t given its history? Just like how LM is pretrained.
There are some typos and grammatical errors. E.g., parenthesis not matched for ""(an example of which can be seen in Figure 2."", ""as well as the change in next-step prediction in"". E.g., spacing is often weird, esp. before ""Figure""s.
More visualizations may be helpful, e.g., when talking about the ""counterfactual case"". | Strengths:
The paper addresses a highly relevant problem, and clearly states it.
The experiments are well-planned and well-executed, directly addressing the research problem posed.
The paper is clearly written, and is a pleasure to read.
Weaknesses:
The choice of the metric used in the part 4.2 (EVIDENCE FOR A CAUSAL ROLE FOR THE REPRESENTATION) is not fully justified, and the calculation process is described too briefly, which may cause confusion. | Strengths
Present new evidence that GPT variant can learn Othello board state, extending previous work on chess
Presents some evidence that their intervention technique to alter the ""emergent world state"" is effective
Using the intervention technique, they produce useful visualizations of ""saliency"" which describe how much a certain board position influences the prediction of Othello-GPT
Weaknesses
There are several issues with clarity in the paper that made it difficult to understand on the first read.
I find the last paragraph of section 4 hard to understand, although I do think I grasped the method after reading the rest of the paper. It would help if the authors updated Figure 2 to show the actual metric/value that is used to determine if a representation is causal or not.
Similarly the description of the counterfactual case is not very clear. Please use notation to clarify the actual metric/value that is used to determine whether the internal representation is of the board ""rather than just a sequence""
In the Related Work section, they authors refer to their work as having ""a focus on the geometry of internal representations"", but no geometry-related work is shown in the main results at all. This is only presented in the Appendix A, and this is not referred to anywhere in the main text.
In Tables 1 & 2, I believe ""randomized"" corresponds to probes trained on random weight GPT, rather than a random dataset. Table 1's caption makes it appear as if it is simply another dataset.
The authors do not present any measures of variance for the probe error rates in Tables 1 & 2 (e.g. standard deviation or confidence interval over random seeds). This makes it difficult to interpret the difference between the probes trained with the actual Othello-GPT model and the ones trained with the random weight model.
The authors also show that nonlinear probes perform better than linear probes, but do not address why this might be the case, despite the fact that they state this is a major contribution of the paper. What are we to take away from this result, other than ""the probe may be recovering a nontrivial representation of board state""? The authors claim that the intervention experiments ""validat[e] this hypothesis"", but they do not discuss this hypothesis again. It appears as if this claim may be supported by Appendix A but they again do not refer to it at all.
Finally, the authors fail to make any additional argument as to why these results would be compelling to a wider audience beyond those interested in games / Othello. In the conclusion the authors write that ""the tools described in this paper—nonlinear probes, layerwise interventions, and latent saliency maps—may yet prove useful in natural language settings"". However it's unclear to me how these specific tools would contribute anything above and beyond current work in NLP using probes, interventions and feature attribution. | Strength
Nice task setting: I like how the authors use board games as toy tasks for studying sequential data. Natural language, due to many confounding factors and properties, is not easy to be used for studies like in this work (e.g., flipping state of a tile). I'd love to see more work along this line, that can potentially help us to better model language, and the world that language describes.
Towards understanding LMs: At the same time, this work propose concrete methods towards making LMs more interpretable and transparent.
Nice visualization method: I like the latent saliency map method, which quite effectively represent dependencies among tiles. This can potentially be used in analyzing 1) word or concept dependencies in language generation; and 2) event or action dependencies in an offline RL setting (e.g., using decision transformer to model sequences of transitions, which essentially is an auto-regressive generation problem).
Writing: I enjoy reading this paper, it's clear in most part of it.
Weaknesses
As the authors acknowledge, it remains unclear how methods proposed here can generalize to broader context, i.e., natural language, which is large language models designed for. I agree that this work is an ""pilot study"" towards that goal, but it would make the work much stronger if the authors can discuss more about, e.g., how board game trajectories connect with language, what are some common property, what are missing, and what are some potential paths towards that goal etc. As I mentioned among the strength, board games are good toy tasks studying sequential data, which gives researchers more control. It's just less clear in the sense of, how do we jump from this to language.
Please refer to the questions and concerns below.",7.5,3.75
25,Encoding Recurrence into Transformers,"Keywords: Recurrent models, Transformers, sample efficiency, gated mechanism","Strengths:
The paper is easy to read, and generally well written.
The paper evaluates the proposed method on various different tasks such as time-series forecasting, code and language modelling. The paper augments the proposed method to various different transformer variants and compares the performance with respect to the unmodified baseline.
Weakness:
The problem of integrating recurrence and self-attention is an important research problem. There exists some existing ways on how to augment transformers with recurrence such as Temporal Latent Bottleneck [1] and Block-Recurrent Transformers [2]. The idea behind TLB is to ""divide"" the sequence into chunks, and within a chunk use self-attention and to access information across chunks the model needs to use recurrence. It would be useful to compare the proposed method to these variants to futher analyse the pros and cons.
It may also be useful to study the proposed method by varying the capacity of the network to see how well the underlying idea scales.
[1] Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning, https://arxiv.org/abs/2205.14794 (NeurIPS'22)
[2] Block Recurrent Transformers, https://arxiv.org/abs/2203.07852 (NeurIPS'22) | It is novel enough to combine the advantages of two famous models (Transformer, RNN). Also, the combining method looks applicable to a variety of scenarios. The experimental results are impressive, showing superior performance to previous Transformer.
I think the draft would become better if there is a more complete explanation and figures about the self-attention with recurrence (RSA) operation. | The motivation is clear and the algorithm is sensible.
The proposed method is tested on several benchmarks.
RNN block discussed in the paper is basically a masked linear aggregation of the tokens (``masked'' means each token can only attend to the previous tokens), with the aggregation weights (
Pmask
) specially designed. It would be helpful if the authors can compare to the baseline where the
Pmask
is learnable, i.e., using a learnable masked linear aggregation. Another baseline would be a learnable unmasked linear aggregation. These comparisons can tell us if it's the RNN formulation that matters or just the linear aggregation.
The authors argue that, transformers on small-scale recurrent data will overfit, thus introducing the RNN module which has a better inductive bias can help prevent the overfitting. However, there is a learnable gate-control parameter to decide whether the model should rely more on self-attention or RNN. Won't that encourage the model to rely more on self-attention in order to overfit the training data?
Since the authors use a linear RNN, I wonder how much model capacity are we losing by linearizing the RNN, i.e., how huge is the gap between linear and non-linear RNN, performance-wise?
In Section 3, the authors denote by
ptotal
the total temporal patterns in the data. How is this value defined?
Section 2.1,
b∈Rdin
-> $b \in \mathbb{R}^{d}.",7.333333333333333,4.0
26,Extreme Q-Learning: MaxEnt RL without Entropy,"Keywords: reinforcement learning, offline reinforcement learning, statistical learning, extreme value analysis, maximum entropy rl, gumbel","I think this paper brings forth some very interesting ideas to the table. Using Gumbel regression to model Q updates is to the best of my knowledge quite novel and in my opinion definitely something worth further exploration. The paper overall was quite easy to read and the authors did a very good job of justifying most claims and the technical choices involved in the algorithm. Some of my comments on some specific points in the paper:
Intuitively I agree with the authors in that I think this EVT-based approach would work quite well in an offline setting (and experiment results do support this) since it naturally introduces conservatism into the algorithm. However I would definitely like to understand better where the source of improvement comes from and especially some additional comparisons with other algorithms in this regard, this could come in the form of a toy example that gives easier visualization.
For the online setting, I'm not fully convinced why this approach would be better than an algorithm like SAC. Performance for online RL do not seem to offer much improvements. The environments used in the paper are generally considered to be fairly easy environments and the number of random seeds used is very small (3 or 4). Also unlike PPO, numerical instability isn't usually a huge issue for vanilla TD3/SAC for these particular environments.
Adding to the above point, it does seem that numerical instability is quite a major issue from a practical perspective, though the authors have introduced ways to mitigate this, I feel that this could still be a huge obstacle to wide practical adoption of this approach.
One thing you mentioned in the conclusion section about potential future directions is to integrate automatic parameter tuning for the temperature. Could you elaborate on some of the challenges for doing this and why using the mechanism for this introduced in [1] would be difficult?
[1] Haarnoja, Tuomas, et al. ""Soft actor-critic algorithms and applications."" arXiv preprint arXiv:1812.05905 (2018). | Strengths:
The paper motivates their algorithm based on theoretical foundations and provides Lemmas to support their final loss functions.
The paper is well-written and easy to follow.
Empirical results against strong baselines show significant improvement
Weaknesses:
As far as I can tell, hyper-parameter tuning is done on the test set, there is no separate validation set.
It will be great to have this algorithm available as open source. | Strengths
Incorporates the MaxEnt RL framework while avoiding the major problem of offline RL (extrapolation error from referring to OOD examples).
Modeling TD-error as a Gumbel distribution seems to be more appropriate compared to modeling it as a Gaussian.
Weaknesses
On offline RL, the proposed method still requires double-Q learning technique even though the method is expected to be more robust to Q-value overestimation.
The experiments table for offline RL (Table 1) is misleading in several aspects:
First, the hyperparameters for the proposed method are tuned dataset-wise (refer to Table 4). However, previous works such as CQL or IQL keep their hyperparameters the same at least for each environment. For example, IQL fixes its hyperparameters the same for all MuJoCo locomotion tasks. Thus, the table results are not a fair comparison. Since the paper does not provide any hyperparameter sensitivity results, it is hard to conclude that the proposed method is actually the new ""state-of-the-art"" as the authors argue. Also, even if dataset-wise hyperparameter tuning is freely allowed, recent works seem to show higher performance on several datasets [1, 2].
Second, the runtime comparison also seems to be misleading. The authors note that the proposed method converges faster than IQL (Figure 6) and runs half the epochs compared IQL. However, based on the source code in the supplementary material, it seems that the proposed method uses a much larger batch size (1024) compared to IQL (256) [3]. Are the authors increasing the batch size and claiming the proposed method converges with much fewer iterations?
Questions
On Lemma 3.4, why is the first expectation over \mu while the second expectation is over \pi?
On Figure 3, what does -DQ on TD3 mean?
[1] An et al., Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble, NeurIPS 2021.
[2] Cheng at al., Adversarially Trained Actor Critic for Offline Reinforcement Learning, ICML 2022.
[3] https://github.com/ikostrikov/implicit_q_learning | Strength
The paper is very well organized, with sufficient background introduction so that readers without much previous knowledge could also understand the context.
The usage of Gumbel regression is well organized, with good theoretical support.
The paper also makes a good connection and shows how the minimizer of the Gumbel regression objective recovers previous update rules that require explicitly policy action distribution knowledge such as SAC or CQL, while XQL does not require the access to the policy during the value function update.
The practical algorithm shows very promising improvement over previous methods, especially in the offline benchmarks. The practical algorithm seems also easy to be built on previous online methods.
Weakness
The motivation for using Gumbel regression seems not very obvious in the continuous action region because it's not obvious how to take the max operator. I am also confused about the presentation of the practical motivation (such as Fig.1) for two reasons:
The bellman error is recorded during the training. However, some value functions we learned during the training may not even represent a valid value function for any policy? Then what does this bellman error represent?
The bellman error is actually not calculated with max, but with the action that the corresponding policy takes. Again I understand this is due to the difficulty from the continuous action space, but this seems to deviate from the theoretical motivation.
Although the paper claims that not requiring the access to the policy during value function update is a merit of the proposed algorithm, I could not see why this is significant in practice: in practice, we still need to train a policy anyway so we could assume we always have the access to the policy information?",7.5,3.25
27,Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search,"Keywords: search, adaptive horizon, verification, deep learning, hierarchical planning","Strengths
I'll be short here. See discussion below on my interpretation of the results.
The paper is mostly well-organized and well written
The ablation study confirms the role of the specific modules.
I appreciated the detailed algorithms in the appendix
The conclusions are very clear, at least for the domains and conditions where it was tested.
Weaknesses
Scholarship: I think that Czechowski et al. (2021) should be mentioned as a starting point. The manuscript seems timid about it. I think a stronger paper would make that clear. Here is two key quotes:
""kSubS is the first general learned hierarchical planning algorithm shown to work on complex reasoning domains Czechowski et al. (2021) (called BF-kSubS there), attaining strong results on Sokoban and Rubik’s Cube, and INT. kSubS can be viewED as a non-adaptive version of AdaSubS realized by a suitable hyperparameters choice: a single subgoal generator and inactive verifier (t lo = 0,t hi = 1).""
""Most of the hyperparameters, both for training and evaluation, follow from Czechowski et al. (2021). The most important parameter of AdaSubS is the set of k-generators to use and the number of subgoals each of them generate.
Czechowski et al. (2021) introduced the idea of a search to achieve the goal in k steps. The paper has multiple references on how they used the same parameter.
Scholarship: the paper does not review the state of the art on this problem. If the state of the art is not in the same scope, then related work should be discussed anyway.
For instance, see Cameron et al, IJCAI 2021, below.
BestFS is not explained well enough
(See before my comment on contrib 2 below)
Given this is the most important baseline, the paper must make sure that the algorithms was properly tuned.
It seems it's a simple modification respect to the others, but perhaps there are hyperparameters that would work better for BestFS in comparison with AdaSubS
No limitations are discussed (see my first question below as an instance)
Questions: PLEASE ANSWER THESE ONES
Does the algorithm keep a list of close nodes?
Do you cache the result of evaluations in Alg 1 or Alg 2?
It seems Alg 2 migh run predictions multiple times on the same nodes.
If a node is visited with k = 8, would it also be visited by GET_PATH with k = 2?
How would AdaSubS behave in the following case?
Suppose a degenerated search space where
There are two action applicable actions in each state, but at least one of them always leads to a dead end.
Suppose further that there is actually only one path to the goal.
For a state in the path to the goal, there are two childs: one to the next state in the plan, the other one child that leads to a dead end by a short binary subtree, all of the deadends.
Suppose the same list of k used in Sokoban: [8, 4, 2]
Suppose that all the models are not very good. They choose uniformly over their options. Let's disable the verifier.
It seems that AdaSubS would expand multiple times the nodes in the subtrees that lead to deadends? Moreover, Alg 2, GET_PATH doesn't seem to have any memory. So, if the models are expensive this could grow quickly.
(If this example doesn't work, please attempt to provide another example where AdaSubS might deteriorate)
Would you comment on the following paper published in IJCAI 2021?
Efficient Black-Box Planning Using Macro-Actions with Focused Effects. Cameron Allen, Michael Katz, Tim Klinger, George Konidaris, Matthew Riemer, Gerald Tesauro. https://arxiv.org/abs/2004.13242
Macro-actions are sequences of low-level actions. They create them by attempting to change a few variables in the problem.
They test in Rubik's cube.
They tested reusing macros in the same domain.
Why does the evaluation support the 2nd main contribution?
End of sect 1 says: ""We present a comprehensive study of adaptive methods, showing that they typically outperform non-adaptive ones"". But given the lack of comments about other potential algorithms, this might not be true in general.
I'd accept something like ""they outperform similar algorithms without the adaptation"". It seems that the argument is that given that BestFS and the studied algorithms share common elements, then the value of the contributions is justified.
How were the hyperparameters set?
I see the values in appendix F, but I don't see how they were set. Page 19:
Based on experimental results, we have chosen generators of 8, 4, and 2 steps for Sokoban, 4, 3, and 2 steps for the Rubik’s Cube, and 4, 3, 2, and 1 step for INT. In the first two domains, the longest generator match the one used for kSubS.
Explaining that would help readers to understand the effort of using the algorithms in another setting.
Please clarify the computation cost of arriving at these hyperparameter values? For instance, how expensive is to realize that a set of hyperparameter values are not the most adequate?
Are the hyper-parameters of BestFS set in the best possible way?
Would you mention the multi-queue algorithms in a revised version of the paper? Where? (See that in my section ""Scholarship on search and planning"")
Other changes
Please mention the number of subgoals generators and the values of k early in the paper.
Table 1 shows that the list k is one of the most important factors for performance.
The number of goals generated changes fundamentally the search space.
In the main body, it should say that BestFS with a trained policy, and how it's implemented. (See my comment on contribution 2)
Minor questions or comments
Page 5: > Rubik’s Cube is a celebrated 3D puzzle with over 4.3 × 10 18possible configurations.
Please cite Korf's paper
Page 9. Verifier: precision and recall
I’d say the verifier increase accuracy only in a small budget in INT.
Page 6: ""Shaded areas indicate 95% confidence intervals."" How many trials?
Page 15, Sect ""Computational budget analysis""
says: Tables 3, 2 and 4 present the number of calls to each component per 1000 episodes.
Is this testing or during training?
Are those 1000 instances the same for all the algorithms?
Is that the total number for 1000 episodes? The common practice in deterministic search is to report numbers per episode.
page 17:
Rubik’s Cube. To construct a single successful trajectory we performed 20 random permutations on an initially solved Rubik’s Cube and took the reverse of this sequence. Using this procedure we collected 10 7trajectories""
are there symmetric movements? Are these optimals path? See comment by Clemens et al.
Page 18:
""ρ BFSworks in the following way. First, it uses a trained policy network to generate actions to investigate. Specifically, for INT we use beam search to generate high probability actions (it is necessary as for INT we represent actions as sequences, following Czechowski et al. (2021)). Then, it uses these actions to get a state that follows a given action (note that all our environments are deterministic). Finally, we treat returned states as our new subgoals, which are easily found in one step by the low-level policy.""
It's not clear what happens in other domains.
page 15: please say this in the main body, not just in the appendix:
used for sampling predictions from the subgoal generators – the only component that outputs a set of predictions. | Strengths: ● The paper is well-structured and easy to read. ● Introduction of novel search algorithm benefiting from both the planning and learning methods. ● The experimental section is very well organized and provides good insights into the complexity of the domains considered. ● State-of-the-art results on inequality theorem prover INT.
Weakness:
Although the paper shows advancement in learning-based planning, the results are not surprising given that there is a large body of work in search-based planning to show that adaptive action selection / heuristic selection is beneficial in planning. See work like ""Real-Time Adaptive A*"" or historical perspective in a planning book like Ghallab et al. See also recent dissertation titled ""Adaptive Search Techniques in AI Planning and Heuristic Search"" by Maximilian Fickert.
A problem with this like of work is that the algos are not complete. They are not guaranteed to always produce a plan even if such a plan exists. This point should be highlighted.
A system architecture diagram depicting the flow of the algorithm would have been more comprehensible. It is hard to understand if all the chosen sub-goals at a given node are being explored. ● It is mentioned that the verifier network is a binary classification model, and if the network fails to predict the feasibility of the sub-goal, the algorithm falls back on to CLLP to decide whether to keep or discard a given subgoal. But it is not clear how CLLP is used for this purpose as it generates an action plan between two given states. Does an empty sequence from CLLP mean the sub-goal is being rejected? ● The authors commented on the out-of-distribution generalizability of the inequality theorem prover INT, however, it could have been interesting to see the status of the other domains. | The idea of learned subgoal generators with adaptive planning horizons is novel. The authors performed large-scale experiments on hard planning problems, comparing their methods both with baselines and also on out-of-distribution problems. That's very good efforts.
Before authors' rebuttal, the paper lacks important details in the main text for understanding. Though the authors stated the size of their training dataset. It's unclear how many problems were used for the results from Figure 2 and Figure 3. It would be helpful to state that in section 4.1 to make the results more convincing. It's also unclear whether the total wall time in Table 2-4 in the Appendix is from solving one problem or multiple problems, and how they compare to BestFS baseline. Without context, 3-26 hours seems too long for the title of the paper to be called ""fast and precise."" | High-level comments
[strong] The paper improves upon the state of the art. Search finds the goal in problem challenging problem instances including longer INT problems than are feasible for other approaches in this space. The results are solid and the analysis is comprehensive. This is a central strength of the paper.
[strong] The choice of experiments is good and appropriate for the work.
[weak] There is no discussion of optimal paths and the quality of the generated paths between the different approaches. For the INT domain, this is perhaps not possible, but for the other two domains, it would be very helpful to understand if the proposed approach improves success at the expense of plan quality, and the extent to which this tradeoff is made. While I do not believe inclusion of this discussion is essential, the paper would be improved by the inclusion of such a discussion.
[weak] Sec. 4.5 (analyzing the quality of individual components) seems to omit a key piece of analysis: how well AdaSubS will perform without the verifier or how well the kSubS baseline will perform with the addition of the verifier. The central distinctions between the two strategies are (1) one versus multiple subgoal generators and (2) the inclusion of the verifier network. I do not believe there are any experiments that seek to understand how impactful the verifier. It could therefore be the case that the low level policy network is not particularly capable and that all of the performance improvement comes from the addition of the verifier, which bypasses that process. Some discussion or additional experiments (or a pointer to where these experiments exist) would be incredibly helpful for understanding.
Smaller comments, questions, and typos:
Abstract: Though the abstract makes sense after having read the paper, the first couple sentences are not particularly clear on their own. Consider revising those to provide a clearer picture of what the central, unique insight is.
Sec. 1: The figure in the introduction, while informative, is perhaps somewhat misleading, since robot motion is not studied in this work. Perhaps it would be more appropriate to use the Sokoban domain instead.
Sec. 2: Typo (I believe) ""variable environment variable complexity"".
In Sec. 3, the discussion of the thresholds could be clearer; it was not clear to me until later on that there were two thresholds.
Algorithm 1: The 'return' statement may not find a path, in which (I believe) the procedure is to keep looping through paths if a feasible path is not found.
Algorithm 1: Also, the first 'for' loop that pushes elements into T appears erroneous. There are not yet multiple values of 'k', since the subgoal network has not been queried; should this simply use k=0 and push the starting state into T?",8.0,4.25
28,From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data,"Keywords: behavior generation, robot manipulation, learning from play","I think this is a very strong paper, showing that existing Behavior Transformers can scale to multimodal, reward free play data. In general, I really believe in the results in this paper, and am excited for the possibilities of using such an approach for general offline policy learning on real (and simulated) robotic tasks.
The weaknesses of this work are generally just weaknesses around the feasibility of visual goal-conditioning in the wild. In the real world, it’s not clear whether (given a new task, or a combinatorial explosion of objects/states) we’ll be able to provide robots with meaningful “goal images” that capture what we want to happen; for example, across all the tasks in this work, only a handful of object positions change from start state to goal state; in heavily cluttered environments with dynamic objects, it feels like such an approach would really suffer (in sample efficiency and maybe in just general applicability). This is somewhat shown in the videos, where we add random objects (though that’s also just because the environment is so out of distribution). | Strengths
Simple method building on BeT.
Intuitive and well-motivated approach for the problem statement (BeT handles multimodality well, makes sense to use it for learning from play data)
Strong results in simulation and on real hardware.
Weaknesses
Limited novelty beyond BeT, basically just adding goal conditioning. But it's largely an empirical paper and the results are strong so not a big issue. | Strengths:
I like the spirit of developing a unified algorithm for imitation learning from a variety of domains.
I like the real-robot experiment setup and appreciate the effort put into collecting the data and evaluating the policy on a physical robot platform.
Weaknesses:
My first main concern about the paper is its technical novelty. As stated by the authors themselves, the paper is a somewhat straightforward combination of two prior works, Shafiullah et al., 2021 for the transformer architecture and the multi-modal action learning and Lynch et al., 2019 for goal conditioning. As much as I appreciate the comprehensive empirical study, I had a hard time justifying the technical novelty of this paper given that the main focus of the ICLR conference is on learning methods themselves.
My second main concern is the baselines used in the paper. The strongest baselines are BET and Play-GCBC, but they are “set to fail” since the proposed method is the combination of the two methods. Given the baseline choices, I’m not exactly sure the point that the authors hope to convey through the experiment. Is the main contribution of the paper the transformer architecture and the multi-modal behavior policy proposed by BET? Because 5 out of the 6 baselines do not have these components. Is the main contribution of the paper goal-conditioning? Because the strongest baseline BET does not have goal conditioning and obviously will not be able to achieve the specified goal.
There are also other missing baselines. For example, IRIS [1] and Mandlekar and Xu 2020 [2] are both goal-conditional imitation methods that focus on disentangling multimodal behaviors. Neither is mentioned nor compared. ImplicitBC (Florence et al., 2021) that aims to learn multi-modal behaviors can also be trivially adapted to be conditioned on goals, similar to how the authors adapted BET to be conditioned on goals.
[1] IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data [2] Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. | Strength:
Goal-conditioned learning is a clean and simple formulation, compared to Q learning.
C-BeT addresses the shortcomings of BeT by conditioning on goals to resolve ambiguities.
This paper presents results in both simulated and real-world datasets and shows that C-BeT outperforms baselines in simulated benchmarks.
Weakness:
The core idea is doing GCBC with BeT as the BC backbone, which is only incrementally novel.
The criteria of choosing baselines is not super clear. A concurrent work of Play-LMP, RIL (Gupta et al.), that outperforms GCBC in the Kitchen domain isn’t included while GCBC is in. A concurrent work of GoFar, PLATO (Belkhale et al.), that outperforms Play-LMP isn’t included while WGCSL (which is shown to be not as good as GoFar) is included.
The result metric for simulated tasks is confusing, what is the unit? Is it the number of successes over how many total tasks? Why not just use the standard success rate? Did you run multiple different seeds?
The reason why BeT would outperform goal-conditioned methods isn’t well explained. It is a little counterintuitive and leads to the suspicion that the baseline models are not sufficiently tuned to work with the selected domains. Why not reuse the Play-LMP dataset/tasks? How would C-BeT and BeT perform in tasks without artificially introduced multimodal data?
The type of generalization is very limited.
The setup is fixed and the policy theoretically could have learned the door and knob tasks without the visual observation and be more robust to visual perturbations.
The generalization test of “fresh” demonstrations are still of the same tasks.
References:
Gupta, Abhishek, et al. ""Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning."" Conference on Robot Learning. PMLR, 2020.
Belkhale, Suneel, and Dorsa Sadigh. ""PLATO: Predicting Latent Affordances Through Object-Centric Play."" arXiv preprint arXiv:2203.05630 (2022).",6.75,4.75
29,Git Re-Basin: Merging Models modulo Permutation Symmetries,-''-,"Strengths:
This paper studies an important open conjecture from Entezari et. al. and provides substantial evidence for it, while stating caveats clearly (requirement of high width)
The proposed algorithms to find the permutations are interesting. I am surprised that the weight matching algorithm works at all, since it can speed up the compute required to solve the permutation problem by a large amount.
The paper is well-written
Weaknesses:
I find the claims regarding the practical utility of merging models trained in a distributed way slightly exaggerated. You don’t see the same improvements in the combined model’s accuracy (Figure 9), and the combined model does much worse than ensembling. Moreover, even ensembling would be a weak baseline if you are trying to combine many models that have been trained on smaller splits of the data. Thus, it is hard for me to see how this method could be helpful in federated learning or distributed training. Nevertheless, I don’t think this particular claim is central to the paper and the results are interesting anyway.
Neutral questions
Do the authors have any intuitions for why Figure 3b shows so many bumps? Perhaps the training of one of the models is slower? Maybe the right comparison would be interpolating models of approximately equal loss even if they are slightly shifted epochs.
What are the two different lines in each panel of Figure 2? | Strengths
This is a genuinely exciting paper. I believe it will inspire further research into understanding the loss landscapes as well as merging models and related areas. I think the paper can potentially be quite impactful.
As I will argue below, I think many of the pieces of this work have been explored before. To me, the main strength of this paper is that it provides a new perspective, or way of thinking about the loss landscapes, even if somewhat obvious in retrospect.
Weaknesses
I like this paper, so the weaknesses that I point out are somewhat nit-picky.
W1: Many of the key pieces of this work have been considered before separately. I think some additional discussion of related work is needed.
W2: It appears that the linear mode connectivity results may be somewhat brittle.
W3: Some of the interpretations about the SGD inductive biases are not very well supported by the experiments in my opinion. | Strengths
The paper investigates an essential question in optimizing neural networks: Why do many local minima exist in the loss landscape with similar performance? The paper provides strong evidence for the conjecture that these minima all correspond to the same network modulo permutation symmetries, which provides a significant boost to the community's understanding of gradient descent landscapes. To that end, the paper introduces three novel and computationally efficient permutation selection methods that permute a model's activations or weights to match a target network's behavior. The methods are grounded in concepts from combinatorial optimization and are equipped with convergence guarantees. The paper opens up a variety of exciting research directions:
Can the methods be extended for models with slightly different architectures?
Does the method also work for models trained on fundamentally different datasets?
Can we leverage these methods to combine different networks specializing in specific tasks to create a network capable of performing all tasks simultaneously?
Can we leverage these methods to interpolate between two models in a subtractive manner, e.g., subtract a biased model from an accurate model without loss in accuracy?
Weaknesses
The paper claims that neural network loss landscapes contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units. However, the paper only demonstrates that the proposed permutation selection methods induce linear mode connectivity for two given networks (as far as I can tell from the experiment description, the results have not been aggregated over multiple independent runs). As a result, the paper can only reasonably claim that for some networks, the permutation symmetries allows projecting them into the same loss basin. Thus, the claim either has to be reformulated, or the experiments have to be repeated for many network initialization parameters to provide evidence for the claim in its current form.
Moreover, some experimental details could be more precise (see clarity below). Finally, the paper posits a hypothesis on the width of models for ImageNet in section 5.1, which is relatively straightforward to test, but does not conduct a corresponding investigation.",8.666666666666666,4.333333333333333
30,Graph Neural Networks for Link Prediction with Subgraph Sketching,"Keywords: Graph Neural Networks, Link Prediction, Data Sketching","I am the author of one of the main baselines compared in the paper. I believe that this is an excellent paper with the following significant contributions:
It unifies the several existing state-of-the-art methods for link prediction prediction and does systematic analysis to show their actual strengths of design.
It is fascinating to see how the computational redundancy in subgraph-based neighbor counting can be eliminated by the message passing form of graph sketch using hyperloglog and minhashing. This is one of the most innovative and smartest GNN designs I have seen in a while.
The experiments including those in Appendix D show that the proposed method perform very well by both accuracy and speed. The main claims in the theoretical parts are well substantiated by the experiments.
Weaknesses: I don't see it as a strong negative but I have a question about Fig. 2: it is a bit surprising to see that DRNL (SEAL) can outperforms DE by such a huge margin on Pubmed, considering that SEAL's node labeling function is not more expressive that DE's one-hot distance encodings. Can any explanation be provided on that? Also just a minor typo: the citation after ""Distance Encoding (DE)"" on Page 4 seems incorrect. | S1. This paper is overall clearly written and easy to follow.
S2. The proposed method is very well motivated with comprehensive analysis on why subgraph GNNs works well.
S3. The proposed method is able to successfully combine the advantages of SGNN as well as normal GNNs for link prediction.
W1. Part of the notations in Sec. 2 are a bit confusing. For example. is
Zuv
a matrix / set of node embeddings or a vector of subgraph embedding? And why is the
u
in
yu
(Eq. (1)) bolded, but not bolded elsewhere (e.g., in the following paragraph)? I suggest the authors to follow the common way of denoting vectors and matrices. E.g., using bold lowercase for vectors and bold uppercase for matrices. Currently most but no all of the paper follows that.
W2. I assume the complexity in Sec. 5 and Table 1 are time complexity. If yes, it would be nice if the authors can also provide the space complexity analysis.
W3. As one of the goal of the proposed method is to obtain the effectiveness of subgraph GNNs while still being efficient and scalable, I think it would make the evaluation more comprehensive if the authors can also include the runtime of normal GNNs in Table 3 for comparison.
Minor ones:
W4. Missing some recent relevant link prediction citations. E.g., [1][2]
W5. In the first paragraph of Introduction, why are points i and ii bolded by not iii?
[1] Learning from Counterfactual Links for Link Prediction, ICML 2022
[2] Neural Link Prediction with Walk Pooling, ICLR 2022 | Strengths
The essential factors for the performance of existing subgraph-level GNNs are well analyzed through the experimental observation of the SGNNs.
The proposed idea of effectively estimating intersections and cardinalities without subgraph construction is interesting and novel. By leveraging structural features, the proposed method addresses the inefficiency of existing subgraph-based methods and the automorphic node problem in MPNN.
Experiments conducted on six link prediction benchmarks show both the effectiveness and efficiency of ELPH and BUDDY compared to recent GNN-based baselines.
Weakness
Performance between baselines in ogbl-collab does not seem to be compared fairly. The performance of SEAL in ogbl-collab seems to be the performance using the validation set for training whereas other GNN-based methods (GCN, SAGE, Neo-GNN) don’t seem to use the validation set for training.
It would be good if there were comparisons with GNNs with existing proposed structural features (e.g., DE [1] and node2vec [2]).
[1] Li, Pan, et al. ""Distance encoding: Design provably more powerful neural networks for graph representation learning."" Advances in Neural Information Processing Systems (2020)
[2] Grover, Aditya, and Jure Leskovec. ""node2vec: Scalable feature learning for networks."" Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016. | Strength:
The analysis on the components of existing SGNN methods for link prediction is well-conducted and the results are informative, which may inspire new thoughts on SGNN-based link prediction.
The idea that passes subgraph sketches as messages is novel.
The empirical results render the efficiency and effectiveness of the proposed method.
A more scalable variant is implemented, which prepropagates the node features in the preprocessing stage. This further mitigate the scalability issue of SGNN-based link prediction such as SEAL.
Weaknesses: I found the expressiveness analysis is less exciting. Here, more powerful means the capability of distinguishing automorphic nodes. But this can be realized by simple techniques such as adding random features. So, I don't think this is a good theoretical justification of better performance. Can you provide some comments on this?",8.5,3.75
31,Image as Set of Points,"Keywords: Clustering, Image Processing, Context Cluster, Representation","[strength]
This is a very insightful work towards general visual backbone design. The fusion of SuperPixel idea as well as PointNet structure is natural and elegant. I believe this work reveals the potential of point cloud modeling for 2D image processing, which is certainly a great contribution to the community and is worth exploring.
I would say that CoC is modeled in a way that fits well with the nature of visual signal processing, as it has the inductive biases that are as useful as those of convolutional networks (such as locality and multi-scale).
The interpretability is also as good as convolution (CoC embodies spatial selectivity, while convolution exemplifies the idea of pattern matching). Besides, CoC acts like the deformable convolution and is clearly more flexible and general than ordinary convolution that operates on regular grids.
[weakness]
Potential representation weakness. As an architecture similar to ConvNet and PointNet, CoC could be significantly weaker than Transformers in terms of global modeling capabilities (e.g., interacting two spatially distant objects). What are the authors' insights on this problem?
Related work. I find the first paragraph in Related Work is way more relevant to CoC than the latter two. It would be much appreciated if the authors could devote more space to the first paragraph, e.g., introduce [R1,R2,R3,R4] in more detail and discuss them with this paper in terms of method philosophy.
Figure clarity. I recommend using more contrasting colors when visualizing. Currently these two figures (Fig. 4 and 6) do not illustrate the clustering effect of CoC models very well.
Font formats. The spacing between characters in the headings is too small, making them look cramped. Also, there seems to be too much use of bolding or italics in the body text. I would suggest using a more concise format.
[open questions]
I have more questions as follows and would like to discuss them further with the authors:
From Fig. 5(b), the CoC block has similarities with DAT [R5]. Can the authors further analyze or compare them?
I don't think it's necessarily a disadvantage that the methods [R3, R4] can only be used for specific tasks like segmentation; in other words, a point-based (or SuperPixel-based) modeling might be inherently more useful for dense prediction tasks (detection, segmentation, etc.) than classification. The authors mentioned that they expected to see more work that seamlessly integrates CoC and DETR in the future. This is reasonable, but I still expect the authors to come up with some prototypes of CoC for dense prediction :).
[R1] Varun Jampani, Deqing Sun, Ming-Yu Liu, Ming-Hsuan Yang, and Jan Kautz. Superpixel sampling networks. In ECCV, 2018.
[R2] Zixuan Huang and Yin Li. Interpretable and accurate fine-grained recognition via region grouping. In CVPR, 2020.
[R3] Fengting Yang, Qian Sun, Hailin Jin, and Zihan Zhou. Superpixel segmentation with fully convolutional networks. In CVPR, 2020.
[R4] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In CVPR, 2022a.
[R5] Xia, Zhuofan, et al. Vision transformer with deformable attention. In CVPR, 2022. | Strengths:
To the best of the reviewer’s knowledge, the topic of considering an image as a set of points and extracting features from it for vision tasks is original and very interesting.
The proposed method that uses the clustering algorithm as the basic build block is novel and of significance to the community.
The evaluation plan of the paper is comprehensive. It provides experiments on standard vision tasks like image classification and object detection/segmentation and applications for point cloud inputs like object classification.
The evaluation results show that the method provides improvements on various tasks over the CNN and ViT baselines (though not outperforming the state-of-the-art approach).
Weaknesses:
By using the region partition mechanism, the set of points is no longer unorganized but becomes structured based on their locality. Additional experiments are required to clarify the role of the region partition.
Before applying the context clusters operation, the region partition operation, which is similar to the shifting windows in Swin [1], is introduced to reduce the computational cost. The authors seem to imply that the region partition trades off performance for speed. However, the locality introduced by the region partition could also bring useful inductive bias for the encoder. Therefore, additional experiments are required to answer the following questions:
If the region partition operation is removed in the clustering process, could the model achieve similar or better performance? What would the clustering map be like in this case?
It would be nice to introduce Swin as one baseline to investigate this problem.
[1] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., & Guo, B. (2021). Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv. https://doi.org/10.48550/arXiv.2103.14030 | The paper is well written (although requires an editorial read) and the experiments provided are detailed.
The methodology could be more clearly presented though. It should be immediately understandable but the reader has to re-read to follow properly. As a case, I would like to be explicitly told how the context clusters are believed to capture the content better.
The citations are incorrectly used throughout. The use of italics and bold in paragraphs seems unprofessional and I would prefer it not used.
Why are some figures bold in Table 1? It is not clear what this means.
The authors state ""See appendix for more experiments."" It would read well if a short explanation of what is in the appendix is provided here. | Strength:
The idea is very interesting and this new architecture could be an alternative architecture for CNN, ViT. It provides a new perspective on how the information of an image is represented.
The idea should be of great interest for the representation learning and computer vision community.
The computation is a hard problem if using clustering on 224x224 image, while the authors use a smart way to address it, similar to Swin Transformer.
The performance is impressive as compared with recent popular architectures.
The propose method also generalizes well on downstream tasks like object detection and segmentation.
The paper is well organized and the writing is good.
Weakness:
The paper claims to bridge the gap between representation of image and point cloud. It seems to work well on PointNet. But it is kind of hard to understand. The authors use fixed centers (Fig.5) to save computation, how does this work if the discrete pixels are sparsely sampled from one image? It might be helpful if the authors can provide more explanation about this. It would also be interesting to see the performance of dynamic centers? For example, maybe use only 2 iteration for center updates?
The proposed method may have good interpretation ability, but not better than CNN or ViT. The figure 4 might need more explanation since the framework is very different from other architectures. The method seems to cluster similar contexts in early stage, but it is not easy for reader to get this insight from figure 4.",8.5,3.75
32,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,"Keywords: equivariance, sample efficiency, pose detection, symmetry, SO(3)","Strength:
The paper aims to solve an interesting and challenging problem: estimating 3-D pose from 2-D images, which has broad applications in 3-D computer vision and 3-D imaging.
Weaknesses and questions:
Although the writing is in general ok, there are some missing details in the implementation. I encourage authors to provide a more detailed description of each layer and operations in the proposed neural network. For example, the authors did not clarify how exactly equation (2) is implemented in the Fourier space of SO(3).
The idea of this work is not sufficiently motivated. I can see the advantages of learning the distribution of SO(3) can mitigate the issue of symmetry. However, it remains unclear how the S2 convolution and the operations shown in Figure 2 are essential to the success of the proposed method. More explainations and experiments are expected to demonstrate the effectiveness of each module of the proposed network.
The idea of this work is not novel. It seems to me that proposed method is a combination of ideas from previous works such as group convolution, Fourier expansion on SO(3) and etc. Learning distribution on SO(3) to break symmetry is also not a new idea, see [1].
The proposed method is not the only work that predicts rotations from 2-D images, so there are some missing references. See [2] that uses each 2-D image to predict the pose of protein molecule for 3-D reconstruction. I also encourage the authors to have some discussions that compare this work with your method, since they are able to obtain reliable pose estimates from 2-D images using a simple neural network.
[1] Synchronizing Probability Measures on Rotations via Optimal Transport.
[2] CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images | Strength:
The paper proposed a novel orientation distribution estimation method from simple image features. The idea of mapping image features to a sphere and further a 3D manifold is very interesting.
SO(3)-equivariant Group convolution is devised for rotation estimation on the manifold. This brings a new idea to the field and I think it is inspiring and could be further extended to other applications.
The paper is well-written and easy to follow.
Weakness:
The evaluation is only conducted on images with a single object, well cropped and centered in the images. I am wondering how the method performs in other more cluttered and occluded datasets (e.g. YCB-V and LM-O). Learned or ground truth detection/segmentation may be used for this purpose.
I would like to hear the authors’ comments on how well the proposed method to unseen object instances/categories.",7.0,2.5
33,In-context Reinforcement Learning with Algorithm Distillation,"Keywords: Reinforcement Learning, Transformers, Learning to Learn, Large Language Models","While I find the result of this work interesting as it shows that it is possible to do more efficient offline meta-RL when the problem is formulated in the right way (e.g. using long context), I have a couple of concerns about this work:
Meta-RL: Using context/history in meta-RL has been studied in previous works [1,2,3] and all those works show that encoding past histories leads to significant improvement in performance (see results in [2]). Although the setting of those papers are online-RL, the similarity is uncanny and this work should have discussed those extensively in the paper (I'd suggest adding a section to discuss this). The only difference that I see with this paper and those works is use of transformer vs recurrent models to encode the context aside from online vs offline setting. Also, is it fair to say that the main contribution of this paper is to adopt the use of context/history in the decision transformer?
Choice of baseline: This paper uses RL^2 as a baseline. However, previous works [1,2] showed that RL^2 has poor performance in comparison to other meta-RL methods (see figure 2 of [1] ). Hence, choosing a better baseline method could provide a better conclusion about this paper.
Improvement over behavioral policy: Authors claim in multiple parts of the paper that ""AD learns to output an improved policy relative to the one seen in its context."" I am confused about this as I don't see the results support this claim. Take figure 4 as an example, AD doesn't outperform RL^2. Am I missing something here?
Ablation studies: ""Can AD learn a more data-efficient RL algorithm than the one that produced the source data?"" experiment shows that AD outperforms source algorithm. The setting of this experiment can be inconclusive as the source method and AD have different model sizes (?) and it appears to me AD uses larger network sizes. If that is the case, it is not a fair comparison. Moreover, ""Training the Sequence Model"" in page 5 can be a good experiment as long as it is a fair comparison in terms of model sizes. Finally, it would be useful if authors add a table in the appendix illustrating AD and baselines network/model sizes for each experiment in the paper to provide a better picture about the setting of the experiments.
Context/History: The paper says dataset D contains a set of ""learning"" histories. However lines 3 and 4 of Algorithm 1 show otherwise. In particular, Algorithm 1 shows that D contains ONLY expert data as optimal policies are used to collect data. This weakens the entire discussion of policy improvement in the paper as data contains only expert data.
[1] Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables, ICML 2019 http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf
[2] Meta-Q-Learning, ICLR 2020 https://openreview.net/pdf?id=SJeD3CEFPH
[3] Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs, ICML 2022 https://proceedings.mlr.press/v162/ni22a/ni22a.pdf | Strengths
I think the evaluation of this paper is very very good. The diversity of environments is great, and most of all, I was impressed by the many different questions the author asked themselves about the proposed approach, from out-of-distribution, to training on partial demonstrations or the importance of the length of the context. Great job!
The paper reads very clearly. In part this is because it sets out to do one job and do it well, and I appreciate that.
The idea is insightful and potentially useful in the long run for meta-RL.
Weaknesses
I think the originality of the proposed approach is good enough to pass the bar for acceptance, but I wouldn't consider it great. It is relatively close to GATO and RL2. It is also close to multiple works that are not cited but very relevant: Bootstrapped Meta-learning [Flennerhag et al., Outstanding Paper award last ICLR] as well as Upside-Down RL [Schmidhuber] and, less closely, the learning-to-optimize community [Li&Malik '16, Andrychowicz et al. '16].
In multiple places, the comparison to the standard (non-learned) RL algorithm needs to be described more in favor of the latter rather than the proposed approach. For instance, it is argued that the ""algorithm"" coming from AD generalizes out-of-distribution, but this wouldn't apply to widely different tasks: an RL algorithm applies to image-input problems and torque-based inputs just as fine, but the algorithm from AD wouldn't have nearly that level of generality (and which other methods in meta-RL have started to achieve). Furthermore, the sentence ""the source algorithm produces many single-task agents [...] while AD produces a single generalist agent"" while technically true is quite incomplete, as AD needs to store each individual history to be able to represent each specialist agent.
A point that concerns me from the evaluation was that in the main figure, RL2 is shown only in its asymptotic value, as if it was an Oracle whose performance couldn't be put as just another curve (which may surpass AD). One may argue that it could be unfair for AD, as it is online rather than offline, but the same can be said about the Source being on a non-meta setting and not exploiting meta-training data, being at a disadvantage w.r.t. AD. I would really appreciate a clarification on this. | Strengths:
The approach presented in this paper is at the same time very simple and intellectually stimulating;
The empirical investigation is detailed and answers interesting questions one might have about this type of approaches;
The presentation of the ideas is clear and concise.
Weaknesses:
While I believe the careful investigation of such an idea is definitely worth even without an immediate implication on how to leverage it in a more standard context (i.e., when should I both to imitate my RL algorithm instead of simply running it?). It would be better to have a direct discussion on which ones would be the implication of improving this type of methods in the long run;
Is the context length the only blocker in scaling this method to more complex tasks? From the current paper, it is not clear whether more complex transformer models (with longer context sizes) could actually be enough to scale to the usual reinforcement learning benchmarks. | Edits:
intro: “does not improve through trial an error”, “and” missing a “d”
Strengths:
Well written; this paper was a pleasure to read.
To the best of my knowledge, this work is novel.
The fact that this approach worked was quite surprising to me. This “surprise factor” makes this an important and interesting topic in my opinion.
I also think that this kind of “in-context” learning (in other words, “learning” only within what-can-be-viewed-as-analogous-to the model’s short-term memory, without updating parameters) is an important thing to study. I hope this will inspire future work that better studies the intersection of this kind of learning with more traditional learning.
Weaknesses:
The paper would greatly benefit by a more clear early definition of “in-context”, instead of making the reader infer it through context. (There is sort of a definition in the abstract, but even that is more of an implied definition than a definition. I suggest adding a more clear definition to the early introduction.)
One might argue that the contribution is slightly limited: the authors simply propose a new idea and show that it works. However, given the novelty and importance of this topic, I am not very concerned about the extent of the contribution.",7.25,4.5
34,Is Conditional Generative Modeling all you need for Decision Making?,"Keywords: Offline Reinforcement Learning, Conditional Generative Modeling, Sequential Decision Making, Diffusion Models","Pros: This paper is well motivated and easy to follow. Inspired from diffusion models in vision domain, this paper formulate the decision making process as a condition generation problem and which can naturally achieved by leveraging diffusion models. I like the way that this paper very clearly introducing the technical background and formulate the problem. From the experimental results, the proposed Decision Diffuser is promising on a couple of evaluating tasks.
Cos: My main concern is the limited technical novelty. The key contribution of this work is to formulate the decision making problems as a conditional generation problem. Based on this, this paper train a diffusion model on offline datasets. However, it is more like an application of diffusion models on decision making tasks. I don't see obvious novelty from either model design and/or training objectives. The evaluations are weak. It only evaluate the DD variants on a few tasks. No comparisons of DD with the other state-of-the-art approaches are shown. Also, no ablation and discussion are shown. It is not convincing to me without such extensive study. | Main contributions, Impact
Use of classifier-free guided diffusion. Classifier-guided diffusion requires a Q-value estimation procedure such that Q-values can be used to guide diffusion (which has been explored in Janner et al. 2022). The current work avoids this via classifier-free guidance which requires low-temperature sampling in datasets where demonstrations are of mixed quality. Empirically, this leads to improvements over previous methods - with a grain of salt it seems to be easier to pick out the high performing trajectories than learning good Q-value estimates. Impact: low to medium - it remains unclear under what exact conditions (both theoretically and empirically) classifier-free guidance / low-temperature sampling outperforms Q-value guidance and why.
Diffusion of state-trajectories only, rather than state-action trajectories (which is compared to in the paper and shown to perform worse). This comes at the cost of requiring a good inverse dynamics model. Current impact: low - similar to 1) the empirical results are in favor of the version proposed in the paper but it remains unclear what the practical cost of requiring the inverse dynamics model is; in particular how robust the method is against errors and imperfections in the inverse dynamics estimate (is performance very brittle or quite robust). Under what conditions is it better to rely on an approximate inverse dynamics model, and when is it better to diffuse state-action trajectories?
The paper shows how to incorporate combinations of constraints and skills: by identifying subsets of training trajectories corresponding to an individual constraint/skill and augmenting them with a label that can later be used to condition on. The approach is fairly straightforward, but interesting. Impact: low - the approach is currently only minimally explored in the paper and further conceptual/theoretical characterization of the kinds of constraints and the corresponding requirements for the dataset would be needed for higher impact.
Strengths
Very well written paper
Empirical results show benefits of proposed method compared to high-quality SOTA methods on standard benchmarks
Incorporating constraints and combining skills via constraints is interesting.
Weaknesses
Often the paper’s main aim seems to be to reach the minimal result sufficient for publication, which is mainly to achieve good benchmark results. While this is a valid strategy to avoid risks, it also creates a sense of lack of ambition and depth. I personally think there are some very interesting ideas presented in the paper but they often seems to be addressed in a minimal sense. Beyond the empirical comparisons, a strong characterization of the advantages and disadvantages of these ideas is missing; and I would be very excited to see such a discussion and analysis (both empirically and theoretically). I think it would help raise the significance of each of the main contributions mentioned above.
To make the point above more concrete:
It remains unclear when precisely and why classifier-free guidance and state-prediction only is preferable - when and why is it beneficial to avoid estimating Q-values at the cost of requiring an inverse dynamics model (how robust is each approach to errors in the approximate Q-values/dynamics, what are the conditions w.r.t. the dataset that make low-temperature sampling a good choice, etc).
It remains unclear what exact conditions are needed for constraints/skills to be “combinable” (see comment under improvements).
The current experiments on combining skills are a bit hard to interpret - while they lead to somewhat visually different gaits it is unclear whether these should be considered successful combinations of skills. What’s missing is a clearly stated and (quantitatively) measurable goal for combining skills (which is well defined in the case of combining constraints).
Improvements
Naively it seems that the approach introduced allows to combine individual constraints / skills, as long as there is a separate subset of training trajectories for each constraint/skill and the combination at test time is a conjunction of constraints such that the intersection of the corresponding sets of trajectories constitutes a set of valid solutions (which corresponds to the situation in Fig 1). Is this a hard condition? Can something theoretical be said about the allowed combinations of individual constraints (conjunction, disjunction, exclusive disjunction)? What happens theoretically and empirically if satisfying a combination of constraints requires novel trajectories that have no support in the training data (i.e. such that simply “filtering” out the right training trajectories via conditioning and some mild generalization is not sufficient)?
Combining skills needs some quantitative goal/metric, and an empirical evaluation. The videos are interesting and promising, but it is unclear how to interpret the results, let alone how other methods could be compared in terms of performance.
Perturbation analysis comparing the Diffuser and Decision Diffuser in terms of robustness to noise or errors in the Q-value estimates/inverse-dynamics model respectively. While theoretical results would be nice, empirical results are probably easier to obtain and might already be insightful. Related: how well do both methods handle quite noisy dynamics? Since the latter two requires running additional experiments, potentially even designing toy environments, I do not have a hard expectation to see these results after the rebuttal; but they would certainly make the paper stronger.
Discussion of limitations should be expanded
What are restrictions/requirements on the dataset, particularly w.r.t. allowing for combinable constraints/skills?
The current method is limited to fully observable MDPs, partial observability and latent variables might pose a hard to overcome problem, with self-delusions [1] potentially being a fundamental obstacle).
It seems unlikely that the method can be used to improve significantly beyond the performance of demonstrations (unlike RL, at least in theory).
Minor comments
A) What is the relation between constraint conditioning as presented in the paper and more general task conditioning (via natural language prompts) as in e.g. DeepMind’s Gato?
B) Sec. 3 “It is useful to solve RL from offline data, both without relying on TD-learning and without risking distribution-shift.” - what are the conditions/requirements for the latter to be possible (at least informally)? In order to not risk distribution shift strong conditions on training-data coverage seem required.
C) What is the fundamental difference between constraints and skills - are they more than simply two different names to refer to subsets of trajectories in the training data?
D) “In contrast to these works, in addition to modeling returns, Decision Diffuser can also model constraints or skills and generate novel behaviors by flexibly combining multiple constraints or skills during test time.” - while this has not been explored in previous publications, is there a fundamental problem that prevents previous methods from using the same approach as used in this paper (i.e. augmenting data with a one-hot indicator for constraints/skills, and conditioning on that during generation)?
E) P4, typo: “choices fof diffusion”
[1] Shaking the foundations: delusions in sequence models for interaction and control, Ortega et al. 2021 | Strengths:
The paper is well written. Diffusion modeling for planning is an exciting and timely contribution given that diffusion models are beating state of the art in various other tasks in synthesis.
The proposed technique is shown to tackle compositionality of skills and constraints at inference time, which is interesting.
Weaknesses:
One limitation of diffusion models is that the generations are slow. It would be great if authors can comment on the runtime characteristics of the proposed model.
It would be good to discuss limitations of the model. Some questions I have:
How are the out-of-distribution generalization characteristics of the model?
Can the model capture variable length trajectories?
I am intrigued by the skill composition idea. Currently only “AND” style composition is demonstrated. What other compositions does this method can support?
How does the method perform with limited data?
Other comments:
The loss function used to train the model combines losses which are used to train the diffusion model and inverse dynamics models equally. What implications does equal weighting have?
The paper relies on diffusion models and class conditioning capabilities of such models (i.e. classifier free guidance) but I found it odd that the modeling choice is not mentioned in the abstract.
What is the performance metric used in Figure 4? Similarly, Table 3 is lacking an explanation of the metric used. | Strength
Modeling decision-making as the conditional generative model is novel and also interesting to the community.
Via conditional generative process, the method shows the ability to compose skills in order to maximize the rewards. This is impressive.
The paper in general is of high quality in terms of supportive experiments, and motivation.
Weakness
I'm not an expert in decision-making/Reinforcement learning. So I'm not entirely sure if I follow the problem definition. To be specific, what's the design choice that leads to the ability to compose skills? And do other reinforcement learning-based method have the same ability? It would be better to make it simpler.",7.0,3.0
35,Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification,"Keywords: Bayes error, best achievable error, irreducible error","Strengths:
This paper is well-written, and there are many mathematical formulations to formally illustrate the logic and soundness of the proposed method.
The experimental results seem promising.
Weaknesses:
The proposed method seems simple. However, it is not clear in practice how a practitioner implements the whole method.
The method is supposed to be instance free. However, I still see that real instances would be desired in experiments. | Strengths:
The concept is conceptually simple and well-motivated.
The presentation is clear overall.
The experiments involve ``real-world'' scenes which may bring practical interest
The proposed estimators works (except on ViT and some datasets) better than other trained models in estimating the Bayes error.
Questions:
Some reference are missing. For example, page 4 has a ''Sec. ??'' and page 8 has a ''App. ??''.
One paper [1] might be worthy to discuss (and maybe compare) but seems to be missing. It is also a model-agnostic algorithm for estimating the Bayes error, although they require the access to instances.
I might be wrong but since human labeler are also imperfect, why is it not possible that ViT can outperform human in providing reliable confidence?
[1] A Model-Agnostic Algorithm for Bayes Error Determination in Binary Classification | Strength: The proposed estimator is simple to implement. Also it is nice that the authors show some guarantee of the proposed estimator.
Weakness: I feel the proposed estimator does not really resolve the core of the Bayes error estimation. For the theory part of the paper, e.g., Theorem 4.1 and Proposition 4.2 and 4.3, the assumption is r_i= p(y=+1|x_i) is known. But to me this is actually the core and hard part of Bayes error estimation, since given r_i one can easily get the Bayes estimator.
In the empirical section of the paper, p(y=+1|x_i) is estimated using deep models, but as we all know from the uncertainty estimation literatures, deep models are not doing a good job calibrating their output scores to p(y=+1|x_i), and their outputs are usually highly screwed. As a result I am not quite sure how I should trust the estimated Bayes error.
Experiments are also kind of lacking. I only see two real-world datasets used. To fully show empirically the proposed estimator work I would like to see more numbers on other datasets. | Strengths
The authors provide many rigorous proofs to support their claims.
The empirical results provide further support, showing bias where expected, etc.
Weaknesses
Theorem 2.2 is introduced from previous work without much context around why it is useful or important for this work. It would be helpful to the reader to provide some context of where and why it will come into play later in the text.
Until section 3, there is no mention of where the soft labels come from. It seems that the soft labels would have to be soft labels that are given from the true distribution of the data, which it seems would be impossible to attain in most realistic circumstances. Although, this is explained later, I think it would reduce reader confusion by at least alluding to this fact and that it will be dealt with later.
Section 3, paragraph 3: Why can uncertainty labels not recover
p(y=+1|x)
from
miny∈±1p(y|x)
? If one has access to one, doesn't it guarantee we know the other? If
miny∈±1p(y|x)=0.1
, then isn't
p(y=+1|x)=0.9
If this is a subtle misunderstanding about the meaning of soft labels and uncertainty labels introduced in Section 1, then I think there needs to be more time devoted to explaining those concepts in detail.
Theorem 3.3 seems to show that if one has a noisy corruption of the true class of an instance, then
β^
is an unbiased estimate of the Bayes error. Later on in page 4, the authors state ""In practice, one idea is to ask many labellers for each instance and then use the histogram of the hard labels as a noisy soft label."" Therefore wouldn't this just produce an unbiased estimate of the Bayes error of the aggregated labellers, which is not in fact an unbiased estimate of the true Bayes error? For example, the estimated Bayes error of a ResNet on CIFAR-10H is higher than that of the human annotated
β^
in figure 4. This might be an obvious point, but I still think it should be stated clearly somewhere in the text.
It would be interesting to see the other baseline performances on something other than the toy dataset. For example, ensembles tend to not show much diversity in decision boundaries when considering toy datasets, and therefore often do not outperform simple MLP baselines by much. This changes when the dimensionality of the data increases. It would therefore be interesting to see what happens with the Bayes error of an ensemble of at least one of the network baselines in figure 4.
Minor
Page 4: reference error, ""which we use later on in Sec. ??""
Page 8: reference error, ""(App. ??)""",7.25,2.5
36,Language Modelling with Pixels,"Keywords: representation learning, nlp, transformers, language model, masked autoencoder","Strength: I commend the authors for the innovation here. Although a simple idea, it is highly effective and clever, and presents a very exciting alternative to BERT & friends. I look forward to future research on what these models encode and share across languages/writing systems.
Questions:
It would be useful to have a more detailed discussion on the fertility of the pixel maps, ie., how many patches correspond to a word on average and the relationship to some of the tasks. For example, I am curious if there is a relationship between this and the question answering performance of PIXEL as the authors allude to as well.
It would help to order the languages in the results tables based on [UNK]%. and insert a row showing the difference between BERT and pixel for each result table.
Any hypotheses for why its performs much worse than BERT on COLA specifically?
I understand the space and time constraints but I would be excited to see that perhaps training PIXEL on two very different script systems leads to big performance gains than traditional models and PIXEL trained on english only. Especially if the other language has a rich morphology for example.
How hard was hyper-parameter tuning for this model? what do the loss curves look like? | Strengths:
This paper takes on a new and interesting approach to a very well-established masked language model pre-training/fine-tuning paradigm. It is quite an interesting and refreshing setup.
The paper includes an extensive set of experiments, covering POS tagging, dependency parsing, extractive QA, GLUE, XNLI, NER.
Being able to non-trivially handle new scripts from only fine-tuning data is an impressive feature of the system.
Weaknesses:
It would have been really interesting to see the byte-based vocabulary baseline (something like ByT5). Since most of the experiments involve shorter sequences (right?), is length of the resulting byte-based sequences a concern? In some cases where the vocabulary clearly has very poor coverage, comparing against BERT seems like a slightly unfair comparison with BERT mostly fine-tuning on a sequence of unks.
The discussion on patch span masking is a bit unclear. How was the resolution of the rendered text chosen? Within the same height and sequence length, how many words tend to end up in a patch? Or do patches often contain subwords? I may have missed these details but it seems relevant for understanding how things work. Were there any experiments that varied these parameters to clarify the decisions? A figure illustrating the approach with an example might also be helpful.
The robustness experiments currently lack decided conclusions/takeaways. It doesn’t seem like Pixel is firmly more robust than BERT. Also, the orthographic attack graphs might need error bars?
What if Pixel is provided inputs that are rendered vastly differently during fine-tuning? It would fail to generalize right? Or would the approach be to run OCR and get the text, render it with the Pixel text renderer and then predict? It might be worth adding a discussion that goes over some of these points, and more generally delves into design decisions that are crucial.
Nit: maybe the paper title should be masked language modeling with pixels. | Strength
Avoid vocabulary bottleneck problem.
Show more robustness to orthographic attacks and code-switching
Perform better on non-Latin languages on syntactic tasks and semantic tasks comparing to Bert.
Demonstrate the possibility of pixel-based language model without injecting a priori glyph knowledge.
Weaknesses
Average performance on downstream tasks still lags far behind Bert and mBert.
Data preprocessing (text rendering) is tricky and laborious and will cause inevitable latency in real application.
The generated text pixels during pretraining is blurred and fuzzy which makes the model learning less explainable.",7.333333333333333,3.6666666666666665
37,Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection,"Keywords: Deep Reinforcement Learning, The Arcade Learning Environment, Human World Records, Behavioral Control","A key strength of the paper is in the significance of the results. Setting a new state-of-the-art in Atari is a significant accomplishment, and will be of interest to the RL community.
A weakness of the paper is in a lack of clarity around key technical details, combined with a lot of redundant and unnecessary explanation characterizing the space of possible techniques. The paper spends 3-4 pages (p. 3-6) on redundant and repetitive explanations of behavior space vs. behavior mapping. Many parts of these pages are repeated (for example, the ""Behavior Formulation"" paragraph on p.4 is fully subsumed in other parts of the section and does not need to be repeated). However, the paper spends relatively little time justifying or giving intuition for why the central contribution of the paper (Eq. 9) is the right approach. Most importantly, the explanation of how the MAB behavior selection interacts with Eq. 9 is left very unclear. It would seem that to apply Eq. 9 the MAB would need to select N policies, but this is not stated in the text, which instead says that Eq. 9 will be applied to a single
Φk
, which does not make sense. This lack of precision makes the paper hard to replicate. | Paper seems well written, key concepts are explained, and claims are supported.
The results are very good. As many games of the ALE benchmark require exploration, the significantly improved results show that the method is highly effective at exploring. I'm impressed by the effectiveness of this approach.
The behavior construction and selection method is novel to me although seems too complicated to get it to work well in practice. But the authors provide important hyperparameters and pseudo algorithms. I think this is a good work that demonstrates advances of exploration leads to unprecedented performance on Atari games. | Main Concern
This situation is unfortunate. The submission is excellent (simple approach, great empirical results, nice writing), and the authors clearly spent a large amount of time to make it as good as it is. But there is existing work that was arxived (https://arxiv.org/abs/2106.06232v1) more than a year ago and recently published in ICML (https://proceedings.mlr.press/v162/fan22c) that proposed essentially the same methodology and achieves similar empirical results. I will refer to this existing work as GDI.
Irresponsibility of the omission
GDI was published by a relatively lesser known research group among the reinforcement learning community and has not received as much attention as it deserves, so I can see how the authors may not have known about it. However, even if this omission was unintentional, I feel it was still irresponsible: When making a submission with claims of novelty, the submitters are staking their credibility on the claims of novelty being factual; the submitters appear to have made such claims without having done due diligence.
Work is not concurrent
I do not think the submission can claim concurrency with GDI since GDI has been on arxiv since June 2021.
Similarity of LBC and GDI implementations
In their implemented algorithms, both works use continuous MAB algorithms to optimize the temperatures of softmax functions and mixture parameters over policies. There are many instantiation-level differences, but I feel that the implementation in the submission can meaningfully be described as an instance of GDI (and vice versa). If the authors disagree with this characterization, I ask that they provide detailed evidence for their claims.
Similarity of empirical results
Across various metrics, the submission reports results (e.g., 10077.52% mean human normalized score and 24 human world records) that are a bit better than GDI (e.g., 9620.98% mean human normalized score and 22 human world records) after 5 times more samples (1B for LBC vs. 200M for GDI). It is hard to make a precise comparison, but, if anything, I might say that GDI's empirical results are a bit more impressive, given that it achieved almost as good results with 5x fewer samples.
Value of submission
Although I feel that there is not algorithmic novelty and that the empirical results are not substantially better, I think the submission as is offers value in the sense that it is much more clearly written than GDI. Since the idea behind GDI/LBC is, in my view, quite important for modern RL but not currently widely understood by the community, I think it deserves to have a paper where it is explained more clearly than it was in GDI. That said, I do not think it would be fair to the authors of GDI to re-brand the same idea under a different name and merely mention GDI as related work.
Other thoughts
Measurement notation
The submission chooses to break the optimization target into two terms that it calls a diversity-based measurement and a value-based measurement. I am not sure what purpose doing this serves, as the submission never actually comes back to this point when discussing the actual algorithm it implemented. I think the best way to resolve the issue would be to use a single symbol V to refer to a measurement and note in the text that this measurement may be decomposable into some weighted combination of diversity and value. Alternatively, if the authors do not want to do that, I think they should at least describe the algorithm that they implemented in terms of these two measurements at some point in the submission so that readers are able to make the connection.
""Goal-directed"" terminology
The submission repeatedly uses the terminology ""goal-directed"". To me, this feels like unnecessary jargon. The submission suggests that Agent57 uses a goal-directed meta controller, but I could not find any usage of ""goal-directed"" in the Agent57 paper. If the submission does not want to remove this terminology, I think the submission should at least make it clear exactly what it means by ""goal-directed"" and how this usage departs from or is the same as that in existing reinforcement learning literature.
EfficientZero final performance
The submission states:
From Figs. 3, EfficientZero (Ye et al., 2021) achieves remarkable learning efficiency in smaller data volumes but fails to obtain comparable final performance as other SOTA methods like Agent57 or Muzero.
This statement may be superficially true, but it is misleading to readers because it is comparing apples to oranges. The numbers reported in the submission for LBC use four orders of magnitude! more samples than EfficientZero. It is entirely plausible that the final performance of EfficientZero would be comparable to that of MuZero given the same number of samples. Thus, saying that it does not achieve the same ""final performance"" does not strike me as appropriate.
On this note, I do not think it is even necessary to include EfficientZero(100k) in these plots. It is not a relevant baseline for the submission due to the drastically different sample usages.
Closing Thoughts
As articulated above, I feel that the submission is a great piece of work and, even given existing work, offers value to the community. I hope that, through conversing with the authors, we will be able to agree on appropriate revisions that will make the submission acceptance-worthy. However, I anticipate that these changes may be quite substantial -- i.e., going well beyond simply mentioning GDI as related work. I think they would likely require the submission to be re-written from the perspective of providing additional clarity and empirical confirmation for GDI, rather than that of proposing a novel methodology. To me, it is important that the authors of GDI not be denied credit for having had an important idea simply because they are not well-known RL researchers.",8.666666666666666,4.333333333333333
38,Learning on Large-scale Text-attributed Graphs via Variational Inference,"Keywords: Language Model, Graph Neural Network, Node Classification","Strengths:
Interesting formulation of GLEM. Jointly optimizing the LM and the GNN within a variational EM algorithm constitutes an elegant formulation of the problem.
The paper has performed experiments on large-scale datasets.
Weaknesses:
The paper has mostly examined datasets that contain textual features. However, similar settings arise while dealing with the task of text classification using graph-based models. This problem, although it is highly relevant to the formulation studied here, it is not discussed at all in the paper. In the past, several approaches have been introduced for graph-based text classification — none of them is mentioned in the paper. For instance, one of the very first papers for this task “Graph Convolutional Networks for Text Classification” at AAAI ’19, follows a similar formulation where a graph composed of document nodes is constructed. Is there any specific reason why such methodologies have not been used?
Despite targeting scalability, the paper does not discuss the convergence of the proposed model. The training step is quite complex, therefore studying the convergence of the model is important.
The settings of structure-free inductive learning are not very clearly presented. Since GLEM assumes that there is a graph structure that captures the relationships among text nodes, it is not straightforward to extend it to an inductive setting. How is this problem tackled here? | Strengths
Simple but powerful idea; the authors prove that their method outperforms previous baselines on three benchmark datasets. This work has enough impact on the field of representation learning on TAG.
Weaknesses
Lack of ablation studies; In E-step and M-step, there are hyperparameters that balance the weight of two terms. However, I fail to find any design choice or ablation study on this. In addition, more details (optimizer, learning rate, batch size, …) should be described in the paper (at least appendix) to fully reproduce the methods and experiments in this work.
Suggestions
Questions
What is the difference between GLEM-LM and GLEM-GNN? Do they differ based on the starting point of optimization (E-step or M-step first)?
Typos & suggestions
Table 3: Dataset name (ogbg-papers) is missing in Table 3.
Table 4: In Arxiv-MLP experiments, the boldface on diff is wrong. (on -1.67 instead of -1.58 | Strength
Clear motivation: this paper is clearly motivated to target on the problem of joint learning LM and GNN efficiently.
Mathematically principled: the variational EM framework is elegant and very smart. Although variational EM is very well studied and already used for learning GNNs, I think the application of variational EM in this paper is still novel.
Solid experiment: the experiment is conducted well, with solid comparison with GNNs and LMs and joint methods.The improvements are also clearly marked in Table 2.
Efficiency and scalability: The efficiency (quick enough training time) and scalability (setting larger batch size) is also clear in table 5.
Weakness
Multiple approximation happens in training: the authors have clearly discussed all the places where approximation happens, which on itself is a good thing. Yet all these approximations (mean-field, weak-sleep, pseudo label, balancing parameters .etc) intuitively makes the final objective deviate very much from the original variational bound (or not sure if the final objective is a bound anymore). Although being orthodoxical to the original variational objective may not be practically good, it would still be mathematically more principled if the authors could establish more rigid analysis of the final objective (e.g., its relationship to the likelihood). | Strength:
The general idea of this paper is performing joint training for LM and GNN and letting them help each other during training. This might be done with heuristic approaches. But in this paper, it is interesting to wrap up the alternative training of LM and GNN into an EM framework, which adds credit to the technical depth of the paper.
The datasets and the baselines are well selected and the experiments are comprehensive in general including different settings and comprehensive ablation studies.
Weaknesses:
Some of the performance improvement of the proposed method is a bit marginal. For example, in Table 2, GLEM-GNN (the proposed method) is marginally better than the second best (X_{GIANT}) on arxiv and products (e.g., 75.90 ± 0.19 VS 76.74 ± 0.08 with RevGAT), not mentioning that GLEM-GNN did not outperform X_{GIANT} on papers.
Several clarity issues (please see comments in the next section).",7.5,3.5
39,Learning where and when to reason in neuro-symbolic inference,-''-,"Strengths
The proposed method presents an interesting and elegant way of combining the relative strengths of neural and symbolic reasoning approaches. The use of a learned module to decide when to invoke the symbolic reasoning module is a nice solution that avoids the circular challenge of using a symbolic module to check whether the neural solver is correct, and the approach works surprisingly well (one might have expected that the learned mask-predictor would suffer from the same limitations that the neural solver does in the first place).
The method performs well against other neurosymbolic approaches (and against purely neural or purely symbolic approaches) on a challenging visual sudoko task.
The model is robust to perceptual noise, and can solve problems in an efficient manner.
Weaknesses
The major weakness is that the approach still requires the use of handcoded, task-specific knowledge, i.e. the rules of sudoku are programmed into the symbolic module, rather than being able to learn the rules or receive the rules in natural language as human reasoners do. However, despite this limitation (which is broadly shared by current neurosymbolic methods, and a major open challenge), the proposed method constitutes an elegant solution to a specific problem in integrating neural and symbolic approaches, and should be a useful contribution to the literature.
Other notes
There is an interesting connection between the proposed approach and the 'two systems' perspective in psychology (e.g. [1] and [2]).
[1] Kahneman, D. (2011). Thinking, fast and slow.
[2] Sloman, S. A. (1996). The empirical case for two systems of reasoning. Psychological bulletin, 119(1), 3. | PROS
The method is significant in that it targets a known weakness of existing NeSy approaches.
The proposed pipeline (and accompanying training procedure) is intuitively appealing.
The empirical results on visual sudoku look promising.
The text is generally well structured and easy to follow.
The related work section is reasonable and gives an overall fair representation of existing approaches.
CONS
The evaluation only considers visual sudoku.
The results do not report the time required for training the various models.
Improvement over the symbolic baseline can be small if the neural method makes many mistakes.
Some references are missing.
Some aspects of the pipeline are a bit unclear. | Strengths:
The paper presents the novel idea of selecting which facts need to be changed in order to make the prediction complaint with the constraints. This effectively reduces the search space for the solver, and it might be a useful intuition for future works.
The presented method is intuitively easy to understand.
Weaknesses:
The paper needs significant rewriting. It lacks a lot of details, and while intuitively is easy to follow, when it comes to the details it becomes a very fuzzy. This is the main problem I have with the paper, which I though think can be solved with some effort. See comments on Clarity below for more details on how to address the issue.
The experimental analysis has been done on a single task which limits the understanding on how this model would perform in real-world datasets.",8.0,4.333333333333333
40,MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting,"Keywords: long-term forecasting, local and global context, multi-branch architecture, different potential patterns.","Strength: The overall paper is well written and most of the concepts are concisely explained. As the paper stated, both local patterns and global dependency are crucial for long-term time series forecasting. The design is well motivated. The benchmark results are also promising to demonstrate the effectiveness of the framework.
Weaknesses: Despite the state-of-the-art accuracy, it seems that the main improvement comes from the regression-based trend-cyclical prediction. Trend-cyclical analysis in section 4.2 is not enough to understand how the regression influence the prediction. I suggest evaluating the regression and the mean prediction separately from the full benchmark performance. Also, the results show that MICN-mean has comparable performance (sometimes even worse) than FEDformer, while FEDformer is using mean trend prediction. I can’t help but thinking if FEDformer with regression trend prediction can surpass MICN. More discussion in trend-cyclical prediction is needed. Another thing that can be discussed more is the intuition behind isometric convolution. The motivation of the linear complexity is good, but the paper is unclear about how isometric convolution is going to capture the correlations between local time region. Besides, there exists many works that focus on the efficiency of self-attention and many has linear complexity as well (e.g., Linformer [1], Fastformer [2], …). If the primary motivation behind isometric convolution is the complexity then these works should be discussed as well. I also have concerns over the explainability of the local-global structure. Self-attention is good at making interpretable prediction (can be crucial for real-life application, as the paper mentioned), whereas I don’t see how MICN can be interpreted at a first glance. It is sad when the paper argues “the framework can deeply mine the intricate temporal patterns” but fails to interpret them. This is also one of the reasons I am curious about how linear self-attention can perform instead of isometric convolution. Other minor suggestion:
The abstract says that the structure can model patterns separately and “purposefully”. The wording is vague and confusing. Does it indicate that we can incorporate external knowledge into the model?
In section 3.1, briefly mentions that the term (e.g., “AvgPool”, “kernel”) is from the convolution perspective. I first thought the “kernel” represents the kernel function.
The difference between ablation table 5 and table 13 can be explained more. [1] Wang, Sinong, et al. “Linformer: Self-Attention with Linear Complexity.” ArXiv.org, 14 June 2020, https://arxiv.org/abs/2006.04768. [2] Wu, Chuhan, et al. “Fastformer: Additive Attention Can Be All You Need.” ArXiv.org, 5 Sept. 2021, https://arxiv.org/abs/2108.09084. | The topic of this paper address the problem of long-term series forecasting. The topic is very interesting and the paper is well written. The authors provide a comprehensive review of the related papers. Mathematical models and figures are proposed to understand the problem and the solution provided by the authors. Extensive experiments are provided to demonstrate the effectiveness of the proposed method.",7.0,4.0
41,Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning,TL;DR: We train a bot that places first in a no-press Diplomacy tournament with humans by using human-data-regularized reinforcement learning and planning,"Strengths
The paper tackles a problem that is less well-studied (compared to typical single-agent environments.)
The paper builds on previous works methodically. The proposed algorithm is intuitively clear. Each idea is well-motivated and the overall algorithm is easy to follow.
The experiments are well-constructed and nicely detailed. A number of strong baselines are used.
The overall algorithm, Diplodocus, clearly improves the state-of-the-art.
Weaknesses
I didn't spot any major technical issues with the paper. A couple of minor suggestions and questions.
How might a standard MCTS (AlphaGo style) fare? While unsound for the reasons mentioned, it might make the experimental section more complete.
What would be needed to make the error bars in Table 2 smaller?
I'd be curious to better understand how performance varies as a function of human player level. For example, can Diplodocus outperform weak human players? This would likely require a more complex tournament setup and is likely outside the scope of the paper. | The proposed algorithm is sound. It has some novelty, although it is mostly a combination of existing algorithms. There is also a theoretical analysis of the DiL-PiKL component.
The empirical performance seems also impressive, and the expert analysis certainly sheds further insights into the strength and weaknesses of the algorithm variants.
It is clear that regularization is mainly added to inject some cooperation flavor into the algorithm. A large chunk of the human play does not involve cooperating moves, but moves that aim to improve the position of the player (maximize the reward). Because regularization is uniform across all moves, it is forced to be small enough to avoid drop in performance (even if the human moves are reasonably good), which leads to weaker and less coherent cooperation, when needed (as indicated by the experts). | Strengths
This paper is exceptionally well written. It provides an accurate and concise narrative of all modern work (ie., those containing DRL; it would be nice if they referenced some pre-DRL work in this space) in a very easy-to-process exposition; especially, as it pertains to the motivation behind each innovation.
The innovation focuses on a real problem: fixed-coefficients in compound losses limiting the scope of discoverable strategies. The propose a reasonable solution of sampling throughout training and theoretically show that in certain cases this maintains convergence garuntees. The convergence garuntees are for their modified objective and not the true objective, and I would be interested to hear the author's thoughts on why this is reasonable and how their work might also show convergence to the true objective? The true objective should be to learn an equilibrium (maximize utility), and the human-like component of the loss should only seek to influence selection from equilibrium.
The paper also contains empirical results on full no-press Diplomacy with agents and humans including qualitative results. They perform notably strong against within the agent population, but the quantitative results including humans are inconclusive. The qualitative results are mixed, but generally positive towards the proposed agents. While the results are not overwhelmingly positive it's good to see such diverse and interesting analysis. I would suggest that the authors also consider in future work inquiring from human-players about finesse or attempts to identify the agent player during play. Perhaps have some probability that no agents are present?
Weaknesses
A large concern I've had with this work and its predecessors is that it concerns the equilibrium selection problem without adequate discussion of its tradeoffs. This work penalizes an agent towards behaviors similar to a single representative policy that are meant to represent an entire population of interest. Artifacts of construction of the representative also play a large role in the equilibrium selection process by are not discussed. Why should a single policy be sufficient to represent a target population of interest? What about biases about the representation of individuals within the population as it pertains to the representative? How would this effect performance across diverse sub-sets of the target population and to those outside of the population?
Could the authors please elaborate on why the beta distribution should be considered common knowledge? (Sec 3, Par 2).
This work proposes effectively using a distribution over behaviour policies to generate training data. I am curious how the authors arrived at the coefficients that represent the types within this population? Moreover, how diverse are these policies? I should expect that adding diverse experiences would cause the agent to require much longer training, but Fig3 makes it seem that the agents require effectively no training?
As the authors are aware the country played has a large impact on the success of an agent (Appendix). This makes comparing agents on such a small number of games challenging when attempting to normalize out the power that they played. The Elo bias numbers associated with each power that the authors construct are so large that would render any attempt to understand Table 2 unfair. I would have preferred that more controlled studies were performed to really try and understand the differences with the least confounding variables. I do appreciate that human studies are very expensive and the results within are still nice.
The claim of ""Mastery"" is exceptionally strong considering the inconclusive evidence about this method being measurably different from vastly simpler baselines.
The main methodological change is related to influencing the human-likeness coefficient, but the experimental results do not focus on trying to understand the changes, but instead solely care about a particular game.
It's not clear to mean that being human-predictable is a good objective to optimize towards. Consider what Expert 1 said (pg13): ""I think it is quite predictable in what motivates ... That's potentially exploitable, even with the strong tactics"". This is a very astute observation, and I tend to agree with the underlying idea. We would like to select human-like equilibria for cooperative, but this may hinder the policy's performance when cooperation is no longer needed. A particularly interesting problem in Diplomacy where the amount of cooperation required is fluid over the course of the game.",8.0,3.3333333333333335
42,Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach,"Keywords: Multi-objective Optimization, Machine Learning","Strength: This is an open and important problem in MOO.
Questions:
I am pretty unsure about the correctness of Lemma 4 which indicates the Lipschitz continuity of the lambda. Dontchev & Rockafellar, 2009, Theorem 2F7 does not imply this Lemma.
Using the notation in Dontchev & Rockafellar, 2009, Theorem 2F7, that result holds locally (given
x
, we have a
μ
) but when we approach the Pareto stationary point (
x
changes), that
μ
might change and we cannot have a global bound of the
μ
. In other words, the Lipschitz constant of
λ
should actually increase (to infinite) when we are closer to the stationary.
The lambda can be viewed as a solution map of a dual problem of a quadratic programming problem and its Lipschitzness is a very classic problem. To my best knowledge, all of the existing estimations of the Lipschitz are local, not global. Some literatures [1,2,3]
[1] On the continuity of the minimum in Parametric quadratic programs. [2] On the Lipschitz Behavior of Optimal Solutions in Parametric Problems of Quadratic Optimization and Linear Complementarity [3] Sufficient Conditions for the Lipschitz Continuity of QP-based Multi-Objective Control of Humanoid Robots | Strength:
The problem studied is important.
The writing is easy to follow.
Weakness:
It is not clear why multi-objective optimization problems should be formulated as bilevel optimization problems. This critical point should be discussed clearly. At present, it is a weird combination of those two problems.
The theoretical analysis is mainly adapted from the bilevel optimization literature. This makes the contribution incremental. The authors should highlight the challenges and the different parts compared with existing bilevel optimization literature.
This paper assumes that the loss function is Lipschitz continuous. With this strong assumption, existing bilevel optimization literature can achieve the convergence rate
O(K−1/2)
, which is much stronger than the convergence rate in this paper.
In lemma 2, the number of objectives
M
should depend on the number of iterations. This is definitely NOT feasible in real-world applications. | Strengths:
The paper is well-motivated and well-written, the story line and analysis is easy to follow for experienced readers;
The MoCo algorithm proposed in this work is the fisrt stochastic multi-gradient method that can converge to Pareto stationary point under mild condition, which is a decent contribution to the field.
Weakness:
I did not find any obvious weakness of the paper. The notation is a little bit heavy, maybe it is better to add a notation subsection.",8.0,3.6666666666666665
43,MocoSFL: enabling cross-client collaborative self-supervised learning,"Keywords: Self-supervised Learning, Collaborative Learning, Split Federated Learning, Momentum Contrast","Strengths: The chosen topic of collaborative SSL is an important and timely topic. The paper is clearly written. Performance is shown to be better than SOTA in some cases.
Weaknesses:
The novelty is limited as the work represents a combination of known methods. The design choices made during this combination all seem right but are also either already known or easily anticipated (like latent vector concatenation to create a large batch size for the server-side model, sharing of features and more frequent synchronization). As for the target-aware variation, it is not clear whether the availability of local clients' training data (however small) is justified. Sure, as the authors mention, Bhagoji et al., 2019 considers the possibility of server being able to validate local models, but note that Bhagoji et al. mean to raise the bar for attackers (in an effort to prove that attacks are possible). In contrast, the present authors actually consider the possibility of using the local client data to guard against MIA attacks (and to reduce communication overhead following the feature extractor freezing). My impression is that this is a highly non-standard FL setting, and I would want to see more convincing references.
Specific questions/issues include:
The obvious baselines FedEMA and FedU are missing from Table 2 and Table 3. From Table 5, the results for Non-IID cases with Nc=5 or Nc =100 are better than IID cases which is counter-intuitive. Could the authors provide justification for this performance gain? Can the authors explain why the latent vector size in Table 1 changed from 5000 to 39.1 MB? Is it just because the backward gradients are not included? In Figure 7, how the GFLOPs for computation are measured? Specifically, does it include the FLOPs needed for backpropagation? The authors claim this technique to be competitive in cross-silo setting. However, the paper requires having batch size in range of 100-200 at the server-end which seems impractical considering there exists millions of samples/clients. Could the authors comment on this? Eq (2) finds that having a large batch-size can maintain hardness of negative samples, which seems to contradicts the empirical results in Table 6 where increasing the batch size further to 400 degrades the performance. Reasons? Did the authors implemented/reproduced the main results of baselines works e.g. FedEMA? It seems like the results are exactly as in the original paper. How does this method compared against a FedMoco (Nanqing Dong, Irina Voiculescu: “Federated Contrastive Learning for Decentralized Unlabeled Medical Images”, 2021; arXiv:2109.07504) which also utilized MoCo in federated learning setup? Fig 3 and Fig 7 contains computation results per clients only. How much computation burden is added to the server side? | Strength:
The practical value offered by MocoSFL is interesting: I think MocoSFL is a very practical solution that can support a large number of clients (1,000 clients) to conduct collaborative self-supervised learning (SSL).
The proposed MocoSFL has experienced enormous evaluations: The proposed MocoSFL takes into account both IID and non-IID settings, as well as the cross-client and cross-silo cases; experiments are carried out in both simulated and real hardware devices.
The proposed scheme takes into account not only model performance, hardware requirements, and communication overhead, but also privacy concerns.
Because most IoT devices have limited memory and most data is unlabeled in reality, the investigated problem is both timely and important to the community.
The paper's writing is good, and the overall structure is clear.
Weaknesses:
In addition to model inversion attacks, I am wondering whether MocoSFL with the TAResSFL module has the potential to defend against other privacy attacks, such as the Membership Inference Attack.
Although 1,000 clients are already a SOTA achievement, it would be great to also analyze how many clients can be supported by MocoSFL at most. Is there any turning point here?
I am very interested in the possibility of landing MocoSFL in ubiquitous IoT devices, so I want to know what is the min hardware requirement to implement MocoSFL, It is better to provide more discussions on this part?",7.0,4.0
44,Modeling content creator incentives on algorithm-curated platforms,"Keywords: Nash equilibria, producer incentives, attention monetizing platforms, recommenders, differentiable games, exposure game","Strengths:
The paper is well-written and flows very smoothly. Despite the paper's modest space, it provides the required context and necessary explanation in a good job.
Most parts of the paper have clear motivations. Many of my questions are well answered in the paper.
Formulation and theoretical analysis of the exposure game in the recommender system is insightful. Most papers studied the recommender system in a fixed manner. I agree understanding the interaction effects/dynamics between the producer and customer is important and useful.
The analysis of diversity and exploration is interesting. The paper mentioned the game may concentrate on uniform distribution, and exploration may incentivize content that is uniform and broadly appealing rather than diverse. From the recent papers, I also found deeper models can achieve diversity/coverage and accuracy at the same time. These two concepts may not be contradictory. If could further reveal the relationship between these two, it'll be very interesting.
The paper identified several factors that can influence the recommendation seriously and provided a pre-deployment audit tool that can benefit the community.
Weaknesses/Questions:
In eq. (2), the paper introduced the temperature parameter
τ
to control the spread of exposure probabilities. Many of the follow-up analyses and experiments are built on this. However, most recommender systems didn't include this in their objectives. Could you please explain the rationale or connectivity between these?
On page 3, I'm confused about the full control assumption. What's the difference between full contry and partial control and why it can abstract away the explicit model of producer actions? Is it possible to list some concrete examples?
For experiments, how are the producers defined? In my opinion, there are only user/item information in these data sets.
For the
ϵ
-LNE solver in eq. (4), is this the paper originally proposed or adapted from others? I'm not familiar with the NE-based methods. It seems it's also updating the model with the gradient. Could you please illustrate a little more on the common points and difference between this one and the normal gradient descent method used to optimize ML model? | Strengths:
The topic is very interesting, and important.
The authors use serious game-theoretical approaches to assess the likely outcome of the strategic considerations by content creators
The results are, so far as I can tell on an extremely quick read, more or less correct.
The results tell us something interesting - more explorative recommender systems can help to avoid over-targeting by producers, but might reduce the diversity of content that is created.
Weaknesses:
There are several problems in the proofs (see below).
Insufficient details are given about the experimental results. Section 3.2 was extremely difficult to follow - the plots are very small, with poorly explained axes; which concept of clustering is being used; etc etc.
The claims about switching from non-negative to unconstrained feature spaces having a strong effect is only actually present in Proposition 1, in the case of two players and a two-dimensional feature space (since features are constrained to have norm 1, what has actually happened is that the feature space becomes a line instead of being a circle, but with higher dimensions there is less topological distinction between non-negative and unconstrained).
Problems in the maths:
The proof of Theorem 2 appears to be incomplete. In the hardmax case, the given proof says that for any s_1 that is close to c_1, there is a better position on the geodesic between c_2 and c_3. This means that switching to that point will get better reward than staying at s_1, in the absence of player 2. However the whole point is that there is a player 2, who needs to be taken into account. The logic would need to go something like: for any s_1, player 2 will select this particular s_2, which means that player 1 is not playing a best response to player 2. It is quite possible that the theorem is true (I do not have the time to check) but the proof is not correct.
In the statement of Proposition 1, what is the point of defining \hat{c}? It is a scaling of E(c), to which the first (and only) thing that is done is a further scaling to get \bar{c}.
In the proof of Proposition 1, I think more care needs to be taken. In the first part, what if there is no such theta_m? For example if there is a theta_m such that P(theta_c >= theta_m) >0.5 and P(theta_c <= theta_m) > 0.5 (i.e. P(theta_c=theta_m) is positive, and it just so happens that this is right in the middle of the distribution). For the non-existence when d>2, same complaint as for the proof of Theorem 2 - you need to complete the argument instead of just showing that s_1 is not a best response to itself. (I’ve just had a thought - are you assuming everything is symmetric, so a pure Nash equilibrium must have all s_i equal? That seems to be an anomalous piece of logic - just because the game is symmetric does not mean that the equilibrium must be symmetric - although is this actually the result that is buried inside the 2nd paragraph of the softmax part of the proof, and does that apply t hardmax too?).
I have not checked the proofs of Lemmas 1 and 2.
Proposition 2 is not proved | Strengths
The proofs of this paper are very (sometimes too) elegant.
The model proposed would be highly beneficial when used to assess recommendation systems and potential bias that could emerge prior to deploying them to production.
The paper is well written and, except for the proofs and a few other points, easy to follow.
As the author claim: their work “apply equally to classical matrix factorization and deep learning-based systems.” Which is appreciable, given how deep learning-based systems are still underresearched.
The authors acknowledge the limitations of their model and do not present it as a miracle solution.
Weaknesses
Page 2: You define
Sd−1
after using it. Please define it before/within Definition 1.
Figure 3, plot B & C: if 1 producer is at midpoint, shouldn't the utility reach
1n
at
λ=0.5
? Or did you not normalize the vector
s1
?
Page 6: You mention “Riemannian second derivative test” Can you please be more specific? The reference is long, dense, and do not mention this term.
Page 17, Figure 7: It seems to me like there is an issue with the two central plots as
f(θ)=∇θ1u1(s)
but the plot do not seem to show this relationship.
f(π4)=0
but
u1(s)
appears decreasing at that value of
θ=π4
.
Page 18: It seems to me like the partitions
X(i)
and
Y
are unnecessarily complex, I didn't fully get the grasp on them. Isn't there a simpler way to deal with it?
Not really weaknesses:
Page 3: ""motivated creators will actively optimize their exposure using trial-and-error, making complete information and full-control an imperfect yet not unreasonable model of their behavior."" Or is it? This trial-and-error is done in an always-changing environment (many hours of video uploaded every second on youtube), so is it that informative?
Page 6: “Exploration effects are typically thought of as having negative impact on user experience through immediate reduction in quality of service as a result of suboptimal recommendations.” Being trapped in an ""algorithmic bubble"" with always the same type of recommendation is critisized and not appreciated.
Proposition 1: isn't there a typo? ""For
n=2
non-negative softmax games"". Shouldn't it be
n≥2
?
Page 15, proof of proposition 1:
P(θ≥θm)=12
. What is
θ
?
Page 16, typo:
Pc=12(δc1+δc2)
, should be
δc2
.
Page 17, typo: ""
c3
then
c2
"" should have been ""than"".
Page 17, typo: ""
c1
(resp.
c3
)"" it seems to me like they should be switched.",8.666666666666666,3.6666666666666665
45,Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics,"Keywords: Embodied AI, Adaptation, Visual Navigation","The reported results are very strong, the authors test the proposed method to both modest and severe drifts, as well with multiple disabled actions. They compare the proposed method to a variety of relevant baselines; including model-based RL, meta-learning, and sim2real methods; significantly outperforming all of them in almost all experiments, with a greater performance gap on bigger drifts.
The proposed model is very well-motivated and sensible, and I appreciated the detailed ablation experiments demonstrating the importance of both action embeddings and order-invariant action selection.
The main weakness of the paper is that the proposed method is evaluated on only one environment, AI2-THOR. While using embodied AI environment makes sense given the motivation for the proposed method is in robotics applications, I don't see a reason why this method wouldn't be more generally applicable in RL. The paper would be stronger with the addition of experiments on other environments, e.g. Mujoco, and some even simpler environments where the authors could more easily interpret the learned action embedding and policies, as well as shed light on the limitations of the method.
Some questions for the authors:
How many environment interactions does it typically take to learn action semantics in a new environment?
Do you have thoughts on how the proposed method could be adapted to continuous actions?
How exactly is action selecting
mi
and how is
mi
computed? I assumed you're selecting the last memory embedding transitions made under action
ai
, hence only transitions made under action
i
influence the embedding
ei
; however it would be good to make this less ambiguous in the paper.
How well does the method perform in a non-stationary setting (i.e. where the action semantics change throughout the lifetime) or a changing distribution over actions (e.g. different drift sizes for each rotate action)? These questions are beyond the scope of the paper, but they seem like interesting application areas, and it would be good to get a sense of the method's limitations. | Strengths:
The experimentation section is strong to support most of their claims.
In their experiments, authors claim to outperform meta-learning approaches without requiring, computationally taxing, meta-learning training.
Out-of-distribution generalization (avoiding disabled actions)
For example, if the action a_i is Rotate(30◦) - this is a good example
Weakness:
The approach is not tested on (a) another sim environment like Gibson, MP3D AI Habitat (b) real robot - usually sim2real is not easy for this embodied AI tasks
It seems defective wheel plays a major role in errors. It can be due to surface change, sensor failure, environment noise. Some error modelling technique could have been pursued.
How is the learned RL model applicable to different robot type with different hardware sensors and motors? [A Limitation]
Embedding is not explained clearly - internals
The performance of the drift correction is tied up with the specific goal task at hand | Strength
This paper proposes an approach that is useful to deal with action uncertainty in test time. This will add robustness to embedded agents.
Compared to data augmentation which is arbitrary, this paper shows a systematic approach to deal with action drifts.
Weaknesses
The approach assumes that the disabled actions can be remapped to one of the state changes the agent has seen before in training in order to take the right actions. For rotations, it is possible to cover all rotations in training, but for translation, it depends on the step size, for the translation that is really out-of-distribution state changes, this approach still cannot recover from that. This is shown as the lower success rate for the 0.4m drift. The authors need to make this assumption clear to the reader.
One of the claims of the paper is that the embedding of state changes can help recover from action drift, but in the ablation study, LAC’s performance still drops when the drift increases which invalidates the claim of the paper. It is unintuitive why the action impact (i.e. state change) embedding doesn’t have an impact without OI.
The ablation doesn’t really show OI is helpful as AAP uses a transformer head but LAC uses a linear head. The difference in performance can be the difference in model capacity. A fairer comparison will be a transformer head with positional encoding.",7.666666666666667,5.0
46,"Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve","Keywords: Variational Autoencoders, VAEs, Hypernetworks, Response Functions, Hyperparameter Tuning","Strengths:
The proposed hyper-network construction seems to be theoretically principled. This is because for linear VAEs, the paper provides a theorem that proves that indeed the optimized hyper-network can output parameters for the optimal linear
β
-VAE.
The hyper-network is of practical importance since it bypasses the need to retrain
β
-VAEs for different values of
β
, while fine tuning for optimal
β
.
The hyper-network has similar size and training time as the
β
-VAE it outputs.
The results have been empirically verified on MNIST, OMNIGLOT, CIFAR10 and CelebA datasets, where it is shown that the rate-distortion curve created using the the networks computed from this hyper-network matches the one that is obtained by repeatedly training
β
-VAEs for different values of
β
. | Strengths
Very well-written paper with a clear motivation.
Strong empirical results on diverse datasets. In particular, the paper addresses the important questions that naturally arise: (1) does MR-VAE introduce performance degradation? (2) does MR-VAE generalize to larger, complicated architectures? (3) how sensitive is MR-VAE to the choice of its (only) additional hyperparameter
(a,b)
? The experiments convincingly and positively answer all these questions.
Weaknesses
I did not find any particular weakness. | Paper strengths: The authors start from simple log-linear Gaussian VAEs and prove theoretically that a simple affine transform of the encoder/decoder weights parametrised by
β
provides the dependence of the optimal wights on the hyper-parameter
β
. They conjecture that the hyper-network for complex, non-linear Gaussian VAEs is obtained by applying such affine transforms of the base weights in each layer of the encoder/decoder pair. The experiments compare achievable data likelihoods for VAEs (with fixed architecture), when learned with fixed betas (aka response curve), with the one obtained from learning the hypernetwork. It is striking to see how good the corresponding rate curves are matched by the hypernetwork for different VAE variants.
Paper weaknesses: There are a few conceptual choices and issues which remain unclear for me: I was not able to fully follow the proofs given in the supplementary material for the log-linear Gaussian VAEs:
Step from (18) to (19,20): Taking the expectation over the data and maximising it w.r.t. to
μ
will give a
μMLE
that depends on the other model parameters. It remains unclear how to derive the gradients of the objective w.r.t. the latter.
Theorem 1: It remains unclear to me, why it is necessary to represent the parameter matrices of the model as products of matrices? Why are then the affine transforms applied directly to the VAE weights for the non-linear models in (8,9)?
It remains unclear to me, why applying the affine transform in all layers of the encoder/decoder of non-linear VAEs is sufficient for obtaining the required hypernetwork. Is this may be related to the known rescaling properties of ReLU networks? Would this work also for other non-linearities?. And more general, can this approach be used also for non-Gaussian VAEs with discrete latent variables?",8.0,4.0
47,Near-optimal Coresets for Robust Clustering,"Keywords: clustering, outlier, robustness, coreset","Strengths:
The robust clustering problem is important in practice. Coresets are very relevant in large scale and dynamic applications where going maintaining and processing all the data points is impractical.
The algorithm is simple and intuitive.
The experimental results on four public datasets are promising. In particular, the authors' coresets have significantly better quality than trivial ones, e.g. uniform sampling. In addition, the coreset generation time seems to be >10x faster than the time if one were to run robust clustering without a coreset. This shows that coreset generation can be used as a preprocessing step to significantly speed up robust clustering.
Weaknesses:
I would like to have seen a comparison with the algorithm of Bhaskara et al. In theory it gives a
O(1)
-approximation, but how does it do in practice compared to the coreset approach? Also, there are other, simpler heuristics for coreset generation that would be nice to be considered. For example, what happens if I just perform uniform sampling on the clusters of Bhaskara (plus the outliers)?
There is some confusion with symbols in various places:
P
vs
X
,
n
not defined, what is
c∗
in Lemma F.1, etc. | S1: Strong results and significant improvement of the state of the art. S2: Timely contribution as the non-robust version has been only recently settled and having robust algorithms is a hot topic. S3: Great practical performance despite the large dependence on k and epsilon.
W1: There is still some gap between upper and lower bounds. | strength:
the result improves the coreset size by expotential factor in k abd m, by bringing it down to nearly optimal (m + ply(n,eps^(-1))).
it presents extensive experimental results on real world datasets.
weakness/suggestions:
a comparative discussion about its coreset vs exponential size coreset will be useful to motivate towards the probem
are the groups consist of rings with at most 1 point? The discussion about groups could be further improved.
a proof sketch of lemma 3.6 and 3.7 in the main paper could be useful.",8.0,3.3333333333333335
48,Near-optimal Policy Identification in Active Reinforcement Learning,"Keywords: reinforcement learning, contextual bayesian optimization, kernelized least-squares value iteration","Strengths:
The authors study the interesting problem of best policy identification in episodic reinforcement learning
The authors propose a fairly simple algorithm, easy to implement and with performance guarantees
The sample complexity bound does not scale with the state and action space sizes
The authors perform a thorough empirical evaluation explicitly identifying the limitations of the algorithms and the settings in which it performs better than the baselines
Weaknesses:
The central point of the algorithm is choosing states to sample from he generative model based on the perfomance gap, i.e. the difference between the upper and lower bounds on the V-function of the state. The authors should cite other works that performs selection on similar metrics based on this uncertainty. For example, there is a large body of work in the best-arm identification literature that uses the difference between upper and lower bounds to select which arm to choose, [1], [2] etc.
The paper does use a fairly common assumption from the literature, but nonetheless it should be more self contained. I would have expected to see a discussion on how restrictive is the RHKS assumption as well as a comparison of the guarantees provided non only with methods taking the same assumption but also with other method that make less or more limiting assumptions. For example, a comparison with methods that assume linearity of the reward and value functions would improve the clarity of the paper.
A bit more discussion on why the bound presented loses the dependency on the state and action space sizes might also improve the paper. Especially when comparing with the algorithm presented in Cher et. al. 2019 at the end of Section 4 which also makes the RHKS assumption, has a dependency on
Γk
but also on the state and action spaces.
The algorithms adds an additional parameter
βt
, fairly important since it is used to define the upper and lower bound on Q-values, central for the ""exploration"" method used in AE-LSVI. Unfortunately, no discussion on the selection of this hyperparameter is provided. The authors present all the results with the value 0.5 (why 0.5). Instead, an empirical evaluation of the effect off the value of
βt
is warranted.
While I am mostly positive on the experimental section, I am surprised with the choice of DDQN as a baseline. DDQN was chosen as a SOTA method for online RL, but there are more advanced methods right now for online RL, especially it would have been beneficial to see other online RL methods that use uncertainty estimates just like AE-LSVI like [3] [4] or [5]. Any of them would make a better baseline then DDQN but especially [3] since it makes decisions also based on upper and lower bounds on Q.
[1] Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, Sébastien Bubeck, Multi-Bandit Best Arm Identification, NeurIPS 2022
[2] Marta Soare, Alessandro Lazaric, Rémi Munos, Best-Arm Identification in Linear Bandits, NeurIPS 2014
[3] Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, Michael I. Jordan, Tactical Optimism and Pessimism for Deep Reinforcement Learning
[4] Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Deep Exploration via Bootstrapped DQN
[5] Alberto Maria Metelli, Amarildo Likmeta, Marcello Restelli, Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters | Strengths:
Despite being a combination of multi-decade-old ideas, the eventual algorithm is simple, solid, and novel.
The theoretical results are thorough and sufficiently informative, while being a corollary of existing work.
The experiments are comprehensive enough and the reported results support the central claim of the paper.
Weaknesses:
The baselines chosen for comparison are rather arbitrary and not state-of-the-art. | The contributions and strengths of paper are listed above. Below, I list some points to discuss:
Page 3. It was mentioned that the goal in this setting is to identify an
ϵ
-optimal policy while minimizing the number of necessary episodes
T
. This differs from the standard setting on cumulative regret minimization. The paper would benefit from adding more motivations to justify such an objective. Is this objective commonly used in the generative model setting?
Under the generative model setting, the agent has access to a simulator that can be used for adaptive data generation. The optimal policy that maximizes the cumulative reward in simulators is not necessarily maximizing that in real applications, due to potential difference from simulator and real-world environments. Would it be better to consider the more realistic setting with sim-to-real gap?
Instead of obtaining
sht
according to Equation (12), we can alternatively define some uncertainty measure for the value and directly selects
sht
that minimizes the uncertainty. Would this procedure work?
While kernel methods enjoy nice theoretical properties, their empirical implementation requires tuning of the kernel bandwidth. What kernel functions did you choose in the numerical experiments? Are the results sensitive to the choice of the bandwidth? How would you suggest practioners to select this hyper-parameter?
In continuous action setting, the author(s) proposed to first discretize the action space and then apply the proposed method. Can your methodology be extended to the continuous action setting? Alternatively, can we adaptively discretize the action space to improve the performance (see e.g., http://proceedings.mlr.press/v80/lee18b/lee18b.pdf; https://arxiv.org/pdf/2007.00717.pdf; https://openreview.net/pdf?id=rvKD3iqtBdk)?
The current methodology and theory are developed under the episodic MDP setting with time-varying dynamics. Can these results be extended to time-homogeneous MDP models?",8.0,3.3333333333333335
49,Offline Q-learning on Diverse Multi-Task Data Both Scales And Generalizes,"Keywords: offline RL, multi-task Atari, large models","Strength
The paper is very well-written.
Design choices are well-explained and motivated.
Experiments/ablations are abundant, and experimental results are convincing.
The paper offers very convincingly results that offline RL algorithms can train very large models on a massive amount of data. This paves the way for a large unified decision-making foundation model (so to speak).
Weakness
As thorough as this paper is, I find it relatively disappointing that despite many different offline RL algorithms having been proposed over the last 2-3 years, the authors only used CQL. This opens up several interesting empirical questions:
The paper makes it seem that DR3 (feature normalization), categorical representation of return values, and Q-function architecture matter a great deal. Does the offline learning algorithm matter at all? Would TD3 + BC also scale? Would BCQ scale? It's definitely difficult to evaluate every single offline RL algorithm, but it would be great if at least one other offline RL algorithm is chosen to compare with CQL.
With a huge amount of data (400M transitions and 2B transitions), one might wonder if pessimism is no longer needed since all possible (s, a) might have already been covered -- is the pessimistic penalty really necessary? An ablation study can train with TD3 directly or any off-policy Q-learning algorithm.
""...making too many TD updates to the Q-function in offline deep RL is known to sometimes lead to performance degradation and unlearning"" [1]. On such a large dataset with 40 games, is there an optimal stopping point for training? Is there performance degradation if you train longer? Or do you think because of DR3, performance degradation won't happen anymore?
[1] DR3: VALUE-BASED DEEP REINFORCEMENT LEARNING REQUIRES EXPLICIT REGULARIZATION | Strengths:
The obtained results are very impressive, from the effectiveness of the methods to enable strong scaling trends, to the ability to dramatically surpass the scores of the training trajectories, to the strong online and offline fine-tuning results.
The proposed modifications were well motivated and the ablations illustrate the contribution of each change.
Well written, with formatting that makes it easy to follow the key contributions and points.
Weaknesses:
None of the results except Figure 8 have error bars despite the fact that the error bars in Figure 8 indicate there may be substantial variance.
As far as I can tell, the MT Impala-DQN* setting is not described in/around Figure 5. | Paper strengths and contributions
Motivation and intuition The motivation for addressing the problem of learning general-purpose representations in offline Q-learning is convincing.
Novelty In my opinion, studying extrapolation beyond datasets even when trained entirely on a large but highly suboptimal dataset in order to increase model performance when the model capacity increases seem novel.
Technical contribution
The idea of utilizing standard feature extractor backbones from vision (i.e., the Impala-CNN architectures) to improve performance as model capacity increases is intuitive and convincing.
The outcomes of combining and empirically verifying the effectiveness of several techniques should be helpful for the research community.
Clarity
The overall writing is clear. The authors utilize figures well to illustrate the ideas and framework. Figure 3 clearly shows an overview of the network architecture.
The paper gives clear descriptions in both theoretical and intuitive ways. The notations and the formulations are well-explained.
Ablation study Ablation studies are comprehensive. The proposed framework consists of multiple components. The provided ablation studies are helpful for analyzing their effectiveness of them.
Experimental results The presentation of the experimental results is clear. Particularly, Figure 7 provides clearly understandable results showing pre-trained representations from Q-learning enable positive transfer to novel games and lead to significant performance gain compared to return-conditioned supervised learning methods and representation learning approaches.
Reproducibility Given the clear description in the main paper and the details provided in the appendix, I believe reproducing the results is possible.
Paper weaknesses and questions
Experiment The experiments could be more diverse. While the claims and the results look promising, the proposed method is only evaluated in Atari games. I believe evaluating in robot manipulation domains, locomotion domains, or navigation domains would make this work a lot stronger. | Strengths:
The paper shows that augmenting CQL by only three, rather simple but broadly applicable design choices enables learning a much improved single multi-task policy from suboptimal expert data.
The method shows a nice scaling behaviour for models of increasing capacity
Finetuning and adaptation experiments to new game variants are very compelling
Convincing and superior experimental results over baseline
Ablation studies to assess the relevance of each model choice.
The paper is well-structured and easy to follow.
Nice presentation of results
Weaknesses:
The paper makes a reference to an anonymous work from which the backbone architecture seems to be motivated. Unfortunately, I could not find this work in the supplementary material to gain further insights into the design decisions and the importance of this component.
The importance of the backbone architecture is unclear as there are missing ablation studies.
Minor: the policy still necessitates individual heads for each game and it would have been interesting to see how limiting this particular design choice is for obtaining the reported performances.
Minor:
p.5: “[...] we found that this this speeds […]”",8.0,4.0
50,Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization,"Keywords: Deep Reinforcement Learning, Offline Reinforcement Learning, Value Regularization, Continuous Control","The paper is well-written and does a good job in explaining relevant concepts. The theoretical analysis gives interesting insights and shows also how prior algorithms (IQL and CQL) fit into this framework. Further relevant work is nicely summarized in the context of the paper and put in comparison to the proposed method. The proposed method Sparse Q-learning (SQL) is derived from the theoretical framework under reasonable and well-explained assumptions. SQL achieve SOTA performance on the D4RL benchmark and also strong results for the noisy and the small data regime.
Some typos: p.2 ""implicit value regularization"" -> ""implicit value regularization"" p.2 ""IQL remains much close to the..."" -> ""IQL remains much closer to the..."" p.2 ""...similarities to IQL However..."" ""...similarities to IQL. However..."" p.3 In the last equation defining the policy evaluation operator for V: There should be a plus instead of a minus p.4 ""... adds a entropy..."" -> ""... adds an entropy..."" p.5 the sentence "" U∗(s) can be uniquely solved from the equation obtained by ..."" appears twice in two consecutive paragraphs. p.9 ""This simulates the situation where the dataset is fewer and has limited state coverage near the target location because the data generation policies maybe not be satisfied and are more determined when they get closer to the target location"" what is meant here?? | Strengths:
Overall good writing quality and clarity.
Interesting theoretical results that might help better understand offline methods that rely on some form of regularization terms.
Empirical results on D4RL and other 2 comparisons seem to show SQL has stronger performance than alternative methods, and some of these methods are recent methods with good performance.
Technical details provided for reproducibility.
a lot of discussion on related works, easy to understand how it relates to prior works
Weaknesses:
Thank you for providing technical details including your hyperparameter search range. However, I am a bit concerned that it seems you performed a hyperparameter search on each dataset individually, and then select the best-performing one. This makes sense, but for the methods you compare to, are they using the same hyperparameter for all tasks, or do they also search hyperparameter on each individual dataset and report the best one?
Similarly, in the case when you compare SQL to CQL and IQL on the noisy and small data regime settings, did you also do a thorough hyperparameter search for IQL and CQL?
Maybe I missed sth but have you provided empirical evidence and figures to demonstrate that your method has a more ""sparse"" policy than the other competitive methods?
What is the final hyperparameter you selected for each individual dataset? (You mentioned the best is selected for each dataset, would be good if you also report the actual numbers)
What about computation efficiency? How fast in wall-clock speed does your method compare to others?
Hyperparameter sensitivity: for example can you provide some results on how performance changes with different alpha values? This will help other researchers understand how hard it is to tune your method on new tasks.
Not really important:
some minor typos
Other Questions:
Will your code be open sourced? | Strengths:
The paper is well written, especially the theoretical section.
The proposed SQL method is theoretically well-motivated. The theory also points out interesting connections to existing methods including CQL and IQL. Furthermore, SQL only requires one hyperparameter as opposed to two in IQL.
The implicit value framework derived in the framework seems like a general framework for instantiating in-sample Q-learning methods based on
α
-divergences (although most methods might turn out to be intractable).
Weaknesses
The improvements on the D4RL benchmark do not seem to be statistically significant -- Reported results are likely within 1 standard deviation of baseline results and thus the SOTA claim may not be valid.
Error bars are not reported for any of the baselines in Table 1 and 2 -- however, the standard deviation is known to be large for D4RL datasets. See https://openreview.net/pdf?id=Y4cs1Z3HnqL for a paper that reports standard deviation for some of the baseline methods.
The proposed method seems more complex to implement than existing methods such as IQL, while resulting only in marginal gains.
Possibly Unfair Comparisons: It seems that per-task hyperparameter tuning is done for SQL while the baseline methods' results seem to be copied from prior papers which used the same hyperparameter for a given domain (Kitchen / Antmaze / MuJoCo).
It's not clear whether ""sparsity"" plays an important role in performance of SQL -- this is not explored much in the paper despite the title.",8.0,3.6666666666666665
51,On the Sensitivity of Reward Inference to Misspecified Human Models,"Keywords: reward learning, inverse reinforcement learning, misspecification","Strengths: the paper is very well-written, and I appreciate the attempt to explore different types of human irrationality (e.g. myopia), and the experiments include real human data (albeit w/ a very simplified form of misspecification via simple re-weighting)
I am a bit confused why measuring reward misspecification by distance in parameter space works except for very restrictive, simple linear reward models. If the reward function is a NN, then it is well known that two networks that behavior similarly can have widely different parameters. Any necessary assumptions here should be clearly stated.
I am likewise confused why KL diverge between demonstration policy and the human model is the right way to measure distance. Given the motivations discussed in the paper of modeling humans via noisy-rationality, systemic biases, etc., it seems like humans may deviate from an idealized model in ways much stronger and more state-specific than the KL measure assumes. While I like the idea of fitting certain types of irrationalities (myopia and illusion of control) into this framework, it feels a bit cherry-picked to only study irrationalities that satisfy the specific notion of distance the authors chose. | The paper studies an important problem: To infer reward functions, a human model is necessary, but it is infeasible to infer it accurately. This is because inferring it would at least require the human's reward function, which is initially unknown. This problem becomes more important as AI systems attempt more complex tasks where human behavior is nowhere near optimal and cannot be modeled as optimal. If we know the relationship between reward errors and human model errors, we can better understand if we can improve reward learning by learning better human models. This understanding also helps clarify if reward learning is a viable strategy at all for complex tasks where humans are increasingly suboptimal. Both the optimistic and pessimistic result yield useful insight on these questions. The results are clear and succinct and to my knowledge novel, providing a clearly significant improvement in our knowledge of the important question at hand.
Weaknesses
In the first and last section, the authors make an optimistic conclusion overall, saying that their results imply that reward inference with misspecified human models is not very difficult. The positive conclusion needs some strong caveats stated in the first and last section to avoid false impressions. Though I think these caveats should be stated, I don't think they detract, and in fact add to, the value of the paper. By order of importance:
Firstly, the introduction speaks of mild assumptions for the positive conclusion that a slightly misspecified human model will lead to small errors in the inferred reward. However there is an assumption baked into the choice of error metric for human models: the KL(human_policy | model_policy). To assume that the human model error is small, this KL divergence assumes that the human policy would never take an action that the human model places ~0 probability on. Otherwise, the KL will be large, allowing large reward errors, and challenging the paper's positive conclusion.
Aside from noting this caveat, the authors could reproduce their results with a sufficiently different metric such as the reverse KL. Furthermore, the authors could note that having a small forward KL requires that the human model 'covers' the support of true human, as otherwise the KL would be large. This is an interesting result, as it means that, all we need is to learn human models with good coverage. (And it is not a problem if the human model sometimes takes actions that humans would not take).
Second, the authors construct a case where the inference error is large, but then argue that their construction is unlikely to occur in practice and that this supports their optimistic conclusion. However, they have not ruled out that other constructions exist. Some of these may be more likely in practice.
Third, the paper could justify its choice of reward distance metric, the squared distance between reward parameters. In particular, a small error in inferring the reward parameters might still lead to a large error in the resulting return and policy. | Pros:
High clarity wrt motivation, related work, assumptions and conducted theoretical and experimental work
Important topic - it is a baseline for numerous human-centered RL approaches focussing on AI alignment
Careful evaluation
Cons:
Minor open questions to reward comparison / generalization to other reward learning settings. How, for example, does recent work on equivalent-policy invariant comparison fit compared to the chosen reward similarity? Would it be possible derive similar results here or are such approaches prone to get caught in problems for adversarial samples? Is the derived bound dependent on chosen IRL methods / methods that assume a dataset of expert states and actions or would it transfer to reward functions learned based on trajectory comparisons or other kinds of datasets?
The related work could also other works on reward learning, e.g. from human preferences",6.333333333333333,3.0
52,On the duality between contrastive and non-contrastive self-supervised learning,"Keywords: Self-supervised learning, contrastive, non-contrastive","I am a big fan of unification papers. There are several new self-supervised methods appearing every year each claiming SOTA performance, but every method makes its own design choices and so conceptual comparisons can be difficult. This paper shows that many design choices are not essential, and is able to transform SimCLR into VICReg and vice versa while using the same network architecture. The authors also show that simply tuning some of the SimCLR hyper-parameters increases its performance on ImageNet to the SOTA level. I think this is a very welcome contribution and a very strong accept.
All the experiments were run on ImageNet for 100 epochs (only two final runs were made for 1000 epochs). Given that ImageNet optimization is costly, I was wondering if the authors could also demonstrate some of the results e.g. on CIFAR-10 where running 1000 epochs is not a problem. Or is CIFAR-10 ""too simple"" for the purpose of this study? | Strengths:
This paper promotes an interesting link between two variants of self-supervised loss which are often used in practice. Linking these two variants is a very original and interesting line of research, and the fact that the authors demonstrate that the two are very closely related is very useful to provide a better understanding of these techniques.
The theoretical part of the paper is useful, in that it shows that each of the two variants of self-supervised loss can be used to derive bounds for the other one. This is done in Equations 6 and 7 in the main paper. While the upper bounds in these equations are quite loose (for example, in Equation 6 the upper bound has a difference of the order
N2−N
), I believe that it should be argued that the lower bounds are more important. In Equation 6, normalizing the embeddings means we are optimizing
Lc
, and the lower bound tells us that
Lnc
cannot be arbitrarily large either (similarly in Equation 7). I think this is an important connection between the two styles of self-supervised learning.
The experimental part of the paper is also insightful, in that it supports the proposition of the authors that the two styles of self-supervised learning are closely related, so with sufficient tuning both should achieve similar performance. This is in line with the author's theoretical results. Moreover, in the Appendix the authors also compare the relationship between the losses in practice. These experiments serve to fully support the close connection between the two variants of self-supervised learning.
Weaknesses:
The authors demonstrate how several well-known self-supervised learning methods fall in their framework of sample contrastive and dimension contrastive methods. To fully complete this comparison, I believe that methods such as MoCo, BYOL and DINO should also be included in this discussion (in Proposition 3.2), since they are also very well-known. Knowing how these methods align in this framework would be useful (mainly because BYOL and DINO also make use of a second teacher modle during training).
As a minor issue, for the sake of clarity I believe that in their definitions of
Lc
and
Lnc
, the authors should include the alignment term between the representations, even if it is the same across the two. This would make it easier for the reader to recognize that the main difference across methods is the regularization term used in each.
Minor issues: There is a typo in Section 5.1, third line: “strateoes” -> “strategies” | Strength: The paper conducts many experiments towards substantiating the claims. The contribution is novel, to the extent of my knowledge, making a better understanding of the self-supervised learning methods. The work has potential for further theoretical research.
Weaknesses:
The presentation is not clear, which needs significant revision. More details are given in the Clarity section.
It seems this paper only considers SimCLR and VICReg, while the title tries to cover the whole contrastive and non-contrastive learning methods.
Lack of focus on invariance criteria. Although the claim is made that different similarity criteria are equivalent from an optimization point of view, experimental evidence in support of the claim is missing. I am not convinced that the differences between contrastive and non-contrastive learning methods only rely on the objective in Definition 3.2. I don't think different contrastive learning methods can be simply represented using definition 3.2, e.g., how can you show SimCLR can be written with Definition 3.2? Thus, I think there are many other factors in different contrastive learning methods, an using the designs in this paper cannot cover these.
Lack of KNN evaluation. The paper speaks of the equivalence of the two contrastive regimes and illustrates the equivalence through the gram and covariance matrices, besides empirical evidence with downstream performance. We see that the off diagonal elements in specifically the gram matrix are significantly lower than their diagonal counterparts. But this begs the question of whether the performance of dimension-contrastive methods are at par with sample-contrastive methods for KNN based evaluation of features. KNN performance based experiments will be helpful towards reinforcing the claim of the equivalence of the two regimes.
Claim that the small batch size issues with contrastive methods are misleading (Section 5, Clearing up misconceptions). The paper adequately demonstrates the merits of hyperparameter tuning and use of a better design choice for the projectors may alleviate the problems with small batch sizes, but it is in contrast with the work of [1] where they identify a systematic bias in the updates in SimCLR. This leads us to believe that a better landscape of hyperparameters or projector architectures may be available for large batches as well, reinstantiating the problem with smaller batch sizes in sample contrastive methods, which may require more experiments in those settings. Furthermore, keeping the architecture consistent for fair comparison, we still witness a drop in VICReg performance as the embedding dimensions decrease (Figures 1 and S4, Table S5).
[1] Yuan, Zhuoning, et al. ""Provable stochastic optimization for global contrastive learning: Small batch does not harm performance."" International Conference on Machine Learning. PMLR, 2022. | Strengths: This is a very insightful paper that provides a unifying view of the contrastive and non-contrastive self-supervised learning methods. From a theoretical perspective, they proved that the sample-contrastive criterion is equivalent to the dimension-contrastive criterion up to the normalization of the embedding matrices. In the experiments, they further empirically verified that the performance of several SOTA sample-contrastive and dimension-contrastive methods can be closed. I believe this unifying view of contrastive and non-contrastive self-supervised learning can help us gain a deeper understanding of these methods. And many techniques and advantages of one family can be transferred to the other, as demonstrated in this paper. It might also be possible to improve the performance by designing a self-supervised learning algorithm that exploits both sample-contrastive and dimension-contrastive criteria.
Weaknesses: For the theoretical part, it's proved that the sample-contrastive criterion is equivalent to the dimension-contrastive criterion when the embedding matrix is doubly normalized. However, when the embedding matrix is not doubly normalized, the bounds in Lemma 3.4 and Corollary 3.4.1 is pretty loose. So I wonder if it's possible to prove tighter bounds in some more restricted theoretical settings.",7.75,3.75
53,PaLI: A Jointly-Scaled Multilingual Language-Image Model,-''-,"Strengths:
This work demonstrates a new multilingual pretrained vision-language model with the following distinguished properties: SOTA performance on multimodal tasks without catastrophic forgetting language-only understanding capabilities; multilingual pretraining while maintaining SOTA performance on English-only tasks.
An interesting empirical insight that may benefit future VL pretraining: large-scale vision pretraining may not benefit vision-only tasks, but could improve VL pretraining tasks by a large margin.
Sufficient in-depth analysis and ablation studies are performed for better model understanding.
Weaknesses:
Similar architecture for vision-language pretraining has early been proposed in various works, for example in [a]. To the best of my knowledge, the major difference is to use encoder-decoder architecture (mT5) for downstream VL pretraining.
[a] MERLOT: Multimodal Neural Script Knowledge Models. NeurIPS 2021.
Intuitively, the pre-trained model can be used for multimodal machine translation task. However, most tasks are still traditional VL tasks, such as VQA.
Both image captioning Sec. 4.1 and VQA Sec 4.2 require extra finetuning on downstream datasets. What are the results of zero-shot performance? Is it possible to generate target answers with few-shot demonstrations by prompting?
Can you show training and inference time duration comparisons with current VL pre-trained models across different scales of the proposed model? | Strengths
The proposed encoder-decoder model design for language and vision tasks is simple and leverages existing pre-trained models to reduce training costs.
Finds that scaling the vision component leads to better accuracy per parameter/FLOP compared to language components.
PaLI outperforms the state-of-the-art on a large array of language and vision tasks such as image captioning, visual QA, language understanding, and zero-shot image classification. Notably, it even outperforms fixed-vocabulary approaches using an open-vocabulary generative setting.
Weaknesses
Analysis of the computational cost & time required for training and comparison with other models such as Flamingo are missing.
The paper shares model and dataset cards, however, it is not clear if the actual artifacts will be shared publicly which can hinder reproducibility and lead to wasted time for other researchers.
Other:
When comparing to other models, it would be useful for the reader to show the size of the baselines. E.g. Tables 1 and 2.
What are the 1-shot and 5-shot performances of PaLi on the INet dataset? It reaches a very close performance to Flamingo even with a 0-shot but it would be interesting to also show if it can outperform the 5-shot version with a few examples. | Strengths
The paper proposes an effective scaling strategy that significantly improves downstream performance on wide array of multimodal(vision+language) tasks
The VIT-e model trained for this work seems to be really helping multimodal tasks and will be a very useful contribution to the community if released
Experiments are quite extensive and thorough. Paper is well written and detailed.
The paper does a thorough ablation on the different pre-training mixtures and usefulness of the different objectives on downstream tasks, which is valuable.
Questions
I am slightly concerned with the technical novelty of the work as it is widely known that most of the multimodal benchmarks continue to perform well when the vision backbone/component of the model is better.
Language only tasks seem to have lower performance compared to T5XXL base model even with PaLI-17B. How does this change with smaller PaLI models where there is a smaller vision model?
It is unclear what is the impact of multilingual data and would have been great to see ablation with and without using multilingual data.
The performance on some of the datasets like VQA are way above human performance of ~81, which is a bit surprising. On tasks like VQA it has been evident from some time that better visual models have room for improvement, however to improve further, larger models must also get ambiguous answers correct. Although we are deduplicating and removing any overlap between pretraining and text data, do you think it might be possible there is some leakage, not in a full sample form, but either only on text side or image side? Can you provide more details about the deduplication? | Strength
This paper consistently shows state-of-the-art performance across multilingual image captioning, visual question answering, and zero-shot image classification tasks. Especially, the VQA performance in an open-vocabulary generation setting is inspiring.
Weakness
W1. Weak argument on the joint scaling of the vision and language components. The third paragraph in Introduction argues that the visual component gives a better return on investment (RoI). Figure 2 seems to be the evidence for this claim; however, we cannot remove the possibility of other explanations since there is only one condition for each variable, the language side is from 1B to 13B (too wide interval) compared with that the visual side is from 2B to 4B. The return on investment can be non-linear. One suggestion is that we need the report of ""PaLI-5B (L, ViT-e)"" to see the RoI where the visual component cannot exploit the large language component. Emphasize that, according to this result, one of two major claims can be rejected. The rating might be lowered if there is no reasonable explanation or experimental support.
W2. Weak support on multilingual pre-training. Since the proposed models are pre-trained with multilingual datasets, we expect them reasonably works on multilingual downstream tasks. Table 2 and Table 4 Right (MaXM) only show the multilingual downstream performances, while Table 4 Left shows the cross-lingual for English-answers only (xGQA). However, this xGQA experimental setup does not firmly support the need for multilingual pre-training in cross-lingual tasks since the model architecture is not controlled (MPT may underperform because of model architecture or pre-trained datasets; we cannot discern by this experiment). Here, one suggestion is that, if PaLI-17B is pre-trained with the subset of the English-only WebLI, this model will significantly underperform the (original) multilingual PaLI-17B for the cross-lingual xGQA benchmark (even exploiting translations for multilingual tasks)?
W3. Exclusiveness in the study. The authors do not intend to release the dataset and pre-trained models considering the risk of exposure to ""unknown biases or stereotypes, or propagate inaccurate or otherwise distorted information."" In this vein, how can the community reproduce the results? The current Reproducibility Statements do not discuss this issue and the impact on research communities studying this topic, seriously enough.",7.5,4.0
54,Personalized Federated Learning with Feature Alignment and Classifier Collaboration,"Keywords: Federated Learning, Personalization, Collaboration","Strengths: The proposed method is well-motivated. The use of global feature centroids in local training is straightforward and practical, and the idea of forming classifiers combination coefficient selection as a quadratic programming problem is elegant. Evaluation and ablation studies seem convincing.
Weaknesses: The proposed framework tackles the specific label distribution skew challenge in federated learning. It would be nice if the authors could discuss whether the method generalizes to other challenges caused by heterogeneous data distributions in federated learning, for example, when the number of classes is different (if so, whether the method needs any adaptations; if not, why the method has certain limited applications). | Strength
The proposed framework is novel. The decomposition of feature learning and linear classification helps customize local clients.
The performance is theoretically analyzed based on a simplified model and numerically validated by experiments on several datasets.
Weakness
The heterogeneity of data considered in this paper is a specialized one. Generally, the heterogeneous local data distributions will be different in both total number of samples and number of feasible label classes. Current setting assumes each client has the same size of data and observes the full set of label classes. It is interesting to see the performance of the proposed method when the local data distributions are heterogeneous enough.
In Eq. (3), the authors assume there only exists one centroid for each label class. This assumption needs to be better explained. It is likely that samples in one label class may come from two different centroids. Simply averaging all the features as one centroid may not always be true.
In Eq. (4), the authors mention
α
is optimized to reduce the local testing loss. It is not clear whether the local test data is different from the test data used to show Table 1. | Strength:
The improvement of experimental results is significant.
Weaknesses:
The description of some details in the proposed method is not clear.
Need to make comparisons with some recent PFL works to verify the effectiveness. | Strength
The structure of the proposed FedPAC is reasonable and easy to follow. The studied problem (personalized federated learning) is interesting and different applications may benefit from it. the experiment results are extensive and impressive The authors also give a theoretical analysis.
Weaknesses
The description of motivation could be improved. The paper could benefit from describing personalized federated learning in more detail.",7.25,3.75
55,REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH,"Keywords: pruning at initialization, graph theory, Ramanujan Graph, sparse neural networks","General comments
Why use the third largest eigenvalue of the adjacency matrix? Usually the spectral gap is taken to be the difference between the first and second largest eigenvalues, which is guaranteed to be positive if the graph is connected.
Strengths
The idea of using expander graphs to guide pruning is interesting and unifies existing work.
Weaknesses
The decision to not use existing measures of spectral expansion is not entirely convincing. You mention that the Ramanujan condition is too pessimistic for non-regular graphs when
dmax
is used, but why not use something like the second eigenvalue of the normalized Laplacian?
The decision to pass to all
K
-regular subgraphs seems somewhat arbitrary and possibly expensive to compute.
The pruning measure proposed does not leverage the weights of the network at initialization, since it does not sparsify the network based on sensitivity of the loss function. | Strength - Most prior works on PaI focus on “training signals” such as gradients. This paper studies a complementary angle of graph theory - Although there exist prior work connecting Ramanujan Graph and PaI, they did not consider the pseudo-randomness and irregular bi-graphs in practical sparse NNs, and may prefer “collapsed” naïve random graphs (overly expansive) - The authors rigorously analyzed the limitation of Ramanujan Graph in modeling irregular graphs (practical DNNs) at high sparsity, and propose to relax the assumption on the eigenvalue upperbound - The authors further proposed another metric to detect and avoid “trivial randomness” at high sparsity, by characterizing the same eigenvalue’s lower bound. - Together, the authors demonstrate the novel finding, that a valid and desirable sparse NN architecture only exists when its graph adjacency matrix satisfies certain spectrum condition, e.g., its third-largest eigenvalue is both upper- and lower-bounded properly - The authors experimentally validated their findings on SOTA PaI methods including ERK, SNIP, and SynFlow.
Weakness - The paper does not construct any new PaI approach, only observing how PaIs are correlated with their graph properties. It would be great if the authors could briefly share future insights of improvements. - Only CIFAR-10 dataset is used in experiments. I’d suggest the authors to report on 1-2 more datasets in order to make their empirical evidences more consistently convincing. | Strength 1. This paper went beyond intuition and builds a new mathematical framework for understanding PaI. The differences from previous PaI studies (gradient- and graph-based) are clearly elaborated. 2. This paper also went beyond simply pulling off-the-shelf Ramanujan graph metrics to analyze DNN architectures (like prior art did). Instead it did several important modifications tailored for irregular bi-graphs and high sparsity levels, both are demanded in the PaI situation. 3. The authors are the first to identify a crucial limitation of Ramanujan property in analyzing irregular graphs, i.e., we could dismiss valid expanders due to the overly limiting requirements while retaining only graphs with random sparse-graph structures when analyzing high sparsity. They proposed a new metric called Iterative Mean Difference of Bound (IMDB) to mitigate the gap. This finding could be of independent interest to other applications where Ramanujan graph is relevant. 4. The authors also highlight the danger of being overly expansive, which risks graphs deteriorating into randomness. They proposed a lower constraint to avoid the danger of randomness. 5. Overall, the authors show the key knob to be relaxing the strong upper bound constraint on the third large affinity matrix eigenvalue µ(G), such that the resulting graph’s connectivity correlates strongly with its final performance. Further checking the lower bound for µ(G) could indicate whether a sparse structure deteriorates into randomness. Experiments align well with their theories.
Weakness 1. I did not follow what differences the authors were referring to, between “trivial random sparsity” and “structurally meaningful masks”. What’s the criteria? 2. Experiments are relatively limited. For example, only CIFAR-10 dataset is used as the testbed. Will the observations scale up to larger datasets? Also, could the authors find out whether some PaI methods can be obvious winners under their proposed metrics?",7.333333333333333,3.3333333333333335
56,ReAct: Synergizing Reasoning and Acting in Language Models,"Keywords: Language model, agent, reasoning, decision making","This is a strong paper with a clear motivation, thorough evaluation, and strong results. I think ReAct-style prompting is a clear win for tasks involving reasoning and embodied actions, and would love to see it adopted. | Strength:
The proposed approach is well justified. The performance of the model is evaluated deeply, and the limitation of the approach are deeply investigated. Also, the combination of ReAct with COT is evaluated to boost the performance, which shed more lights into the limitation, and provide opportunities for future directions. | Strengths
A diverse set of tasks spanning both multistep QA style tasks (HotpotQA, FEVER) as well as sequential decision making tasks. It's cool to see both of these types of tasks unified under a single framework, and showing how LLMs can leverage structured action spaces to make headway on both of these tasks.
Quite interesting scaling results across different sized models, showing that ReAct really begins to shine with increased model scale (a similar finding to the original CoT paper).
Weaknesses
ReAct alone works fairly well for some tasks, but self-consistent Chain of Thought often outperforms ReAct alone, and leads to fairly dramatic increases in performance. The best models use a smart combination of both CoT and ReAct, using heuristics that likely maximize performance on the dataset(s) (see ReAct -> CoT--SC strategy), which makes the performance of ReAct by itself slightly less impressive (though I still think this is a useful contribution for the community)
This method relies heavily on prompting to be able to predefine the space of allowed API commands: there needs to be an example usage of each action in the prompt to make the model aware it can take the action. This is infeasible for many environments that do not have an action space that is sufficiently constrained to fit into a single prompt.
Inconsistent prompting format. The sequential decision making tasks have thoughts/inner monologues specifically encoded as an action that has no effect on the environment (e.g. think[For 3 ounce bottle...], observation OK). This is the ""sparse reasoning"" that avoids cluttering the LLM history as authors claim on page 8. However, this is differrent from how thinking is formulated in the Section 3 (Knowledge Intensive Reasoning) where thoughts are not sparse and instead appear before every action. What's the moviation for the authors presenting slightly different methods here?
The decision making setting, as authors note, is not significantly different from Huang et al. (2022) and other uses of language models for decision making, though it's nice to see this approach unified with the multi-step reasoning tasks.
Lack of reproducibility (see Reproducibility section below).
Minor
Typo: bullet points page 5 - one should be ""CoT-SC -> React"", I think?
Is there a way to add self-consistency to ReAct, such that it samples multiple reasoning traces and actions and uses the majority action, for example? SC seems extremely effective for Chain of Thought tasks, and it seems like something similar could work here.
I don't think there are sufficient ethical concerns to flag this for ethics review, but authors should perhaps discuss dangers of hooking up a large language models to an API/action space with side effects. How might we prevent a model from looking up inappropriate/sensitive information or taking harmful actions in an environment?",8.0,4.333333333333333
57,Relative representations enable zero-shot latent space communication,"Keywords: relative representation, zero-shot, stitching, invariance, latent communication, isometry, representation learning","Strength:
The empirical findings in this paper are very impressive and interesting.
The paper is extremely well-written and easy to follow.
The experiments involve broad domains of machine learning and are of interest to a general audience.
The experiment arguments involve rich aspects of analysis and thus are convincing and impressive.
Weakness:
The experiments in sec. 4.1 are somewhat weak compared with those in the remained sections; only one case is considered. Adding an extra case of computer vision task may further enhance it.
The existence of this phenomenon is well proven, but the source and deep reason for it to happen are not presented as strongly as the former.
In Sec.4.1 and Tab. 1, it will be better if the authors can add references and more detailed explanations for the two metrics Jaccard and Mrr. It is a bit hard for me to understand how to calculate them and their roles directly from the current context. | Strengths
The fundamental idea is pleasingly simple
The authors included experiments to validate their motivation and intuition
A comprehensive suite of empirical results on the ultimate task (stitching), including multiple datasets, domains, and variations (eg. differences in architecture vs. data) were included
Weaknesses
For the main tasks (stitching) presented in section 5 I do not think the authors made it sufficiently clear which models were trained using relative embeddings, and which models used the relative embeddings purely as a post-hoc adjustment step. The emphasis on zero-shot (in the title, section header, and throughout), and the fact that earlier experiments (eg. section 4.1) were performed by creating the relative embeddings post-hoc may leave the reader with the impression that the relative embeddings can allow existing pre-trained networks to be stitched together. To my understanding, this is fundamentally impossible. At a minimum, the parts of the model downstream of the ""stiching"" must always be trained using relative embeddings, because the function which takes an absolute embedding to a relative one is not (in general) invertible. Of course, this also opens two opportunities for future work: (a) For instances where the mapping from absolute to relative embeddings is not strictly invertible, perhaps it is still essentially invertible on the data manifold. For example, a large component which contributes to the lack of invertibility is the vector normalization operation, however it has previously been argued that regularization results in embeddings which are essentially on a sphere. In such a setting, we could ""learn"" the inverse via gradient descent, and attempt to use this to stitch together networks without fine-tuning either part. (b) Can we choose the anchor nodes to improve the extent to which the transformation is essentially invertible?
I have a few issues with Section 4.1 - Word Embeddings. First, using 300 randomly drawn parallel anchors seems very weak - if the words were drawn uniformly randomly then they are very likely all rare words, and as such would seem to serve as a very poor choice for anchors. This also may explain why the Jaccard similarity did not increase as substantially as might have been expected. I also take issue with this sentence: ""The average Jaccard distance reported in Table 1 (right), says that the neighborhoods of the relative representations are matched exactly 34% of the time in one direction, and 39% of the time in the other one."" At least to my understanding, that is not what the average of Jaccard similarity would show. In fact, one could obtain those metrics with neighborhoods which never exactly match. Perhaps what was meant is that the words in the neighborhoods matched exactly, as the authors then go on to mention the discrepancies are likely due to semantic differences. I agree, and as such I wonder why such a course-grained evaluation was used in the first place? The setting we are in is as follows: given some embeddings A and B, find some new embeddings C and D such that the all-pairs similarities between elements of A are proportional to those between elements of C (and similarly with B and D), however the similarities between equal elements of C and D are minimized. One could simply measure and report on that metric.
No theoretical analysis is included. It is, of course, increasingly common for papers to rely fundamentally on empirical results, however it seems possible to prove results relating the number of anchor entities, variation in the training data, and potential accuracy on the task. Even making minor preliminary theoretical statements on these aspects would strengthen the paper.
I was not able to find details on how the 2-dimensional representation in on the left half of Table 1 was created. Presumably, some dimensionality reduction technique (eg. tSNE) is used, however that raises into question the usefulness of drawing conclusions from such a picture as the dimensionality reduction introduces a number of factors which may impact the resulting representation. (One could imagine various dimensionality reduction techniques with noise or hyperparameters which, for the same exact embedding, result in different representations.) At the very least, the authors should explain how these pictures were derived.
Typos / Suggestions
Abstract: I would recommend including something about the anchor entities in the abstract, as (to my mind) that is essential to understanding the high-level idea. Perhaps this sentence: ""In this work, we propose to adopt pairwise similarities as an alternative data representation, that can be used to enforce the desired invariance without any additional training."" could be changed to: ""In this work, we propose to use the similarity between each representation and a fixed set of anchor representations, and demonstrate that this can enforce the desired invariances without any additional training.""
Page 1: ""The underlying assumption is that the learned latent spaces should be the best encoding given the data distribution, the downstream task, and the network constraints. In practice, however, the learned latent spaces are subject to changes even when the above assumptions remain fixed."" The writing here presents this as though this is contradictory, but of course it is not - the latent representation which is best for the downstream task given network constraints is not unique.
Page 2: ""more in general"" -> ""more generally""
Page 7, Figure 4: I don't think this figure highlights the results as well as it could. Consider making the figure 5 rows tall, and group based on absolute vs. relative first (i.e. rows would be original, abs ae, abs vae, rel ae, rel vae). This would make it very clear that absolute struggles, while relative works well. (I would make similar suggestions for Table 3 and 5)
Page 7: ""prove that representations are invariant to training stochasticity""is a bit too strong, as I believe any mathematically rigorous interpretation of this statement would be provably false. Something like ""support our claim that relative representations are more robust to training stochasticity"" seems more accurate, given the results presented.
Page 8: ""obtaine"" -> ""obtaining""
Page 9: ""allow to stitch modules"" -> ""allow stitching modules"" | Strengths:
Simple and at the same time very innovative idea. Its simplicity makes it extremely relevant and applicable across settings, domains, architectures and training paradigms, suggesting great potential for impact, both conceptual as well as practical.
Thorough examination and empirical demonstration of some of the potential of this new technique.
The paper does not have any apparent major weaknesses of relevance. There are however technical details that could benefit from some clarification. For instance, the paper considers the training modality of training models with relative representation, but while doing that it is not readily clear whether this happens by also backpropagating through the anchors. It would be beneficial to clarify that, and whether and if there are any differences between doing that (backpropagating also through the anchors) vs only backpropagating through the data.",8.666666666666666,4.0
58,Rethinking the Expressive Power of GNNs via Graph Biconnectivity,"Keywords: Graph Neural Networks, Expressive Power, Weisfeiler-Lehman test, Graph Transformer, Biconnectivity","Strength:
Very interesting and innovative study on biconnectivity problem.
The theoretical discussion on ESAN and proposed Graphomer-GD are comprehensive and clear.
Weakness:
The organization of the paper is a bit hard to follow. I am not fully understand of explaining ESAN in the main paper since eventually author proposes a more efficient variant of graphomer. I suggest focus on Section 4 since section 3 and 4 seems quite parallel to me...
The synthetic experiment is missing some details such as number of graphs and size of the graphs. The numbers reported in Table 1 lack of number of repetitions and variations.
The efficiency of the proposed method. Can you share with us the amount of parameters of Graphormer-GD and other baselines ? What's the empirical running time on ZINC? | Strengths
The paper is well-written, and the authors did a good job reviewing the related literature.
The introduced biconnectivity metric relates to important concepts in network science and is well-motivated.
The paper provides theoretical insights into existing methods and represents an excellent contribution to the graph ML community.
The proposed Transformer-based architecture is simple, relatively novel, and naturally derives from the paper's findings.
Weaknesses
Very limited empirical evaluation setup. | Pros:
The motivation of proposing such a metric is natural and fundamental, since graph isomorphism testing is too abstract to guide practical design of GNNs. And I agree that biconnectivity plays a key role in justifying any meaningful high-order substructures, such as those in bio-chemistry.
The theoretical of recent popular frameworks is pretty comprehensive. It includes almost all popular works with detailed discussion of counter-examples and proof.
The proposed GD-WL is efficient but still powerful enough to solve biconnectivity. It Graphormer implementation enjoys splendid performance on synthetic experiments and real-world benchmarks against strong baselines.
Con:
Since there are so many results in the main text, it would be great to present all these results in a comprehensive table near the introduction, to make it easier for any audience to get the message precisely.
Typo:
In appendix B.4, bolded Note -> Node",8.666666666666666,3.6666666666666665
59,SAM as an Optimal Relaxation of Bayes,"Keywords: bayesian deep learning, sharpness-aware minimization, variational bayes, convex duality","Strength
Using the Fenchel biconjugate to bridge SAM and the Bayes objective is an interesting and insightful idea, and the resulting relaxation of the Bayes objective has shown good empirical performance. Moreover, the authors have conducted a variety of experiments and ablation studies on both toy and real datasets, providing deeper understanding of the proposed method.
Weaknesses
While the equivalence between SAM and the relaxed Bayes objective is interesting, it doesn't explain why the relaxed Bayes objective (or SAM) has better generalization performance than the original Bayes objective. In other words, it seems the ""maximum loss"" performs better than the ""expected loss"" despite the established connection, the reason of which is unclear.
The baselines used in the experiments are too weak. For instance, test accuracies around 95% and 75%, respectively, on CIFAR-10 and CIFAR-100 would be more convincing.
The readability of Sec. 2 needs improvement. | Strengths:
Numerical experiments, to some extent, justify the method proposed in the paper.
The idea is nice and can lead to some insights about how SAM works (but sadly, more on that is in the weakness part)
Weakness:
Actually, losses (12) and (1) are very different from each other, as in (1) we take the supremum over a neighbourhood and (12) we look at all space of possible values
ε
. So, the related term
l(m+ε)
in (12) can be arbitrarily larger than the corresponding term in (1), making the whole approach inspired by Fenchel biconjugates but not equivalent to it. We also note that in Fenchel biconjuates, we search for a convex analogue for the function we optimize. This makes little sense in general deep learning, as the loss is very far away from convex with many symmetries and local optima observed.
So, we can't directly optimize (12).
The accuracy of this approximation is, thus, should be the main concern in the paper. This is not true now. The experiments follow a common pipeline for DL papers comparing to other approaches and making reasonable ablation studies.
what is the difference between ResNet-20-FRN and ResNet-20? The paper [1] reports similar results for ResNet-20 in Tables 2 and 3 for different variants of SAM for CIFAR10 and CIFAR100. For CIFAR10, they obtain 93.82% accuracy (c.t. bSAM 92.16% in the paper). For CIFAR100 they obtain 71.40% (c.t. bSAM 68.22% in the paper). What is the difference in the protocol that provides such a decrease in quality for your approach compared to the baselines?
Suggestions:
I suppose that the biconjugate for the Exponential family is worth deeper examination, as it leads to convex problems, and thus, everything should be correct and can lead to interesting theoretical insights and analysis (also, I suppose that you can generalize Theorems to the paper in this more general case).
[1] Kwon, Jungmin, et al. ""Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks."" International Conference on Machine Learning. PMLR, 2021. | Strengths:
A novel connection between two important ideas (SAM and Bayesian learning) that required a non-trivial amount of complex mathematical reasoning.
A well-motivated, novel practical method driven by the theoretical results.
Strong experimental results on a wide set of numerical benchmarks with a strong set of baseline methods.
A clear, polished write-up with consistent notation. A precise comparison to the baseline SAM-Adam method with algorithmic differences highlighted line-by-line.
Experiments to understand the sensitivity of the method to
ρ
, as well as additional ablation studies in the appendix.
Weaknesses:
The toy experiment could be more informative: have authors considered doing non-linear classification with a NN instead? This might accentuate the differences between uncertainty estimates.
I would recommend replacing the MNIST datasets in Table 1 with alternative, more challenging datasets. MNIST-based datasets are likely too easy for meaningful comparisons between methods, as indicated by more ambiguous results on these.
A (brief) discussion on the computation cost of the method and how it compares to SAM/SAM-Adam would be useful.",6.333333333333333,3.0
60,Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier,"Keywords: reinforcement learning, sample efficiency, resets","Strengths
Well written, intuitive approach
Strong performance with a very simple idea
Exhaustive experiments and interesting analysis and discussions
Weaknesses
Given the recent surge of leveraging pre-trained representations for vision-based RL, it would be an interesting investigation to see what would happen when considering such a pre-trained perception module.
Investigating the role of replay ratio scaling for model-based approaches, especially the effect on the world models and corresponding policies obtained from the models, would be an interesting addition to the paper, but I don't think this is a necessary and could be a future work. | Strengths:
[Clarity & Quality] The paper is basically well-written and easy to read.
[Novelty] Analyses to better understand the reset method [1] are conducted.
In the original reset paper [1], the reset method was not compared to methods with a sample efficient method, such as REDQ. The experiments in the paper under review show that the reset method is generally more efficient than the existing sample-efficient method.
Experiments in tandem and iterated offline settings are conducted. In addition, the effect of combining online updates with offline updates is evaluated. This combination approach addresses the temporal performance degradation of resetting, which was one of the main limitations of the reset method.
Implementation decisions for discrete control problems and the trade-off between computational resources and performance are also analysed.
The experiment results and findings should be beneficial especially to practitioners. Also, as a reset method for RL is a relatively new one and experiments about it have not been stacked so far, the experiment results provided in the paper under review will be beneficial for a future advance of the reset method.)
Weaknesses
[Novelty] Not so much progress has been made from [1].
There is only a few technical difference from the original reset method: for continuous control, the proposed reset method is based on ""updates"" steps rather than ""environment steps"", and for discrete control, the Shrink and perturb style encoder update is used.
As with [1], the reset interval is treated as a hyperparameter, and it is still unclear the optimal timing for reset (i.e., we still need hyper-parameter tuning for it).
[Clarity ] It is unclear how much performance is sensitive to the hyperparameter of the reset interval.
For experiments for both discrete and continuous environments, reset intervals are set to magic numbers. No detailed discussion about how this hyperparameter affects overall performance is provided. This makes understanding how the insights observed in the experiments are generalizable a bit difficult.
Minor comments
Recent approaches in continuous control leveraged high replay ratios as a strategy to improve sample efficiency through the use of ensembles of value functions (Chen et al., 2021) or normalization strategies (Smith et al., 2022).
The following approaches also use ensemble and regularization in high replay ratio settings.
Takuya Hiraoka and Takahisa Imagawa and Taisei Hashimoto and Takashi Onishi and Yoshimasa Tsuruoka, Dropout Q-Functions for Doubly Efficient Reinforcement Learning, International Conference on Learning Representations, 2022
Wu, Yanqiu and Chen, Xinyue and Wang, Che and Zhang, Yiming and Zhou, Zijian and Ross, Keith W, Aggressive Q-Learning with Ensembles: Achieving Both High Sample Efficiency and High Asymptotic Performance, arXiv preprint arXiv:2111.09159, 2021
To answer this question, we resort to what we call iterated offline RL, ...
Is this the same setting as the following paper?
Matsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang, Deployment-efficient reinforcement learning via model-based offline optimization, arXiv preprint arXiv:2006.03647, 2020
Replay Ratio Scaling: Change in an agent’s performance caused by doing more updates for a fixed number of environment interactions.
Regarding SR-SAC, does replay ratio here mean, critic update ratio, actor update ratio, or both? In the REDQ paper, replay (UTD) ratio means critic update ratio (actor is updated only once per environment step).
The progressive loss of ability to learn and generalize of neural networks and its interaction with RL was only one of the recent empirical discoveries about deep RL.
This sounds like an exaggeration. Why ""only"" one? | Strengths
Clarity: The paper is written clearly and generally easy to understand.
Framing/Contribution: The paper’s contribution is clear to understand.
Experiments/Results: The authors perform extensive experiments that are convincing of the usefulness of high replay ratios when combined with periodically fully/partially resetting network parameters.
Weaknesses
Clarity:
In the related works section, first paragraph, many italicized terms are used but not explained. I was familiar with all terms except primacy bias, which I had to look up separately while reading. But in general, these terms should be quickly defined to make the paper easier to read.
The first paragraph of 5.1.2 is written poorly and hard to read, please fix it.
Experiments:
IQMs are listed (which is great) but the authors should probably explain IQM and the “Fraction of runs with score >
τ
” statistics for readers unfamiliar with the benchmarking paper that introduced these metrics.
While the provided analysis on different algorithm design choices in light of high replay ratios is useful, I’m not sure that what the authors analyzed is the most sensible thing to analyze. The paper is about replay ratio scaling through resetting network parameters. Therefore, there should be analysis on two things: 1) different replay ratios, and 2) how/when to reset network parameters. The authors provide analysis of 1) through trying different replay ratios, but insufficient analysis of 2) as in Section 4 they define their reset strategies and just stick with it. I think the paper really should have this analysis to be more useful as an empirical study to inform design choices for other researchers.
In combination with the experiments in Section 5, it would be useful to have an experiment with an offline RL algorithm (perhaps IQL or CQL). Offline RL is increasingly popular and also being deployed on real robots, so the authors should study their replay ratio increases with an offline algorithm to make the paper more useful for researchers.
Minor details:
“The number of agent updates per environment step is usually called replay ratio, and most standard algorithms were designed to have values around or below 1” → not sure if they were designed to have values around or below 1, I think this statement would be more accurate if it was something like “most standard algorithms are trained with a replay ratio around 1.”
“under tasks switches” → “under task switches”
“with a same algorithm” → “with the same algorithm”
“for evaluation and comparisons, we follow the protocol suggested by Agarwal et a. (2021)” missing a period",8.0,4.333333333333333
61,Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions,"Keywords: diffusion models, score-based generative models, sampling, score estimation, Langevin, stochastic differential equations","Despite the strong claims, the paper seems to deliver on most aspects so everything listed in the summary of contribution should be considered as a core strength of the paper.
However each of the strong point presents some slight limitations that could be adressed more clearly.
Removing the LSI assumption is a strong result, but ends up introducing new assumptions of smoothness, bounded moment, and of accurate estimation of the score function 1.a) As the authors explain in the paper, this smoothness assumption is not satisfied when the input distribution lies on a manifold, which is probably the same regime where the accurate score function estimation is feasible at all. However most current SGM models also implicitely assume that q has full support, highlighting that this might be a limitation of the whole field. To make the analysis more relevant the failure modes of the theoretical analysis should shed some light on which modifications should be introduced in the models and training procedures to relax this assumption and be able to automatically detect the manifold and avoid this pitfall, but this would be an important contribution by itself. The authors instead give a good second choice by looking at a simplified but realistic model where the distribution lies in a ball around the origin. However the number of iterations grow quite quickly with the radius (e.g. ~R^8) making it vacuous. 1.b) The accurate estimation of the score function is the central point of the paper, but the authors spend very little time explaining how this quantity is computed. Beyond a quick description of score matching, they mostly refer the reader to Vincent 2011, but this source actually highlights how minimizing (13) is actually very hard even after the score matching rewriting, and the authors even mention hardness results at the end of page 2. It would be good to point out some more context on when this estimation problem can be solved in an efficient manner. 1.c) Unlike the other assumptions that are discussed mutliple times in the paper, the moment assumption receives very little attention. For example, the authors should try to justify why their proposed bound of m_2 < d in the discussion of Thm. 2 should hold, especially considering a bound e.g. m_2 < d^2 would result in different value for N and eps_score than the ones reported in the introduction. | Strength: The results are pretty impressive given the state of the field.
Weakness: The results shown under the manifold hypothesis seem incomplete. | Strengths:
Well-written and presented clearly.
Clearly discusses the relationship to other work in this area. While I am not an expert in the area, I thought this was especially well-done.
Bound is in terms of the L2 error of the score estimate.
Bound does not assume log-concave data distribution.
Assumptions and limitations of the results are described clearly.
Explores the consequences of this result with respect to critically damped Langevin diffusion, a variant of the simpler diffusion process. | Strength:
The presentation is clear.
The proof ideas are simple and fairly straightforward. As the authors mentioned, they seem robust enough to generalize to other settings beyond the OU process considered in the present work.
Weaknesses:
This paper does not really deal with DDPM in the sense of the cited (Song et al. 2021b). First, at least in (Song et al. 2021b), DDPM refers to a specific SDE with time-inhomogeneous diffusion. Although the authors briefly mention that the analysis can be extended to that case, I don't think it is such a straightforward matter. For instance, the first step in Section 4 would break down, as
p~0≠γd
. How do we handle this?
Another major difference is, in (Song et al. 2021b), the sampling part is done by a predictor-corrector sampler; see Appendix G. Incorporating this source of error is quite important as it does reduce the variance of the sampling procedure significantly. Instead, the authors opted for an exact sampler by taking advantage of the time-inhomogeneous OU process, which, echoing the above, is not available for DDPM.
One thing that is unclear to me is the scaling issue as follows. Since both TV and KL are invariant to scaling, instead of
q(x)
one may consider a new measure
q(cx)
for any
c>0
. Inspecting the bound in Theorem 2, it is clear that the first term is invariant, the second moment in the second term scales accordingly, whereas the
εscore
term is again left as constant. On the other hand, intuitively, as
c→∞
the error in the score estimate might shrink (since this corresponds to shrinking the moments of
q
).
To conclude, the bound in Theorem 2 doesn't seem to capture the scale invariance of TV/KL, suggesting that there might be some artifact in the proof. (That being said, I acknowledge that my argument above is no less vague than the authors', so this should not be taken as a major criticism.)
I find the argument on the deficiency of underdamped Langevin fairly weak. On one hand, as the authors have noticed, no definitive statement is given. On the other hand, all of the bounds in this paper are either given by KL or TV, but these two are not really the ""right"" metric for underdamped Langevin (Desvillettes and Villani 2000).
The comparison to (De Bortoli 2022) is not entirely fair since the convergence metric there is given in the Wasserstein distance, which makes sense under the manifold hypothesis. The Wasserstein distance is ideal for this setting as it does not rely on the measures being absolutely continuous to each other.
As the authors acknowledged, a serious limitation is that the score estimate part is assumed away, whereas in practice estimating the score is the bottleneck.
Desvillettes and Villani 2000, On the trend to global equilibrium in spatially inhomogeneous entropy-dissipating systems: The linear Fokker-Planck equation.",8.0,3.25
62,Scaling Up Probabilistic Circuits by Latent Variable Distillation,-''-,"Strengths:
a novel method to improve the performance of large scale probabilistic circuits.
motivating empirical evaluations on three image datasets to show the performance improvements over very large circuits.
clear writeup with good examples. Weaknesses: I didn't find major weaknesses in the technical aspects of the paper. Please see some questions and comments below. | The authors present a well-written and technically sound paper.
The empirical evaluation is outstanding for TPMs, both in terms of scale and model performance.
It would be nice to have some more comments on the impact of LVD with regards to the rest of the model. How would you do imputation? How is the performance of your model when you marginalize the LVs? | Strengths:
S1. This work clearly demonstrates a novel method to exploit all the capacity of PCs using auxiliary and non-tractable methods from Deep Learning. This is extremely relevant for the PC community to close the gap with existing DL approaches.
S2. The practical speed-up obtained by their method (section 4) is quite relevant as well.
S3. Experimental results are strong.
Weaknesses:
W1. While the idea is novel, I felt a bit let down when I read that the approach need to be ""engineered"" depending on the dataset, architecture, and deep generative model. The two instantiations of the approach look really ad-hoc. I would've hoped to see some sort of general guidelines, or desiredata for the method to be successful.
W2. On a similar note, I feel a deeper ablation study would help a lot. There are too many questions open:
How important is the deep generative architecture to obtain good results?
Do other PCs architectures benefit from the proposed approach? So far, it is only tried in a modified version of HCLT.
K-means is also prone to fall into local optima. How much do the results vary with different K-means initializations?
How does the algorithm perform if we condition (Fig. 5) on deeper layers?
W3. I might have missed it, but it is not clear to me how many times each experiment was repeated. In any case, standard deviations are not reported.
W4. No samples were reported, so one cannot assess the qualitative improvements. Are the new generated samples better as well?
Questions:
Q1. Any intuition on why the order to generate the latent samples is reversed with respect to the HMM in the example of section 2?
Q2. Could you clarify why is the upper part of Fig. 5 supposed to be
p(z1,z2)
. While I agree on the bottom modules, I do not see how the top part represents the marginal of
z
.
Q3. I understand LVD is HCLT using the proposed training approach. Why is there so much difference in number of parameters between both approaches in Figure 7? Is it because of the ""minimum modifications required"" described in the text? Why not comparing with that modified architecture without LVD?
Q4. What do you mean by ""semantics"" in the manuscript? I really struggle to understand what ""semantic-aware"" means.",8.0,4.333333333333333
63,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,"Keywords: System 2, Logical reasoning, Language Models, Large Language Models, Reasoning, Neuro-symbolic, Neural Symbolic, Interpretability","Strengths
Clarity: The paper is well-written and easy to follow. The problem, and motivation are clear and the method directly follows from them.
Novelty: Splitting reasoning into modules has been explored but to my knowledge, using something like the proposed SI method has not been seen.
The SI framework significantly improves performance on the reasoning tasks over vanilla CoT/ scratchpad approaches.
Selecting a subset of information ensures that the language model is restricted by the information it uses for reasoning.
Weaknesses
One missing baseline for the framework is where the CoT prompts are framed as SI prompts without splitting SI into two separate modules.
SI using larger language models: I expected to see how models a magnitude larger than 7B would perform with the SI framework. This would help us understand how important the specific components of the framework are.
For example, a larger model might not make facts up in the selection step when using greedy decoding (with temperature = 0).
Or, one might not need two separate calls to the LM for S and I, simply using a CoT script with steps for selection and inference is all that one might need.
Other
Reproducibility: The method is straightforward to implement and test. The authors provide the prompts but access to the larger 280B language model is restricted and the baseline results would be impossible to replicate. | trengths
This framework of SI is different from all previous frameworks, as illustrated in Figure 1. SI use LLMs to produce each reasoning step one at a time, which is similar with ProofWriter. While ProofWriter can only answer “Prove this statement to be True/False” style question, SI is able to solve reasoning problems.
The designs of splitting of each step of reasoning into selection and interference makes the model more unlikely to make up information to answer and it cannot ignore previous reasoning when getting the answer.
The authors conduct extensive experiment and provide a comprehensive evaluation of LLMs on a set of 46 tasks. They show that LLMs are good at simpler single step logical inference in 5-shot generalisation settings, but struggle with harder problems.
Using SI framework, the 7B model shows impressive results and even outperform 280B LLM baseline using COT framework. Also, the SI framework has ability to produce a causal reasoning trace and is able to recover from errors.
Weaknesses
Some important details are not presented. The algorithm now just simply halts after some times of steps. So will the result differs from different numbers of steps, and how.
The results are not convincing enough. In the paper, the authors just show the results of 7B SI. I wonder know that if the 280B version using SI will outperform all previous models.
There are many case studies, more quantitative metrics to evaluate the effect is better.
There are some typos in this paper, which causes additional burden for understanding. For example:
typo in page 2, “Figure 1” paragraph, “indicate” -> “indicates”
typo in page 5, “We use prompt...” paragraph, “the the following form” -> “the following form”
typo in page 26, “We have seen...” paragraph, “which we we now” -> “which we now”
typo in page 26, “When observing...” paragraph, “we us” -> “we use” | Strength
Neat idea: The idea of the two step selection-inference framework can be seen as a way to leveraging human prior knowledge in prompt engineering. As the authors discussed in the motivation section, this to some extent bridges neurosymbolic AI with recent large-scale deep learning approaches. There are a few neat ideas I like in this paper:
Ranking negative log-likelihood for selection instead of directly prompting LLM for a list of outputs. This could effectively avoid the potential trouble of LLM generating something uncontrollable or hard to parse. Although I have to say this requires the LLM output probabilities to be available, which is not always the case (for practitioners who has less ML knowledge, or using online LLM APIs).
""Occlude"" question from the inference module. This is in some sense adding an information bottleneck.
Empirical value: I'd love to see research along this line, which can potentially have great empirical value. Better prompting strategies can be useful in general and this is not limited to the ML community.
Significance test: I appreciate the authors include significance test in their experiment and result section.
Interpretability, transparency, and humans-in-the-loop: Compared to vanilla in-context learning and prior methods such as COT, the proposed framework can have a better interpretability and help researchers to better understand how an LLM makes certain decision. I also like the opportunity of humans-in-the-loop that the SI framework brings (e.g., users help to verify/filter selected evidence).
Explicit knowledge representation / memory: In some sense, the iteratively growing set of evidence can be seen as a working memory, or a representation of past experience that one can easily retrieve from. Thinking broadly, from a sequential decision making or robotics perspective, instead of multi-step reasoning, I can imagine something like the SI framework being used to facilitate multi-step planning.
Weaknesses
Please see my question 3 below regarding the
H
and
K
values.
The paper suffers from a few accessibility issues. Please see the minor issue and typos section below. | Strengths:
The paper presents a comprehensive study of 46 reasoning tasks and gives an understanding of how good are the current LLMs in single step and multi-step reasoning problems.
The proposed selection-inference algorithm is a very interesting and novel idea. It makes the reasoning trace more interpretable.
The paper demonstrates that this approach will enable small language models to outperform baseline large language models that doesn’t use SI algorithm
Weaknesses:
It would have been a more interesting comparison to see how this method performs in comparison to other recent works such as self-consistency, verifiers, etc. The proposed method has its own merits, but comparing with more methods would have been more interesting. | Strengths : This is a good paper with good motivation. The work can have a positive impact on the field of multi-step reasoning. However some aspects are not entirely clear (more details in the next section). Some improvement suggestions are listed below:
Weakness : The Selection algorithm relies on the manual selection of K: number of facts to include in the prompt. This makes the framework hard to use in practice as manually selecting the number of required facts for each Selection step can be very tedious. Did the authors explore setting this parameter to something large (say 4 for all steps, for all tasks)?
Similarly and most importantly for the entire process of Selection & Inference the steps are repeated until a predefined number of steps. Hence the model is not responsible for identifying when it has answered the question and when it needs to stop this process. Do the authors have some thoughts about how to resolve this issue in future work? It would be nice to discuss this in the final section of the paper.
Question : Do the authors have some intuition why in Figure 6b the baseline model performs better as the depth increases? The opposite would be a more natural behavior (like in the case of the SI model).
Recommendation : Although to a smaller scale, and with models trained from scratch, the idea of using language models to generate step by step reasoning chains was first explored in “Measuring systematic generalization in neural proof generation with transformers” (Gontier et. al, 2020). The paper should compare their approach to this one in the Related Work section. Similarly, the authors could consider testing their Selection & Inference framework on the CLUTRR benchmark (Sinha et. al, 2019) in addition to Baby & ProofWriter.",7.6,4.0
64,SimPer: Simple Self-Supervised Learning of Periodic Targets,"Keywords: Periodic learning, Self-supervised learning, Representation learning, Periodic targets, Periodicity","Strengths
(Clarity) The paper is very well organized and clearly written. The figures are also well made and very informative.
(Novelty) The paper and its proposed techniques are novel as far as I can tell. The topic is clearly important (for its e.g. medical & health applications) yet doesn’t seem to be as popularly studied as other ML/SSL topics.
(Quality) The paper is of good quality in my opinion. The algorithm is well designed for periodic signals to maximally extract useful information (via frequency augmentation) to be better learned (via periodic feature similarity & generalized loss). The technical details are all correct as far as I can tell. The empirical evaluation is also comprehensive, covering 6 different SSL (and related) tasks against popular baselines, even though the selection of baselines could be further improved (see weaknesses).
(Significance) The proposed techniques are simple, easy to implement and experimentally highly effective, making the algorithm (SimPer) a strong, potentially impactful baseline for future researchers & practitioners to use and/or improve upon.
Weaknesses
(Evaluation, Minor) The algorithm is currently only benchmarked against 4 SSL baselines and a supervised counterpart, not SOTA algorithms on each of the datasets (e.g. the MAE for Countix appears much weaker than Table 7 of Dwibedi et al.). While SimPer is likely orthogonal to those SOTA algorithms and thus might be applied jointly, it’s still non-ideal to completely omit the SOTA baselines and the discussion on how SimPer could further boost the SOTA numbers. | Strengths
The method improves results over self-supervised methods such as SimCLR.
The paper is well written and easy to follow.
The paper advances simple augmentation technique for temporal contrastive learning framework.
Weaknesses
Although the approach is interesting, the experimental section would require additional results to benchmark thoroughly against CVLR. For example, this submission does not include results on Kinetics-400. (as opposed to a subset)
There has already been self-supervised learning approaches in the area of human physiology. This submission does not compare with this ICCV paper. [ref 1]. This paper already explores the idea of maximal cross correlation. (MCC)
It is not clear if the evaluation protocol is good enough for representations. Especially, for computing a single HR value over a large window. In other words, even if the representations are very bad for large portion of the video, the evaluation protocol would not be able to detect it.
Additional comparisons with baselines: The results on known traditional methods are better on SCAMPS dataset than the proposed method. These baseline methods have not been to be included in this paper. It would be beneficial to the reader to have a comparison in the paper.
References
[ref 1] The Way to my Heart is through Contrastive Learning: Remote Photoplethysmography from Unlabelled Video, ICCV'21 [ref 2] SCAMPS: Synthetics for Camera Measurement of Physiological Signals, NeurIPS'22 | Strengths
The overall idea and the individual design choices are well-motivated and explained very clearly in the text and supported by suitable illustrations.
It is transparently stated that the method makes use of an inductive bias to construct suitable augmentations and loss functions.
The empirical analysis is very comprehensive. It clearly shows the benefits of the proposed method and provides helpful ablations.
The writing and exposition of the paper are very clear.
Weaknesses
It is not sufficiently clear how to choose the sequence length and how important the hyperparameter might be for the encoding of periodic information. For instance, a very long sequence might contain multiple subsequences that exhibit different periodicities. I think it would be helpful to include an ablation for the sequence length.
Do I correctly understand that the baselines SimCLR, MoCo, and BYOL use individual frames as inputs, whereas CVRL and SimPer use sequences as inputs? In that case, it would not be surprising that the frame-based approaches do not encode periodic information, which would be difficult to estimate from a single frame.
It would be interesting to measure not only the periodic information that is encoded, but also how much other information (object class, etc; depending on the dataset) is retained in the encoding. For instance, Figure 6 shows that the spurious feature (digit appearance) is encoded relatively well for the training data, even though it is not an invariant feature that would help discriminate between positive and negative pairs.
Minor comments and questions:
In Figure 4a, why do you plot fewer data points when you decrease the training dataset size? Are the learned representations not based on a test dataset of a fixed size?
Why is the performance of the proposed method on the LST dataset not much better compared to the baselines?
In the context of the Nyquist sampling theorem, it would be helpful if you briefly stated its results and why it is relevant.
Did you decrease the margins for the abstract?",8.666666666666666,3.0
65,Simplified State Space Layers for Sequence Modeling,"Keywords: sequence models, state space, S4, RNN, transformers, long range arena","The proposed layer, s5, replaces the frequency-domain approach used by S4 with a recurrent, time-domain approach bypassing non-trivial steps needed for S4. The method achieves high performance with comparable complexity of s4.
The source code link provided in the paper isn't working, The authors should publish anonymized code for the review purposes with the correct link. | [+] The paper is very well written with a concise and clearly structured background section.
[+] All experiments are repeated multiple times with different seeds, leading to more stable and thus scientifically valuable results.
[+] Detailed and comprehensive comparison with S4 and its components with helpful illustrations and descriptions.
[+] The reviewer highly acknowledges the inclusion of concurrent work in the comparison.
[-] Although all experiments are repeated multiple times, no error bars are reported. Especially in cases where multiple methods perform similarly, a corresponding deviation measure proves to be really helpful for judging.
[-] It is not clear what the reported scores in Table 1 are. Assuming the scores to be accuracy scores, the average accuracy provides only limited insight under the assumption that the tasks are of varying difficulty (i.e. different number of classes with different relative class sizes, etc.). The reviewer suggests reporting the average rank for each method instead.
[-] Regarding the results reported in Table 3: are the results marked with an asterisk (*) computed on 5 seeds, and all others (i.e. „CRU (our run)“ and „S5“) on 20 seeds? If yes, this might skew the result. The reviewer suggests using the same test protocol for all methods in this case. If not, the reviewer suggests clarifying the use of different seeds. | This paper is very well written, precise, and easy to follow. I found the background on S4 layers incl. the diagram very useful and more accessible than the original paper, and I also think the authors made a good choice in explaining their extension S5 by contrasting them to S4. It is also great to see the easy-to-implement jax code in the appendix.
The experimental evaluation is extensive, with many and the most important baselines, evaluated on commonly used long-range-dependency benchmarks. It is especially nice to see ablation studies in Appendix E, evaluating different model sizes (making it more or less similar to S4) as well as initialisation and continuous-time (vs. direct discrete-time implementation), showing that these are the most important features.
I did not find major problems or concerns. But I would be interested if the relation between S4 and S5 (App. D) could be made with less restrictions. Especially assumption 2 seems a bit strict? I would also have liked to see how the model deals with uncertainty/unpredictability, which is often the case in forecasting (e.g. the stock market would be an extreme case). The sequential image classification tasks are a bit unrealistic/artificial. But I do realise this is not the goal of the paper, and previous work from this domain also does not evaluate in such settings.",8.0,3.3333333333333335
66,Sparse Mixture-of-Experts are Domain Generalizable Learners,"Keywords: domain generalization, mixture-of-experts, algorithmic alignment, visual attributes","Strengths
I liked the idea of applying mixture of experts for solving the problem of DG. I resonate with the authors, that it is a bright direction for explorations in DG
The experiments are thorough and the results appear promising and t
Weaknesses
My major concern with the paper is that some important recent papers or results/discussions from cited papers are missing.
(a) For instance, why did the authors ignore the MIRO+SWAD numbers in Cha et al. 2022. It clearly and comprehensively outperforms the proposed method on all the datasets.
(b) Author missed in important recent work by Sivaprasad et al. [A], which makes a similar observation that backbone plays a more crucial role in DG compared to tailored algorithms. They show that ERM with Inception Resnet backbone can achieve 89.11% accuracy on PACS (outperforming the proposed approach on the particular dataset). That clearly dilutes the first contribution of the paper. I suspect more similarities with that work as well in terms of distribution shifts etc. A discussion is warranted.
(c) Finally, were the experiments in Table 1 averaged over multiple runs?
[A] Sivaprasad et al. Reappraising Domain Generalization in Neural Networks, arXiv 2021 | Strengths:
The paper tackles an important problem of domain generalization and proposes a good direction to approach it, by designing models that are well aligned with the desired task.
The formalization of alignment between models and the task is sound and useful for guiding the design of future architectures.
The proposed model seems to be beneficial for DG tasks. It is good to see that GMoE obtains good results with standard ERM training, but also benefits from DG algorithms.
It is interesting to see that the experts seem to specialize, as seen by histograms and examples in Section 5.
Weaknesses
The paper did not explain in much detail why conditional statements are good for Domain Generalisation. As this point is crucial in linking the proposed method to DG, it should be better explained. At the high-level Theorem 2 says that the proposed model is good for cases where the true function is composed of similar mechanisms (selection + application of submodule) as the GMoE model (gating + separate FFN). But still, it should be explained in more detail why such functions are preferable for DG. Some intuitions are given in Section 4.2, but expanding them would be good.
The paper could discuss connections to other works that mixture-of-experts for domain adaptation [A], or use experts in similar ViT models [B].
[A] Guo et al. “Multi-Source Domain Adaptation with Mixture of Experts” EMNLP 2018 [B] Rahaman et al. ""Dynamic inference with neural interpreters."" NeurIPS 2021. | The paper focus on an interesting aspect and the paper are overall easy to understand. But some important experiments are missing.
Missing comparison between transformer with ERM and transformer with DG. The core motivation ""empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms"" can just because the transformer is a stronger backbone.
Why is GMoE superior to a normal MoE. The major difference is that it forces a normalized operation on the input of the gating function and uses a learned embedding dictionary to decide the gating. The ablation study of why this is superior is missing from the paper.
The paper states that GMoE relies on visual attributes to have better performance. This is not convinceable from figure 3(b). It seems that e0 and e2 are specialized for background and others (e3,e4, e5) are all activated by almost all the visual attributes. Figure 3(c) is also not convincing to me. It seems no strong relation between experts and attributes. Any network would have similar visualization that some part of it prefers some region in the image.
Overall, the experiment parts seem to be a weakness: not enough experiments, ablations, and convincing visualization. | As a clear strength, the paper provides detailed argumentation to motivate each of its design choices, going beyond a purely empirical study. Most prominently, it motivates the choice of ViT architecture by means of Theorem 1 and the choice of (G)MoE by means of Theorem 2. Both theorems provide a somewhat theoretical grounding of the architectural choices made. The empirical evaluation is extensive and convincing. A minor weakness is that further tweaks in Section 4.3 are made on purely empirical grounds and ablations are relegated to the appendix.",6.75,3.75
67,Statistical Efficiency of Score Matching: The View from Isoperimetry,"Keywords: score matching, log-Sobolev inequality, isoperimetry, relative efficiency, sample complexity","Strengths:
Proofs are clean and straightforward.
Theory corroborates with practical observation, that score matching on multimodal data requires noise perturbations.
Weaknesses:
Theory is mainly descriptive rather than prescriptive --- it is well known that noise-perturbation is crucial for properly learning scores [1].
Non-asymptotic theory is limited to comparing loss values (KL divergence vs expected square error of scores), which does not provide a clear insight into how the resulting densities themselves compare.
Asymptotic theory is limited to learning on distributions in the exponential family.
[1] Song, Y. and Ermon, S., 2019. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32. | Strength:
Score matching is a widely used technique and its statistical analysis is of high interest to the community.
Their theoretical results seem novel and technically solid to me. I found the isoperimetric viewpoint quite interesting.
I found the paper well-written and easy to follow.
Major comments:
Why is Thm. 3 about an unknown direction? It makes sense to me if the goal is to estimate
||θ||
or
w0⊤θ
for a prescribed
w0
. The fact that the lower bound is proved for an unknown vector
w
is confusing to me. Is it possible that there exists some
w′
such that the score-matching estimator achieves better efficiency?
The paper could benefit from adding more explanations and discussions. For example,
How large is
CLS(q)
compared to
CLS(q,P)
depending on
P
? Can you give an example?
Can you provide an example for Thm. 2, just like Thms. 1 and 3?
It would be helpful to give some intuition on the tools used in Sec. 2.
From (1),
Jp(q)−Jp(p)
is equal to
1/2
of the RHS of (6). Prop. 1 seems to be missing a factor of 2. This also holds true in the latter results like Thm. 1.
Minor comments:
Should
p
be smooth in Def. 2?
What is
f
in Def. 3?
The notation
Id
in the paragraph Mollifiers can be confused with the identity matrix.
Jp(q)
should be
Jp(p)
in the equation below (6).
The subscripts MLE and SM in Def. 6 and Prop. 2 are too large. Maybe use \text{}?
In conclusion, ""In tihs paper"" --> ""In this paper"". | Many results of the paper are interesting and insightful. And they are fairly novel since this work is probably the first to discuss the statistical efficiency of score estimator.
However, the clarity of the paper could be improved. I have a few suggestions below. Also, with some more insightful experiments, the motivation of the paper could be boosted.",8.0,3.0
68,Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search,"Keywords: symbolic regression, Monte Carlo tree search, governing equations, nonlinear dynamics","strengths: The strength of the research is its stable symbolic regression compared to prior methods.
weaknesses: The following three points are drawbacks 1 There is no discussion or theoretical analysis of why the proposed method is effective. Therefore, I cannot eliminate the suspicion that the effectiveness of the proposed method over the prior methods depends on the superiority of the hyperparameter tuning. I think it is good to show what kind of ""prior physics knowledge in nonlinear dynamics"" can be implemented by the proposed algorithm, and provide numerical experiments to demonstrate its effectiveness.
2 Where the stability of the estimation results due to hyperparameter settings is unclear. Please explain the optimization method for hyperparameters and the stability of the estimation results with respect to variations in hyperparameters.
3 Where it is unclear whether the hyperparameter tuning of the prior method is appropriate. It is same as above point. In particular, we would like to know why the results in Table 1 do not match the results of similar experiments presented in the paper proposing the prior method. (Table 1 in [Mundhenk et al., 2021]) | Strengths
1. The paper is well-organized and easy to follow.
The organization and the writing of the whole paper are very attractive to me. The introduction and the background sections go over the different lines of existing work and the background one needs before diving into the proposed method in this paper. Figure 1 and Algorithm 1 clearly illustrate the high-level architecture and the underlying details of the method, respectively. Sections 4 to 7 discuss how the method could be applied to different benchmarks and tasks.
2. The proposed method adopts three useful adjustments to the conventional MCTS.
The proposed Symbolic Physics Learner (SPL) machine carefully incorporates the three adjustments to the conventional MCTS, which largely improve the prediction performance and stability.
3. The experimental sections are impressive to me.
The experiments are conducted on four separate cases. The results show a consistent improvement on a bunch of tasks for SPL.
Weaknesses
1. The model novelty.
I am a little bit concerned about the novelty of the proposed method. It looks like an incremental model that makes several modifications on top of a popular model. | The paper is well written and easy to follow. The authors perform a multitude of experiments on symbolic regression and discovery of nonlinear dynamics. Comparison with state-of-the-art related works clearly demonstrates the shortcomings of related works and the efficacy of the proposed method.
I am missing a more detailed description and direct comparison with related works on MCTS for symbolic regression. How is your work different from theirs? Similarly, I would expect a quantitative comparison with existing MCTS methods for symbolic regression.
There is one ablation study on the importance of adjustments to MCTS (sec. 4.2), yet the results are barely mentioned, and I miss some analysis on them. You could have included the results (and the analysis) in the appendix. It is surprising that removing each adjustment separately results in a performance lower than 60%. Can you provide more detailed results, and perhaps explain why that might be the case? | Strengths:
This is a strong paper, and has clear novelty in the area of symbolic regression. The paper has extensive comparison to competing methods, with good results.
Weaknesses:
The theoretical aspects of the paper could be improved.
The theoretical basis of this approach should be highlighted in comparison to existing methods of symbolic regression. In addition, since it is based on stochastic methods for which guarantees have been given for other applications, this is a clear lack. Questions include: What type of sampling distributions are used, and can these be modified during search to improve the returned function?",8.0,3.5
69,Tailoring Language Generation Models under Total Variation Distance,"Keywords: language generation, maximum likelihood estimation, total variation distance, text degeneration","STRENGTHS
This is overall a well-rounded paper: it tackles a very important problem, the proposed solution is sound and well-motivated (although I did not carefully check the correctness of the maths), the paper has more than enough substance, and the reported results are solid.
Despite being somewhat math-heavy, the paper is written in a very clear and didactic way. I found it easy to follow and an enjoyable read overall!
Comprehensive (although not entirely convincing, see below) experiments covering 3 downstream applications and a synthetic task, with a good number of baselines.
WEAKNESSES
The experiments are comprehensive, but limited to rather artificial (or at least not the most relevant) scenarios from today’s perspective. For instance, the authors use a 1-layer LSTM trained on 10k sentences as their oracle for the synthetic data experiments. This sounds ridiculous in 2022, when there are dozens of strong language models publicly available. Similarly, the choice of IWSLT14 as the evaluation benchmark for MT and Gigaword for summarization do not seem the most relevant nowadays (e.g. why not run MT experiments in more languages and/or a more recent WMT benchmark)? In addition, all real-data experiments rely on seq2seq models, while it is decoder-only models that are becoming more and more central in NLP.
Connected to the previous point, the authors have a decent number of baselines that they implement, but they do not compare to any number previously reported in the literature. If one checks previously published papers, it seems clear that the reported numbers are rather far from the state-of-the-art. For instance, https://aclanthology.org/2021.emnlp-main.534.pdf reports 38.6 BLEU in IWSLT14, compared to 35.1 for the best system in this paper.
Given the previous points, I think that the experiments in the paper are valid as a proof of concept, but are far from convincing me that I should use the proposed approach when I train my next NLG model. You have convinced me that the approach has some potential, but not necessarily that it is better than standard MLE in the real world. | Experiments are on a synthetic task as well as regular tasks including machine translation and summarization. Experiments are also done on long text generation as well, and I’m glad to see that it works well.
I’m glad that the paper involves discussion of Kang and Hashimoto (2020) as well as Pang and He (2021). One concern is that given that the authors’ motivation and the two papers’ motivation are so similar – all of the approaches (the two papers’ as well as the authors’) propose to downweight unlikely / outlier tokens, I would appreciate some deeper explanation on the pros and cons of each approach, or why the authors’ approach excels.
Update: I read through the paper again, and responded to the authors' rebuttal below -- I don't really have complaints otherwise. I think the motivation and the execution are great. | Strengths: -Novel method -Thorough empirical results (both real and synthetic) -Well written",8.666666666666666,3.3333333333333335
70,Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives,"Keywords: Automatic Machine learning, Hyperparameter tuning, Lexicographic preference","Strengths:
Interesting method with a lot of potential
Strong performance compared to existing state-of-the-art multi-objective HPO methods
Robust and strong anytime performance
Weaknesses:
Requires the order of objective functions as an input by the user (who might not be able to express such an order)
Requires the user to specify goals/thresholds which might be difficult to provide for black-box objectives. Meaning to provide such information the user needs extensive knowledge about the black-box objectives. | Strengths:
The problem is well motivated and thoroughly described; and the proposed use of lexicographic preferences is well justified.
The problem formulation is mostly clear (but see caveats below).
In general I was able to follow the development of the algorithm and its description.
Weaknesses:
The algorithm and its description are somewhat hard to parse. After multiple read-throughs I think I have a reasonable understanding (but not perfect - see questions below), but it would have been easier if the larger ""text chunks"" (e.g. paragraph 2 on page 5) were rewritten using dot-points, tables etc to improve readability. You may also want to consider motivating the approach taken in the algorithm (what is it based on and why) before presenting it.
Have you considered the convergence properties of this algorithm (""big-O"" behaviour, regret bounds etc).
Minor points:
The presentation was inconsistent in the results section - for example the highlighting in table 1 (bold for all within the error bars of the best result) and table 2 (bold only for the ""best"", ignoring error bars).
Why inf (text) in some places,
∞
in others?
Questions:
Regarding (1), do you intentionally allow for solutions that do not reach the goal for some objectives? This appears to be the case (z_*^k is max of the goal and ""best we can do plus tolerance"" if I'm reading it correctly) - perhaps some commentary on why such solutions are acceptable would be of assistance here?
Was there a reason (technical or otherwise) for using randomised direct search rather than an algorithm specifically designed to deal with expensive optimisation problems (Bayesian optimization or similar)?
You mention that the solution found is (potentially) simply one sample of a region of the Pareto front. What if the algorithms finds multiple such solutions? Or is this not possible?
What is the purpose of lines 9-12 in the algorithm? In particular, where do the various ""update equations"" here come from?
What is the termination condition for the algorithm? | Strengths:
The LexiHPO framework is novel to the best of my knowledge, and may be more intuitive to practitioners than dealing with multi-objective approaches that discover Pareto frontiers.
The LexiHPO algorithm is described well and the authors provide intuition about how it was designed.
Extensive experimental results are provided that demonstrate the performance of the algorithm, but also the use-cases presented help to justify the idea of introduce a lexicographic ordering into multi-objective optimization.
Weaknesses:
Only a few sentences are devoted to existing work regarding multi-objective optimization with preferences. I think the paper would be improved if a little more spaced was devoted to explaining why LexiHPO is somehow a better way to incorporate preferences than Paria et al. 2020, Abdolshah et al. 2019 and Zitzler et al. 2008, and also why none of these methods could be used as experimental baselines.
In Section 3 it is stated: ""However one problem with the vanilla lexicographic relations is that they cannot accommodate user-specified tolerances and goals on the objectives."". I found the flow here a little confusing - wouldn't it be better to introduce equations (5), (6), (7) in Section 2.1 since one can argue they fundamentally relate to the setup itself, rather than the algorithm. Whereas the algorithm is responsible for computing the online statistics in equation (8)?
The datasets used in Section 4.2 are very small and the results are not entirely convincing statistically. While I feel that the contributions of the paper stand without this section, I think that in order to convincingly argue that LexiFlow can be used to reduce overfitting it would be necessary to use more datasets and/or perform rigorous statistical hypothesis testing.
Minor corrections:
In the example at the bottom of page 3 it is stated the primary objective is the model's prediction error, but later in the example it is referred to as ""loss"".
In the same example, it is stated that the goal vector should contain +infinity in the first position, shouldn't this be -infinity?
""We also urge methods that are efficient and cost-frugal"" -- ""urge"" is probably the wrong choice of word in this sentence.
In Figure 2 and Figure 3 - is there a reason why the LexiTarget is not shown for all objective?
Section 4.1.3 last paragraph - Figure 4 is referenced whereas I think it should be Figure 3?",8.0,4.0
71,Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks,"Keywords: Domain Generalization, Sequential Learning Model, Dynamic Neural Network","Strength:
Interesting problem setting
Interesting approach with solid results.
Weaknesses:
Paper does not provide enough information to understand the full details (e.g. see comments on integrals and proofs)
Proofs and theorems are not fully clear | The paper approaches the important problem of temporal concept drift with an elegant solution. The draft is quite dense but remains clear. The experimentation is thorough with useful ablation studies and interesting discussions and limitations.
The theoretical results are interesting and show the importance of modelling the concept drift, but do not quite show why the proposed approach should outperform the other state-of-the-art strategies. Importantly, two key claims are unclear:
Why should the variance of the union of domains always be larger than the last domain? For instance, if each domain at time
t
is a gaussian with mean
0
and variance
t
(the variance increases with time, meaning that the union has a lower variance than the last observed distribution)
Why does the posterior always have a lower variance than the prior? I think this claim only stands in expectation. Finally, the formalisation of the concept drift under which the methodology would perform better would be particularly valuable.",6.0,3.0
72,The Lie Derivative for Measuring Learned Equivariance,-''-,"The paper is well-written. I found it easy to read. The storytelling of hte paper is of high quality. The derivations are accurate and the math notation is well-understood.
Strengths
The authors pay attention to the problem which is less studied in the community - the problem of measuring the equivariant properties of various models.
The theory of the paper is solid. And demonstrates a very good understanding of the problem.
The experimental results of the paper demonstrate that such a method is useful for the analysis
No Strengths I don't see any significant issues, which may affect my rating drastically. However, I would like to address several questions and suggestions, which may improve the quality of the paper.
The related work misses a section on why we are so motivated to measure the equivariance error. There were several papers which demonstrated that the model's accuracy and its equivariance error are highly correlated. In [1] the authors take a network and train it by minimizing the equivariance error which leads to a better accuracy than any other counterparts. In [2] the authors compare different scale-equivariant models, conclude that the equivariance error is not 0, and demonstrate that the lower it is the more accurate the model becomes.
[Does not affect my rating. Just a pure interest] Although it was not possible to include this paper beforehand, there is a recent paper which considers a similar problem. In [3] the authors introduce a similar metric for measuring the symmetric properties of the model. What is the main difference between their approach and yours?
Let us consider two neural networks, which perform binary classification. The very last layer outputs a positive or a negative value for either class 1 or class 2. We initialize them with the same weights. However, for network 2 we multiply the output by
106
. Do I understand it correctly, that for network 2 LEE will be
106
larger than LEE for network 1? If so, can we really compare two neural networks based on Eq 4? Do we need an extra step of output normalization?
In supplementary materials, in A3, paragraph 2, you mention that there is an inequality between the
L
of the network and the sum of the
L
of its layers. Let us consider a network, for which the very last layer just multiplies everything by
0
. Such a network is absolutely invariant, thus
L=0
. However, the
L
of the layers of the network can be arbitrary large. Thus, Figure 4 does not really tell us anything about the network as a whole. Could you make a comparison where you plot side-by-side the cumulative loss as a sum per-layer losses. And a series of losses calculated for all truncated subnetworks of the original networks
subnetk=fk∘⋯∘f1
[1] Khetan N. et al. Implicit Equivariance in Convolutional Networks, preprint 2021
[2] Sosnovik I., Moskalev A., Smeulders A. Disco: accurate discrete scale convolutions, BMVC 2021
[3] Moskalev A. et al. LieGG: Studying Learned Lie Group Generators, NeurIPS 2022 | Strengths
The paper considers an important question in the field. Instead of proposing a novel architecture, this work provides a useful comparison of the strengths and weaknesses of existing state-of-the-art methods from the viewpoint of their ability to learn equivariance. It could provide insights for improving equivariance in both models with built-in and no built-in inductive bias or help to make model selection easier.
The experiments are really quite extensive. I agree with the authors' claim that the scale and scope of their analysis are helpful in making the trends clear against the noise and other factors contributing to model performance and equivariance error. Given the goal here is to say something broad about CNNs or ViT in general, it is essential a wide variety of architectures in both classes are considered.
Previously works on equivariant neural networks have not been overly focused on aliasing. The equivariance is usually analyzed from the point of view of continuous signals and equivariance is proved in the continuous case. Empirical measurements are used to assure that discretization errors are acceptable. However, it seems clear that discretization is important and limiting the potential advantages. For example, many applications of E(2)-CNN use small discrete rotation groups instead of continuous groups. This work addresses aliasing head-on and provides evidence of the extent to which it contributes to equivariance error and specifically where (non-linearity).
The definition of LEE and the observation that LEE can be broken down on a layer-by-layer basis seems like an important contribution. Previously, there was not a precise way to quantify the contribution of each layer to EE. Practically, one could measure the EE over a single layer (as is done here for alternate EE metrics, I believe), but this is not guaranteed to combine in any reasonable way.
The result shown in Figure 4 and subsequent insights are very interesting. I appreciate that several different comparisons are done: CNNs vs. ViT, different symmetries, different EE metrics. The finding that non-linearities contribute the most to LEE raises interesting questions. What about the fact early layers seem to contribute more? The observation the contributions to EE for each layer are similar for different symmetry groups suggesting aliasing the root cause is interesting.
Section 6 discusses several ways to improve equivariance learning given that baking equivariance seems to come up short.
Weaknesses / Questions
Since the task is invariant (equivariant with output having trivial rep), the representation must go from having non-trivial group action to trivial group action. In practice, this invariance can be baked in either by mapping to invariants and having later layers be unconstrained or having equivariant layers and then a final invariant layer. Thus for a classification task, it is not necessary for the intermediate layers to have low EE for the final NN to be invariant. I feel this could somewhat undermine the assumptions of the layer-by-layer LEE analysis. Could the authors comment? I would feel more confident if the experiments were performed for a task where the output representation was equal to the input representation, for example, image segmentation.
Another potential aspect overlooked in the setup is the potential for a non-trivial action on the channels of the hidden features. This absolutely key to making equivariant neural networks work. My understanding is that LEE is assuming the channel representation is trivial (though I could be mistaken). Thus we could be measuring high LEE when in fact it would be low if we knew the way the channels should transform.
As noted in the paper, CNNs are often not very equivariant (compared to many other equivariant networks) due to discretization, pooling, and stride. I'd like to see results showing the equivariance error for the CNNs vs. ViTs at initialization and throughout training. If the compared CNNs do not have low EE at initialization, then the argument that they have built-in equivariance is not very strong. I'd feel more comfortable with a comparison between E(2)-CNN and unconstrained networks for rotational equivariance error. In this case, I think the E(2)-CNN architecture would have relatively low EE.
Although it was posted on the arXiv only in October and has now been accepted to NeurIPS (per the schedule), I believe that LieGG (https://arxiv.org/abs/2210.04345) covers some similar territory in terms of using lie algebra action on functions space to analyze the learned symmetry of the model. That work also includes analysis of ""symmetry bias"" (analogous to equivariance error) and per-layer analysis (see Fig.5). I don't consider it to pre-empt this work, both because they are concurrent and because the focus is very different, however, I would like to see a comparison added.
I take some issue with the claim that it is surprising that non-equivariance models attain lower equivariance error after training. Equivariant networks certainly have lower equivariance error at initialization that unconstrained networks, but learning equivariance is necessary in order to get good performance, so it follows that if an unconstrained network achieves better performance than an equivariant one, it may well have lower trained equivariance error (EE). Consider, for example, a regression task. Assuming the ground truth is perfectly equivariant, then the equivariance error will be bounded by 2*(approximation error) for any sample by the triangle inequality. Since you consider a classification task, this relationship is a bit more obscured, but still well represented by your results.
Questions
I'm unclear on the aliasing operation
A
versus the map
Alias(n)
. Can you clarify? In particular, on page 3, it seems
A:N→N
, but on page 6, it seems
A:(R2)Rc→(R2)Rc
, i.e. images to images.
Could section 4 avoid referring to flows and be written in terms of the exponential map
g→G
?
In section 5, you prove that Alias results in translation LEE. I didn't fully understand why this points to the non-linear layers.
Minor Points
The abstract says ""introduce the Lie derivative,"" I would re-word to make clear Lie derivatives are pre-existing work, they are merely being introduced as a method here.
I'd replace ""it's"" with ""it is"" since this is formal writing.
In two places
↦
appears when it should be
→
.
↦
is for elements and
→
is for sets. As in
f:X→Y
mapping
x↦y
.
Page 4 ""the operator
ρ21(g)[ΦYt]
"" Should it be
ρ21([ΦYt])
?
In the notation for LEE in eqn. 4, you might want to include the dependence on
X
.
Section 5, first sentence ""equivariance"" -> ""equivariant""",8.0,4.5
73,The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation,"Keywords: multimodal learning, knowledge distillation","Strength: (a) The author hypothesized that for crossmodal KD, distillation performance depends on the proportion of modality-general decisive features preserved in the teacher network, this hypothesis is refreshing to me. (b) The verification experiments in Figure 2 and 3 are convincing to me. Weaknesses: (a) In some cases, modality general decisive features and modality-specific decisive features of audio and video modalities could be imbalanced, e.g. on VGGSound Event dataset, there could be more audio decisive features than visual features feature, How would the proposed method handle this? | The main strength: authors proposed the concept of modality-general decisive features for cross modal knowledge distillation, and proposed a hypothesis that the key factor for improving the student model performance is that the teacher model is trained based on the modality-general decisive features. Authors provided theoretical and experimental analysis to support their hypothesis. This hypothesis could provide new insight for the reason of when a student model could perform well in cross-modal knowledge distillation.
Main weakness: the way to decide what kind of features are modality-general decisive features are difficult to control and search. The advantage of deep learning is to automatically learn the ""useful"" features for improving the performance, while authors strategy of determining the modality-general decisive features is heuristic, and usually difficult to decide. In particular, how to decide the modality-general decisive features itself is a difficult problem. | Pros:
The proposed Modality Focusing Hypothesis is significant and might raise some interest in controllable cross-model distillation. Rich experiments verify this claim.
The organization of the paper is good in general.
Cons:
Although I admit and like the Modality Focusing Hypothesis, this hypothesis conflicts with the general multi-view assumption, i.e., each view could bring extra useful downstream information [1]. Different views always have some specific information and we always find that learning a multi-modal model is better than every single modality (CLIP), at least in most datasets. Why this phenomenon is different in knowledge distillation? One might argue that view-specific information contains much redundancy [1]. I'd like to see some explanations about this point.
It is not clear to me that how to control the
γ
, e.g., how to calculate
γ
in Table 2? What's the rule? Also, how to quantize this parameter during other experiments?
For MM-IMDB data, why there is more modality-specific information?
The modality Venn diagram is not new enough, as there are plenty of works that use this to analyze the effectiveness of multi-modal learning [1-3]. Specifically, [2] also claims that modality-general information is essential. The author is encouraged to change their claim that treats this as a contribution and do a better job by clarifying the differences between these relevant works and the proposed method.
[1] Self-supervised Learning from a Multi-view Perspective, ICLR'21
[2] Dual Contrastive Prediction for Incomplete Multi-View Representation Learning, TPAMI'22
[3] COMPLETER: Incomplete Multi-view Clustering via Contrastive Prediction, CVPR'21",7.333333333333333,3.0
74,The Role of Coverage in Online Reinforcement Learning,"Keywords: reinforcement learning theory, online RL, offline RL, learnability, general function approximation","Strength:
The topic is important and the idea is novel. Both offline RL and online RL are important topics for the community and the paper serves as a bridge of the two formulations. In particular, the idea that the notion in offline RL can facilitate online exploration is novel and may serve as an inspiring point for future researches.
The story is complete. The authors compare their notion with existing notations for both online and offline RL, which addresses the necessity and novelty of their findings. The example of block MDP is also a good point, which shows how the authors' findings can actually result in a sharper sample complexity, compared with previous works.
Weakness:
Part of the technical contribution is limited, since the algorithm and some of the analysis are directly copied from existing works. However, it shouldn't be considered as a big problem since the paper itself is mainly focused on the analysis instead of the algorithm design, and the paper does show its own novelty in the regret analysis. | Strengths:
The paper addresses an important setting of RL: online RL with access to ofﬂine data
The paper is generally well written
Weaknesses:
However, I have some concerns about the novelty of the main contribution of the paper. A very recent result under review of ICLR 2023 [1] shows that under the Concentrability assumption as Definition 1 (similar to the coverability assumption as the paper proposed), it is enough to achieve an ""optimal"" offline RL, without any interaction with the environment. These results seem correct to me. Then,
If we can access offline data and assume that offline data has good coverage as stated in Definition 2 (lower bound of Concentrability assumption), so do we really need the online RL?
If we really still need the online RL, then it seems that the coverability condition on offline data that the paper proposed is not enough weak to keep a sample-efficient online RL.
[1] https://openreview.net/forum?id=ZsvWb6mJnMv&referrer=%5BReviewer%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2023%2FConference%2FReviewers%23assigned-papers) | Strengths
The paper is well written.
The authors have addressed a crucial challenge of sample efficiency in reinforcement learning and the paper is quite relevant to the community.
The authors have introduced the concept of coverability with taking motivation from the idea of concentrability from offline RL which is quite interesting.
Questions to Authors
It would have been nice to show the effect of coverability coefficient via empirical studies.
Is it possible to have coverability coefficient =0 ?
How difficult is it to construct the confidence sets in practice? Is the approach feasible?
Is it possible to extend this framework to continuous state-action spaces?",7.0,3.3333333333333335
75,Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection,"Keywords: Computer Vision, 3D Object Detection, Stereo Matching","Strengths:
New formulation of the exiting multi-view, multi-frame image based 3D detection methods.
Use both theoretical and empirical analysis by introducing localisation potential, to unveil the importance of longer time horizon.
Impressive performance in nuScenes leaderboard.
Writing is clear and easy to follow.
Weakness:
I have one question regarding the experimental analysis. Did the authors analyse the performance improvement for objects with different speed? It is interesting to see the impact of long term fusion on static, slowly moving and fast moving objects. | Strength:
This work borrows a lot of knowledge and tools from multi-view stereo to help us understand and analysis how temporal context should be used for vision-based 3D object detection. Introducing multi-view stereo to 3D object detection is a valuable direction that probably worth much more research, and to my knowledge this work can be seen as one of the few works that open up this direction.
The analysis based on the proposed ""localization potential"" well-explained the motivation of using both long-term and short-term memory, and the conclusions shown in Fig.4-6 are quite interesting and inspiring.
The final method is quite simple, easy to be re-produced.
Experimental results are very promising, refreshing previous state-of-the-art by a significant margin, showing the importance of using long-temporal context.
Thorough abation study.
Weaknesses:
I think the biggest issue is the presentation. I appreciate the dense technical content, which indeed brings difficulty in clarification; But the presentation could definitly be improved. For instance, a reader who has weak background on multi-view stereo may be confusing when discussing the relationship between multi-vew stereo and 3D object detection, since there is little preliminary knowledge introduced in the main text. Maybe it would be better to elaborate more on background, and defer other stuff to the appendices.",8.0,3.5
76,Token Merging: Your ViT But Faster,"Keywords: token merging, token pruning, inference speed, training speed, throughput, off-the-shelf, fine tuning","Strength:
The idea is simple yet effective and can benefit both ViT training and inference a lot.
The authors clearly have deep insights about the (larger or smaller) ViT designs. The introduction of the literature is clear and informative.
Using matching instead of clustering algorithm makes sense to me, results also demonstrate the expected throughput achievements.
Weakness:
More direct comparisons are desired, e.g., in Tab. 3, you compare ViT-L with other ViTs. How about comparing them apple-to-apple. E.g., ViT-L w/ ToMe vs. ViT-L (attached in appendix; better to remove ahead), MViTv2 w/ ToMe vs. MViTv2, Swin w/ ToMe vs. Swin, etc.
It may be easier to improve the throughput for large ViT models without hurting the model accuracy a lot. How about smaller ones, e.g., LeViT? | Strength
The problem, reducing the computational burden in ViTs, has been an important issue in developing and deploying transformers.
While redundancy in self-attention has already been investigated before, there are few solutions except for pruning-based methods, which require additional attention. The simple strategy in this work shows promising results, even without being trained.
The authors perform concrete studies and experiments across different tasks and architectures, showing the reliability ToMe.
Weakness
This method could be viewed as a parallel implementation of the matching algorithm. We expect more discussion with previous slower variants, such as the clustering algorithm in [1].
The authors mentioned that the method could be used in training to reduce training complexity. One more common use case here is to adopt such idea to recent MIM-based methods such as MAE, where longer pre-training epochs are required. However, the authors have only used it in fine-tuning MAE pre-trained checkpoints.
[1] Token Pooling in Vision Transformers. arxiv 2110.03860. | Strengths
The proposed method of greedily-merging tokens is very elegant and makes a lot of sense to me, especially that the method can work without retraining. This makes the proposed method easy to use for many off-the-shelf transformer-based models.
The paper shows results on three different modalities: images, videos, and audio, and all displayed competitive results. This shows the proposed method is general and robust.
The ablation studies in Table 1 are comprehensive, and show the proposed method is robust under different hyer-parameters.
The qualitative examples in Figure 4 are interesting. Please make sure they are not cherry-picked.
The paper is well motivated, well-written, and easy to understand.
Weaknesses
The comparison to existing efficient transformers in Table 4 are not as exciting as expected, especially given that DynamicViT is also very simple. Most of the advances are in training speed instead of on speed-accuracy trade-off.
It will be good to include other efficient transformer methods for videos and audio to provide more context (if there is any) about the numbers.
[Minor] last line of the code in Appendix D. Should it be return merge(k).? | Strength
The motivation of this paper is clear. For a large ViT model, there is no need to keep all the tokens involved in the decision making as some of them may contain useless information. Discarding some of the tokens that are similar to others can efficiently reduce the inference time.
The proposed approach is simple and easy to follow. It can be added in either the training or inference stage. For users who want to use large-scale models but are with limited computational resources, this paper provides an appropriate tool to conquer this issue.
Experiments show that the performance does not drop much when r is small but the speedup is clear. Moreover, compared to other ViT based models, this paper also shows its advantages.
Experiments on all images, video, and audio show that the proposed approach indeed behaves well.
I really like the visualizations in Figure 4. It seems that for regions with similar textures or patterns, they can be merged for efficient inference.
Weaknesses
Like the author said, the vanilla ViT shows great potential in visual recognition especially after the emerging of MIM-based methods, like MAE and the proposed approach works well with the vanilla ViT. However, in some cases, a pyramid structure of ViT is required especially for downstream tasks, like semantic segmentation. It would be better if the approach could be utilized in ViTs, like Swin Transformer.
From the experimental results, we can see that for large models, increasing the value of r does not lead to too much performance drop. However, for small-sized or even tiny-sized models, the performance drops much. Though the reason is clear but I still think it should be mentioned explicitly.
From Figure 3(a), it seems that after compressing the ViT-L model, the trade-off between the performance and latency is not as good as ViT-B without compression.",8.0,4.0
77,Towards Open Temporal Graph Neural Networks,"Keywords: Temporal Graph Neural Networks, Open Temporal Graphs, Class-Incremental Learning","Strength:
The problem of learning open temporal graph is of great practical value as in many applications where graphs are updated consistently with both new and old node types. The algorithm proposed in the paper clearly improves the state of art temporal GNN algorithms under this setting.
The proposed methods are well motivated, and are supported with both theoretical and empirical results.
The proposed algorithm is very generalizable. It works with basically any temporal GNN model with no or little modifications.
Weakness:
The experimentation process needs further elaboration, especially on how the baseline models are trained and tuned. For example, there might be trade-off between AP and AF depending on how many epochs the model trains on the new task.
While the loss function encourage class-agnostic representation, to what extend the learned representation is class-agnostic can be better depicted besides the ablation study measured by model performances. For example, suppose we cluster the embeddings, will the clusters correlate with classes? | Strength
This paper focues on continuous open temporal graph which is lack of formal study in previous works.
This paper proposes OTGNet which is empirically far better than similar works.
Weakness
Some statements need to be improved for better understanding.
The definition of
Iloss
need to be improved. For example, upweight by
ϵ
need to be formally expanded. I understand that
ϵ
is the extra weighted classification loss only for traidic nodes only after reading the appendix.
Will it be possible that Algorithm 1 select triadics that are not connected with any node in current batch or task? I think those triadics are meaningless. If Algorithm 1 can avoid this, how it is guaranteed? I do not see any step in Algorithm 1 to avoid this.
Class-agnostic representation
Z(t)
need to be clarified. How to really compute
LIB
? I think it is infeasible to compute any expection value in the formula.
Is neural network method the only way to get
Z(t)
? I think this optimization is quite classic, and may have non-parametric approximation. Can you confirm that there is no other approximation?
Besides, I have a minor concern of Algorithm 2:
For every task, you will select new important triadics
Sk
for each class and update them in the memory. Thus the triadic buckect
S
will indeed grow without restriction. Is there any way that you can limit the maximum size of
S
? If so, how will this restriction hurts the performance? | Strength:
This paper proposed a framework to handle the class-incremental task in the temporal graph. The Triad Structure Selection is interesting. By using the influence function, it can select the important triangles. The task is novel and interesting. To accelerate the algorithm, the authors also apply a greedy result by showing that the value function F is monotone and submodular. This is intuitive. The experiments also shows that the model compared with TGN and TGAT have a better result, the ablation result shows that all the module works well. The experiments in the appendix show that the model can learn the class-agnostic knowledge and is able to mitigate the catastrophic forgetting problems. The ablation study is solid.
Weaknesses: I have the following questions:
First, the paper mentioned the heterophily setting in the temporal graph, which can be one main issue. However, in previous research, people found that the heterophily indicates that usually, GNN(such as vanilla GCN) can't work, but the traditional pagerank-based algorithms(APPNP) or other models which have a more powerful aggregator method(such as GCNII, GGCN, or other attention-based models, which implicitly handle the heterophily by a learning-based aggregator) perform well in these heterophily tasks. I think this kind of model should also be a baseline (although I understand the setting is a temporal graph and new coming tasks, so maybe the authors can first ignore the temporal information. ). Moreover, the baselines are not enough, both TGN and TGAT are baselines 2 years ago. Plenty of temporal network models are proposed and should be compared.
Secondly, the time complexity of choosing triads is unscalable. If the testing set is large, or the graph itself is large, the algorithm can't work in a limited time. However, for temporal graphs, we usually have more edges due to the time domain. Some possible work like Causal Anonymous Walk may somehow solve this issue by random walk. And these methods also have the potential to represent more complex motives instead of triangles.
[1] Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks [2] Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks | Strengths:
The problem of class-incremental GNN learning is valid and interesting. Dynamics and new classes are indeed problems in real-world graph learning.
The observation that nodes from existing classes influence nodes from new classes due to homophily is a unique and interesting perspective. It is also shown in the experiments that this indeed poses challenges and affects the results.
The designed methods seem novel and sound. Selecting important triads for replay to alleviate forgetting makes sense. Using influence functions to approximate the importance of triads is interesting.
The experimental results seem very good. The improvements over existing works are significant. The ablation studies are extensive enough to demonstrate the effects of individual components.
Weaknesses and Questions:
Some notations and words can be revised to better describe the methods. For example,
log
should be
log
,
exp
should be
exp
,
sup
should be
sup
, 'representative' should be 'representativeness'.
Some details of the proposed method are not very clearly described. I list several unclear points.
On measuring the representativeness. We know that nodes in a graph influences the learned model in two ways, both in forward propagation (i.e. neighborhood aggregation) and backward propagation (i.e. model optimization via the loss). As far as I can see, the influence function in Eqn. 1 can only capture the latter one, i.e. whether a node appears in the loss, but not the former one, i.e. whether the node propagates its features to neighboring nodes.
Intuitively, the scores
R(gkc)
should influence each other. For example, suppose
i,j,k
and
i,j,m
are both close triads of class
k
,
i,j
are both important nodes (e.g. hubs with high degrees) while
j,m
are not. It is likely that both triads will get high scores. However, it is often sufficient to select only one of them (as only
i,j
are important), i.e. with the presence of
i,j,k
, the score of
i,j,m
will decrease. Is my understanding correct? If yes, how can OTGNet address for the case?
In the triad structure replay, it is said that 'we will replay these triads from old classes when learning new classes'. However, it is not clear how will nodes that are not in selected triads be dealt with. Specifically, will they still participate in the forward propagation? If they are dropped, there will be missing auxiliary information (e.g. nodes in the selected triads will have missing neighbors). If they are not, the training cost is still high (since a whole graph participates in forward computation). Please clarify that.
In Eqn. 10, 11, 12, it seems that
xi(t),zi(t)
are not related to the layer. Does it imply that OTGNet only supports one layer of TGAT?
I appreciate the extensiveness of experiments with the following minor questions.
I suggest some visualization or case study on the class agnostic embeddings
z
and
x
to better illustrate the necessity of learning
Z
.
In Table 6, changing
K
from 1000 to 100 leads to a significant performance drop. However, in Figure 7, it shows that the performance stabilizes with only
M=20
, which means that only 20 triads per class is enough for OTGNet to work. This seems counter-intuitive. From my understanding, it indicates that the triads with high
R
(top
K
) do not necessarily lead to a good selection. Is that correct?",6.5,3.0
78,Towards Stable Test-time Adaptation in Dynamic Wild World,"Keywords: Test-time adaptation, Roustness","Strengh The paper is easy to follow. Authors also include explanations/intuitions to certain phenomena and design choices.
Weakness: Follows are my concerns. More detail below.
The proposed method somewhat lacks novelty.
The empirical evaluation can be improved and strengthened. | Strengths:
This work provides detailed investigation of unstable reasons and careful analysis of failure cases in TTA. According to their investigation and analysis, a sharpness-aware and reliable method is proposed to solve those issues.
Their discussion about norm layer is interesting. They also find weaknesses of only using GN and LN and provide a good solution to deal with such issues. The analysis of test sample gradients and the corresponding removing method is quite novel and efficient.
Their experiments and ablation studies demonstrate that their analysis and hypothesis is indeed true, and the method they proposed is effective to improve the performance of TTA on ImageNet-C.
Weaknesses:
Their analysis about the norm layer is mainly based on empirical evidences. It is good to support their study with comprehensive experiments. However, I still wonder is there any intuitive explanation about choosing GN and LN instead of BN? This could help readers to better understand the analysis.
In Fig. 6, the authors find even with fine-tuned hyper-parameters, gradient clipping yield worse results than no clipping, which is a bit counter-intuitive. Is there any explanation about the failure of gradient clipping even with fine-tuning? | Strength:
The authors introduce wild online TTA settings. For example, the test data may have 1) mixture distribution variation, 2) mini-batch, 3) online imbalanced label distribution variation, which are realistic during application.
Extensive experiments are conducted to illustrate the limitations of Batch Norm in TTA and further observed that batch-independent norms (e.g., Group Norm and Layer Norm) will be more stable than Batch Norm. The experiments provide new insights for the TTA community.
The authors observe that models with Group Norm or Layer Norm tend to collapse, especially when the severity of the distribution shift is high. They then propose a sharpness-aware learning scheme to alleviate the collapse problem, which is technically reasonable and novel within the scope of TTA.
The authors propose a sharpness-aware and reliable entropy minimization method to uniformly address the instability of previous methods under three wild TTA settings. Extensive experiments on ImageNet-C demonstrate the effectiveness of SAR.
Weakness:
This paper focuses on dynamic wild-test time adaptation settings. I am curious about under what circumstances does the third situation arise? Can the author provide some examples?
In Tables 2 and 4, all considered methods (including the proposed method) achieve worse TTA performance than baselines on ResNet50-GN with damaged Defoc. That is, all SOTA TTA methods cannot fit out-of-distribution samples. Could the authors discuss more about this phenomenon?
In Section 4, the description is a bit difficult to understand. Since these figures are informative, it is best to indicate which subgraph is being described when describing.
The numbers in section 4 are somewhat difficult to understand directly and the font size is too small. It's best to optimize these numbers for readability.
In Table 2, were the results for different damages measured with the same sample sequence? | Strengths:
the proposed setting of ""wild"" test-time adaptation is of high practical relevancy and it is considerably more challenging that the standard setting of batched, single-target domain, class-balanced TTA. The authors also motivate and illustrate this setting well (compare Figure 1)
the paper does a good job in empirically identifying problematic parts of existing TTA approaches in the ""wild"" setting, namely (i) batch-norm's implicit single-domain assumption and reliance on large batches and (ii) model collapse caused by large gradient norms.
Pragmatic solutions to these issues are identified that are not inherently novel but shown to be very efficient in the ""wild"" setting: (i) using group/layer norm instead of batch-norm and (ii) using SAR (SAM + entropy-based filtering + model recovery) to prevent model collapse.
Overall, the paper is strong on the empirical side and provides an extensive set of experiments on the different parts of the ""wild"" setting and the different components of the proposed wild TTA procedure.
It is laudable that runtimes of different TTA-procedures are compared in Table 1. I would recommend to use the same unit (seconds) for all methods though. One question: why has SAR nearly the same runtime as TENT? I would expect that the double backward pass causes it to be somewhat slower.
Weaknesses:
empirical evaluation is limited to target domains with common corruptions (e.g. ImageNet-C); investigation of other types of domain shifts like ImageNet-R would provide additional insights in terms of the generality of the proposed procedures.
the paper's comparison of normalization layers has a strong confounding factor in the overall model architecture: while batch-norm and group-norm are studied as part of a ResNet-50, layer-norm is studied as part of an ViT. Thus, a statement like ""GN models are more suitable for self-supervised TTT than LN models since TTT+VitBase-LN is sensitive to different sample orders and has large variances over different runs."" is problematic since it might not be that differences are due to the normalization layer (but rather to the model architectures). It would be preferable to remove the confounding factor of model architecture in this part.
The ablation study of SAR in Table 5 is nice but should be extended to cover all components of the wild setting and not only imbalanced labels
the model recovery scheme seems to be very helpful in the ViT-LN setting. To which extent would it also benefit TENT and EATA? That is: to which extent are benefits of SAR over TENT/EATA due to model recovery and not due to SAR?
not really a weakness, but a natural question arising would be: how would a model trained with SAM on the source domain perform in the comparisons given that SAM was found to be helpful at test-time?",7.25,3.75
79,"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning","TL;DR: We provide a theory to explain why ensemble and knowledge distillation work for Deep Learning. It matches practice well, while traditional theory such as boosting, random feature mappings or NTKs, cannot explain the same phenomena for DL.","Strength:
The authors present theoretical results showing that a single model is guaranteed to have 0 training error while having a high testing error with high probability; the ensemble can provably improve the testing accuracy.
The author shows that the ensemble can be efficiently distilled into a single model. This understanding is fundamentally different from the standard NTK settings.
The idea of ""multi-view data"" is intuitive and provides a natural and convincing explanation for various empirical observations for distillation.
Questions and weaknesses:
One major concern is that the supplementary material cannot be opened due to a file format error, and I'm not able to see the results of self-distillation and empirical evaluation, which seem to be important aspects of this work.
In the example of Section 2.3 and for a single model, why the network can only pick up one of the features in
v1,v2
with the 90% of the data, and only memorize the remaining 10% of the data, instead of learning from the other relevant feature in
v1,v2
? | Strength:
The novelty of the paper is quite strong. I believe the such analysis is new and sound.
There is quite a good match between theory and practice, especially the construction of the multiview dataset is an exciting and meaningful proxy for the practical case while being theory-friendly.
The writing of the main context is well organized. It is easy for the reader to get to know the most critical intuition of the theory.
I like the experiment used in this paper. It is concise but well supports the theory.
Weakness:
I am not able to open the supplementary of the paper but reviewing this paper, reading the appendix is very necessary and thus my reading of the appendix is via the arxiv. The structure of the appendix is good but I feel a more simplified overall technical/intuition is desirable.
To obtain such a theory, there are still a lot of 'artificial' in the problem setup such as the use of smoothed ReLU, and specific restrictions of the data distribution. Those slightly increase the gap between theory and practice. But this point does not downgrade my point, understanding deep learning in theory is very hard and this paper has done excellent work toward that. | Strength
The writing and presentation are good. It is a pleasure to read this paper.
The empirical results on neural tangent features and deep neural networks are novel and interesting. The authors then provide thorough explanations for them, which is appreciated.
The theoretical results are rigorous.
Weaknesses
We know new theories usually motivate new algorithms. I want to know if there are some potential future works to better exploit these theories. | Strengths:
The paper analyzed an underexplored problem of how ensemble works in deep learning methods. It pointed out contradictions when applying previous principles of traditional algorithms and shed light on understanding deep learning.
For the theory side, the analysis was rigorous with clear definitions and concrete examples to help better digest the theorems.
Weaknesses:
I am just curious whether the hypothesis of multi-view data structure can be extended to different data structures such as texts and graphs. Those data are very different from image data discussed in this paper and 'multi-view' might not be applicable to them.
It is not a good practice to include ""self-distillation"" in the title but defer all details about it to the appendix, which was somewhat misleading even though it was due to page limit.",9.0,3.5
80,Transfer NAS with Meta-learned Bayesian Surrogates,-''-,"Strengths
I think this work well addressed the main limitation of MetaD2A which is the meta-learning-based transferrable NAS method. As this paper said, the meta-learning-based predictor proposed by MetaD2A can be exploited to unseen datasets after once meta-training, which significantly reduces the search time for unseen datasets. However, MetaD2A only exploits the meta-learned predictor, can not adapt to the unseen dataset even if the unseen task provides few-shot samples. This work combines the BO method to tackle the problem of transferrable predictors by allowing them to reflect feedback from unseen tasks.
Weaknesses
I think while the contribution of this work in the structure of predictor is limited, this work assigned the part of the paper for them too much. For example, using both transformer-based set encoder and graph encoder together is almost same with MetaD2A, this paper described it as the figure and performed ablation study about that. I think it would be better to emphasis the difference between MetaD2A and this work or BO + predictor as a Figure. | Strengths
Different from (Lee et al., 2019), this paper introduces deep-kernel Gaussian Processes as a probabilistic performance surrogate and intends to adaptively update this surrogate using newly evaluated architectures on the test task.
This paper introduces BO to achieve query-efficient (i.e., search-efficient) optimization.
Weaknesses
From my understanding, this paper follows the idea of (Lee et al., 2019) (i.e., meta-learn the representation of datasets and architectures and then predict the performance of architectures based on this learned representation) and only makes a few changes to the method, i.e., replacing its deterministic performance predictor with a probabilistic performance surrogate (i.e., the deep-kernel Gaussian Processes) and using BO for the optimization based on existing works. In this view, this paper mainly combines techniques from different fields for its method and therefore the novelty of this paper is kind of weak to me.
In the abstract and introduction section, this paper doesn't provide a clear and detailed demonstration of its motivations, which not only makes me feel confused about why this paper is needed in the literature but also makes the contributions and the importance of this paper less convincing to me. Specifically, this paper only gives a brief introduction to the line of NAS using transferred information across datasets without elaborating more on why these existing works can not well satisfy the demand in practice and why the method in this paper is needed. Moreover, the introduction section of this paper lacks a comparison with its most related work (Lee et al., 2019) to justify the necessity of proposing the method in this paper.
Some claims of this paper are not well supported. For example, in the related work section, this paper does not show (a) why the missing trade-off between exploration and exploitation in (Lee et al., 2019) is important and desirable for NAS using transferred information across datasets, and (b) why deep-kernel Gaussian Processes (GP) rather than GPs using the existing kernels are needed in this paper. Moreover, the result of Table 2 in this paper (a) can not unveil the drawbacks of MetaD2A, i.e., the missing exploration vs. exploitation tradeoff and the non-adaptive update in the optimization process and (b) may not support that these drawbacks lead to the stagnated performance of MetaD2A (located at the ""Results for Hypothesis 2"" of Sec. 5).
While this paper claims that the method in this paper can adapt to function evaluations on the test task, Sec. 3 can not well support it because of the missing details of BO. I highly recommend the authors provide the details of how BO works to further justify this point.
The empirical results in this paper show that in most cases the proposed method can only marginally improve over MetaD2A. More experiments that can distinguish these two methods may help to justify the superiority of this newly proposed method.
Questions
What's the training cost for w in your proposed method? While there is only marginal improvement over existing NAS from scratch in Figure 2, the total cost (including the training cost for w and the search cost in the x-axis of Figure 2) for the method in this paper may become a problem in practice since it may be more time-consuming.
As the w in this paper is trained based on the meta-datasets M, how does this method perform for the NAS problem with out-of-distribution datasets? Will it become a big issue for the proposed method? Because when only considering in-distribution datasets, directly transferring the selected architectures from other datasets can already provide competitive performances with compelling search costs as shown in Figure 2. So, the proposed method in this paper may not be necessary for this scenario. Meanwhile, if the proposed method can not guarantee its performance for the out-of-distribution datasets, this may also hinder the application of this method in practice. | Strengths
The motivation of this paper is great, the idea of transfer previous knowledge is straightforward and lack of exploration. To my knowledge, there are previous methods trying to learn from history [1], but using on NAS is novel.
The results on NAS-Bench-201 are pretty robust across six datasets, and it's good to see from the ablation study that both the architecture and the dataset embedding have contributions to the final performance.
Weaknesses
When encounter a new dataset / architecture space, how do you propose a new architecture? Do you need to traverse lots of architectures and compare their score? I don't see discussions of this part in the paper.
Lack of comparison to predictor based methods [2, 3], which also learns an architecture embedding and directly predict the accuracy. It could be better if the authors can show that learning the Gaussian kernel function rivals.
For the learning process of the architecture / dataset embedding, do you just leverage the previous checkpoints or train them by yourselves? If it does, do you include the training cost into the NAS search cost?
Another concern is that all the experiments are shown in the NAS-Bench-201 space, which is pretty small. Is it possible to show the result on larger space, e.g., DARTS space. This can also show that the learnt embedding can transfer across spaces.
[1] Yang et al. ""OBOE: Collaborative filtering for AutoML model selection."" KDD 2019.
[2] Yan et al. ""Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?"" NeurIPS 2020.
[3] Ning et al. ""A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS."" ECCV 2020.",7.333333333333333,3.6666666666666665
81,Transformers Learn Shortcuts to Automata,"Keywords: Transformer, self-attention, group theory, semigroup theory, algebraic automata theory, shortcut learning, theory of deep learning","I am not expert in Computational Theory and Complexity Theory.
Pros
The theoretical contribution is dense but clear
The theorems seem sound
Experimental results complement theoretical results
Cons
Could be published at a conference but should be submitted at a review
Experimental protocols should be made more precise
I am not so convinced by the lessons drawn from the experiments | Strengths: very interesting question and approach, solidly grounded in theoretical computer science, clearly written, strong results.
Weaknesses: the scope and complexity of the ideas make them hard to fully convey in a paper satisfying the length constraints, even with the supplementary material. In particular a lengthier comparison with the related works (and those in section A.5) would be of great interest. We hope the authors will write a longer treatment elsewhere. | This is a highly unusual ICLR paper. This is not to say that ICLR is an inappropriate venue for this work, since this is clearly not the case: the subject matter is of direct and high importance to the machine learning community. This work is unusual because of the breadth and depth of ideas and connections to different parts of the theoretical computer science literature, as well as very topical considerations to the ICLR community such as out-of-distribution generalization, spurious correlations, and neural algorithmic reasoning. For instance, the results presented in this paper are formulated in terms of the Transformer’s ability to simulate automata—-a now classical concept in AI (albeit being essentially conceptually the same as recurrent models). The analysis tools are also unusual, with many algebraic arguments used, and results from classical complexity theory are drawn upon.
A few brief strengths:
A complete set of theoretical results, including general results for any arbitrary automata, sharper bounds for certain (solvable) automata, even sharper bounds for a particular class of automata (grid words), and lower bound results.
Experimental results validating a number of these findings on a number of algorithmic reasoning tasks, taking pains to use the experiments as a chance to bridge some of the inevitable gaps between the theory and practice (such as checking whether gradient-based training actually finds the shortcuts the theory identifies).
Taking care to at least discuss a number of questions a reader might have whilst reading this work, such as the connections to universal approximation,
No weaknesses come to mind, and I do not wish to work to artificially come up with problems. That’s not to say that the work is perfect, of course it isn’t, but I have no major concerns about the validity or significance of the results.",8.0,3.3333333333333335
82,Transformers are Sample-Efficient World Models,"Keywords: deep learning, reinforcement learning, model-based reinforcement learning, world models, learning in imagination, transformers, discrete autoencoders, generative modeling, sequence modeling","Strengths:
Simple and easy-to-understand learning algorithm.
Well-thought-through reuse of standard modeling and algorithmic components.
Outstanding results on Atari 100K benchmark, in particular, bearing in mind the limited compute and expressivity of models.
Weaknesses:
Models and agents are trained per game. I am curious what would happen if one tries to train a multi-game world model and agent in the spirit of Gato https://arxiv.org/abs/2205.06175 or Multi-game decision transformers https://arxiv.org/abs/2205.15241.
Nit: the paper may benefit from using more modest language. I mean, in particular, the “drastically different architecture” claim or not very elegant comparisons to look-ahead algorithms.
A bug that may turn into a feature:
The method needs “more tokens” for games that require the detailed representation of images (as authors put it: “Another kind of games difficult to simulate are mazes with moving enemies, such as MsPacman, BankHeist, and Alien”). This characteristic seems to be a limitation but may lead to an interesting long-context benchmark. Some environments may require more frames and details, leading to a new dataset that can be used to benchmark long-context transformer models. | Strength:
Using Transformers as world models in model-based RL seems novel.
The results are promising.
Weaknesses:
Although it is emphasized in the paper that sample efficiency should be a main consideration for model-based RL methods, I think training for a longer time and show that the proposed methods can achieve good results on most games would also be worth considering to verify the proposed models.
The objectives of training the autoencoder and Transformer is missing in the paper and in the appendix (only mentioned by a few sentences). It would be better to also include those settings in the appendix. | [Pros]
The paper is well-writen and clear to read. the effieciecency and effacacy are good on the atari100k benchmark.
The code is clear and easy to follow.
[Cons]
Noveltiy is insufficient (Some disucussion to [1][2][3] would be appreciated)
Increasing the number of ablations and analyses to improve comprehension of the contributions of various designed parts.(e.g. different tokenizer, backbone)
[1] TRANSDREAMER: REINFORCEMENT LEARNING WITH TRANSFORMER WORLD MODELS https://arxiv.org/pdf/2202.09481.pdf [2] Reinforcement Learning with Action-Free Pre-Training from Videos, https://arxiv.org/pdf/2203.13880.pdf [3] Online Decision Transformer, https://arxiv.org/pdf/2202.05607.pdf
*Some typos: sec2.2 blue arrow-> purple? arrow | Strengths
Loosely speaking, existing successful MBRL algorithms are modifications of Dreamer or MuZero. Showing that another framework for MBRL can be successful is a valuable contribution.
The submission largely follows the guidelines for benchmarking laid out by the statistical precipice paper.
Weaknesses
At least in my opinion, Atari 100k is not particularly well benchmarked. I would say that the only ""good"" algorithm that has been benchmarked by experimenters incentivized to tune the algorithm to maximize performance is EfficientZero. Thus, the extent to which IRIS is good is a bit unclear to me. It would be interesting to see how IRIS performs on the standard Atari benchmark (or, inversely, how something like Dreamer compares to IRIS on Atari 100k).
The statistical precipice paper suggests that results on Atari 100k are reliable (under appropriate metrics) with as few as 10 runs; the submission only uses 5 runs. My gut feeling is that the margin of improvement under various metrics seems substantial enough that it would probably continue to hold under a more reliable number of runs, but it would be good to actually show this.
Addendum: I also concur with reviewer t9Kq: 1) the submission would benefit from additional attention to related work (such as [1],[2],[3]) and 2) additional ablations.
Comments on decision-time planning:
The submission argues: ""Moreover, IRIS could be combined with MCTS, both in imagination and in the real environment. Therefore, methods involving lookahead search should not be seen as direct competitors but rather as potential extensions to learning-only methods."" In principle, this is of course true. However, as a matter of practice, I am not sure it is as clear. I am not aware of any examples of a non-MuZero-like architecture successfully utilizing decision-time planning. It is plausible to me that algorithms like Dreamer and IRIS, which perform well in a background planning regime, may not enjoy much benefit from decision-time MCTS.
Comments on superhuman performance:
Most notably, human experts were surpassed by deep RL algorithms in a multitude of arcade (Mnih et al., 2015; Schrittwieser et al., 2020; Hafner et al., 2021), real-time strategy (Vinyals et al., 2019; Berner et al., 2019), board (Silver et al., 2016; 2018; Schrittwieser et al., 2020) and imperfect information (Schmid et al., 2021; Brown et al., 2020a) games.
I find this sentence is misleading. Deep RL algorithms have not surpassed human experts in most Atari games, as is clearly evidenced by the human world record metric. They have also not surpassed human experts in StarCraft (AlphaStar is only grandmaster level) or DOTA (OpenAI5 played a restricted version of the game and was found to be reliably exploitable by humans).",8.0,4.0
83,Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching,"Keywords: few-shot learning, dense prediction tasks","Strengths:
(1) The problem of few-shot multi-tasking for dense prediction is important and unexplored.
(2) The proposed approach is simple and intuitive, without task-specific inductive biases or extreme computational complexity. The experiments show that it works well.
(3) The writing is clear and helps the understanding of the motivation, desiderata, solution.
Weaknesses:
There are some discussions/comparisons that are missing.
(1) Could you please explain the technical differences between the w/o Matching baseline and DPT? Equivalently, if we add a support set on DPT, how different is this from the proposed model?
(2) The proposed architecture is not specifically designed for few-shot settings. In fact, if we compare the w/o Matching variant from Table 2 to the results of the baselines in Table 1, it does pretty well. This is good but also raises the question of how much the fast generalization is a result of the powerful architecture vs the multi-tasking meta-training. These additional experiments would shed light on this: (a) What is the performance of the proposed model if trained fully-supervised? Does it scale well when a lot of data is there? How does it compare to DPT? (b) How does out-of-the-box DPT work after few-shot finetuning?
(3) From Table 2 it appears that fine-tuning is very important. This is fine as long as we care about the performance on the novel tasks only. But once finetuned, the model could forget the training tasks. This makes the approach less significant from the point of continual learning. Could you evaluate the forgetting, i.e. what is the performance on the training tasks after finetuning on the novel tasks?
(4) Training for more tasks jointly is a harder job for the model but makes the dependency on the support set stronger. How important is that aspect for the proposed model? If you train on 5 tasks and test on 5 vs 8-2 vs 2-8, how does the performance change? It would be nice if you keep 2 novel tasks as fixed and then train the model on 2, 5 and 8 tasks and show how the performance on the two held-out tasks changes.
Questions:
(a) Is the whole support set used in a forward pass? That's fine in 10-shot, but when you have 275 support images, do you still feed all of them at the same time? Would it make sense to finetune on 275 support images but use only a fixed set of them during inference? In that case this approach could scale to large datasets as well.
(b) Why do you think that you need to finetune the model? Isn't the support set enough to provide the task description? Could somehow the support set predict the task-specific parameters to use? | Strengths:
The idea is novel and intuitive. I very much enjoy many designs of the architecture. Just to list a few:
The image encoder has shared and separate portion of parameters across different tasks. Leveraging the bias-tuning idea, the few-shot learning optimizing biases should be very efficient.
Multi-channel labels are split into single-channel ones. This design enables flexible joint learning of multiple tasks
The token matching module provides a straightforward way of aggregating labels from the support set. It is also multi-resolutional to support learned aggregation on different abstraction levels.
The results are impressive. VTM outperforms few-shot learning baselines significantly. The qualitative results also appear to be better than baselines.
The paper is well-written and designs are well-motivated.
Weaknesses:
Limitation on model architecture. The method requires a same architecture for all tasks. This design may lead to suboptimal architecture for different tasks. I don't see an easy way to enable flexibility of architecture for different tasks within the proposed paradigm.
Limitation on training data. I could be wrong on my understanding of the experimental setting but apparently in the meta training, labels of 8 training tasks are all used for each image example. Although Taskonomy supports such a setting with comprehensive labels, in the real world it could be too expensive to obtain so many dense labels for each image. Have you tried an alternative setting, where in the meta training stage, each training image only comes with one type of annotation? This setting will be more realistic and can be extended to real-world datasets by combining multiple datasets for different dense understanding tasks. | This paper was clear, presented an interesting new problem and has excellent results. I much enjoyed reading it. Questions and comments I had are below:
What is the computational cost of the system, compared to a fully-supervised model (DPT) inference? Given already-encoded support set features, what the cost to execute one query inference, including query image encoding and matching?
Was any loss or gradient balancing needed between task types (between the different loss measures), and how was this applied?
While the motivation of nearest-neighbor lookup is clear, I think the multiheaded attention is more sophisticated than simple NN on key to label tokens, particularly with multi-scale features, as it is here. Since each head outputs a projection of values, which are linearly combined, based on a projection of keys and queries, different parts of different support images can be used for different components of the label token. In particular, there can be differences in scale: Low frequency parts of the token requiring wide contexts can match to one relevant set of support features, while high frequency edge transitions requiring local context for the same query prediction and task might match to other support features, even from support images having completely different global context or layout. How much have you seen this occur, and how important is the number of heads in the MHA matching? What other sorts of components have this behavior? Some discussion of this would be nice.
intro sec 1: ""the model learns the similarity in image patches that capture the similarity in label patches"": This seems it would require a relatively large number of tasks of different types, in order to cover differently relevant ways of matching, and indeed the 80/20 5-fold cross-eval does seem to supply a fair number. What about varying the number training task types --- in what ways does the method fail?
sec 5.2: discussion on why the method outperforms few-shot semantic segmentation baselines: the two points here, while likely large factors for most tasks, don't explain the performance difference for semantic segmentation task, where the model described in the paper also performs better. Do you have any ideas about this difference, and can it explain a large portion of the gains for other tasks as well?
Decomposing the problem to single-channel output tasks also may have a factorization effect, that stands to improve performance --- does it?
Are biases in the encoder tuned for each output channel separately, or jointly for all output channels in the test task (for multi-channel tasks like SN)",9.333333333333334,3.6666666666666665
84,View Synthesis with Sculpted Neural Points,-''-,"Strength:
The proposed point sculpting technique is novel. The method achieves good performance on all benchmarked datasets and has faster training and inference speed when compared to NeRF or other point-based baselines.
Using spherical harmonics in the feature space and eliminating the need for a dir encoding MLP seems to be a novel but useful technique.
Weaknesses:
According to Appendix B.2, the authors applied different MVS networks (at least different weights) when evaluating on different datasets. Especially, when evaluating on LLFF data, they finetune the MVS model scene-by-scene. This may cause concern/confusion because 1) the time for COLMAP and scene-by-scene fine-tuning should be taken into account when comparing, rendering the method less efficient for these scenes; 2) it is unclear why not using COLMAP point clouds directly. The authors should clarify these and ideally use a same generalized MVS model for a fair comparison.
Pulsar is a very related direct baseline of this method. However, a comparison with Pulsar seems to be missing in this paper.
The intuition behind employing a U-Net for rendering is unclear. Is it possible to use per-pixel MLP to render the features (MLP: feat -> RGB)? Is the U-Net pre-trained and weight-shared across datasets? | Strength
The proposed point sculpting can effectively improve the completness of the reconstruction.
The proposed point dropout layer can alleviate the over-fitting problem by giving all points a chane to get optimized even if they are covered by others in the input views.
The proposed high-dimensional spherical harmonics point features can effectively capture non-Lamberian visual effects.
The proposed method is 100 times faster than the popular NeRF approach in rendering.
The proposed method can produce better visual quality than NeRF and other point-based methods.
The proposed method achieves SOTA in terms of SSIM and LPIPS metrics.
The proposed method allows easy fine-grained scene editing due to its point-based representation.
Weaknesses
Examples in the Appendix show that the proposed method is good at capturing complex textures and strong non-Lambertian effects, but tends to produce over-smoothed results for tiny structures. This may suggest that the proposed spherical harmonics point features are effective in modeling the view-dependent appearance of the scene but the point-based representation may not be good (dense) enough to capture fine scene geometry. The over-smoothed results may also be related to the use of U-Net for final rendering. Finally, the averaging of 2 results generated from 2 random point subsets may also contribute to the problem. More analysis should be carried out regarding this issue.
In 3.2, it was mentioned that U-Net was employed to convert the 2D feature map F of a target veiw into RGB image. However, in 4.2 (point adding step), it was mentioned that the point features were treated directly as RGB values during rasterization without using U-Net in optimizing the existing point. Please clarify how can the features be treated directly as RGB values in this step.",7.0,4.5
85,Visual Classification via Description from Large Language Models,"Keywords: vision-language models, CLIP, prompting, GPT-3, large language models, zero-shot recognition, multimodal","Strength:
This idea is quite novel and intuitive.
The experiments have clearly demonstrated the benefits of the proposed methods.
Weakness:
the aesthetics of the figures could be further improved. some potential questions were not answered
generalization: how this generalizes to other tools other than CLIP and GPT?
how do you weight the prob. of a set of features linked with ""or"" logic? for example, in the appendix, the lemur could be ""black, grey, white, brown, or red-brown"", do you just sum these prob up? A better way might be using max? | Strengths:
The paper is generally well written and a fun read. It has a interesting experiments and shows that interesting results in explainability can be achieved almost for free from manual intervention by harnessing GPT-3 descriptions and models in the mould of CLIP.
A nice set of interesting experiments showing that the CLIP results on ImageNet can be improved upon from text descriptions automatically obtained from GPT-3 as opposed to the manually specified in the original CLIP paper. There is just one reservation here in that it is not clear which text prompts were used for the CLIP results in table 1.
Weaknesses:
The original CLIP paper has already demonstrated that prompt engineering can have a significant impact on the zero-shot recognition results. So this finding in the paper is not new.
The experimental evidence presented is nice. But some of these experiments are a little sparse and a little artificial. This is especially true for the ""acquiring and utilizing novel information"" learning experiments. It is fun to see that ""Wordle"" and ""Ever Given"" images can be recognised from their GPT-3 descriptions but it is obviously far from a complete study of the model's capabilities on this front and it is a bit unclear what it demonstrates. ""Ever Given"" is a proper noun and the name of a specific container ship. It is not surprising that CLIP cannot do image retrieval based on a proper noun which was generally unknown at the time of training CLIP.
The paper mentions that very little in the way of prompt engineering is done. The engineering that is performed is mainly done to ensure a list of descriptions is created by GPT-3. How sensitive are the results to the temperature and max token length used in the language model? Were the default values used or were they set using some form of hyper-parameter sweep.
There are some technical details missing or brushed over. So for example:
In equation (1) how exactly is \phi(d, x) computed? Is a cos similarity used, does it use a softmax to compute the probabilities, etc...?
For table 1 for the CLIP column what is the text prompt used to acquire the classification? Is it just a classname or does it use the prompt ""A photo of a {label}"" as suggested in the CLIP paper as the basic prompt that boosts performance over just using the classname.
In figure 1 and figure 4 there are bar charts showing the compatibility between a text description and an image. What exactly are the numbers being displayed? Also the horizontal scaling seems to be different between the different examples and thus gives an inconsistent visual cue via the length of the bars.
Figure 4 gives some qualitative results for the explainability for particular images. But it would be nice to have some more quantitative measures of how well the explainability is doing or thoughts on how to do this or show which explanations are most useful for each class etc... Also would some calibration be required to decide which descriptions are actually ""valid/present"" for a given image. | Strengths:
The proposed idea is simple and well-executed. The paper is also well-written.
Visual-prompt-engineering can be a heuristic and tedious process. It is refreshing to see that LLMs can help to automate this process. The prompt for GPT-3 is also well-designed to guide the model in generating visual descriptors.
The method provides interesting additional capabilities such as explainability and novel category classification.
Weakness:
Could the proposed prompt-generation method generalize to other domains other than common objects and animals? It is necessary to evaluate on more datasets as in the CLIP paper, such as food/scene/satellite/medical images. In order for the method to be practical, it should at least not be harmful on the more difficult domains.
It should be made clearer (rather than in the footnote) that the CLIP results in Table 1 uses a single base prompt, which I assume is ""a photo of {class_name}""? Does the proposed method include the same base prompt with class_name?
It is suggested to include the result for the hand-crafted CLIP prompts in Table 1.
There exist other methods to construct visual descriptors that can offer similar advantages such as explainability. For example, using WordNet (K-LITE) or wikipedia articles [a]. It would be good to see some comparison between descriptors generated by GPT-3 and descriptors retrieved from knowledge base.
[a] Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions. Bujwid and Sullivan. ACL 2021.
Minor suggestions:
One of the entry in Table 1 has an inconsistent number of digits.",8.0,4.0
86,"When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?","Keywords: vision-language models, clip, contrastive learning, retrieval, vision-language pretraining, multimodal representation learning","There is no major weakness. I will just note two issues I noticed which do not affect my rating of the paper.
In terms of the three challenges presented in the paper, it seems they can be addressed by explicitly considering how the challenge is formed (e.g., adding shuffled captions during training as hard negatives). However, this does not imply that the problem is solved. This reminds me of what is discussed in Teney et al., 2020, where once we know how the “challenge” dataset is constructed, we could use such information to train a model that performs well on the challenge dataset; but the model will likely fail on cases not foreseen during dataset creation.
I wonder what should be the intended use of the benchmark. If simply adding shuffled captions can solve the challenges pretty well, what should be the next move?
Teney et al., On the Value of Out-of-Distribution Testing: An Example of Goodhart's Law.
I would have appreciated more discussions on relation to negative mining and contrastive learning.
Minor comments:
Most tables are in the appendix. While figures send a strong message, I would appreciate having a few tables in the main paper.
NegCLIP has two improvements: generating neg captions and sampling hard-neg images. Is there an ablation study on this?
What’s the batch size of fine-tuning CLIP? | Strengths:
The discussion of the problem and presentation of the dataset and solution is extremely clear; the paper is very well-written.
The paper presents a clear problem with existing systems and the drawback of using retrieval as an evaluation method. The presented benchmark is offering a ""bare minimum"" sort of evaluation for such systems.
Weaknesses:
The fonts in some of the figures is really small. I also strongly suggest putting numbers on the actual bar charts; it's difficult to interpret the results otherwise.
The numbers for the experiment in 2.3 (evaluating COCO/Flickr30k on perturbations of captions) should be in the main paper, not the appendix.
It is suggested that the reason these models ignore word ordering so much is that they really are trained as keyword identifiers, as required for image retrieval, and there's no reason to learn ordering. However, what happens if you incorporate better priors on the captions? I would imagine that a large language model would place very low probability on most of the perturbed captions (I could be wrong, though), and a VLM that uses features from a general large language model would be able to distinguish the obviously grammatically incorrect examples from the true caption.
I was hoping the dataset would be a bit more of a scaled-up Winoground dataset, because Winoground directly tests all four settings (perturbation of relations in text and perturbation of relations in image). However, this evaluation set only seems to test perturbation of relations in text.
Questions:
Why are there so few relations and attribute pairs in ARO, as described in Section 2.1?
Practically, how are some of the perturbations done? With operations on top of the parse tree?
Does the experiment in Section 4 backprop through both text and image features in CLIP? | Strength:
This paper is well-written and easy to follow.
The discovery that existing visual-language models could not handle complex relationship understanding seems to be well-supported.
Weakness:
The proposed benchmark seems to be very similar as the Winoground, which is also cited in the related work. This, to some extent, limits the novelty of this work to the community. Could authors test the proposed composition-aware hard negatives on the Winoground to see if the method can improve these natural images capturing object relationship?
The proposed composition-aware hard negatives needs to know the targeted composition beforehand, which greatly limits the generalization and the potential effectiveness of this work.
""Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation"" by Qin et al is closely related to this work and should be cited in Related work. | Strengths: (1) Bring up an interesting and critical issue of not sensitive to compositional order of the current large VL models (2) Four new tasks based on existing dataset that do not need human annotations to validating the aforementioned issue (3) a simple yet effective treatment to the issue by adding compositional-aware hard examples (4) Good results on the newly proposed tasks
Weakness: (1) Hard to control each example in the 4 tasks, for example, swapping adj for the caption ""a cute girl is walking a little dog"", it would be good to have human check if the semantic is actually not valid",7.0,4.0
87,WikiWhy: Answering and Explaining Cause-and-Effect Questions,"Keywords: NLP, Question Answering, LLM, Dataset, Explanation","Strengths:
The paper introduces a high quality resource that fills a gap regarding evaluation of LLM’s ability to not only provide answers about causes and effects but also explain why using a wide variety of knowledge.
The paper is well-written and provides clear descriptions of how the data was collected, providing enough information to verify the quality of the resource.
The benchmarking experiments convincingly demonstrate the difficulty of the task. This resource could be a key driver for research into this problem space. | Strength:
It is the first work that investigates Q&A over cause-effect reasoning with explanations.
The authors create a consolidated pipeline to define the problem and build the dataset, resulting a WIKIWHY dataset covering 11 topics. -The authors propose new evaluation metrics for this cause-and-effect question answering task. Experiments verifies the reasoning ability of LLMs.
Weakness:
The problem formulation in which a sequence or a set of explanations are required make this task intrinsically difficult to evaluate. | Strong
The Cause and Effect QA with text generation problem is interesting (WikiWhy tasks 2 and 3)
The automatic evaluation metric proposed is interesting.
Weak
(Minor) Table 1 has messy formatting. The Size and Topic columns should be right-aligned to support easy comparisons.
Some hyper-parameters are missing (see the Clarity, Quality, Novelty, and Reproducibility section below).
(Minor) The Conclusion states, ""With this paper, we release WIKIWHY, a Question Answering dataset interested in understanding and facilitating the improvement of LLMs’ reasoning capability."" The word ""interested"" is a strange choice for this sentence. I recommend rephrasing.",7.333333333333333,3.3333333333333335
88,Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms,"Keywords: Optimization acceleration in deep learning, network optimizers, deep learning optimizer, deep learning algorithm","Strength: The paper is clear and well written. It presents an acceleration algorithm and apply to Adam, AdamW, LAMB and SGD. It has both theretical analysis and experiments to support the claims. The idea introduced is nice.
Weaknesses:
Better to include more comparisons with other methods, such as the line search methods.
To consider the s_k norm seems not necessary. What if s_k is partial, like 1/2? | Strength:
The incorporation of the PPM regularization term with a recently-proposed formulation of nesterov-type acceleration is novel and seems to be useful.
There proposed Win acceleration can be added to several different types of optimizers (e.g., Adam, SGD, AdamW, LAMB).
The authors provide convergence guarantees in the non convex setting for their acceleration technique using both Adam and AdamW. These guarantees improve upon prior work to the best of my knowledge.
Experiments are performed on a wide variety of architectures (CNNs, ViTs, LSTMs, transformers). X-Win improves upon baseline optimization techniques in nearly all cases.
The proposed acceleration technique seems to reduce the need for hyperparameter tuning.
Weakness:
The discussion of adaptive optimization techniques is lacking. It is widely recognized that adaptive methods suffer poor generalization relative to SGDM in certain settings (see “The marginal value of adaptive gradient methods in machine learning” and “Towards theoretically understanding why sgd generalizes better than adam in deep learning.”). This has led to research in alternative optimizers like AMSGrad, Yogi, M-SVAG, etc. The authors mention AdamW, but do not mention the rest of this work or sufficiently address the generalization ailments of adaptive methods. I think because of these issues, claiming that optimizers like Adam/AdamW are “default optimizers to train DNNs” is somewhat problematic/incorrect. SGDM is still the default choice for most computer vision tasks, where event recent adaptive algorithms like AdamW achieve inferior performance.
The description of the proposed method (especially in the introduction) is lacking in clarity. I believe the paper could be greatly improved if the authors spend time to improve the clarity of these initial explanations of their technique.
No discussion of how the computational complexity of the proposed method compares to the other methods (maybe I missed it?). You need to state this comparison, do we incur extra cost by adding Win?
Small stuff:
Small writing errors: beginning of Section 3 (“accelerating and also stabilizing optimization”, “At below”), end of section 3.1 (shouldn’t “bias correlation” be “bias correction”?).
Putting AdamW, Adam, and LAMB variants all into Algorithm 1 is a bit confusing, though I’m not sure if there is a better way to present this.
It is not completely clear to me from your description how Win+LAMB compares to vanilla LAMB. Making this comparison a bit more directly would be helpful in my opinion.
It is a bit weird that Table 1 appears after Table 2 in the writing.
For the loss plots (Fig. 1), I’m not sure whether your method actually converges faster, or if it simply converges to a better loss. In some cases, the convergence does appear to be faster, but in others it converges at (seemingly) the same to a better loss. It would be helpful to provide a similar plot with some of the other baseline optimizers (e.g., PAdam, RAdam, Adabelief, etc.) if it is does not require too much extra effort. | The technique presented is well motivated theoretically, and is straightforward to implement. The experiments are quite diverse and show promising improvements across the board.
The organization of the paper could be a bit better --- right now the authors present the fully general technique and then specialize it to various algorithms, it might be better to present the technique for various algorithms one at a time, starting with SGD, which is the simplest algorithm.
The combination of weight decay and Nesterov momentum has been applied in other optimizers already (LARS, Shampoo, several others).",8.0,3.6666666666666665
89,​​What learning algorithm is in-context learning? Investigations with linear models,"Keywords: in-context learning, transformers, sequence models, deep learning, meta learning","Strengths: This paper appears to be the first to investigate the neural mechansisms underpinning in-context learning for the very simple but non-trivial hypothesis class of linear regression models. The authors' finding that LLMs seem to mimic sensible and even Bayes-optimal learning algorithms is surprising and will surely motivate further research into understanding the power and limits of in-context learning.
Weaknesses: As the authors admit, their findings are suggestive but the exact mechanisms of in-context learning for linear regression are still not understood.
Additionally, there were some experiments I was surprised the authors did not perform:
examining how the dimensionality of the regression problem affects the learning algorithm derived for an LLM with a given capacity.
determining whether the LLM derives Bayes optimal learning rule for other choices of p(w) and p(x). In particular, ridge regression is the correct choice for the p(w) used. But would the LLM mimic Lasso regression if p(w) was a Laplace prior over the regression coefficients? | Strength
The paper provides theoretical insights on how ICL implements learning algorithms. This can be used to introduce new paradigm for training ICL
The paper is supported by experimental analysis on simulated data related to linear regression to validate the theoretical results
The paper investigated different scenario such as the case of noisy data
Weakness
The paper derives the results for a single step of larger iterative algorithm
The presented results are derived for the case of linear regression and it is unclear how it extends to non-linear problems.
The experiments are conducted on toy datasets with normal distributions.
The probing experiments are not very conclusive | (The requests from the authors during the rebuttal phase is tagged below.)
STRENGTHS:
Scope and relevance: Given the surge of interest in in-context learning, this paper is very timely in taking on a very important topic.
Significance of contributions: The paper does a great job illustrating how a transformer might implement an iterative optimizer in its weights. While I expect this work not to be the final word in how a transformer architecture might learn to be a linear function approximator, the questions asked by the authors seem to be the right ones and the provided answers seem novel and meaningful.
Clarity: The main body of the paper is written very well.
""RAW"" operator is useful: While not discussed in the main body of the paper, the raw operator defined and used in the appendix for the constructions seem like a quite neat way to think about constructive proofs for transformers.
WEAKNESSES:
Dependence on the GELU activations: If I'm not mistaken, the authors make use of the numerics of the GELU activation to show how a the feed-forward layers can implement the dot product and matrix multiplication operations. I have two concerns about this:
It's not clear how small the activations need to be for the approximation to hold. (REQUEST) Would it be possible to add a plot in the appendix that shows how well
x∗x=x2
is approximated with the GELU approximation? The x-axis could be the values between
−0.1
to
+0.1
, and the y-axis would be the error in the approximation, or the proportion of the error to the x value.
Transformers that don't use this activation also display ICL capabilities, including the ability to be linear function approximators (happy to be corrected about this - perhaps GELU is much more important than I anticipate).
In light of this, I'm not sure if the construction of
mul
truly represent what the transformers might be learning (happy to be convinced otherwise during the rebuttal) . (REQUEST) Perhaps it'd be good to at least briefly discuss the dependence on GELU to get the
mul
construction in the discussion section following Theorem 2?
Mechanistic interpretability results lack control groups: As the authors acknowledge, interpreting probing results is often tough. Hence, the results presented in Section 5 appear to be speculative as of now. (This limitation is acknowledged by the authors). (REQUEST) One approach to strengthen the results would be to produce Figure 4 (left and right) with randomly initialized, slightly trained and fully trained (but poorly performing in ICL) transformers. If these also yield comparable loss values, then discussion of the results of Section 5 should be reconsidered.
Making Figure 2 denser to confirm a claim: The authors claim that for all values of
σ2
and
τ2
, the right parameter that gives the best fit also is the one that minimizes Bayes risk. Figure 2, which seems to be what this sentence is based on, is a bit too sparse to really reach this conclusion. If it's possible, it would be great to make the plots a bit denser to see the trends a bit more clearly. That is, sample tau and sigma a bit denser to add more measurement points? Marking the MSPD between the transformer and the predictor using the optimal ridge parameter with a different marker for each value
τ
and
σ
on the plots could also bring extra clarity perhaps? (nitpick) Lastly, perhaps a colormap that has a gradient determined by the value of the ridge parameter could also make the plot easier to read.
Ambiguity in Figure 4 (right): Figure 4 (right) isn't very clear to me. What are the x and y axes showing?
QUESTIONS TO AUTHORS: (a bunch of REQUESTS):
Position embeddings: What kind of position embeddings did you use? (REQUEST) Space permitting, could you briefly discuss how you expect the choice of position embeddings to affect ICL result? If I'm not mistaken, the derivations in the appendix assume absolute position embeddings a'la the original transformer paper.
Gradient descent with n steps as another benchmark predictor: I was wondering if you considered observing whether a few steps of gradient descent (not just a single step) matches ICL performance of medium-shallow transformers? If the initial weights are initialized with zeros, gradient descent will presumably consistently lead to an increase in the norm of the implied weight vector. Since having an L2 penalty on the weight vector also has a similar effect, perhaps this is why ICL performance of medium-shallow transformers are matched well with the predictors with a ridge term. (Analogously, perhaps gradient descent with a couple of steps has low MSPD with ridge regression with an appropriately chosen lambda?)
MLP used in the probe experiments: Could you give further details regarding the MLP probe used in Section 5?",8.0,4.0
