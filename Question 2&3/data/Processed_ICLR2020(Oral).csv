,Title,Keywords,Strength_Weakness,Review_Rating
0,A Closer Look at Deep Policy Gradients,...,"Review: This is an interesting and important paper, it emphasizes and analyzes how policy gradient methods modify their objective functions and how this leads to training differences (and often errors w.r.t. the true objective). I have some minor comments on terminology used that I would like to see properly defined within the paper, but otherwise believe this should be accepted for its useful insights.
Assorted Comments:
+ Maybe I simply have a difference of opinion or have misunderstood, but I am hesitant to agree that the work is comparing the surrogate *reward* function, but rather the surrogate objective. You'll notice that in the TRPO paper, it is called a surrogate objective not a surrogate reward: https://arxiv.org/pdf/1502.05477.pdf .
+ I think better specification of what exactly is being plotted (pointing to an equation) or defining very concretely what is a surrogate reward or true reward (which I suspect is the objective) will make this paper much clearer.
+ In fact, it was a bit unclear whether the comparisons were of the sampled/observed reward function R(s,a) (provided by the environment and sampling regime) or the objective function often the advantage A(s,a) (or the surrogate objective, GAE, etc.) I assume it should be the latter, but the wording of the paper makes this a bit unclear. I suggest discussing things in terms of objectives not rewards -- unless in fact the paper does approximate reward functions in which case this should be specified in much more detail.
+ Also, in a lot of places it seems like there's a mixup between rewards and returns. I think typically in the literature reward = r_t and return = V_t (sum of reward). Perhaps, in places the paper truly speaks of rewards, but from the context it seems as though it mainly refers to returns. Examples: "" Evidently (since the agent attains a high reward) these estimates are sufficient to consistently improve reward"" "" This is in spite of the fact that our agents continually improve throughout training, and attain nowhere near the maximum reward possible on each task"" | Review:
[Summary]
This paper empirically studies the behavior of deep policy gradient algorithms during the optimization. The conclusion is that, while these methods generally improve the policy, their behavior does not comply with the underlying theoretical framework. First, sample gradients obtained with a reasonable batch size have little correlation with each other and with the true gradient. Second, a larger batch size requires a smaller step-size. Third, the value baseline is far from true values and only marginally reduces variance, yet it considerably helps with optimization. Finally, the optimization landscape highly varies with the choice of objective function and the number of samples used to estimate it.
[Decision]
I vote for acceptance. To the best of my knowledge, the findings of this paper are new and not predictable by the current theory. These negative results have some merit as they call for theory that explains the behavior of these algorithms, or an algorithm whose behavior is predictable by the current theory. The paper is well-written, with a few small issues in presentation that should to be addressed in the final revision.
[Comments]
In Fig. 4 (b) it does not look like that the value error is high. It is said that ""the learned value function is off by about 50% w.r.t. the underlying true value function."" This sentence should be clarified or visualized.
What is \pi in Eq (13) in A1? If it is the agent's current policy, how is it different than \pi_\theta? If \pi corresponds to the distribution of state-action pairs in the replay buffer, how can one obtain a policy \pi that has led to this distribution of states in order to construct the importance sampling ratio?
In 2.2, the claim that a learned value baseline results in significant improvement in performance should be supported by results or reference to previous work.
Figs. 6 and 7 compare the loss surface with different objectives and sample regimes. Do these factors (objective and sample size) affect the part of the parameter space that is visualized (by changing the origin and the update direction), or are they only used to evaluate the values on the z-axis for the same area in the parameter space? Observing a different landscape in a different part of the parameter space is not surprising.
[Minor comments]
- Is V_\theta_{t-1} in Eq (4) a function of state? If so, a (s_t) is missing before the plus sign. | Review: The paper explores a critical divergence between theory and practice, emphasizing that while deep policy gradient algorithms seem to work in certain cases, they don't seem to be working foor the reasons underlying their derivations. It particularly looks at how closely the sample-based approximation of the objective's gradient aligns with the true gradient of the objective, how accurately learned values match the true expected returns, and how well the optimization landscapes of surrogate objectives line up with the objective of maximizing the return.
I propose accepting this paper, as it reveals a key gap in our understanding of why policy gradient methods work. Such emphasis can suggest why deep RL results tend to be inconsistent and irreplicable, and spark future work on closing the gap between theory and practice. Further, the paper is overall well written.
I primarily would like clarification on the optimization landscape visualizations:
1) Is the step direction the direction of the update actually performed at that time step?
2) Would moving diagonally in this space correspond to a mixture of following the update direction and a normally-distributed random direction? Concretely, in the true reward plot at Step 0 for few state-action pairs in Figure 8, does this suggest that mixing a random direction with the update direction would be better than moving cmopletely in the step direction?
Minor:
Typo in citation ""...policy improvement theorem of Kakade and Langford Kakade & Langford (2002)""",7.333333333333333
1,A Generalized Training Approach for Multiagent Learning,AddPublic Comment,"Decision: Accept (Talk) | Review: Review Update (18/11/2019)
Thank you for the detailed replies and significant updates to the paper in response to all reviewers. You have comfortably addressed all of my concerns and so I have updated my score. I think the paper has improved significantly through the rebuttal stage and therefore the update in my score is also significant to match the far larger contribution to the community that the paper now represents.
--
This paper considers alpha-rank as a solution concept for multi-agent reinforcement learning with a focus on its use as a meta-solver for PSRO. Based on theoretical findings showing shortcomings of using the typical best response oracle, the paper finds a necessity for a new response oracle and proposes preference-based best response.
The theoretical contributions help further the community's understanding of alpha-rank but the method remains somewhat disconnected from other recent related literature. Therefore, I think the paper's subsequent impact could be significantly improved by making more direct comparison to recent results. Specifically:
1) In the 2-player games comparisons are currently made to PRD based on its use in Lanctot et al (NeurIPS, 2017) instead of the more recent PSRO Rectified Nash approach proposed by Balduzzi et al. (ICML, 2019). Please make this direct comparison or justify its exclusion.
2) The preliminary MuJoCo soccer results in Appendix G significantly increase the relevance of this work to the ICLR community given the prior publication of this environment at ICLR 2019. However, the results are currently incomplete. In particular, to again strengthen the link to existing work, comparison of the method proposed in this paper to the agents trained by population based training in Liu et al. (ICLR, 2019) would be a more informative comparison than the preliminary results presented in comparison to the naïve uniform meta-solver.
3) Appendix A includes a brief literature survey. This is important material to position the paper in relation to existing work, particularly for readers not familiar with the area that will rely on this to understand the paper as a self contained reference. Please move this section into the main body of the paper and expand to fully credit the work this paper builds upon.
Minor Comments:
In Appendix C.4 should the reference to Figure C.7 be to Figure C.7a specifically? and the reference to Figure C. 7a be to Figure C. 7b-f inclusive? If so, I believe the available joint strategies in step 4 is missing (1,1,2) as shown in Figure C. 7f. | Review: This paper extends the original PSRO paper to use an
α
-Rank based metasolver instead of the projected replicator dynamics and Nash equilibria based metasolvers in the original. To this end, the paper modifies the original idea of Best-Response (BR) oracle since it can ignore some strategies in
α
-Rank defining SSCC to introduce the idea of _preference-based_ Best-Response (PBR) oracle. The need for a different oracle is well justified especially with the visualization in the Appendix. The main contributions that the paper seems to be going for is a theoretical analysis of
α
-Rank based PSRO compared to standard PSRO. From the PBR's description (especially in Sec 4.3) it seems the paper is intereseted in expanding the population with novel agents rather than finding the ""best"" single agent which is not well defined for complex games with intransitivities. Nevertheless, it seems that BR is mostly compatible with PBR for symmetric zero-sum two-player games.
The paper performs empirical experiments on different versions of poker. First set of experiments compare BR and PBR with
α
-Rank based metasolver on random games and finds that PBR does better than BR at population expansion as defined. The second set of experiments compare the metasolvers.
α
-Rank performs similarly to Nash where applicable. Moreover it's faster than Uniform (fictitious self-play) on Kuhn. Then the paper tacks on the MuJoCo soccer experiment as a teaser for ICLR crowd.
Overall the paper is quite interesting from the perspective of multiagent learning and I would lean towards accepting. However the paper needs to clarify a lot of details to have any chance of being reproducible.
** Clarifications needed:
- Tractability of PBR-Score and PCS-Score
It's unclear how tractable these are. Moreover these were only reported for random games. What did these scores look like for the Poker games? Could you clarify how exactly these were computed?
- It's somewhat unclear what the lack of convergence without novelty-bound oracle implies. Does this have to do with intransitivities in the game?
- Dependence of
α
?
The original
α
-Rank paper said a lot about the importance of choosing the right value for
α
. How were these chosen? Do you do the sweep after every iteration of PSRO?
- Oracle in experiments?
The paper fails to mention the details about the Oracles being used in the experiments. They weren't RL oracles but more details would be useful.
- BR not compatible with PBR, albeit not the other way around, meaning one of the solutions you get from PBR might be BR, but can we say which one?
- For MuJoCo soccer was it true PSRO or cognitive hierarchy. In general, the original PSRO paper was partly talking about the scalable approach via DCH. This paper doesn't mention that at all. So were the MuJoCo experiments with plain PSRO? What was the exact protocol there? From the appendix it's unclear how the team-vs-team meta game works with individual RL agents. Moreover how are the meta-game evaluation matrices computed in general? How many samples were needed for the Poker games and MuJoCo soccer?
- The counterexamples in Appendix B3 are quite interesting. Do you have any hypotheses about the disjoint support from games' correlated equilibria? | Review: The paper studies α-Rank, a scalable alternative to Nash equilibrium, across a number of areas. Specifically the paper establishes connections between Nash and α-Rank in specific instances, presents a novel construction of best response that guarantees convergence to the α-Rank in several games, and demonstrates empirical results in poker and soccer games.
The paper is well-written and well-argued. Even without a deep understanding of the subject I was able to follow along across the examples and empirical results. In particular, it was good to see the authors clearly lay out where their novel approach would work and where it would not and to be able to identify why in both cases.
My only real concern stems from the empirical results compared to some of the claims made early in the paper. Given the strength of the claims comparing the authors approach and prior approaches, it seems that the empirical results are somewhat weak. The authors make sure to put these results into context, but given the clarity of the results in the toy domains I would have expected clearer takeaways from the empirical results as well.
Edit: The authors greatly improved the paper, addressing all major reviewer concerns.",6.75
2,A Theory of Usable Information under Computational Constraints,...,"Review: The paper presents a generalization of classical definitions of entropy and mutual information that can capture computational constraints. Intuitively, information theoretic results assume infinite computational resources, so they may not correspond to how we treat ""information"" in practice. One example is public-key encryption. An adversary that has infinite time will eventually break the code so the decrypted message conveys the same amount of information (in a classical sense) as the plaintext message. In practice, this depends on computational time.
The authors' approach is to first restrict the class of conditional probability distribution p(Y|X) to a restricted family F that satisfies certain conditions. Unfortunately, the main condition in Def 1 that the authors assume is not natural and is only added to ensure that mutual information remains positive. However, putting this aside, the subsequent definitions that general entropy, conditional entropy, and mutual information are well-motivated.
The authors, then, show that many measures of ""uncertainty"" can be viewed as ""entropies"" under this generalized definition including the Mean Absolute Deviation and the Coefficient of Determination.
The overall framework can justify practices that we commonly use in machine learning, which would be justifiable using classical information. One important example is Representation Learning, which is a post-processing of data to aid the prediction task. According to classical information theory, this post-processing shouldn't help because it cannot add more information about the label Y than what was original available in X. Under the formulation presented in this paper, postprocessing can help if we keep in mind information about Y in X are hard to extract to begin with.
In terms of practical applications, the main advantage of the new definition is that F-information can be estimated from a finite sample, simply because F is a restricted set. However, this restriction helps compared to using state-of-the-art estimators for Shannon mutual information as shown in the experiments.
Finally, the literature review section is quite excellent.
I find the overall approach to be quite interesting and definitely worth publishing. The only suggestion I have is that the authors include immediately after Definition 1 a concrete example that illustrates it. For example, suppose that Y is a scalar and X is a noisy estimate of Y. Suppose we restrict F to the family of Gaussian distributions. That is, with side information x, f[x](y) = N(x, s). Without side information, f[empty](y) = N(u, s). The functions f are parameterized by u and s.
Is this a ""predictive family""? To make sure I understand it correctly, can you please walk me through the Eq 1 for this particular example?
Some minor remarks:
- Reference Shannon and Weaver was published in 1963, not 1948.
- In Page 5, ""maybe not expressive"" should be ""may not be expressive"". | Review: Summary
The paper introduces a framework for quantifying information about one random variable, given another random variable (“side information”) and, importantly, a function class of allowed transformations that can be applied to the latter. This matches the typical scenario in machine learning, where observations (playing the role of side information) can be transformed (with a restricted class of transformation-functions) such that they become maximally predictive about another random variable of interest (“labels”). Using this framework, the paper defines the notion of conditional F-entropy and F-entropy (by conditioning on an empty set). Interestingly, both entropic quantities are shown to have many desirable properties known from Shannon entropy - and when allowing the function class of transformations to include all possible models F-entropies are equivalent to Shannon entropies. The paper then further defines “predictive F-information” which quantifies the increase in predictability about one random variable when given side information, under a restricted function-class of allowed transformations of the side information. Importantly, transformations of side information can increase predictive F-information (which is the basis for the notion of “usable” information), which is in contrast to the data processing inequality that applies to Shannon information and states that no transformation of a variable can increase predictability of another variable further than the un-transformed variable (information cannot be generated by transforming random variables). The paper highlights interesting properties of the F-quantities, most notably a PAC bound on F-information estimation from data, which gives reason to expect F-information estimation to be more data-efficient than estimating Shannon-information (particularly in the high-dimensional regime). This finding is confirmed by four types of interesting experiments, some of which make use of a modified version of a tree-structure learning algorithm proposed in the paper (using predictive F-information instead of Shannon mutual information).
Contributions
i) Proposal of a framework for measuring and reasoning about information that transformed random variables have about other random variables, when the class of transformation functions is restricted. Interesting properties are highlighted and corresponding proofs are given. Important conclusions to Shannon-information measures are drawn.
ii) PAC guarantees for estimating F-information quantities from data. A nice result that justifies some optimism about the scalability of F-information estimation.
iii) Modification of a tree-structure learning algorithm, and application to four types of experiments with comparisons against methods for estimating Shannon(-mutual)-information.
Quality, Clarity, Novelty, Impact
The paper is very well written, the motivation and main results are clear and connections to known measures for information in complex systems are drawn (which often appear as corner-cases, or unrestricted cases of F-information). I am not an expert on various information measures, thus I cannot fully judge the novelty of the framework (given that the central idea is fairly simple and quite elegant, the main work lies in the proofs and connections to other frameworks). However, I have not seen the framework being discussed in the machine learning literature before. I personally would rate the potential impact of the F-information framework as high because it addresses many problems that Shannon-(mutual-)information has (hard to estimate, generality means complete blindness against model-classes). The experiments in the paper already illustrate how F-information could be very useful for a range of ML problems that cannot be tackled by strong competitor methods based on Shannon-information estimation. My only criticism is that the paper does not clearly state current limitations and shortcomings and does not comment on the difficulties / potential problems with solving the variational problem that is part of the definition of (conditional) F-information. I currently vote and argue for accepting the paper, though my assessment is of medium confidence only, and I am happy to take issues raised by the other reviewers and the rebuttal into account. I have not checked the proofs in the appendix in great detail.
Improvements
i) Please add a short section of current shortcomings and caveats, especially with regard to applying the methods in practice.
ii) Please comment on solving the variational optimization problem (the infimum) which is part of the definition of (conditional) F-information. In particular, are there any theoretical statements / bounds / etc. to be made for the case where the infimum is not found exactly - does the measure degrade gracefully or can small errors in this optimization lead to wildly varying/divergent F-information? From a practical point-of-view: how was this optimization done in the experiments (particularly when involving a neural network model), how much computational overhead did this optimization add (and how does it compare against other methods, e.g. in terms of wall-clock time or other reasonable metrics, the more the better)?
iii) This is a minor one and feel free to completely ignore it. The name F-information might easily get confused with the use of f-divergences, perhaps there is a better, more informative name. Also, while I personally like the term “usable” in the title, I’m not so sure about “computational constraints” - the latter somehow suggests that the method has small computational footprint, or can easily scale to different computational budget. Perhaps there is a way that more strongly indicates that this refers to restrictions on the model-/function-class (which the term “usable” does already to some degree admittedly).
Minor Comments
a) Have you had any thoughts on how F-information could be used in a rate-distortion / information-bottleneck type framework for a theory of “relevant usable information”? This is probably beyond the scope of this paper, just out of curiosity.
b) The paragraph above 3.3 almost sounds a bit like Shannon (and the data processing inequality) was wrong. I’d rather phrase this as a “no-free-lunch problem” - while the DPI and Shannon (mutual) information is very elegant, it is necessary to make further assumptions/restrictions (the function class of allowed transformations) to make more fine-grained statements and define more precise (but less general) informational-quantities tailored to the specific function class.
c) When choosing function classes that allow for universal function approximation, would F-information degrade to Shannon information?",8.0
3,Adversarial Training and Provable Defenses: Bridging the Gap,AddPublic Comment,"Review: This paper was very clearly written and easy to follow. Kudos to the authors. In particular, the experimental evaluation section was exceptionally clear. Thanks to the authors for making the paper so easy to review. The “Main Contributions” section was excellent as well as it allows the reader to quickly understand what the paper is claiming.
The introduction & related work section was very clear, and seemed to quickly get the reader up to speed.
Minor critiques:
- It’s not clear to me that the network size is actually as impressive an improvement as is implied. Barring an extensive hyper-parameter search that demonstrated that this network architecture is the smallest possible that could achieve the presented results, I strongly suspect that applying techniques from papers like EfficientNet [1] or MobileNet would allow the authors of Mirman et. al (2018) to reduce the number of parameters required to achieve their results. I don’t think this takes away from the paper, though- the results are strong despite that. I would encourage the authors to weaken the claims that the only better network is 5 times larger.
- In general, I would have liked to see more evaluation- e.g. I would have liked to see more results with a variety of perturbations (2 through 8, not just 2 & 8), and on a variety of datasets.
Questions to the authors:
- How robust is the algorithm to architecture choice?
- What happens if you change the test set? E.g. instead of evaluating on the first 1000 images, what if you evaluate on another random subset? Does that make a difference? I’m concerned that the subset of the test set the authors are using for evaluation isn’t representative of the entire test set.
- Is the current architecture the largest network that can be run? I would be interested in seeing how network size affects the performance of your technique.
- What hyper-parameter tuning did you do? What other network architectures did you try?
- How do the comparison methods compare in terms of training time/machines used? E.g. do all the methods reported in Table 1 use similar amounts of computing power?
Overall, this is a great paper, with some interesting results presented in a tight, clear manner. While I would like to see more experiments on larger datasets- e.g. ImageNet- the results seem solid and absolutely worthy of publication.
[1]: https://arxiv.org/abs/1905.11946v2
[2]: https://arxiv.org/abs/1704.04861 | Review:
Summary:
This paper provides a promising new general training methodology to obtain provably robust neural networks (towards adversarial input perturbations). The paper provides promising experimental results on CIFAR-10 by obtaining state-of-the-art certified accuracy while also simultaneously improving clean accuracy. The paper is overall well-written and the algorithm is clearly described.
Important questions to be answered:
I find the need to clarify my understanding and request for more information in order to make a decision.
--Methodology/motivation for the method: I am trying to understand abstractly what the proposed layerwise training is trying to optimize. To be concrete, let's compare to the relaxation of Wong and Kolter (which this paper uses in the instantiation of layerwise adversarial training). What's the exact difference?
One way to view this is the following: The same training objective, but a different way to optimize. The new proposal to train involves freezing weights until one layer iteratively starting from the input layer. It is possible that this kind of training provides some inductive bias in finding better solutions. Is this an appropriate understanding?
However, the paper’s experimental results unfortunately change the certification procedure. In other words, they haven’t evaluated the same training objective as that of Wong and Kolter. Hence, it’s not clear if the gains are from the better networks, or better certification method, or network being better suited for certification by the method used. The phrase “same relaxation” is not appropriately used. Their certification procedure uses a different (and tighter) relaxation.
--Effect on latent adversarial examples: I am unable to understand why this training procedure would reduce the number of latent adversarial examples. The definition of latent adversarial examples seems to suggest that it’s the gap between the actual set of activations corresponding to the input perturbations and the convex hull. However, the proposed layerwise adversarial training procedure involves replacing the actual set S_i with the convex hull C_i when freezing things till below i-1. I do not follow how the proposed method tries to make C_i = S_i. Implicitly the optimization objective does try to make C_i small because the bounds being optimized are tighter when C_i is small. But this is true even for normal certified training, and not sure what changes in the new training procedure.
Specific experimental results that would help:
--Certified accuracy on using the same LP based certification procedure used in Wong and Kolter with the new layerwise trained networks
--The paper’s own certification procedure (a combination of previous methods) on the network from Wong and Kolter or a note on why that doesn’t apply (if it doesn’t)
--The paper currently provides only one data point to suggest this training method is superior. Would be good to try SVHN or MNIST. MNIST is perhaps “essentially” solved for small \eps. But would be good to see if the training method offers gains at larger \eps. In general, would be good to see more consistent gains.
--The paper reports results on first 1000 examples of CIFAR10 test set. From my personal experience, there is a lot of variability in the robustness of test examples when evaluated on 1000 random test instances. Especially since the paper doesn't take a random subset, it might be good to make sure the gains are consistent on some other subset. The Wong et al. baseline is evaluated on the entire test set for example, and hence might not be a fair comparison? What's the Wong et al. accuracy on just the first 1000 test exampes
Overall, I am leaning towards accept but need some conceptual and empirical clarification from the authors (detailed above). | Review: Summary: the paper introduces a novel protocol for training neural networks that aims at leveraging the empirical benefits of adversarial training while allowing to certify the robustness of the network using the convex relation approach introduced by Wong & Kolter. The key ingredient is a novel algorithm for layer-wise adversarial (re-)training via convex relaxations. On CIFAR-10, the proposed protocol yields new state-of-the-art performance for certifying robustness against L_inf perturbations less than 2/255, and comparable performance over existing methods for perturbations less than 8/255 (where the comparison excludes randomized-smoothing based approaches as proposed by Cohen et al.).
The proposed methodology seems original and novel. The concept of latent adversarial examples, the layer-wise provable optimization techniques and the sparse representation trick are interesting in their own regard and could be valuable ingredients for future work in this direction. The improvement over the state-of-the-art on CIFAR-10 for perturbations less than 2/255 is significant (although I wouldn't call it substantial). For perturbations less than 8/255 the picture is less clear. The authors' explanation that they couldn't achieve state-of-the-art certified robustness because of smaller network capacity makes sense, however, it also highlights that their protocol doesn't scale as well as previous approaches.
I am not concerned about the missing comparison with randomized smoothing-based approaches (I find the rationale provided in Section 2 convincing).
The discussion of the relatively weak performance of previous provable defenses on page 3 is a bit vague, e.g. the statement that ""the way these methods construct the loss makes the relationship between the loss and the network parameters significantly more complex than in standard training"", thus causing the ""resulting optimization problem to be more difficult"". To me, these are one and the same thing, and a bit more rigour in the argumentation would be advisable here, in my opinion.
-------------
I acknowledge I have read the authors' response and also the other reviews/comments which confirm my opinion that this paper is worthy to be published at ICLR.",7.333333333333333
4,BackPACK: Packing more into Backprop,...,"Review: This paper presents a Pytorch framework for experimenting with first and second order extensions to standard gradient updates via backpropagation. At the time of writing, the implementation supports feed-forward networks where there is a strict module ordering (by which I mean residual connections are not possible).
The framework enables researchers to easily experiment with techniques which involve modifying the update according to quantities computed over the batch of gradients (i.e. before they are summed to form the standard SGD update)—these are ‘first-order’ extensions—and it also makes use of the block-diagonal factorisation of the Hessian outlined in Mizutani & Dreyfus as well as Dangel & Hennig to enable the computation of second order quantities via ~ ‘hessian prop’.
I think the paper is a strong accept: the framework has some limitations in the current form (mostly in terms of what architectures are supported), however it still provides a very useful and extensible tool for researchers to efficiently experiment with a variety of more complex optimisation architectures. This is (as the paper states) a large bottleneck for much optimisation research in deep learning.
In section 2.3 you state that generalised Gauss-Newton (GGN) is guaranteed positive semi-definite. It would also be nice to add a sentence as to when (even intuitively) the Fisher information coincides with GGN; (in practice, as the GGN uses a (possibly rank-bounded) sample size of ‘N’, while the Fisher is the expectation under the data generating distribution, one could argue that even when they should be ==, it would only be as N->\infty). | Review: This is a good paper. The authors present a software implementation which allows one to extend PyTorch to compute quantities that are irritatingly difficult to compute in PyTorch directly, or in other automatic differentiation frameworks, particularly if efficiency is a concern. Issues of this kind have been discussed at length within the community, particularly on GitHub, and related issues with optimization of automatic differentiation code have motivated other software developments, such as Julia's Zygote package. Having wasted a large amount of my time implementing the WGAN gradient penalty from scratch - which, to implement scalably, requires one to use both forward-mode and reverse-mode automatic differentiation simultaneously - I appreciate what the authors are trying to do here to make research that requires more sophisticated automatic differentiation more accessible.
---
Detailed remarks below.
* The paper's title is not very informative about what the paper is about. The authors should choose a different title - perhaps something like ""BackPACK: user-friendly higher-order automatic differentiation in deep networks"" or something similar but less long.
* The authors focus on KFAC-type methods as their key illustrated use case, but actually software frameworks like this are also helpful for certain GAN losses - WGAN-GP in particular. These losses require one to compute a term that involves the norm of a certain gradient of the output. The gradient of this gradient can be handled efficiently with Hessian-Vector products, which in principle are computable efficiently via automatic differentiation, but in practice a huge pain because of the need to use both forward-mode and reverse-mode automatic differentiation and lack of first-class support from automatic differentiation packages. Provided I've not misunderstood BackPACK and that it would help in making such computations less tedious (and I can't verify this myself, as my own implementation of WGAN-GP was not in PyTorch), I would highly recommend the authors to add an extra paragraph discussing this particular use case, because this would increase the paper's impact on the community by connecting it to another literature which is not mentioned in the paper.
* The entire paper could do with talking about backpropagation less, and automatic differentiation more, because it illustrates that the concerns addressed are not solely limited to deep networks, even if the package does primarily target them.
* P2: these issues are not limited to the Python community, and specialized automatic differentiation software has also been developed for Julia. The authors should cite Innes [1,2] and related papers from that literature.
* Figure 1: from the sample code, I worry about how generic BackPACK is. I think the package authors should be careful not to specialize too much to particular kinds of deep networks, particularly since a much wider variety of models and network architectures are starting to be used with automatic differentiation.
* P2: capital \Omega notation is confusing, please replace with capital \Theta.
* P2: L_2 regularization should more technically be \ell^2 instead.
* P4: please cite Baydin [3] who provides a very nice review of automatic differentiation. It may help explanation to introduce dual numbers, which make forward-mode much easier to understand.
* P6: please write out ""with respect to"" for ""w.r.t."".
* P7: I really liked this section. The simplicity of implementing the example method using the authors' software framework feels compelling to me. However, genericness is still a concern: by analogy, every deep learning framework can do MNIST easily, but some of them make it much harder to do customized or advanced implementation than others. The latter cases are often the ones that matter to practitioners. It's hard to tell how easy it will be to implement something the authors did not foresee or consider - but this will necessarily be the case in any software paper.
* P8: ""and in part driven"" - missing a comma.
* P8: please spell out ""Table"" in ""Tab. 1"".
[1] M. Innes. Don't Unroll Adjoint: Differentiating SSA-Form Programs. NeurIPS, 2018.
[2] M. Innes. Flux: Elegant Machine Learning with Julia. Journal of Open Source Software, 2018.
[3] Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark. Automatic differentiation in machine learning: a survey. JMLR, 2017. | Review: This paper adds a very interesting and useful feature to existing autodifferentiation for training neural networks. The second-order information can be backprogated just as the first-order ones, which can be used to accelerate training.
This idea, although according to the paper, is developed upon existing works, still, strongly attracts as the second-order information is crucial for training and perhaps visualizing the landscape of neural networks. I vote for an acceptance as this brings a significantly important feature to PyTorch, and the author's good experiments results and open-sourced code.",8.0
5,Building Deep Equivariant Capsule Networks,AddPublic Comment,"Review: This paper combines CapsuleNetworks and GCNNs with a novel formulation. First they modify the CapsNet formulation by replacing the linear transformation between two capsule layers with a group convolution. Second they share the group equivarient convolution filters per all capsules of the lower layer. Third, they change the similarity metric from a lower-upper similarity into a pairwise lower similarity and aggregation which makes it keep the equivarience. Since the cij does not depend on upper capsule anymore they only perform 1 routing iteration (no modification of the routing factors).
One assumption in CapsNets is that each part belongs to one whole. Therefore, the normalization in Alg.2 usually is division by degree^k_i. The proposed normalization formula for c_ij seems to encourage that each upper capsule only receives one part. Is this a typo or is there a justification for this?
The discussion on ideal graph on page 5 is interesting. But the points made are not used later on. I expected the results to have an analysis or at least a show case that indeed if you transform the resultant graphs stay isomorphic.
One goal for CapsuleNetworks vs GCNNs is the hope for handling different transformations and not only rotations that one can grid with group convolutions. But, the experiments only report on rotation, translation as a transformation. Reporting results by training on MNIST, testing on AFFNIST could shed light on this aspect of SOVNETs.
Conditioned that the last two points will be addressed in the rebuttal I vote for accepting this paper since they suggest a novel formulation that brings some measures of rotation equivarience guarantee into CapsNets. Also their results suggest that there is no need for per Capsule filter bank and several refinements to get rotation robustness (it would be interesting to check the performance of a simple capsnet with shared parameters). In the appendix there is a comparison with GCNNs on fashion MNIST which shows they have better performance than GCNNs. I would advise reporting GCNNs for all the experiments in the main paper.
------------------------------------------
Thank you for updating and expanding the paper. The extra experiments, isomorphism analysis and their response regarding the attention vs part-whole makes the paper much stronger. Therefore, I am increasing my score. | Review: In this work, a method was proposed to train capsule network by projectively encoding the manifold of pose-variations, termed the space-of-variation (SOV), for every capsule-type of each layer. Thereby, the proposed method aims to improve equivariance of capsule nets with respect to translation (rotation and scaling).
The proposed method is interesting and the initial results are promising. However, there are various major and minor problems with the work:
- There are various undefined functions and mathematical notation, such as the following:
- Please give formal and precise definitions of groups and group representations for readers who are not familiar with mathematical groups.
- What are GetWeights and Agreement used in Algorithm 1?
- Please define “routing among capsules” more precisely.
- How do you calculate Pool() more precisely?
- In the paper, the results are given for a general class of groups. However, it is not clear how these results generalize even for some popularly employed groups, such as Z^n, S_n, SO(n), SE(n) etc., with different symmetry properties, base space, and field type.
- Please check the following paper for a detailed discussion on group equivariant CNNs with different group structures, and elaborate the theoretical results for particular groups (e.g. at least for p4m used in experiments) :
T.S. Cohen, M. Geiger, M. Weiler, A General Theory of Equivariant CNNs on Homogeneous Spaces, NeurIPS 2019
- Please define the norms used in Algorithm 2.
- How do you calculate accuracy of models? Are these numbers calculated for a single run, or for an average of multiple runs? If it is the former, please repeat the results for multiple runs, and provide average accuracy with variance/standard deviation. If it is the latter, please provide the variance/standard deviation as well.
- Have you performed analyses using larger datasets such as Cifar 100 or Imagenet? It would be great to provide some results for larger datasets to explore their scalability.
- Please define accuracy given in tables more precisely, use dot ""."" at the end of sentences in captions.
- There are several typo/grammatical errors, such as the following:
-- Homegenous -> homogeneous
-- for with prediction networks
-- Please proof-read the paper in detail and fix the typo etc.
After the discussions:
Most of my questions were addressed and the paper was improved in the discussion period. Therefore, I increase my rating.
However, some parts of the paper still need to be clarified. For instance;
- GetWeights: To ensure that the predictions are combined in a meaningful manner, different methods can be used to obtain the weights. The role of GetWeights is to represent any such mechanism.
-> Please define these methods in detail and more precisely, at least in the Supp. mat.
- Agreement : The Agreement function represents any means of evaluating such consensus.
-> This is also a very general concept, which should be more precisely defined.
- Our theoretical results (Theorem 2.1 and Theorem 2.2) hold for all general groups, and the particular group representation defined in the main paper.
-> Could you please give a concrete discussion on generalization of these results and the proposed algorithms for discrete and continues groups? For instance, how do these algorithms and results generalize with Z^n and SO(n)?",7.0
6,CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning,AddPublic Comment,"Decision: Accept (Talk) | Review: This paper proposed a new synthetic dataset (CATER) for video understanding. The authors argue that since current video datasets are heavily biased over static scenes and object structures, it is unclear whether modern spatial-temporal video models can learn to reason over temporal dimension. In order to address this problem, they design this fully observable synthetic dataset which is built upon CLEVER, along with three tasks that are customized for temporal understanding. They further conduct a variety of experiments to benchmark state-of-the-art video understanding models and show how those models more or less struggle on temporal reasoning.
Overall this paper is well-written and easy to follow. The problem is well-motivated, and the claims are mostly supported. The diagnosis in this paper provides useful insights that could be contributive to both vision and learning communities.
My primary concern is to what extent can the new dataset (CATER) add to existing video datasets that are also explicitly designed for long term spatial-temporal reasoning, such as video VQA datasets TGIF-QA[1]/SVQA[2]. In addition to the comparison between CATER and three action recognition datasets (Kinetics/UCF101/HMDB51) as presented in Table 3., it would be more interesting to see how video understanding models that are specifically designed for those video VQA datasets will perform on CATER.
[1] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758–2766, 2017.
[2] Xiaomeng Song, Yucheng Shi, Xin Chen, and Yahong Han. Explore multi-step reasoning in video question answering. In 2018 ACM Multimedia Conference on Multimedia Conference, pages 239–247. ACM, 2018. | Review: The paper introduces CATER: a synthetically generated dataset for video understanding tasks. The dataset is an extension of CLEVR using simple motions of primitive 3D objects to produce videos of primitive actions (e.g. pick and place a cube), compositional actions (e.g. ""cone is rotated during the sliding of the sphere""), and finally a 3D object localization tasks (i.e. where is the ""snitch"" object at the end of the video). The construction of the dataset focuses on demonstrating that compositional action classification and long-term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation-based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues.
A variety of models from recent work are evaluated on the three proposed tasks, demonstrating the validity of the above motivation for the construction of the dataset. The primitive action classification task is ""solved"" by nearly all methods and only serves for debugging purposes. The compositional action classification task is harder and shows that incorporating LSTMs for temporal reasoning leads to non-trivial performance improvements over frame averaging. Finally, the localization task is challenging, especially when camera motion is introduced, with much space for improvement left for future work.
I am positive with respect to acceptance of this paper. It is a well-argued, thoughtful dataset contribution that sets up a reasonable video understanding dataset. The authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with real-world data, but still it can serve a useful and complementary role similar to the one CLEVR has served in image understanding.
I have a few minor comments / questions / editing notes that would be good to address:
- The random baseline isn't described in the main text, it would be good to briefly mention it (this will also help to clarify why the value is particularly high for tasks 1 and 2)
- The grid resolution ablation results presented in the supplement are actually quite important -- they demonstrate that with a small increase in granularity of the grid the traditional tracking methods begin to be the best performers. As this direction (of increased resolution to make the problem less artificial) is likely to be important, a brief discussion of this finding from the main paper text would be appropriate
- p3 resiliance -> resilience
- p4 objects is moved -> object is moved
- p6 actions itself -> actions themselves; builds upon -> build upon
- p7 looses all -> loses all; suited our -> suited to our; render's camera parameters -> render camera parameters; to solve it -> to solve the problem
- p8 (Xiong, b;a) and (Xiong, b) -> these references are missing the year; models needs to -> models need to
- p9 phenomenon -> phenomena; the the videos -> the videos; these observation -> these observations; of next -> of the next; in real world -> in the real world | Review: This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR. The new dataset is the first to account for all of the following fundamental aspect of videos: temporal ordering, short- and long term reasoning, and control for scene biases. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. In contrast, the authors show that with the proposed dataset, models without spatiotemporal reasoning largely fail.
The paper should be accepted as it addresses a major shortcoming of all existing video understanding datasets. It does a good job at summarizing the deficiencies in existing datasets, clearly motivating the need for a new dataset. The claims are backed up with solid experiments, ablating models and data parameters adequately. It is mostly well-written (except for section 4 which would benefit from extensive proofreading) and does a good job at covering relevant work. One drawback is of course the synthetic nature and limited domain of objects and actions. On the other hand, this makes the setup highly controllable and reliable. I like the fact that each task comes both with both static and moving camera.
Improvements and Questions:
Some relevant datasets are missing. For example, the Moving MNIST and Robot Pushing datasets could be added to Table 1.
I suggest having a train / validation / test split (like CLEVR), rather than just a train and validation split.
In particular for Task 3 more frames seem to give dramatic improvement. Why did you not run with more than 64 frames?
Did you consider downsampling the videos to allow running on all the frames?
I’m missing details on the resolution of the generated videos?",6.75
7,Causal Discovery with Reinforcement Learning,-''-,"Decision: Accept (Talk) | Review: In this paper, the authors propose an RL-based structure searching method for causal discovery. The authors reformulate the score-based causal discovery problem into an RL-format, which includes the reward function re-design, hyper-parameter choose, and graph generation. To my knowledge, it’s the first time that the RL algorithm is applied to causal discovery area for structure searching.
The authors’ contributions are:
(1) re-design the reword function which concludes the traditional score function and the acyclic constraint
(2) Theoretically prove that the maximizing the reward function is equivalent to maximizing the original score function under some choices of the hyper-parameters.
(3) Apply the reinforce gradient estimator to search the parameters related to adjacency matrix generation.
(4) In the experiment, the authors conduct experiment on datasets which includes both linear/non-linear model with Gaussian/Non-gaussian noise.
(5) The authors public their code for reproducibility.
Overall, the idea of this paper is novel, and the experiment is comprehensive. I have the following concerns.
(1) In page 4 Encoder paragraph, the authors mention that the self-attention scheme is capable of finding the causal relationships. Why? In my opinion, the attention scheme only reflects the correlation relationship. The authors should give more clarifications to convince me about their beliefs.
(2) The authors first introduce the h(A) constraint in eqn. (4), and mentioned that only have that constraint would result in a large penalty weight. To solve this, the authors introduce the indicator function constraint. What if we only use the indicator function constraint? In this case, the equivalence is still satisfied, so I am confused about the motivation of imposing the h(A) constraint.
(3) In the last paragraph of page 5, why the authors adjust the predefined scores to a certain range?
(4) Whether the acyclic can be guaranteed after minimizing the negative reward function (the eqn.(6))? I.e., After the training process, whether the graph with the best reward can be theoretically guaranteed to be acyclic?
(5) In section 5.3, the authors mention that the generated graph may contain spurious edges? Whether the edges that in the cyclic are spurious? Whether the last pruning step contains pruning the cyclic path?
(6) In the experiment, the authors adopt three metrics. For better comparison, the author should clarify that: the smaller the FDR/SHD is, the better the performance, and the larger the TPR is, the better the performance.
(7) From the experimental results, the proposed method seems more superiors under the non-linear model case. Why? Could the authors give a few sentences about the guidance of the model selection in the real-world? i.e., when to select the proposed RL-based method? And under which case to choose RL-BIC, and which case to selection RL-BIC2?
(8) What’s training time, and how many samples are needed in the training process?
Minor:
1. In the page 4 decoder section, the notation of enc_i and enc_j is not clarified.
2. On page 5, the \Delta_1 and \Delta_2 are not explained.
3. For better reading experience, in table 1,2,3,4, the authors should bold value that has the best performance. | Review: Update: after the revision, I have decided to increase my score to 8.
Original comments:
In this paper, the authors proposed a new reinforcement learning based algorithm to learn causal graphical models. Simulations on real and synthetic data also shows promise.
Pros
1. It's great to see the authors has done a comprehensive comparison with the other methods, especially under different simulation scenarios.
2. The novel idea of applying reinforcement learning to DAG search sounds intriguing. Reinforcement learning offers a powerful tool for policy evaluation and decision making. It’s good to see that the author can successfully extend such toolbox to the field of causal structure learning. To the best of the author’s knowledge, such idea has never been considered by previous work in causal graphical models.
Cons.
1. In the introduction section, the authors claimed that “GES is not guaranteed in the finite sample regime”. This seems to be incorrect. For example, the Nandy et al. paper tackles exactly the finite sample problem.
In conclusion, overall this is a sensible idea, although some of the preliminaries still remain to be polished. | Review: This work addresses the task of causal discovery. The proposed contribution is to apply prior work which uses reinforcement learning for combinatorial optimization to structure learning. Specifically, the proposed optimization problem seeks to maximize a penalized score criterion subject to the acyclicity constraint proposed by Zheng, et al. Empirical results show the proposed method performing favorably in contrast to prior art.
Overall I think this is a sensible idea, and the authors do a nice job of exposition, and empirical evaluation.
My concerns are as follows:
* The novelty is somewhat limited, since the paper is combining two previously proposed ideas (combinatorial search and the acyclicity constraint) for structure learning.
* The paper is loose with technical points. Specifically, the authors claim to use the additive noise model, but then make no restrictions on f(). In this setting, it is fairly well known that we can only hope to learn up to the Markov equivalence class (not the fully directed graph), but there is no mention of this in the paper.
With all of this said, I think overall the paper is an interesting addition to the causal discovery literature.",7.0
8,Comparing Rewinding and Fine-tuning in Neural Network Pruning,"Keywords: pruning, sparsity, fine-tuning, lottery ticket","Review: *Summary*
Extending the observations of Frankle et al. (2019, ""The Lottery Ticket Hypothesis""), this paper examines ""rewinding"" as an alternative to fine-tuning in a typical network pruning process. After training to convergence for T iterations, the k% of weights with the smallest magnitude are pruned (set to zero). Typically, in fine-tuning, the remaining weights are continued to be trained for several more iterations at a small learning rate. With ""rewinding"", the remaining weights are reset to their values in iteration 0<=t<T and are trained for T-t iterations with the original learning rate schedule.
Experiments with this method (and an iterative variant) show that (a) rewinding achieves 2-5x smaller networks matching the unpruned accuracy (with comprehensive tuning of the rewinding parameter t); (b) selection of rewinding iteration is important, but is flexible within a large range for higher sparsity pruning.
*Rating*
The paper is very well written with good exposition, thorough notes and citations for all methodological choices,
and an explicit statement of the limitations of the work.
A few considerations relevant to the rating:
(1) Novelty: The idea for rewinding is not novel, as acknowledged clearly in the paper. Frankle et al. (2019, ""Stabilizing the Lottery Ticket Hypothesis"") showed that rewinding to an early iteration of training (0<t<T) yielded better accuracy than rewinding to iteration t=0 for VGG-19 and ResNet-18 on CIFAR-10. However, Frankle et al. did not consider fine-tuning as it was not relevant to lottery ticket discovery. This submission studies the tradeoffs of rewinding vs. fine-tuning.
(2) Thoroughness: The paper considers ResNet-20 and VGG-16 with CIFAR-10 and ResNet-50 with ImageNet. Conclusions would be strengthened with additional combinations of networks and datasets.
(3) Acknowledged limitations: As noted, the paper doesn't consider any pruning criteria other than weight magnitude, nor does it consider structured pruning. The latter in particular is important for applications where prediction speed on commodity hardware is a limiting factor.
As it is, I think this paper a worthy (if limited) contribution to the understanding of network pruning.
*Notes*
Table 1/Figures *: note which dataset is used for each architecture
Figures 2-3: It seems that many values are clipped by the legend range of +/- 0.5%. Consider showing the figure with a larger range or adding such a figure to the appendix. | Review: Building on the results of [Frankle et al, 2019], this paper seeks to utilize rewinding as a core procedure in pruning neural networks, in combination with the usual fine-tuning procedures. Specifically, [Frankle et al, 2019] demonstrate that it is possible to find sparse subnetworks, such that rewinding weights to their initial values and retraining from that initialization, yields test accuracy similar to the original network. While this is already a form of pruning, the submitted paper explores a wider space of pruning procedures that utilize rewinding as a subroutine.
This wider framework includes the choice of rewind point (e.g. rewinding to partway through training rather than to initialization) and how to balance computation budget between rewinding (and retraining for an equivalent number of epochs) vs continuing to fine-tune a network. Experiments cover this hyperparameter space, as well as the range of desired sparsity level (pruning amount). Results show rewinding (to a point 30% - 60% into training) dominates any amount of fine-tuning, if moderate to high sparsity is desired.
The empirical study conducted by this paper is useful and complements the results previously reported in [Frankle et al, 2019]. However, the paper itself is light on novelty, as the core ideas were already established by [Frankle et al], and the application of them here is relatively straightforward. The extensive experiments here add value to the conversation about the lottery ticket hypothesis, but are not otherwise ground-breaking. | Review: This paper does an in-depth evaluation of the notion of rewinding pruned networks to the weights at a previous point in training, and then re-training the pruned network from then on. This is in comparison with fine-tuning a pruned network, where the retraining continues from the network's current weights. The authors focused on vision networks and unstructured magnitude pruning in their evaluation.
The paper is well-written and easy to follow, and the authors have done a good job of empirically comparing rewinding against fine-tuning in a number of different scenarios.
My main concern with the paper is that it seems too incremental to stand on its own. Rewinding is a notion that was already explored in the Lotter Ticket Hypothesis (Frankle et al., 2019), so this paper seems to be more of an extension of that work. The take-away message from this paper (stated by authors in their conclusion) is that practicioners ""should explore rewinding as an alternative to fine-tuning for neural network pruning"", but that's a case that was already made by the LTH paper. The current work certainly gives more weight to that claim, but I don't feel the contributions are strong enough on their own to justify a full conference publication.
I encourage the authors to continue working on this, as it is very interesting and can be very useful. Some ideas to make the paper stronger:
- Given that this is a purely empirical paper, it'd be better to not limit the experiments as much. Can you run on non-vision networks? What about mobile-net? Can you try different pruning techniques? etc.
- How do the different methods compare in terms of accuracy/sparsity versus FLOPs?
Finally, two minor comments to improve the writing:
- First sentence of 3.1: s/that meet that the accuracy/that match the accuracy/
- First sentence of 3.1: s/than fine-tuning can/compared to fine tuning",7.333333333333333
9,Contrastive Learning of Structured World Models,-''-,"Review: This paper aims to learn a structured latent space for images, which is made up of objects and their relations. The method works by (1) extracting object masks via a CNN, (2) turning those masks into feature vectors via an MLP, (3) estimating an action-conditioned delta for each feature via a GNN. Learning happens with contrastive losses, which ask that each feature+delta is close to the true next feature, and far away from other random possibilities. Experiments in simple synthetic environments (e.g., 2D geometric shapes moving on a black background) show encouraging results.
This paper has a simple, well-motivated method. It is clearly written, and easy to understand. The evaluation is straightforward also: the paper merely shows that this model's nearest neighbors in featurespace are better than the nearest neighbors of World Model (2018) and PAIG (2019). Also, some visualizations indicate that for these simple directional manipulations (up/down/left/right motion), PCA compressions of the model's states have a clean lattice-like structure.
It is impressive that the model discovers and segments objects so accurately. Perhaps this could actually be evaluated. However, I do not understand why results are so sensitive to the number of object slots (K). This seems like a severe limitation of the model, since in general we have no idea what value to set for this.
Although I like the paper, I am not sure that there is sufficient evidence for the method being something useful. Yes, H@1 and MRR are high, but as the paper itself implies, the real goal is to improve performance (or, e.g., sample efficiency) in some downstream task. Given how simple these domains are, and the fact that data is collected with purely random exploration, it is difficult to imagine that there is any significant difference between the training set and the test set. For example, if you make 1000 episodes of 10 steps each in Space Invaders, you practically get 1000 copies of the same 10 frames. I worry that all the evaluation has shown so far is that this model can efficiently represent the state transitions that it has observed.
The authors note that it was beneficial to only use the hinge on the negative energy term. This seems unusual, since a hinge on the positive term allows some slack, which intuitively makes the objective better-formulated. Can the authors please clarify this result, at least empirically? | Review: This paper tackles the problem of learning an encoder and transition model of an environment, such that the representation learnt uses an object-centric representation which could favor compositionality and generalisation. This is trained using a contrastive max-margin loss, instead of a generative loss as previously explored. They do not consider RL or follow-up tasks leveraging these representations and transition models yet.
They perform an extensive assessment of their model, with many ablations, on 2 gridworld environments, one physical domain, and on Atari.
The paper is very well motivated, easy to follow, and most of its assumptions and decisions are sensible and well supported. They also provide interesting assessments and insights into the evaluation scheme of such transition models, which would be of interest to many practitioners of this field.
Apart from some issues presented below, I feel that this work is of good quality and would recommend it for acceptance.
1.The model is introduced in a very clear way, and most decisions seem particularly fair. I found the presentation of the contrastive loss with margin to be clear, and the GraphNet is also well supported (although see question below). However, two choices are surprising to me and would deserve some clarification and more space in the main text, instead of the Appendix:
a.Why does the object extractor only output a scalar mask? This was not extremely clear from reading the main text (and confused me when I first saw Figure 1 and 3a), but as explained in the Appendix, the CNN is forced to output a sigmoid logit between [0, 1] per object channel. 
This seems overly constraining to me, as this restricts the network to only output “1 bit” of information per “object”.
However, maybe being able to represent other factors of these objects might be necessary to make better predictions? 
This also requires the user to select the number of output channels precisely, or the model might fail. This is visible in the Atari results, where the “objectness” is much less clear.
 Did you try allowing the encoder to output more features per objects?
Obviously this would be more complicated and would place you closer to a setting similar to MONet (Burgess et al. 2019) or IODINE (Greff et al. 2019), but this might help a lot.
b.It was hard to find the dimensionality D of the abstract representation
zt
. It is only reported in the Appendix, and is set to
D=2
for the 2D gridworld tasks and
D=4
for Atari and the physics environments. These are quite small, and the fact that they exactly coincide with your assumed sufficient statistics is a bit unfortunate. 
What happens if D is larger? Could you find the optimal D by some means?
2.The GraphNet makes sense to me, but I wondered why you did not provide
atj
to
et(i,j)
as well? I could imagine situations where one would need the action to know if an interaction between two slots is required.
3.Similarly, the fact that the action was directly partitioned per object (except in Atari where it was replicated), seemed slightly odd. Would it still work if it was not directly pre-aligned for the network? I.e. provide
at
as conditioning for the global() module of the GraphNet, and let the network learn which nodes/edges it actually affects.
4.In your multi-object contrastive loss, how is the mapping between slot k in
zt
and
z~t
performed? Do you assume that a given object (say the red cube) is placed in the same
k
slot across different scenes/timesteps? This may actually be harder to enforce by the network than expected (e.g. with MONet, there is no such “slot stability”, see [1] for a discussion).
5.It was unclear to me if the “grid” shown in Figure 3 (b) and 5 is “real”? I.e. are you exactly plotting your
zt
embeddings, and they happen to lie precisely along this grid? If yes, I feel this is a slightly stronger result as you currently present, given this means that the latent space has mirrored the transition dynamics in a rather impressive fashion.
6.Related to that point, I found Figure 3 b) to be slightly too hard to understand and parse. The mapping of the colours of the arrows is not provided, and the correspondence between “what 3D object is actually moving where” and “which of the coloured circles correspond to which other cubes in the image” is hard to do (especially given the arbitrary rotation). Could you add arrows/annotations to make this clearer? Alternatively, presenting this as a sequence might help: e.g. show the sequence of real 3D images, along with the trajectory it traces on the 2D grid.
7.Figure 4 a) was also hard to interpret. Seeing these learnt filters did not tell much, and I felt that you were trying too hard to impose meaning on these, or at least it wasn’t clear to me what to take of them directly. I would have left this in the Appendix. Figure 4 b) on the other hand was great, and I would put more emphasis on it.
8.There are no details on how the actual test data used to generate Table 1 was created, and what “unseen environment instances” would correspond to. It would be good to add this to the Appendix, and point forward to it at the end of the first paragraph of Section 4.6, as if you are claiming that combinatorial generalization is being tested this should be made explicit. I found Table 1 to be great, complete, and easy to parse.
9.It would be quite interesting to discuss how your work relates to [1], as the principles and goals are quite similar. On a similar note, if you wanted to extend your 2D shape environment from a gridworld to a continuous one with more factors of variations, their Spriteworld environment [2] might be a good candidate.
References:
[1] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P. Burgess, Alexander Lerchner, “COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration”, 2019, https://arxiv.org/abs/1905.09275
[2] Nicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra, Alexander Lerchner, “Spriteworld: A Flexible, Configurable Reinforcement Learning Environment”, https://github.com/deepmind/spriteworld/ | Review: The construction and learning of structured world models is an interesting area of research that could in principle enable better generalisation and interpretability for predictive models. The authors overcome the problem of using pixel-based losses (a common issue being reconstruction of small but potentially important objects) by using a contrastive latent space. The model otherwise makes use of a fixed number of object slots and a GNN transition model, similarly to prior approaches. The authors back up their method with nice results on 3D cubes and 3-body physics domains, and reasonable initial results on two Atari games, with ablations on the different components showing their contributions, so I would give this paper an accept.
The comparisons to existing literature and related areas is very extensive, with interesting pointers to potential future work - particularly on the transition model and graph embeddings. As expected, the object-factorized action space appears to work well for generalisation, and could be extended/adapted, but setting a fixed number of objects K is a clearly fundamentally limiting hyperparameter, and so showing how the model performs under misspecification of this hyperparameter is useful to know for settings where this is known (2D shapes, 3D blocks, 3-body physics). The fact that K=1 is the best for Pong but K=5 is the best for Space Invaders raises at least two questions: can scaling K > 5 further improve performance on Space Invaders, and is it possible to make the model more robust to a greater-than-needed number of object slots? On a similar note, the data collection procedure for the Atari games seems to indicate that the model is quite sensitive to domains where actions rarely have an impact on the transition dynamics, or the interaction is more complex (e.g. other agents exist in the world) - coming up with a synthetic dataset where the importance of this can be quantified would again aid understanding of the authors' proposed method.",8.0
10,Convolutional Conditional Neural Processes,"Keywords: Neural Processes, Deep Sets, Translation Equivariance","Decision: Accept (Talk) | Review: The paper describes a method for model neural for neural processes considering translation-equivariant embeddings.
The paper seems to be quite specific topic. Maybe, the author could add more empirical results to it to show the impact on translation-equivariant examples. The theoretical claims seem to be valid. So the question is a bit open what are the applications. The empirical results are also narrow as there is not much other competitive work. The results seem to be increment extension to previous work.
The work looks solid to me, currently I am probably not able to appreciate and judge relevance to its full extend. I would judge, it is more of interest to view specific people working on this - maybe, the authors could for the final version make this more clear.
The questions that should be more addressed maybe is also the applications - why is this relevant and how does it improve your specific cases. Why do we want to develop this. State of the art is quite relative if authors come from a quit narrow area which not much papers on the topic and data sets.
One of the main points of the paper did not get clear how does translation-equivariant helps to solve or improve the empirical results. Could you add some examples where this improves results.
I remain ambivalent. It seems to be solid work with not much convincing applications and somewhat incremental. Maybe the authors might address this in their introduction more. The motivation remains unclear to me and hence difficult to judge its potential and impact. | Review: -- Summary
This paper considers the problem of developing neural processes which
are translation-equivariant. The authors derive a necessary and sufficient
functional form that the neural process \Phi function must exhibit
in order to be permutation invariant, continuous and translation
equivariant.
Using the derived functional form, the authors construct a
translation-equivariant neural process, the convolutional conditional
neural process.
Results in several experimental settings are given: 1d synthetic
experiments, an astronomical time-series modelling experiment, a
sim2real experiment, and several image completion experiments. All
the experiments show performance improvements over the AttnCNP, the
main baseline tested against. In the astronomy setting the authors
test against the winning kaggle entry, against which they get better
log likelihood. The authors give several qualitative experiments,
including image completion tasks from a small number of pixels.
Proofs of all the theorems and full details of all the experiments
are given in the appendix, along with ablations of the model.
-- Review
Overall I found this paper very impressive. It is clear how the theoretical
results motivate the choice of architecture. The fact that Theorem 1
completely characterises the design of all translation-equivariant
neural processes is a remarkable result which precisely specifies the
degrees of freedom available when constructing a convolutional NP.
The implementation gives state of the art results against
the AttnCNP while using fewer parameters on a variety of tasks. The image
completion tasks are impressive.
It seems that the authors close an open question posed in (Zaheer 2017)
regarding how to form embeddings of sets of varying size by embedding
the sets into an RKHS instead of a finite-dimensional space. This in itself
is an interesting idea, and I am interested to see how this embedding method
be applied outside of the CNP framework.
The experimental results are comprehensive and diverse, showing good
performance on both toy examples and more real-world problems. The ablations
and qualitative comparisons in the appendix are helpful in showing where
the ConvCNP outperforms the AttnCNP.
My main criticism of the work is that it's very dense, requiring a few
passes to really grasp the theoretical contribution and the concrete
architecture used in the ConvCNP. I would recommend enlarging figure 1
(b), which is illuminating but quite cluttered due to the small
size. Perhaps the section on multiplicity could be moved to the
appendix to make space as it seems for all real-world datasets the
multiplicity would be equal to 1.
Misc Comments
- It would be good to have a brief discussion of why the ConvCNPPXL performs
very badly on the ZSMM task, while being the best performing method in all
of the other tasks. I couldn't find such a discussion.
- Did the authors try emitting a 36-dimensional joint covariance matrix over the
six-dimensional output in the plasticc experiment?
- In the synthetic experiments, for the EQ and weak periodic kernels it would
be nice to see the `ground truth' log-likelihood given by the actual GP,
just to have some idea of what the upper bound of LL could be.
- In appendix C.2 Figure 6, what is the difference between the `true function' and the
`Ground Truth GP'? I thought the true function was a gp... | Review: The paper introduces ConvCNP, a new member of the neural process(NP) family that models translational equivariance in the data, which uses convolutions and stationary kernels to aggregate the context data into a functional representation.
This problem is well-motivated as there are various domains where such an inductive bias is desirable, such as spatio-temporal data and images, and will help especially with predictions for out-of-distribution tasks. This inductive bias was never built into NPs, and it remained unanswered whether the NP can learn such a behaviour. This paper shows that the answer is negative and that one needs to make modifications to create such inductive bias.
The architecture of the ConvCNP is motivated by theory that completely characterises the set of translation equivariant functions Phi that maps sets of (x,y) pairs to bounded continuous functions that map x to y (disclaimer: I haven’t read through the proof in the appendix, so will not make any claims on its correctness). Theorem 1 defines the set of such functions using rho, phi and psi, and the choices for each on on-the-grid data and off-the-grid data are listed in Section 4. There are ablation studies in Appendix D.4 that justify the choices.
Overall the paper is very well-written and clear for the most part, with helpful pseudo-code and well-laid out quantitative + qualitative results, and a very detailed appendix that allows replicating the setup. The evaluation is extensive, and the results are significant.
- The results on 1D synthetic data show a noticeable improvement of the ConvCNP compared to the AttnCNP, with improved interpolation as well as accurate extrapolation for the weakly periodic function. I do think however that a more competitive baseline for AttnCNP would have been to parameterise the logits of the attention weights as a periodic function with learnable length scale (e.g. stationary periodic kernel), since this is another way of building in periodicity into the model. Arguably this is more explicit and restrictive than the translational equivariance built into ConvCNP, but would have made for a more interesting comparison.
- Having said that, I like how the evaluation was performed on a variety of stochastic processes - previous literature only used GP + EQ kernel, but here more challenging non-smooth functions such as GP + Matern kernels and sawtooth functions are explored - and it’s very convincing to see the outstanding performance of ConvCNPs here.
- It’s also nice to see results on regression tasks on real data (sections 5.2, 5.3), which was never explored in the NP literature as far as I know. 5.2 shows that ConvCNPs can be competitive against other methods that model stochastic processes, and 5.3 shows an instance of where ConvCNPs do a reasonable job whereas (Attn)CNP fails.
- The results on images is also extensive, covering 6 different datasets (including the 2 zero shot tasks), and show convincing qualitative and quantitative results. The zero shot tasks are nice examples that explicitly show the consequences of not being able to model translation equivariance in more realistic images composed of multiple objects/faces.
I have several comments/questions regarding the disccusion & related work section:
- One link that might be worth pointing out regarding functional representation of context is that ANP (or AttenCNP) can also be seen as giving a functional representations of the context; the ANP computes a target-specific representation of the context, which can be seen as a function of the target inputs.
- I think it’s incorrect to say that latent-variable extensions enforce consistency. Even with the latent variable, if the encoder is seen as part of the model, then the NP isn’t consistent (pointed out in the last paragraph of section 2.1 in the ANP paper). So there still are issues regarding AR sampling. There does however seem to exist variants of NPs that satisfy consistency e.g. https://arxiv.org/abs/1906.08324
- What is preventing the incorporation of a latent variable in the ConvCNP? Is this just something that can be easily done but you haven’t tried, or do you see any non-trivial issues that arise when doing so e.g. maintaining translation equivariance?
Other minor comments:
- Are there any guidelines on choice of filter size of CNN in the image case? E.g. have you chosen the filter size of ConvCNP such that the receptive field is smaller than the image, whereas it’s bigger for ConvCNPXL? It’s not clear why having a bigger receptive field allows to capture non-stationarity, and it would be helpful to expand on that, perhaps in the appendix.
- Also it’d help for the sake of clarity to explain why AttnCNP uses significantly more memory than ConvCNP, i.e. because memory for self-attention is O(N^2) where N=HW is the number of inputs, whereas for convolutions it’s O(HW).
- I think it’d also help to state explicitly in the body that AttnCNP is ANP without the latent path when it is introduced.
- typos: first paragraph of Section 2: Z_M <- Z_m (twice), finitely <- infinitely, Appendix D.1: separabe <- separable
Overall, I think this is a very strong submission and I vote for its acceptance.",6.0
11,Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning,...,"Review: This article presents cyclical stochastic gradient MCMC for Bayesian deep learning for inference in posterior distributions of network weights of Bayesian NNs. The posteriors of Bayesian NN weights are highly multi-modal and present difficulty for standard stochastic gradient MCMC methods. The proposed cyclical version periodically warm start the SG-MCMC process such that it can explore the multimodal space more efficiently.
The proposed method as well as the empirical results intuitively make sense. The standard SG-MCMC basically has one longer stepsize schedule and is exploring the weight space more patiently, but only converges to one local mode. The cyclical SG-MCMC uses multiple shorter stepsize schedules, so each one is similar to a (stochastic) greedy search. Consequently, the cSG-MCMC can collect more diverse samples across the weight space, while the samples of SG-MCMC are more concentrated, but likely with better quality (as shown in Figure 3).
Personally I would like to see how Bayesian deep learning can be applied to real large-scale applications. Probabilistic inference is expensive; Bayesian model averaging is even more expensive. That's probably why recent literature focuses on variational inference or expectation propagation-based approaches. | Review: The paper propose a new MCMC scheme which is demonstrated to perform well for estimating Bayesian neural networks. The key idea is to not keep lowering the step sizes, but -- at pre-specified times -- go back to large step sizes.
The paper is timely, the proposed algorithm is novel, and the theoretical analysis also seem quite novel.
My key concern is that with MCMC sampling it is often quite difficult to tune parameters, and by introducing more parameters to tune when step sizes should increase, I fear that we end up in a ""tuning nightmare"". How sensitive is the algorithm to choice of parameters?
I would expect that the proposed algorithm is quite similar to just running several MCMCs in parallel. The authors does a comparison to this and show that their approach is significantly faster due to ""warm restarts"". Here I wonder how sensitive this conclusion is to choice of parameters (see nightmare above) ? I would guess that opposite conclusions could be reached by tuning the algorithms differently -- is that a reasonable suspicion ?
It is argued that the cyclic nature of the algorithms gives a form of ""warm start"" that is beneficial for MCMC. My intuition dictate that this is only true of the modes of the posterior are reasonable close to each other; otherwise I do not see how this warm starting is helpful. I would appreciate learning more about why this intuition is apparently incorrect.
Minor comments:
* on page 4 it is stated that the proposed algorithm ""automatically"" provide the warm restarts -- but is it really automatic? Isn't this a priori determined by choice of parameters for the algorithm?
* It would be good to use \citet instead of \cite at places, e.g. ""discussed in (Smith & Topin, 2017)"" should be ""discussed by Smith & Topin (2017)"". This would improve readability (which is generally very good).
* For the empirical studies I think it would be good to report state-of-the-art results as well. I expect that the Bayesian nets still are subpar to non-Bayesian methods, and I think the paper should report this. | Review: The paper develops a cyclical stepsize schedule for choosing stepsize for Langevin dynamics.
The authors prove the non-asymptotic convergence theory of the proposed algorithm. Many experimental results, including ImageNet, are given to demonstrate the effectiveness of the proposed method.
Here I suggest that authors also need to point out that the continuous-time MCMC is the Wasserstein gradient flow of KL divergence. The bound derived in this paper focus on the step size choice of gradient flows. This could be a good direction for combining gradient flows studies in optimal transport and MCMC convergence bound for the choice of step size.
Overall, I think that the paper is well written with clear derivations. I strongly suggest the publication of this paper.",7.333333333333333
12,Data-dependent Gaussian Prior Objective for Language Generation,AddPublic Comment,"Review: This paper introduces the use of data-dependent Gaussian prior, to overcome negative diversity ignorance problem that includes the exposure bias problem for sequence generation models. In addition to the usual MLE (teacher forcing) criteria, the authors add the KL divergence between the prediction and the Gaussian PDF on the word embedding space. Experimental results show that the proposed method consistently improves the performance of the state-of-the-art methods for neural machine translation, text summarization, storytelling, and image captioning.
I lean to accept this paper. The proposed method is well motivated and shown to be effective in several tasks for language generation.
I have some major comments about the evaluation function $f(\cdot)$. The authors propose to define it as a Gaussian distribution.
- While this choice seems to be reasonable, I would like to know how its standard deviation can be defined. If it is a hyperparameter, the sensitivity of different deviations for the performance should be experimentally reported. A small valued deviation would make the KL divergence close to zero, while a large one makes its convergence slow.
- Another way to remedy the problem of KL divergence above is applying Wasserstein distance instead of KL divergence. I would like to know if the authors have investigated the use.
Minor comments:
- White space should be inserted between ""sequence-to-sequence"" and ""(seq2seq)"" on the third page.
- If the authors define a sequence using a bold and italic font as $\boldsymbol{y}$, each token can be represented using an italic font to distinguish each token and the entire sequence: $\boldsymbol{y}=<y_1,...,y_l>$. Otherwise, the sequence can be defined as $\mathcal{Y}$ if the authors like to represent each token as a vector.
- There is a typo on the fifth page. The word ""dada-independent"" should be ""data-independent."" | Review: This paper proposes to add a prior/objective to the standard MLE objective for training text generation models. The prior penalizes incorrect generations/predictions when they are close to the reference; thus, in contrast with standard MLE alone, the training objective does not equally penalize all incorrect predictions. For the experiments, the authors use cosine similarity between fastText embeddings to determine the similarity of a predicted word and the target word. The method is tested on a comprehensive set of text generation tasks: machine translation, unsupervised machine translation, summarization, storytelling, and image captioning. In all cases, simply adding the proposed prior improves over a state-of-the-art model. The results are remarkable, as the proposed prior is useful despite the variety of architectures, tasks (including multi-modal ones), and models with/without pre-training.
In general, it is promising to pursue work in altering the standard MLE objective; changes to learning objective seem orthogonal to the modeling gains made in many papers (as evidenced by the gains the authors show across diverse models). This paper opens up several new directions, i.e., how can we impose even more effective priors? The authors show that it's effective to use a relatively simple fastText-based prior, but it's possible to consider other priors based on large-scale pre-trained language models or learned models. In this vein, a concurrent paper ""Neural Text Generation with Unlikelihood Training"" has also shown it effective to alter the standard MLE objective. I think it would be nice to discuss this paper and related works. Overall, I think the approach is quite general and elegant.
My main criticism is that the writing was unfocused or unclear at times. The intro discusses a variety of problems in generation, before explaining that the authors only intend to tackle one (""negative diversity ignorance""). It would have been more helpful to read more text in the intro that motivated the problem of negative diversity ignorance and the proposed solution. The second paragraph in the Discussion in Section 4 is rather ambiguous and hand-wavy. It would be nice to see the authors' intuition described more rigorously (i.e., explicitly describing in math how the cosine similarity score is used in the Gaussian prior, or describing in math how the central limit theorem is used). Some of the existing mathematical explanation in section 4 could be made simpler or more clear (the description of f(y) seems to be a distraction since it doesn't end up in the final loss).
I would have also appreciated more analysis. After reading the paper, I have the following questions (which the authors may be able to address in the rebuttal):
* Do off-the-shelf fastText embeddings work well? How important is it to train fastText embeddings on the data itself? If off-the-shelf embeddings worked well, that could make the method easier to use for others in practice.
* How does the gain in performance with D2GPo vary based on the number of training examples? Priors are generally more helpful in low-data regimes. If that is the case here as well, you might get even more compelling results on low-data tasks (all tasks attempted here are somewhat large-scale, as I understand)
* Qualitatively, do you notice any difference in the generations? How does the model make mistakes (are these more ""semantic"" somehow, i.e. swapping a different synonym in). Perhaps the gaussian prior has some failure modes, i.e., where it increases the probability of very incorrect/opposite words because they have a similar fastText representation. These kinds of intuitions would be useful to know
I also have one technical question:
* When you compare against MASS (Song et al. 2019), do you use the same code and/or pre-trained weights from MASS, or do you pre-train from scratch using the procedure from MASS? (The wording in the text is somewhat ambiguous.) I'm just wondering how comparable the results are vs. MASS, or if it would be useful to know how your version of the pre-trained model does.
Despite my above questions/concerns, I think the proposed method or its predecessors could provide improvements across a variety of text generation tasks, so I overall highly recommend this paper for acceptance. | Review: The paper introduces a new Gaussian prior objective, ""D2GPo"", that addresses the fact that in sequence generative models, all incorrect predictions are penalized equally by MLE, a phenomenon which the authors refer to as the negative diversity ignorance drawback. The proposed objective is simple to implement and can easily be added on top of a regular cross-entropy loss. The paper shows that the new objective shows consistent improvements across a wide range of tasks.
- Something that perturbed me a lot when reading Section 4, is that the function f should take as input 2 arguments, and not just 1. Typically, in Equation 7, from what I understand the numerator should be exp(f(y'_i, y_i)/T), and the denominator the sum over the exp(f(y'_j, y_i)/T). In other words, the value of f assigns a score to a word y'_j which depends on the target word y_i. This is also what suggests Figure 1: f depends on the target word. I think this should really be clarified in the paper, and f needs to be defined formally. In Figure 1, what is the exact value of f when the model outputs ""armchair"" / when the model outputs ""armchairs"". If I did not understand correctly, please point me out.
- The cross-entropy loss in a generative model is essentially the KL divergence loss KL(Q, P) where Q is the true distribution (i.e. a one-hot vector) and P is the model output. In Equation 7, when T -> 0, the distribution Q becomes this one-hot vector. In that case, from what I understand, the D2GPo objective is actually very similar to the initial MLE objective, except that you compute KL(P, Q) instead of KL(Q, P), and that Q is not exactly one-hot because you consider a T > 0. Can you confirm this? Also, any reason why you considered KL(P, Q) instead of KL(Q, P)?
- What value did you use for the temperature T? Did you study its impact?
- Same question for lambda in Equation 6, what is the value you considered, and did you study its impact?
- As mentioned in the introduction of the paper, one drawback with traditional neural generative, is the generation diversity, i.e. the fact that generations are generic, and an easy way to see this is to observe that generations are mostly composed of the most frequent words in the vocabulary. Did you evaluate whether a model trained with D2GPo has more diversity, and is more likely to generate rare words compared to a regular model trained with MLE only?
- Figure 1 in the appendix is helpful. It would be nice to move it to Section 4.
- In the related work, you write ""the Transformer provides us with a more structured memory for handling long-term dependencies"", which sounds a bit odd. There is no explicit memory component in the transformer, the ability to handle long-term dependencies comes instead from the self-attention mechanism.
Overall the paper tackles an interesting problem which I feel has received surprisingly little attention from the community. The paper is well written, and has a lot of experiments supporting the method. But I think Section 4 needs to be clarified, and some experimental details are missing. Also, more information about the impact of T and lambda (at least on one type of experiments) would be very useful to have.",8.0
13,"Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",-''-,"Review: Batch active:
This paper proposes a novel approach to active learning in batches. Assuming a neural-network architecture, they compute the gradients of each unlabeled example using the last layer of the network (and assuming the label given by the network) and then choose an appropriately diverse subset of these using the initialization step of kmeans++. The authors provide intuitive motivation for this procedure, along with extensive empirical comparisons.
Overall I thought the paper was well written and proposed a new practical method for active learning. There were a few concerns and places where the paper could be clearer.
1. The authors keep emphasizing a connection to k-dpp for the sampling procedure emphasizing diversity. They provide a compelling argument for the kmeans++ but in Figure 1 it is unclear why k-DPP is the right comparison point. For example, you could imagine building a set cover of the data using balls at various radii and then choosing their centers.
2. The paper emphasizes choosing samples in a way to eliminate pathological batches. Considering this is a main motivation, none of the figures really demonstrate that this is what BADGE is doing compared to the uncertainty sampling-based methods tested against. Perhaps the determinant of the gram matrix of the batch could be reported for both algorithms?
3. While reading the paper, the set of architectures used was hard to find. Maybe I just missed it, but it would be useful to have this information. In particular, in Figure 3, there are absolute counts, but I wasn’t sure how many (D,B,A,L) combinations there were.
4. Finally, recent work in Computer Vision has shown that uncertainty sampling with ensemble-based methods in active learning tends to work well. I understand that it is hard to compare to the myriads of active learning algorithms out there, but they deserve a mention. See [1] below.
Overall I think this paper is a good empirical effort that I recommend for acceptance.
[1] Beluch, William H., Tim Genewein, Andreas Nürnberger, and Jan M. Köhler. ""The power of ensembles for active learning in image classification."" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9368-9377. 2018. | Review: The paper proposes a new method for active learning, which picks the samples to be labeled by sampling the elements of the dataset with highest gradient norm, under some constraint of diversity. The aforementioned gradient is computed w.r.t. the predicted label (rather than the true label, that is unknown) and diversity is achieved by sampling via the k-MEANS++ algorithm.
The paper is well written and while the experiments look thorough, the motivation to support the proposed method seem too weak and unconvincing as does the discussion of the results, which is why I am leaning toward rejection.
I am willing to amend my vote if the authors provide stronger (not empirical) motivations on why using the gradient norm w.r.t. the predicted label is a better metric than those in the literature, and More comments below.
Detailed feedback:
1) The paper lacks a proper motivation as to why using the norm of the gradient is a better metric than the many others already present in the literature. In particular, I cannot think of any case where it would be best to use that than the entropy of the network’s output distribution, even though the empirical results seem to suggest otherwise. Specifically, while I believe that in many cases it will be similarly good, if we consider the case when the network is able to rule out most of the classes but is unsure on a small fraction of them, the entropy will better reflect this uncertainty than the norm of the gradient of the predicted class.
Generally speaking, I believe that the use of the norm of the gradient of the predicted class should be much better motivated, being the core idea of the paper. Stating that it is cheap to compute and empirically performs as well as k-DPP in two experiments is not convincing enough in my opinion.
2) I wonder how much of the performance of BADGE is due to k-MEANS++ and how much to the choice of using the gradient norm. Please perform an ablation study where you can e.g., replace the gradient norm with the entropy, or replace k-MEANS++ with random sampling, and discuss the results.
3) How is the embedding “ground set” space determined for k-MEANS++? How are the centroids determined? In which space? It is unclear to me how k-MEANS++ is used in the context of the norm of the gradients. Please improve the explanation in the main text.
4) Please add a curve for k-DPP to the plots in the main text, rather than having separate plots for it in the appendix. Also, it would be interesting to compare against Derezinski, 2018 as well, if that’s the current state of the art (which is what I infer from your text, but I might be wrong).
5) The paper builds on the claim that the gradient norm w.r.t. the prediction is a lower bound for the gradient norm induced by any other label, yet Proposition 1 that proves it is in Appendix B. This prove is central to the proposed idea and should be in the main text.
6) The authors claim that to capture diversity they collect a batch of examples where the gradients span a diverse set of directions, but it’s unclear to me that k-means++ actually accomplishes that. Where is the *direction* of the gradient taken into account in the algorithm?
7) The “discussion” section is really a “conclusion” one, and indeed a proper in-depth discussion of the experiments is missing. Please expand the comments on the experimental results.
8) The metric to compute the “pairwise comparison” looks quite convoluted. Is it common in the literature? If so, please add a reference. If not, can you motivate the use of this specific formula?
9) The random baseline seems to be very competitive. Why is that? Please provide your intuition. Could this be indicative that the baselines have not been tuned properly?
10) Introduction: the sentence “[deep neural networks] successes have been limited to domains where large amounts of labeled data are available” is incorrect. Indeed, neural networks have been used successfully in many domains where labelled data is scarce, such as the medical images domain for example. Please remove the sentence.
11) Introduction: please add a sentence to explain what a version-space-based approach is.
12) Is Figure 2 the average over multiple runs or a single run?
13) Notation: please do not use g for the gradient (g^y_x) and for the intermediate activations (g(x; V)).
14) The lower margin seem too wide. Please make sure you respect the formatting style of the conference.
Minor:
- Notation: if you must shorten g^{\hat{y}}_{x} please do so with \hat{g}_{x} and equivalently shorten g^{y}_{x} as g_{x}
- Notation: in the pairwise comparison, please don’t reuse i to denote an algorithm (it is used a few lines before to compute the labeling budget)
- Please add reference to Appendix A when k-MEANS++ is first referred to in page 2.
- Page 3, when Proposition 1 is mentioned add reference to the location where it’s defined.
Typos:
- Page 2: expenive -> expensive
- Page 5: Learning curves. “Here we show ..” -> Remove “here”
- Figure 3: pariwise -> pairwise
- Page 7: Apppendx E
----------------------
Updated review:
I thank the authors for for taking the time to address all my comments, and clarifying some of the misunderstandings I had. I am happy to revise my score accordingly. | Review: This paper introduces an algorithm for active learning in deep neural networks named BADGE. It consists basically of two steps: (1) computing how uncertain the model is about the examples in the dataset (by looking at the gradients of the loss with respect to the parameters of the last layer of the network), and (2) sampling the examples that would maximize the diversity through k-means++. The empirical results show that BADGE is able to get the best of two worlds (sampling to maximize diversity/to minimize uncertainty), consistently outperforming other approaches in a wide-rage of classification tasks.
This is a very well-written paper that seems to make a meaningful contribution to the field with a very good justification for the proposed method and with convincing empirical results. Active learning is not my main area of expertise so I can’t judge how novel the proposed idea is, but from an outsider’s perspective, this is a great paper. It is clear, it does a good job explaining the problem, the different approaches people have used to tackle the problem, and how it fits in this literature. Below I have a couple of (minor) comments and questions:
1. Out of curiosity, it seems that it is standard in the literature, but isn’t the assumption that one can go over the whole dataset, U, at each iteration of the active learning algorithm, limiting? It is not that cheap to go over large datasets (e.g., ImageNet).
2. MARG seems to often outperform the other baselines but it doesn’t have a reference attached to it (bullet points on page 5). Is this a case that a “trivial” baseline outperforms existing methods or is there a reference missing?
3. In some figures, such as Figure 2, there are shaded regions in the plots. It is not clear what they are though. Are they representing confidence intervals? Standard deviation? They are quite tight for a sample size of 5.
4. In the section “Pairwise comparisons” it reads “Algorithm i is said to beat algorithm j in this setting if z > 1.96, and similarly … z < -1.96”. It seems to me that the number 1.96 comes from the z-score table for 95% confidence. However, if that’s the case, it seems z should be much bigger in this context. With a sample-size of 5 (if this is still the sample size, maybe I missed something here), the normal assumptions do not hold and the t-score should’ve been used here. What did I miss?
In terms of presentation, Proposition 1 seems to be a very interesting result. I would move it to the main paper instead of leaving it in the Appendix. I also think the paper would read better if it didn’t use references as nouns (e.g., “algorithm of (Derezinski, 2018)”). Finally, there’s also a typo on page 7 (Apppendx).
---
>>> Update after rebuttal: I stand by my score after the rebuttal. This is a really strong paper in my opinion. I appreciate the fact that the authors took my feedback into consideration.",7.333333333333333
14,Differentiable Reasoning over a Virtual Knowledge Base,"Keywords: Question Answering, Multi-Hop QA, Deep Learning, Knowledge Bases, Information Extraction, Data Structures for QA","Review: The paper studies scaling multi-hop QA to large document collections, rather than working with small candidate lists of document/paragraphs (as done in most of the previous work), a very important, practical and challenging direction.
They start with linking mentions to entities in a knowledge base. Every iteration of their mult-hop system produces a set of entities Z_t, relying on entities predicted on the first representation Z_{t-1} and the question representation. In order to make training tractable, they mask 'attention' between Z_{t-1} and Z_t (actually mentions corresponding to Z_t). They also use top-K relevant mentions at train and test time. As the attention score is based on dot-product, they can plug-in the approximate Maximum Inner Product Search to avoid computing the attention score for every mention in the collection. The architecture is essentially end-to-end trainable (except for specialized pretraining discussed below).
Whereas itself the architecture is not overly novel (e.g., the architecture does feel a lot similar to models in KB context, and also graph convolution networks applied to QA), there is a lot of clever engineering and the novelty is really in showing that it can work without candidate preselection.
My main worry is pretraining. In both experiments (MetaQA and the new Wikidata Slot Filling task), they pretrain the encoders using a knowledge base, and the knowledge base directly corresponds to the QA task. E.g., for MetaQA the questions are answerable using the knowledge bases, so relation types in the knowledge base presumably correspond to relations that need to be captured in the QA multihop learning. This specialized pretraining appears to be crucial (88% for pretraining vs 55% with BER), presumably because of the top K pruning. Though a nice trick, it is likely limiting as a knowledge base needs to be available + it probably constraints the types of mult-hop questions the apporach can handle. Also, some of the baselines do not benefit from using the KB, and, in principle, if it is used in training, why not use the KB at test time? (I see though that for the second dataset pretraining seems to be done on a different part of Wikipedia, I guess, to address these concerns).
I was not sure how the number of hops T was selected for the model, it does not seem to be defined in the paper. Do you pretend that you know the true number of hops for each given question?
The authors experiment with reducing the size of a KB for pretraining. It apparently does not harm the first 1 hop questions, but 2 and 3-hop. Do the authors have any explanation for this? Related to the previous question, does it mean that the model does not learn to exploit the hops for t > 1?
The evaluation is on MetaQA and on the newly introduced Wikidata task, whereas most (?) recent multi-hop QA work has focused on HotpotQA (and to certain degree WikiHop). Is the reason for not using (additionally) HotpotQA? Is the model suitable for HotpotQA? If not, does this have to do with pretraining or the types of questions in HotpotQA?
The model definition in section 2.1 is not very easy to follow. E.g., it is not immediately clear if the model applied at every hop is the same model, and not clear how the model is made aware of the current search state (e.g., which part of the question has already processed / how the history is encoded) or even of the hop id.
I would really like to see more analysis of what the model learns at every hop.
--
After the rebuttal -- I appreciate the detailed feedback, extra experiments, and analysis. `I increased my score. | Review: The paper proposes a model that can perform multi-hop question-answering based on a textual knowledge base. Results show that the proposed model -- DrKIT -- performed close to or better than the state of the art results over MetaQA and WikiData datasets. Ablation study is offered to show that a few tricks in the model are necessary to make it work, and comparisons with baseline models such as DrQA and PIQA are presented. The paper also provides additional means to speed up the DrKIT model using the hashing trick and approximated top-k methods.
The paper is a good one and I vote for its acceptance. Besides achieving good performance, the proposed DrKIT model makes sense, and all the parts are necessary components based on the ablation study results. In addition, the ablation study and the speed-up methods are great addition to the model to make it work better.
With this generally positive assessment said, I do have a few questions below that I hope the authors could provide some response. These are on top of the high quality of the paper, and should be best regarded as suggestions for future work.
1. In equation (3), do G and F have to be TFIDF features? The likes of word2vec and GloVe (and also pershaps fastText) are trained based on co-occurences of adjacent words, and I would imagine that they will improve over TFIDF. This is just an intuition and I could be wrong, but it would be very helpful to hear the authors' opinions.
2. The ablation study mentioned that the softmax temperature helps with the model. This is a nice observation, but is there any intuition behind why that is the case? I could imagine that it could be because the gradients of a saturated softmax function is small and therefore results in slow training of the model. If this is the case, both low temperature and high temperature will fail to work. It would have been better so show both ends of failing extremes in an ablation study.
3. Can you discuss the similarity between DrKIT and multi-hop End-to-End Memory Networks [1]? It looks very much like an expansion of it with a fixed retrieval mechanism and by expanding the answer to a set rather than a single vector.
[1] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, End-To-End Memory Networks, NIPS 2015 | Review: This paper introduces a new architecture for question answering, that can be trained in an end-to-end fashion. The model expands the entities in a given question to relevant mentions, which are in turn aggregated to a new set of entities. The procedure can be repeated for multi-hop questions. The resulting entities are returned as candidate answers. The approach relies on an index of mentions for approximate MIPS, and on sparse matrix-vector products for fast computation. Overall, the model processes queries 10x faster than previous approaches. The method provides state-of-the-art results on the MetaQA benchmark, with significant improvements on 3-hop questions. The experiments are detailed, and the paper is very well written.
A few comments / questions:
1. Do you have any explanation of why taking the max instead of the sum has a significant impact on the 2,3-hop performance, but only gives a small improvement for 1-hop questions?
2. The same observation can be done for the temperature lambda=1 vs lambda=4, so I was wondering about the distribution of the entities you get on the output of the softmax (in Eq 4). Is the distribution very spiky, and Z_t usually only composed of a few entities? In that case, I guess lambda=4 encourages the model to explore/update more relation paths? Something that works very well in text generation is to not just set a temperature, but also to apply a softmax only on the K elements with the highest score (so the softmax is applied on K logits, and everything else is set to 0). Did you consider something like this? It may prevent the model from considering irrelevant entities, but also from considering only a few ones.
3. Given the iterative procedure of the method, I wonder how well the model would generalize to more hops. Did you try for instance to train with 1/2 hops and test whether it can generalize to 3-hop questions?
4. In Section 2.3, you fix the mention encoder because training the encoder would not work with the approximate nearest neighbor search (I assume this is because the index would need to be rebuilt). However, the ablation study suggests that the pretraining is critical, and one could imagine that fine-tuning the mention encoder would improve the performance even further. Instead of considering a mention encoder, could you have a lookup table of mentions (initialized with BERT applied on each mention), where mention embeddings are fine-tuned during training? The problem of the index is still there, but could you consider an exact search on the lookup table (exact search over a few million embeddings is slow, but it should still run in a reasonable amount of time using a framework like FAISS, and it would give an upper-bound of the performance you could achieve by fine-tuning the mention encoder).",8.0
15,Dynamics-Aware Unsupervised Discovery of Skills,-''-,"Review: This paper introduces an unsupervised learning algorithm Dynamics-Aware Discovery of Skills (DADS) for learning low-level “skills” that can be leveraged for model-predictive control. The skills are learned by maximizing the mutual information between the next state s’ and the current skill z conditioned on the current state s. Maximizing this objective corresponds to maximizing the diversity of transitions produced in the environment, while making the skill z be informative about the next state s’. The idea is that using this objective leads to learning a diverse set of skills that are predictive of the environment. The skills z correspond to a set of action sequences, which are represented by a distribution \pi(a|s,z). Because the above objective is intractable to compute (because it relies on the true dynamics p(s’|s,a)), it is variationally lower bounded using the approximate dynamics q_{\phi}(s’|s,z), which represents the transition dynamics when using a certain skill and this variational lower bound is optimized to produce the optimal q_{\phi}(s’|s,z) and \pi(a|s,z).
In the second phase, model predictive control is used to do planning for a new test environment where we have access to the reward function. This corresponds to simulating multiple trajectories using the learned transition dynamics and skill function, computing the reward of each trajectory according to the reward function, executing the first action of the most optimal trajectory and repeating. It is mentioned that planning is done in the latent skill space, which enables easier longer-horizon planning.
Experiments are performed to show that: (1) the learned skills exhibit low-variance behavior (which means that the skills have predictable behavior when used for model predictive control); (2) Model predictive control performs favorably compared to other relevant baselines.
Overall, I feel this is a very well-motivated and interesting submission with very thorough experiments. | Review: The authors try to incorporate intermediate-level skills into model-based RL, which is an essential problem in the RL field. The algorithm works well even in the case of high-dimensional state and action spaces. Their contributions are in four aspects:
(1)they propose an unsupervised RL framework for learning intermediate-level representations, i.e. skills, based on maximizing the mutual information between the future state and current skill given the current state. This procedure is well-motivated and the mathematics is easy to follow.
(2)they reformulate model predictive control (MPC) in the latent skill space.
(3)their method is compatible with the idea of continuous skill spaces, which seems to give rise to more diverse trajectories and hence offers greater utility.
(4)their method yields low-variance behavior while maintaining enough diversity.
It’s an accept for me. On one hand, using model-free unsupervised RL methods to learn intermediate-level skills is not a new idea. on the other, approaching this problem via mutual information is, as far as I know, new to this field. Although, the novelty of this approach remains undecided, this method seems to work well enough compared to model-based, model-free and hierarchical RL methods. Their analysis from the perspectives of continuous skill space and skill variance also seem to hold.
Nevertheless, the study would benefit from more comparison with other methods using intermediate-level primitives (apart from DIAYN). Moreover it would be interesting to show this method works in scenarios apart from locomotion. I wonder how well the approximation p(z|s) \approx p(z) works in non-locomotion tasks. Otherwise, the authors should mention this method is somewhat limited to locomotion tasks in the main text.
Others:
Typo:
1.Page 3: “maximally informative about about …” remove the redundant “about”;
2.Page 8, first line in section 6.3 “is to be enable use of planning algorithms…” may be changed to “is to take advantage of planning algorithms”. | Review: This paper proposes a novel approach to learn a continuous set of skills (where a skill is associated with a latent vector and the skill policy network takes that vector as an extra input) by pure unsupervised exploration using as intrinsic reward a proxy for the mutual information between next states and the skill (given the previous state). These skills can be used in a model-based planning (model-predictive control) with zero 'supervised' training data (for which the rewards are given), but using calls to the reward function to evaluate candidate sequences of skills and actions. The proposed approach is convincingly compared in several ways to both previous model-based approaches and model-free approaches.
This is a very interesting approach, and although I can already think of ways to improve, it seems like an exciting step in the right direction to develop more autonomous and sample efficient learning systems. I suggest to rate this submission as 'accept'.
Regarding the comparison to model-free RL: although it is true that no task-specific training is needed, a possibly large number of calls to the reward functions are needed during planning. It would be good to compare those numbers with the number of rewarded trajectories needed for the model-free approaches.
My main concern with the proposed method is how it would scale when the state-space becomes substantially larger (than the 2 dimensions x and y used in the experiments). The reason I am concerned is that the proposed method uses brutal sampling to search for good trajectories in z-space and action-space. It looks like the curse of dimensionality will quickly make this approach unfeasible. Also, it would be nice to have the learning system discover the important dimensions in which to plan (the x and y in the experiments), rather than having to provide them by hand.
A minor concern is the following: is it possible that the optimization could end up discovering a large number of highly predictable (and diverse) but useless skills?
In the related work section, 1st paragraph, in the list of citations, it might be good to also include the work on maximizing mutual information between representation of the next state and representation of the skill (Thomas et al, arXiv:1802.09484).
The definition of Delta (page 9) is strange: it is said that Delta should be minimized but Delta is defined as proportional to the rewards (which should be maximized). Maybe a sign is missing. Also, why not simply define the rewards as being normalized in the first place, so that the metric IS the accumulated reward rather than this unusual normalized version of it.",8.0
16,Fast Task Inference with Variational Intrinsic Successor Features,...,"Review: Summary:
This paper proposes an algorithm to combine the ideas of unsupervised skill/option discovery (Eysenbach et. al., 2018, Gregor et. al., 2016, referred to as “BMI” in the paper) with successor features “SFs” (Barreto et. al., 2017, 2018). While unsupervised skill/option discovery algorithms employ mutual information maximization of visited states and the latent variables corresponding to options (typically discrete), this paper adds a restriction that this latent variable (now continuous) should be the task vector specified by some learnt successor features.
With such a restriction, the algorithm can now be used in an unsupervised pre-training stage to learn conditional policies corresponding to several different task vectors and can be used to directly infer (without training or fine-tuning) a good policy for a supervised phase where external reward is present (i.e. via GPI from Barreto et. al., 2018) by simply regressing to the best task vector.
Such unsupervised pre-training is shown to outperform DIAYN (Eysenbach et. al., 2018) in 3 different Atari suites (including the full 57 game suite) and also ablations to the proposed model where GPI and SFs are excluded individually.
Decision:
I vote for accept as this paper proposes a novel technique to combine mutual information based intrinsic control objectives with successor features, which allow for combining the benefits of both in a complementary way. An unsupervised phase can now discover good conditional policies with successor features which can be used to infer a good policy to solve an external reward task in a supervised phase, with such a policy capable of attaining human level performance in several Atari games and outperforming several baselines such as DQNs in limited data regimes.
Other comments:
- The technique for enforcing the restriction in Eq. 10, as well as being able to use it with generalized policy improvement is a good novel contribution in the paper.
- The detailed comparison with baselines on the full Atari suite is sufficient to back the claims in the paper that the strengths of BMI and SFs do complement each other.
- The fact that fast task inference is sufficient to get good performance is impressive i.e. without the need to fine-tune the best inferred policy.
Minor typos:
- In section 5 para 5, “UFVA” -> “UVFA”, “UFSA” -> “USFA”. | Review: The authors address the problem of finding optimal policies in reinforcement learning problems after an initial unsupervised phase in which the agent can interact with the environment without receiving rewards. After this initial phase, the agent can again interact with the environment while having access to the reward function. To address this specific setting, the authors propose to use the successor feature representation of policies and combine it with methods that estimate policies in the unsupervised setting (without a reward function) by maximizing the mutual information of a policy-conditioning variable and the agent behaviour. The result is a method called Variational Intrinsic Successor Features (VISF) which obtains significant performance gains on the full Atari task suite in a setting of few-step RL with unsupervised pre-training. The main contribution seems to parameterize the successor features in terms of a variable specifying the policy. This variable will be the same as the linear weights in the linear model for the reward assume by the successor features representation. Finally, a discriminator aims to predict the linear weights of the policy from the observed state-feature representation.
Clarity:
I think this is one of the weaknesses of the paper. The writing and clarity could be significantly improved. How do you go from equation 8 to equation 9? How does q lower boun p? In the paragraph after equation 9 the authors mention the score function estimator. But very few details are given. After reading the paper, my impression is that the reproducibility of the results could be very hard, because of the lack of details of the specific implementation. The authors also do not mention that the code will be publicly available after acceptance.
I feel that, to better understand the method, the authors should include experiments in simple and easy to understand synthetic environments which can be more illustrative than ATARI.
What is the difference between the policy parameters theta and the conditioning variable z?
Novelty:
The proposed approach is novel up to my knowledge. I find the idea of parameterizing the successor features in terms of the policy parameters very innovative.
Quality:
The proposed approach seems well justified and the experiments performed indicate that the method can be useful in practice. However, I think it would be very useful to have results on other environments besides ATARI. For example, DIAYN contains experiments on a wide range of tasks, including gridworld style tabular experiments to illustrate what their method does. This work would benefit from similar simpler and easier to understand synthetic environments (unlike ATARI).
Significance:
The proposed contribution seems significant as illustrated by the experimental results and the novel methodological contributions. However, the lack of clarity and the difficulty in the reproduction of the results limit this.
Some minor comments:
I recommend the authors to remove references in the abstract.
Update after the authors' rebuttal:
After looking at the response from the authors, I believe that they have successfully addressed my concerns.
Therefore, I have decided to update my rating and vote for acceptance. I am looking forward to seeing the python notebook with the implementation of the VISR algorithm. | Review: The paper builds upon the idea of Successor Features; this is a principle used to facilitate generalization beyond the finite set of behaviors being explicitly learned by an MDP. The proposed paper ameliorates the need of defining the reward function as linear in some grounded feature space by resorting to variational autoencoder arguments. The derivations are correct, the motivation adequate, the experiments diverse and convincing. The literature review us up to date and the comparisons proper. This is a valuable contribution to the field.",7.333333333333333
17,Federated Learning with Matched Averaging,Keywords: federated learning,"Decision: Accept (Talk) | Review: This paper offers a beautiful and simple method for federated learning. Strong empirical results.
Important area.
. | Review: Edit: Thanks for the thorough and responsive rebuttal! I'm particularly happy to see the additional background on BBP-MAP and the baselines you've added for handling data bias. You've comprehensively addressed my questions and I think this paper should be accepted.
Original review:
The authors extend the recently proposed Probabilistic Federated Neural Matching (PFNM) algorithm of Yurochkin et al. ( 2019) to more kinds of neural networks, show that it isn't as effective for larger models as it is for LeNet-sized ones, and propose enhancements that lead to a state-of-the-art approach they call FedMA. I'm convinced that this represents a meaningful advance in federated learning, although the paper could use some tightening up, and the experiments are somewhat limited.
Some feedback:
- I'd like to see a little bit more description of BBP-MAP, as even though it's not one of the components of the algorithm you directly modify it's still the underlying mathematical primitive. How far is it from having the same effect that the ""best possible"" permutation would? How is it able to allow the number of neurons in the federated model to grow relative to the size of the client models?
- Can you include the ""entire data"" baseline in more of the figures/plots (especially Figure 2)?
- The models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application. Since federated learning is a problem domain motivated more by applied concerns (privacy, edge vs. cloud compute, on-device ML) than other areas of machine learning theory, it would be particularly valuable to see experiments at larger scale (in particular, on larger or more realistic datasets).
- The section that demonstrates how your model addresses skewed data domains is fascinating! That's one area in which your experiments are directly relevant to federated learning in practice, and it's a rapidly growing area of research in itself (e.g. in its relationship to causal learning that Leon Bottou has recently been exploring). Exploring this further could make for a whole separate paper. In the mean time, though, is there some kind of equivalent of the ""entire data"" baseline that would represent e.g. the best known technique for taking into account skewed domains outside the federated context? | Review: Post Rebuttal Summary
---------------------------------
I have nudged my score up to an ""Accept"", based on my comments to the rebuttal below. I hope the authors continue to improve the readability of Sec. 2.1
Review Summary
--------------
Overall I think this is almost above the bar to be accepted, and I could be persuaded with a strong rebuttal. The strengths here are the extensive experiments and the easy-to-implement method. The primary weakness of this paper is that it is a ""straightfoward"" way to extend the BBP-MAP method to CNNs and RNNs, so the methodological novelty is weak relative to the BBP-MAP past work (Yurochkin et al. ICML 2019). Other technical weaknesses limit the ability to use this method on clients with diverse class distributions, which will be common in real deployments.
Paper Summary
-------------
This paper addresses the problem of federated learning, where J separate ""clients"" with disjoint datasets each train a neural network model for a supervised problem, and then try to aggregate all J individual client models into one ""global model"" in a coherent way. The natural problem is that due to hidden units being permutable within one network, naively taking parameter averages across two client models will lead to bad accuracy without first coming up with a consistent ordering of the units in each layer.
Previous work (Yurochkin et al. ICML 2019) has developed a Bayesian nonparametric model based on the Beta-Bernoulli Process (BBP) for the case of federated learning of multi-layer perceptrons. However, the extension to convolutional layers or recurrent layers has yet to be solved, which is the focus of this paper.
This paper's algorithm (Federated Matched Averaging (FedMA), see Alg 1), proceeds by iteratively stepping thru the CNN or RNN layer by layer greedily from input to output. At each layer, we first solve a BBP-MAP optimization (bipartite matching using a BBP maximum a-posteriori objective as cost function, a subprocedure taken direclty from Yurochkin et al.). This obtains a consistent low-cost permutation for each client model. Then, the global model weights for that layer is the average of the aligned client weights. After the current layer update, each client keeps training, keeping all layers up to the current frozen but revising later layers. This layer-by-layer training can be applied to both CNNs and RNNs.
The proposed approach is compared to FedAvg and FedProx on MNIST and CIFAR image classification tasks with CNNs, and Shakespeare text classification tasks with RNNs. Later experiments explore the effect of communication efficiency (MB transfered between client and master), effect of local training epochs, handling biased class distributions, and interpretabilty.
Novelty & Significance
-----------------------
Solving federated learning problems is of increasing practical importance, and certainly trying to do so for CNNs and RNNs (more than just large MLPs) is important. So I like where the paper is going.
Although the method is ""new"", it is more or less a straightforward extension of work by Yurochkin et al. (ICML 2019) to CNNs and RNNs. If you read the last few sentences of Yurochkin et al., you'll see ""Finally, it is of interest to extend our model-ing framework to other architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The permutation invariance necessitating matching inference also arises in CNNs since any permutation of the filters results in the same output, however additional bookkeeping is needed due to the pooling operations."" I view this paper as a well-executed implementation of this ""bookkeeping"". Certainly not trivial, but to some readers perhaps not clearly ""above the bar"" for a top conference like ICLR.
Technical Concerns
------------------
## Concern 1: Client models will not always be alignable after permutation
My first concern is that there will not always be a one-to-one permutation of the neurons learned by two client models with different class distributions. Given fixed capacity at each layer, some clients may learn a filter for ""horse hooves"" (esp. if horse images are common to that client), while other clients may learn a filter for ""snake skin"" (if snakes are more common to that client). I wonder if we can quantify how well the aligned filters match in practice, and if there is any benefit to revising the alignment to allow some client-specific customizations (e.g. by having the global model can learn more units than the client model).
## Concern 2: Use of the BBP-MAP subprocedure poorly motivated
The paper prioritizes a clean and easy-to-implement algorithm to resolve practical alignment issues between client CNN and RNN models. However, I was a bit underwhelmed that the BBP-MAP solution used by Yurochkin et al. was treated as a black-box subprocedure without much justification. I could see 2 preferable alternatives to the current use of BBP-MAP. Either a simpler approach using Eq. 2 with a squared error cost and the Munkres algorithm to solve bipartitite matching to obtain the permutation (which seems more in spirit of the rest of the paper). Or, a more sophisticated probabilistic approach (taking a Bayesian hierarchical model from Yurochkin et al. seriously and forming the estimated global weights from a weighted sums that includes both the clients (weighted by dataset size) and the assumed prior). As it is, I feel the BBP-MAP subprocedure in the current Algorithm 1 is poorly motivated for the task at hand.
Experimental Evaluation
-----------------------
Overall the experiments were extensive and demonstrated several apparent advantages (reduced need to transfer large memory during communication, etc.).
Minor Presentation Concerns
---------------------
Before Eq. 2, you should introduce the ""\theta"" notation
I'm a bit confused about how ""FedMA"" differs from ""FedMA with communication"", even after reading Sec. 2.3. How exactly are communicate costs kept down? What are you sending from master to client at beginning of every ""round"" if not the full global model (all weights of the CNN)?",5.75
18,GenDICE: Generalized Offline Estimation of Stationary Values,AddPublic Comment,"Review: This paper proposes a new estimator to infer the stationary distribution of a Markov chain, with data from another Markov chain. The method estimates the ratio between stationary distribution of target MC and the empirical data distribution. It is based on the observation that the ratio is a fixed point solution to certain operators. The proposed method could work in the behavior-agnostic and undiscounted case, which is unsolved by the previous method.
This paper tackles an interesting problem with an increasing number of studies in the reinforcement learning community and gives a practical algorithm with strong empirical justification, as well as theoretical justification. I think this paper should be accepted.
Detailed comments:
1) This paper provides experiment results in multiple domains, including two continuous control domains which are more complex than experiments in previous OPE methods. The paper also provides many details about the learning dynamics and ablation studies, which is very useful for the reader to understand the result of the paper.
2) The theoretical result is as same strong as previous work DICE and DualDICE, under similar assumptions.
3) I appreciate this paper formalizes the two difficulties of degeneration and intractability, and then explain how those are addressed in a principled way. Degeneration is important and is at least ignored in two similar works on this topic. | Review: In this paper the authors proposed a framework for off-policy value estimation under the scenario of infinite horizon RL tasks. The new proposed method utilize the variational representation of
f
-divergence, which quantifies the difference between
Tτ
and
τp
, where
τ
is the parametric density ratio between the unknown behavior policy data and the target policy. If only if
τ
is the true density ratio, the loss
Df(Tτ||τp)=0
.
Compared with prior work (Nachum et. al 2019), the new proposed framework can generalize the undiscounted case
γ=1
, and the derivation for the new algorithm is quite simple and easy to follow. The experimental results show the advantage of the proposed methods over baseline methods such as model-based, DualDice etc, for both discrete and continuous cases. Moreover, I have two specific questions:
- The choice of
f
-divergence. Although the author mentioned the difficulty of using the dual representation of KL divergence, it would be nice to have an ablation study that shows the effectiveness of various
f
-divergence (Personally I think Jensen-Shannon Divergence may be also a good choice).
-The authors should also have a discussion that similar idea can be generalized to more general distribution metrics such as Integral Probability Metrics, specifically wasserstein-1 distance (similar to wasserstein-gan) or maximum mean discrepancy (Maybe it is unnecessary to conduct experiments, some discussion should be enough to clarify the relationship. I think there is a concurrent submission using MMD metrics).
Overall I think this is a good paper and I recommend for acceptance.
Reference Papers:
- Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. ""f-gan: Training generative neural samplers using variational divergence minimization."" Advances in neural information processing systems. 2016.
- Arjovsky, Martin, Soumith Chintala, and Léon Bottou. ""Wasserstein gan."" arXiv preprint arXiv:1701.07875 (2017).
- Nachum, Ofir, et al. ""DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections."" arXiv preprint arXiv:1906.04733 (2019).
- Anonymous, “Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning”, submitted to ICLR 2020. | Review: Main contributions:
This paper generalizes the recent state-of-the-art behavior agnostic off-policy evaluation DualDice into a more general optimization framework: GenDice. Similar to DualDice, GenDice considers distribution correction over state, action pairs rather than state in Liu et al. (2018), which can handle behavior-agnostic settings. The optimization framework (in equation (9)) is novel and neat, and the practical algorithm seems more powerful than the previous DualDice. As a side product, it can also use to solve offline page rank problem.
Clarity:
This paper is well established and written.
Connection of theory and experiment:
I have a major concern for the theory 1 about the choice of regularizer
λ
. For infinite samples case, the derivation of theory 1 is reasonable since both term is nonnegative. However, in practice we will have empirical gap for the divergence term, thus picking a suitable
λ
seems crucial for the experiment. I think a discussion on
λ
for average case in experiment part should be added. And compared to Liu et al. (2018) which normalized the weight of
τ
in average case, which one is better in practice?
Overall I think this paper is good enough to be accepted by ICLR. The optimization framework can also inspire future algorithm using different divergence.",8.0
19,Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning,...,"Decision: Accept (Talk) | Review: The authors consider two problems: Overcomplete dictionary learning (ODL)
and convolution dictionary learning (CDL).
Dictionary learning learns a matrix factorization of the data
Y = A X
where A is the dictionary and X is the (known to be sparse) code.
Y consists of n rows (sample size) and p columns (dimension).
In the overcomplete version A is n x m where m > n, i.e. the number of learned
features is larger than the sample size.
The CDL problem is a special case of the ODL problem where the dictionary
matrix is known to consist of convolution filters instead of being unstructured.
The authors show that under a given set of assumptions local nonconvex
optimization can be used to find globally relevant solutions.
The basic assumptions are:
(i) unit norm tight frame
(ii) mu-incoherence
(relates the angles of the columns of a, e.g.\ if columns are orthogonal,
they are incoherent / have small mu)
(ii) stochastic model of the code X that says entries are Gaussian and sparse
according to a Bernoulli random variable
The authors present the idea of maximizing the l^4 norm of A^T q in order to
find q as rows of A.
Apparently l^4 norm maximization leads to ""spikiness"" which is exactly
desirable under mu-incoherence.
The authors show (assuming p \to \infty) that the optimization nonconvex
landscape (constrained to the sphere) does not contain any stationary points
without negative curvature.
A saddle avoiding optimizer therefore converges to local minimizers from
random initialization.
The authors also show that the analysis extends to CDL via a preconditioned
initializer.
Finally, they go on to briefly show some experiments that further validate
the theory presented in the paper.
Overall, the authors present a rigorous technical analysis using powerful
mathematical tools for nonconvex optimization (which is relevant to many
machine learning problems).
I am recommending to accept based on the high quality of the work.
But I am not confident as to the accessibility of the paper to the wide
audience of ICLR as it is rather technical.
Perhaps, the complete contribution would be better suited as a journal article.
Notes:
It would have been useful to give some more intuition about what ""spikiness""
of A^T q is, why spikiness exists under mu-incoherence and why l^4 norm
maximization improves spikiness.
I am not sure that the inclusion of the CDL problem is beneficial for a
converence paper and would rather have more space allocated to the intuition on
why the method works for ODL. | Review: This paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.
For the over-complete dictionary setting, given the measurements of the form
Y=AX
, where
A
and
X
denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an
ℓ4
-norm maximization approach to recover the dictionary
A
. This corresponds to maximizing
∥qTY∥44
over
q∈Sn−1
. Interestingly, the paper shows that when
A
is unit norm tight frame and incoherent the optimization landscape of the aforementioned non-convex objective has strict saddle points that can be escaped by along negative curvature. Furthermore, all local minimizers are globally optimal which are close to one of the columns of
A
. This shows that any descent method that can escape the saddle points will (approximately) recover one of the columns of
A
.
For convolution dictionaries, the paper shows that when the underlying filters are incoherent a suitably modified
ℓ4
-norms based objective has only strict saddles over a sub-level set. Furthermore, all local optimizers within this sub-level set are close to one of the convolution filters.
The reviewer believes that this paper presents many interesting and novel results that extend our understanding of provable methods for dictionary learning. As claimed in the paper, this the first global characterization for the non-convex optimization landscape for over-complete dictionary learning. Besides, the paper provides the first provable guarantees for convolution dictionary learning. Overall, the paper is very well written and the key ideas used in the paper are nicely explained in the main body of the paper. The experimental results in the paper also corroborate the theoretical findings of the paper.
Minor comments:
1. In page 2, ""....can be simply summarized by the following slogan."" ---> ""....can be simply summarized by the following statement.""?
2. In page 7, replace ""cook up"" with ""propose""?
------------------------------
After rebuttal
Thank you for the response. Releasing the code for reproducibility purposes is certainly a great idea. | Review: [Summary]
This paper studies the problem of non-convex optimization for Dictionary Learning (DL) in the situation when the underlying dictionary is over-complete (more basis vectors m than the dimension n). The paper proves that the L4 maximization formulation has a nice global landscape and can be efficiently minimized by (Riemannian) gradient descent, when the over-complete ratio m/n is less than an absolute constant. A similar result is proved for convolutional dictionary learning.
[Pros]
The theoretical results in this paper provides a solid improvement over the prior understandings on overcomplete DL, a setting that is practically important yet theoretically more challenging than standard orthogonal/complete DL.
Specifically, the prior work of (Ge & Ma 2017) shows only a nice local optimization landscape when m > n^{1+\eps} and hypothesizes that the global landscape is bad in the same setting (there exists bad local minima out of a certain sub-level set). In comparison, this work proves that at least for m/n <= 3 (roughly), the landscape is globally benign (has the strict saddle property), therefore providing a new understanding that the benign landscape is still preserved from “the other side” where m/n grows mildly above 1. The analysis contains novel technicalities and can be of general interest for understanding the landscape of non-convex problems.
The paper also provides experimental evidence that gradient descent converges globally up until m = O(n^2), a broader regime than suggested by the theory (m <= 3n). (Though when m >= n^{1+\eps}, the reason of global convergence from random init may be far from the present theory, in that there can be potentially exponentially many bad local min yet gradient descent won’t get trapped.)
[Cons]
It is still a bit disturbing to see that m/n needs to be bounded by a fixed absolute constant, rather than *any* constant, for the theory to work. From the proofs it seems like this constant (3) may have the potential to be improved, but it is not quite easy to completely get rid of it?",7.0
20,Gradient Descent Maximizes the Margin of Homogeneous Neural Networks,"Keywords: margin, homogeneous, gradient descent","Decision: Accept (Talk) | Review: This is a strong deep learning theory paper, and I recommend to accept.
This paper studies the trajectory induced by applying gradient descent/gradient flow for optimizing a homogeneous model with exponential tail loss functions, including logistic and cross-entropy loss in particular. This is an important direction in recent theoretical studies on deep learning as we need to understand which global minimizer the training algorithm picks to analyze the generalization behavior.
This paper makes a significant contribution to this direction. This paper rigorously proves gradient descent / gradient flow can maximize the L2 margin of homogeneous models. Existing works mostly focus on linear models or deep linear networks, and comparing with Nascon et al., 2019a, the assumptions in this paper are significantly weaker. Furthermore, this paper provides convergence rates, which seem to be the first work of this kind for non-linear models.
I really like Lemma 5.1. This is not only a technical lemma for proving the main theorem. Lemma 5.1 itself has a nice geometric interpretation. It naturally decomposes the dynamics of the smoothed version into a radial component and a tangential velocity component. I believe this lemma can be useful in other settings as well.
Comments:
The bibliography should be fixed. Some papers are already published, so they should not be cited as the arXiv version, and author lists in some papers have ""et al.""
-----------------------------------------------------
I have read the rebuttal and I maintain my score. | Review: The goal of the paper is to formally prove that gradient flow / gradient descent performed on homogeneous neural network models maximizes the margin of the learnt function; assuming gradient flow/descent manages to separate the data. This is proved in two steps:
1. Assuming that gradient descent manages to find a set of network parameters that separate the data, thereafter gradient flow/descent monotonically increases the normalized margin (rather an approximation of it).
2. The limit points of optimization are KKT points of the margin maximization optimization problem.
While the main body of the paper presents a restricted set of results, the appendix generalizes this much further applying it to various kinds of loss functions (logistic/cross-entropy, exponential), to multi-class classification and to multi-homogeneous models. There seem to be many subtleties in the proofs and the paper seems to be quite thorough. (I must say that I'm not expert enough to assess the technical novelty of this paper over prior works.)
Recommendation:
I recommend ""acceptance"". The paper takes a significant step by unifying existing results on margin maximization and going beyond them.
Technical comments:
- It is clear that in order to define margin meaningfully, some form of normalization is necessary. But a priori,
∥θ∥2L
is not the *only* choice;
∥θ∥L
could also work for any norm
∥⋅∥
. But perhaps the choice of
∥⋅∥2
is special (as Thm 4.4 suggests). It will be nice to have some insights/comments on why this choice of
∥⋅∥2
based normalization is the right one.
- The paper argues that having a larger margin helps in obtaining better robustness to adversarial perturbations (within
∥⋅∥
balls for some choice of
∥⋅∥
). However note that the notion of ""margin"" is not just a function of the decision boundary, but instead depends on the specific function computed by the neural network --- this is unlike margin maximization in linear models, where ""margin"" in determined entirely by the decision boundary. As the paper argues, if we have an upper bound on the Lipschitz constant w.r.t.
∥⋅∥
norm, then we get a lower bound on required adversarial perturbations for any training point. However, this does not mean that training longer is necessarily better because by doing so, we might end up with a larger Lipschitz constant (even after normalizing). So even if the ""margin"" is larger, the actual adversarial perturbations (in
∥⋅∥
norm) allowed might get smaller. So I'm not sure how relevant this result is for adversarial robustness. | Review: This paper studies the implicit regularization phenomenon. More precisely, given separable data the authors ask whether homogenous functions (including neural networks) trained by gradient flow/descent converge to the max-margin solution. The authors show that the limit points of gradient descent are KKT points of a constrained optimization problem.
-I think that the topic is important and the authors clearly made some interesting insights.
-The main results of this paper (Theorem 4.1 and Theorem 4.4) require that assumption (A4) is satisfied. Assumption (A4) essentially means, that gradient flow/descent is able to reach weights, such that every data x_n is classified correctly. To me this seems to be a quit restrictive assumption as due to the nonconvexity of the neural net there is a priori no reason to assume that such a point is reached. In this sense, the paper only studies the latter part of the training process.
I feel that Assumption (A4) clearly weakens the strength of the main results. However, because the topic studied by the paper is interesting and the authors have obtained some interesting insights, I decided to rate the paper as a weak accept.
Typos:
-p. 4: ""Very Recently""
-p. 7 and p. 9: ""homogenuous"" (instead of ""homogeneous"")
----------
I want to thank the authors for their response. However, I will stand by me evaluation and will not change it.
I agree though that assumption (A4) is indeed reasonable, although of course very strong.",8.0
21,GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding,"Keywords: graph embedding, unsupervised learning, multi-level optimization, spectral graph theory","Review: The paper provides a multi-level graph-coarsening approach that can improve the predictive and computational performances of numerous existing unsupervised graph embedding models. The proposed approach is a pipeline consisting of 4 steps, viz: 1) Graph Fusion - that fuses attribute similarity graph with network topology, 2> Graph Coarsening - that reduces the graph size iteratively, 3> Graph embedding - using existing models and 4> Embedding refinement. While such a pipeline for scaling using a graph coarsening and refinement based approach is not new, the authors have carefully designed the pipeline to be effective and be scalable such as without any costly learning components (as in mile). The effectiveness of the proposed approach is evaluated with the node classification task on 6 datasets.
Strengths:
- The paper addresses a very important problem. The paper proposes a well-designed pipeline to scale existing embedding models.
- Experimental results support that the proposed approach is effective, especially in terms of reducing computation complexity.
Weaknesses:
- While the experimental results are convincing on the computation front, I have few concerns on the performance front.
a) 'MILE with the fused graph' baseline is missing. It can been seen from Figure 3 that the incorporation of the attribute graph provides a significant performance benefit. Thus it is necessary to have this baseline to understand the improvement gap w.r.t to MILE. I believe this is a fair comparison to make as the graph fusion component is a commonly used technique in the last decade.
b) Improvements are inconclusive without additional results on other standard non-attributed graph datasets. In Figure 3, ignoring the model with the fused graph, MILE seems to be comparable to GraphZoom overall. As with the existing results, it's not conclusive whether GraphZoom is better than MILE. Also, add variance and report t-test results.
c) That said, it can be seen from Figure 2, that GraphZoom significantly outperforms both DW and MILE(DW) on a large non-attributed dataset. However, it is not clear where the significant increase in performance benefits stems from. More analysis is required here.
- Results on other unsupervised embedding task missings. It is important to evaluate the embeddings additionally for the link prediction task at the least.
Additional comment:
- It would be helpful to incorporate one if not some of the attributed graph embedding model as a base model and baseline, such as Deep Graph Infomax (DGI).
- It should be easy to use a mini-batch version of GCN with MILE and use it for inductive learning.
- It would interesting to see what the performance will be without the refinement step.
If my concerns regarding the experiments are positively addressed, I'm willing to improve the score.
-----------------
After the rebuttal, I have updated my score from 3 to 8 as the authors have satisfactorily responded to the concerns raised. | Review: Summary: This paper proposes GraphZoom, a framework for augmenting unsupervised graph embedding methods by (a) fusing feature information into the graph topology, (b) learning embeddings on a coarsened graph, and (c) refining the coarsened embeddings to obtain embeddings for the original graph nodes. In particular, a nearest neighbor graph over node features is computed and this adjacency matrix is linearly combined with the original adjacency matrix to obtain a graph with feature information ""fused in"". The graph is then coarsened using a spectral approach, embeddings are learned on the coarsened graph (via any strategy), and the embeddings are then refined back to the original nodes (again using a spectral approach). The authors take care to heed the advice of Maehara et al. and remove high-frequency information from the features.
Assessment: Overall, this is a borderline contribution with some interesting motivation, original ideas, and sound derivations. However, the primary limitation of this work is the empirical comparison. First, the empirical comparison includes DeepWalk and GraphSAGE as the two base models, and while these are reasonable models, they are known to no longer be state of the art in this area (e.g., see https://arxiv.org/pdf/1809.10341.pdf). It would be more appropriate to include a more recent and better performing method (e.g., DGI; linked previously), as the reported numbers are very far from state-of-the-art. In addition---and perhaps a more concerning issue---is that seems that a randomly initialized GCN can obtain similar or superior performance compared to the numbers reported in this work (again, see the DGI paper linked above). While it is possible that GraphZoom+DGI or GraphZoom+[some other more recent method] could achieve stronger results, the fact that the current results seem to be below performance of a randomly initialized GCN is a major issue. Stronger empirical results with better baselines and base models would drastically improve the paper.
As another point regarding the empirical results, the datasets used are known to be problematic (e.g., see https://arxiv.org/abs/1811.05868). If these datasets are used, then multiple random splits should be employed and more robust summary statistics should be reported.
Regarding the fusion step, there were also two points that should be addressed in the paper:
1) It seems that this fusion setup is assuming that the network exhibits homophily (i.e., it assumes that nearby nodes have similar features). This is common in many networks (e.g., the benchmarks that are analyzed) but not always the case. Some commentary on when (if ever) this fusion process might *not* be appropriate would improve the paper.
2) The authors state the they use the coarsening process to compute the nearest neighbor graph in order to avoid the quadratic time complexity. However, there are numerous well-established approaches to deal with this issues (e.g., locality sensitive hashing). Why was one of these standard approaches not employed?
Reasons to accept:
- Original and well-motivated idea
- Clearly written paper
Reasons to reject:
- Problematic empirical evaluation (e.g., lacking recent baselines)
- Several performance numbers appear to be below random GCN baseline performance
- General applicability of the approach (e.g., to non-homophilous networks) is not clear | Review: Summary: The authors propose a way to fuse information on nodes of a graph with the topology of the graph in the large scale setting. The proposed approach is done in four phases where (i) the covariates in the nodes of the graph is first mapped in the graph space for fusion and fused using linear combination of the topological graph and feature graph, (ii) the resulting ""adjacency"" matrix will almost surely not be sparse even if the original graph space, so they use eigenvalues of the graph laplacian to coarsen the graph -- remove edges; (iii) they then propose to embed the coarsened graph using ""any"" unsupervised learning technique; (iv) then the embedded representation is refined using iterative procedures. Cheap procedures are introduced to do Phases (i) and (iv). Experimentally the authors see improvements in the performance using their approach compared to the baselines considered.
Novelty: 1. The approach suggested in this paper is already there in MILE Fig 1., the authors mention that MILE requires training GCN but I am not sure why this is critical. The authors mention that ""MILE cannot support inductive embedding
models due to the transductive property of GCN"", can you clarify what this means? I guess one can easily replace GCNs?
2. Covariate adjusted clustering is known to work only when when the features are independent like Stochastic Block Model, see Covariate-assisted spectral clustering by Binkiewicz et al, 2014. Is there a reason why the features that we see on nodes are not correlated?
Results: It is hard to see where the performance improvement actually comes from, if at all. It is interesting to see that the proposed approach saves time and is more accurate in the variety of settings considered, but it is not clear why we see the improvement.
After rebuttal: I have raised my score to 6 after going through the authors' response for my questions, and other reviewers' concerns. While the approach performs well in many datasets (thanks to the authors for providing more experimental evidence!), I'm still not convinced with the authors' response on their fusion step -- it seems to me that node attributes are ""side"" information, that can ""boost"" the signal on the original neighborhood graph. Recall that spectral approaches do have a fundamental barrier -- they fail on ""thin"" graphs (see https://arxiv.org/abs/1608.04845 ). Hence, node covariance/fusion matrix being dense will be a blessing for spectral approaches since they make spectral methods work. However, is this what we want in *all* the cases? I'm not sure. This means that the choice of \beta in their fusion step is *very* important, and I don't see any plots on the sensitivity of their procedure with respect to \beta. I kindly request the authors to include a plot or results showing the sensitivity of the final results with respect to the choice of \beta. Thanks!",7.333333333333333
22,Harnessing Structures for Value-Based Planning and Reinforcement Learning,-''-,"Review: This paper introduces an interesting idea of exploiting the low-rank structure of the value function to reduce the computation complexity of value-based RL algorithms. Instead of working in the reduced space, they focus to operate on the original space and reduce the computation by looking at few elements and inferring the rest. They use a matrix completion/estimation strategy to infer the global structure from a smaller set of samples. They show empirical evidence of the low-rank structure in few classical control tasks (Mountain Car, Inverted Pendulum, Cart Pole), and provide an iterative procedure - Structure Value-based Planning (SVP), that is similar to value iteration but is able to exploit the low-rank structure to reduce the computational time. They also provide a deep RL extension - SV-RL, that can be applied to value-based methods. They test the efficiency of their approach to Atari games.
Overall this paper presents an interesting idea, that also scales to the deep RL algorithms. However, there are a few missing components that need to be addressed in order to fully support the claims. Given these clarifications in the author's response, I would be willing to increase the score.
1) Nature of the regularization for SV-RL.
The authors are proposing a form of regularization that enforces low-rank structure for the value function (and target Q-values in particular for deep RL agents). The authors show that this form of regularization is helpful for improving the learning for low-rank tasks, and for tasks that have a high-rank the performance is worse. This kind of regularization balances having a low-rank and small reconstruction error. However, shouldn’t the regularization also depend on the size of the sub-matrix (the minibatch)? However, I didn’t find any experiments related to how changing it affects performance. Also, is the regularization related to due to the random projections (Johnson–Lindenstrauss lemma)?
2) It is important to note that SV-RL is limited to Deep Q-learning based techniques. So it can’t be applied to any value-based method, especially when the samples in the sub-matrix are correlated.
3) Missing literature that exploits Low-rank structure for planning. There is literature on RL that is based on exploiting the low-rank structure for planning [1, 2, 3].
4) All the experiments are in deterministic environments? Is there a reason behind this?
5) (Optional) This regularization introduces error in reconstructed approximate Q-values. It will be useful to have some analysis on how far it deviates from the optimal value function. There has been work in the field [4, 5] that I believe can be used to help derive an analysis of the kind of approximation error bounds that are being introduced here.
References:
[1] Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954–966, 2011
[2] Pierre-Luc Bacon, Borja Balle, and Doina Precup. Learning and planning with timing information in markov decision processes. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence , pp. 111–120. AUAI Press, 2015.
[3] Sylvie CW Ong, Yuri Grinberg, and Joelle Pineau. Goal-directed online learning of predictive models. In European Workshop on Reinforcement Learning, pp. 18–29. Springer, 2011.
[4] Klopp, Olga. ""Noisy low-rank matrix completion with general sampling distribution."" Bernoulli 20.1 (2014): 282-303.
[5] Recht, Benjamin. ""A simpler approach to matrix completion."" Journal of Machine Learning Research 12.Dec (2011): 3413-3430. | Review: The study is motivated by the observation that the Q-value matrix in reinforcement learning problems often has a low-rank structure. The paper proposes an approach called structured value-based planning or learning, where the Q matrix or the Q function is estimated from incomplete observations based on the prior that it is low-rank. The proposed strategy is demonstrated in stochastic control tasks and reinforcement learning applications.
The paper is clearly written and the experimental results show that the proposed strategy leads to performance gains especially in problems where the Q matrix indeed conforms to a low-rank model. A few comments and questions:
- The assumption that the Q matrix should be low-rank is demonstrated with several experiments. Is there any theoretical motivation or guarantee for this assumption as well?
- The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem?
- The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs?
- Please clearly define the notation used in Section 4.2. | Review: Summary:
This paper develops a method for taking advantage of structure in the value function to facilitate faster planning and learning. The key insight is that MDPs with low rank Q^* matrices can be solved more expediently using matrix estimation methods, both for classical dynamic programming methods (value iteration) and for learning in rich environments using recent model-free deep RL techniques. Thorough empirical analysis is conducted both for value iteration in tabular MDPs and for deep RL in rich environments. These experiments highlight new findings about the role the rank of the Q matrix plays in planning convergence and learning rates.
I view this paper as containing several key contributions: first, the analysis on the role of Q rank in planning and learning---experiments conducted indicate that even complicated environments tend to have low rank Q matrices (when approximated). Highlighting the role of this rank and the corresponding empirical analysis estimating it in benchmark RL and control tasks is, to my knowledge, novel. Second, and perhaps the most significant contribution, is ""Structured Value RL"" (SV-RL), an easy-to-apply method that can be incorporated into many Q-based deep RL methods with little overhead. The empirical results are compelling: across three different variations of DQN-like architectures, the SV RL augmentation tends to improve learning. Presentation of results is rigorous, too, and provide strong evidence that the method works.
As the paper mentions, theoretical analysis on the impact of Q-rank on dynamic programming (and perhaps learning) would be of great interest to the community. I take this analysis to be out of scope for this paper, but could see the work motivating future investigation into these questions.
Verdict: Overall, I take this paper to present many novel insights, establish solid motivation with good writing and examples, and offers compelling evidence about the strength of SV RL. I recommend accepting the paper.
Comments:
C1: The visuals throughout the paper are helpful!
C2: The paper is well written: the use of examples was effective in developing the motivation.
C3: Section 2 is helpful for understanding the ideas developed in the paper. However, there are many well developed planning frameworks for MDPs that trade-off optimality with computational efficiency. It might be worth discussing some of these methods up front. For instance, Bounded Real-Time Dynamic Programming (McMahan et al. 2005) explicitly uses value function structure to improve planning speed, with performance guarantees, as does Focused RTDP (Smith and Simmons 2006). I don't take the computational complexity improvements of the proposed method to be the primary contribution, so just a brief discussion to contextualize the work against other planning literature would be helpful.
C4: While the ""rank"" studied here is of a different form, some discussion of the Bellman Rank work (Jiang et al. 2017) might be useful for differentiating the two notions of ""rank"" at play, and how they are each used to expedite learning. The Bellman Rank is used as a measure of complexity of an MDP---Jiang et al. develop an RL algorithm that has sample complexity that depends on this measure. It is not strictly necessary, but I could see multiple uses of ""rank"" appearing in the RL literature as a means of exploiting structure for faster learning being confusing. If space (perhaps in the appendix if not), a sentence or two differentiating the two ideas might be helpful to readers. Additionally, the study of sparsity in value function representation was studied by Calandriello et al. 2014. If space permits, the paper might benefit from some discussion of the relation to this work.
Questions:
Q1: In the inverted pendulum results, I am curious about the effect of the discretization on plan quality. Specifically: how were 2500 states and 1000 actions chosen? Were different orders of magnitude (for both values) considered? How did this impact SVP? Does the rank change as the discretization becomes more or less coarse? I don't think this is strictly critical for the paper, but a few sentences clarifying this point would be informative.
Q2: Figure 4 provides nice insights into how to scale these ideas to deep RL. How were the four games chosen? Is there anything special that motivated their selection?
Q3: Additionally, I am curious about whether the results from Figure 4 are the consequence of algorithmic decisions, rather than the environment. Is it possible to determine whether different value based methods (or different choices of hyperparameters) lead to different outcomes? For instance, I could imagine a more shallow network, or a tighter bottleneck, leading to Q evaluations that produce higher rank.
Typos and Writing Suggestions:
[Abstract]
- This sentence is quite long, and I had a hard time following it as a result: ""As our key contribution, by leveraging..."". Consider dividing into two sentences.
[Intro]
- Oxford comma: ""control, planning and reinforcement learning""::""control, planning, and reinforcement learning
- ""the structured dynamic""::""the structure in the dynamics""
- ""where much fewer samples""::""where fewer samples""
- Consider rewording: ""almost the same policy as the optimal one"". Is it that the policies are in fact the same? Or that their values are close? Perhaps: ""a policy with near optimal value"".
- When introducing Double DQN and Dueling DQN for the first time it would be appropriate to cite each (end of Section 1).
[Sec. 2: Warm Up]
- ""understand the structures""::""understand the structure""
- ""give a strong evidence for""::""provide evidence that""
- ""exploit the structures for""::""exploit structure in the value function for""
- I think the italicized statement at the top of page 3 could be sharpened. The antecedent currently stating ""why not"" is quite a soft statement compared to the motivation the section develops. Consider changing: ""...why not enforcing such a structure throughout the iterations?""::""...then enforcing such a structure throughout planning can improve the rate of convergence"".
[Sec. 3: Structured ... Planning]
- ""even non-convex optimization approaches (...""::""even non-convex optimization approaches to solving this problem (...""
- ""offer a sounding foundation for future""::""offer a sound foundation for future""
[Sec. 4: Structured ... RL]
- ""Previously, we start by""::""Previously, we started by""
- ""which in deep scenarios""::""which in scenarios with large state spaces"", or perhaps: ""which in deep scenarios""::""which in scenarios where value function approximation is used""
References:
Calandriello, Daniele, Alessandro Lazaric, and Marcello Restelli. ""Sparse multi-task reinforcement learning."" Advances in Neural Information Processing Systems. 2014.
Jiang, Nan, et al. ""Contextual decision processes with low Bellman rank are PAC-learnable."" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
McMahan, H. Brendan, Maxim Likhachev, and Geoffrey J. Gordon. ""Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees."" Proceedings of the 22nd international conference on Machine learning. ACM, 2005.
Smith, Trey, and Reid Simmons. ""Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic."" AAAI. 2006.",7.333333333333333
23,High Fidelity Speech Synthesis with Adversarial Networks,"Keywords: texttospeech, speechsynthesis, audiosynthesis, gans, generativeadversarialnetworks, implicitgenerativemodels","Review: This paper puts forth adversarial architectures for TTS. Currently, there aren't many examples (e.g. Donahue et al, Engel et al. referenced in paper) of GANs being used successfully in TTS, so this papers in this area are significant.
The architectures proposed are convolutional (in the manner of Yu and Koltun), with increasing receptive field sizes taking into account the long term dependency structure inherent in speech signals. The input to the generator are linguistic and pitch signals - extracted externally, and noise. In that sense, we are working with a conditional GAN.
I found the discriminator design very interesting. As the comment below notes, it is a sort of patch GAN discriminator (See pix2pix, and this comment from Philip Isola - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39) and that is could be quite significant in that it classifies at different scales. In the image world, having a single discriminator for the whole model would not take into account local structure of the images. Likewise, perhaps we can imagine something similar in the case of audio at varying scales - in fact, audio dependencies are even more long range. That might be one reason why the variable window sizes work here.
The paper also presents to image analogues for metrics based on FID and the KID, with the features being taken from DeepSpeech2.
I found the speech sample presented very convincing. In general, the architectures are also presented quite clearly, so it seems that we might be able to reproduce these experiments in our own practice. It is also promising that producing good speech could be achieved by a non-autoregressive or attention based architecture.
The authors mention that they hardly encounter any issues with training stability and mode collapse. Is that because of the design of the multiple discriminator architecture? | Review: This paper proposes to enable GAN based TTS in the time domain with the careful designs of the (non-autoregressive) generator and discriminator. There have been various trials of GAN-TTS but not so many success and I'm glad to hear that the proposed method seems to enable GAN-TTS with fast inference thanks to the non-autoregressive property. The method also proposes new objective measures inspired by the image recognition network based on the high-level features generated by end-to-end ASR, which is also another important contribution of this paper.
My concern for this paper is reproducibility. Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator. Apart from that, the paper is well written overall by well describing the trend of GAN studies in the image processing and the application of such image processing oriented GAN techniques to TTS. | Review: I want thank the authors for solving this long-standing GAN challenge in raw waveform synthesis. With all due respect, previous GAN trials for audio synthesis are inspiring, but their audio qualities are far away from the state-of-the-art results. Although the speech fidelity of GAN-TTS is still worse than WaveNet and Parallel WaveNet from the posted sample, it has begun to close the significant performance gap that has existed between autoregressive models and GANs for raw audios. Overall, this is a very good paper with significant contributions to the filed.
Detailed comment:
1, In WaveNet, the conditional features (linguistic / mel-spectrogram) are added as bias terms in the convolutional layers. Did the authors tried this alternative architecture for the generator, which uses the white noisy z as network input (similar as flow-based models, e.g., Parallel WaveNet) and the conditional features as bias term in the convolutional layers?
2, Could the authors comment the importance of serval architecture choices in this work? From Table 1, it seems to me that the ensemble of random window discriminators is the most important (perhaps the only important) contributing factor for the success. For example, the MOS score was boosted from 1.889 to 4.213 by replacing a single full discriminator to the ensemble of RWDs.
3, The notations in Eq. (1) and (2) are messy. Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time.
4, The stable training (NO model collapses) is pretty impressive. Could the authors shed some light on the potential reason? Does the ensemble of RWD regularizes the training? What's your experience for training FullD (does not have random window ) and cRWD_1 (only has one random window discriminator)? Are they still very stable? Also, could the authors comment on the importance of large batch size -- 1024 for stable training of GAN-TTS?
5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).
Yamamoto et al. Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation. 2019.
=== update ===
Thank you for the detailed response.
2, Thanks for the elaboration.
4, It would be very interesting to see an analysis of model stability with smaller batch sizes.",7.333333333333333
24,Implementation Matters in Deep RL: A Case Study on PPO and TRPO,...,"Decision: Accept (Talk) | Review: This paper investigates the impact of implementation ""details"", with existing implementations of TRPO and PPO as examples. The main takeaway is that the performance gains observed in PPO (compared to TRPO) are actually caused by differences in implementation, and not by the differences between the two learning algorithms. In particular, adding to the TRPO code the same implementation changes as in PPO makes TRPO on par with (and possibly even better than) PPO. The clipping objective of PPO is also found to have no significant impact on its performance. This calls for more careful comparisons between algorithms (by minimizing implementation changes and more in-depth ablation studies) than has typically been done until now in the RL research community.
Although this paper is pretty straightforward and does not bring meaningful algorithmic improvements, I still believe it should be accepted as reproducibility and evaluation are a major issue in RL, and people need to be aware of these kinds of implementation differences that can affect the reported results.
My only important concern is that I could not find a link to the code, which I believe is a must for such a paper focusing on implementation. Could the authors please confirm that they will release their code?
Other small remarks:
- Fig. 1 is hard to read, I think more synthetic results could have easily conveyed more clearly the intended message
- When referring to Fig. 2 and 3 please specify ""left"", ""middle"" or ""right""
- Fig. 2's caption should describe the plots in left to right order (also what does ""maximum versus mean KL"" mean?)
- Fig. 3's caption lists mean KL twice on its first line
- ""The trust region for PPO-NoClip bounds KL to a lesser degree"": this is confusing as it sounds like it is ""less bounded"" while it is actually ""more bounded"" (as said in Fig. 3's caption)
- It would help comparing Fig. 2 and Fig. 3 if they both used the same y axis range
- Typo: ""enforcing"" => enforces
Update after author feedback: increasing score to ""Accept"" thanks to the release of the code | Review:
# Summary
The papers studies the effects of code-level optimization on the performance of TRPO and PPO. Details, usually considered as implementation-level particularities, are shown to be of crucial importance for the algorithms' performance.
# Decision
The paper makes an important point, it is written clearly, and the body of evidence is convincing. Therefore, I recommend this paper for publication.
# Suggestions
Make it more clear what is meant by code-level optimizations.
- In Sec. 2, there is a link to Appendix A.2 for a ""full list"", but the list in A.2 does not contain all points from Sec. 2.
- For PPO-M, it is said ""implements only the core of the algorithm"". What exactly does that mean?
- PPO-NoClip is defined as ""PPO without clipping"". Does it mean that it includes all other tricks apart from clipping? Please, be explicit in such places. | Review: Summary
This paper calls to attention the importance of specifying all performance altering implementation details that are current inherent in the state-of-the-art deep policy gradient community. Specifically, this paper builds very closely on the work started by Henderson et al. 2017, building a conversation around the importance of more rigorous and careful scientific study of published algorithms. This paper identifies many ""code-level optimizations"" that account for the differences between the popular TRPO and PPO deep policy gradient algorithms. The paper then subselects four of these optimizations and carefully investigates their impact on the final performance of each algorithm. The clear conclusion from the paper is that the touted algorithmic improvement of PPO over TRPO has negligible effect on performance, and any previously reported differences are due only to what were considered unimportant implementation details.
Review
This paper investigates the claims made by Schulman et al. 2017 carefully, by investigating the impact of PPO's clipping mechanism on maintaining a valid trust-region; the central claim made by PPO's originating paper. The empirical results suggest that PPO is not sufficient for maintaining a valid trust-region, however the ""code-level optimizations"" that differ between the TRPO implementation the PPO implementation are sufficient. The ablation study of the four optimizations studied by the paper shows dramatic and clear results suggesting that annealing stepsizes and normalize rewards make very strong differences in learning performance; much more effect than demonstrated by the differences between TRPO and PPO's core algorithmic contribution as demonstrated in Figure 2 and even more strongly in Figure 3. I find the work included in this paper to be novel and a valuable contribution to the field.
For the above reasons, I recommend to accept this paper for publication at ICLR. In the following paragraphs I will discuss why I only recommend a weak accept instead of a strong accept.
My primary concern with the empirical study is the use of only three random seeds. As demonstrated in Henderson et al. 2017 (which is heavily cited in this paper), using such a small number of random seeds can have very misleading results. Although the effects appear very strong in the empirical studies in this paper, the effects likewise appear strong in Henderson et al.'s Figure 6 where 10 random seeds were split into two groups for the same algorithm. For this paper to make such strong claims about the negligence of the careful scientific study on TRPO and PPO, it would be best if this paper included far more random seeds in its investigation.
My second concern is with the discussion and conclusions drawn from Tables 1 and 2. It appears that the inclusion of clipping plays a strong role in the variance of each algorithm on every domain except Hopper. Specifically, the algorithms that include clipping appear to be much lower variance than the algorithms including clipping. Admittedly using only 3 seeds means that investigating the variance appropriately is near impossible (see the above paragraph), however variance should be considered and discussed in a conversation about the effects of the core contribution of PPO. If clipping leads to more consistent results across runs, even if those results are a little worse, it is still a valid and important contribution.
The paper cites Henderson et al. 2017 in several places. I would point out (perhaps in the introduction) that this paper builds on work already done in Henderson et al. 2017. Specifically, Henderson et al. 2017 investigates the effects of using different codebases for TRPO and shows that these different codebases result in dramatically different performance. The similarity to the investigation in this paper to too close to be unreported. However, I find that the investigation in this paper is much more complete and insightful than that of Henderson et al. 2017 (this paper has a more narrow focus), thus contributes significantly and meaningfully to this ongoing conversation.
Additional Comments (do not affect score)
It might be worthwhile to move the related work section to the beginning of the paper, either merged with the introduction or immediately after. This section is of critical importance to understanding the scope of this paper and for understanding why you are studying what you study. In fact, there is already a bit of duplication between the related works and introduction sections, so the paper could likely gain some additional real-estate by combining these.
I disagree with the terminology ""code-level optimizations"" and I find that it is misleading. This caused a bit of confusion on my first pass reading the paper, as I originally was expected the code differences to be more akin to using Tensorflow vs PyTorch or switching hash table functions, etc. Instead the changes focused on in this paper are changes to the problem specification and algorithm implementation. These are not simply implementation details as ""code-level optimizations"" suggests, but are rather details that necessarily must be included in peer-reviewed works. I don't have a suggested name to switch to, but felt strongly enough to mention it.",6.75
25,Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems,AddPublic Comment,"Decision: Accept (Talk) | Review: The paper describes an algorithm to find diverse patterns in Lenia (a continuous CA system) by using a CPPN to generate initial states, and a stochastic exploration algorithm to mutate parameters of the CPPN + CA parameters. The fitness is the closeness of a generated set of latents to a set of latents produced through one of several possible processes; hand-design, pretraining, or online training on previously generated CA settings.
The core results are in Figures 27 to 31 in an appendix. Initial inspection reveals that handdesigned goal states produce the most interesting non-animal patterns. With regard to animal forms, it appears to me that Online goal learning harms the diversity of animal forms considerably compared to PGL and perhaps HGS. High frequency spatial structure seems to be lost there.
I would like to see a further analysis of maybe 10000s of such images generated, and an understanding of exactly why RGS produces the same kind of red linear patterns, and why HGS produces the distribution of pattern types in Figure 29, and why non-animal types differ in PGL vs HGS, and why high frequency spatial structure is lost in OGL. How robust are these over many runs? The results should NOT be shown just for the first repetition of the experiment but for all independent runs of the experiments, e.g averaged over 30 independent CPPN evolutions, for PGL, OGL, Random, and HGS! | Review: The focus of the presented paper is on formulating the automated discovery of self-organized patterns in high-dimensional dynamic systems. The introduced framework uses cellular automata (game of life) as a testbed for experimentation and evaluation and existing machine learning algorithms (POP-IMGEPs). The goal of the paper is to show that these algorithms can be used to discover and represent features of patterns. Moreover, an extension of SOTA algorithms is introduced and several approaches to define goal space representations are compared.
Overall, I have the impression this is an interesting paper that could be accepted to ICLR. The idea of applying IMGEPs to explore parameters of a dynamic system is novel and interesting, which could also simulate further research in this field. Furthermore, the paper well-written, technically sound, and the results are interesting. The overall contribution of the paper is in applying IMGEP algorithms to exploring parameters of dynamic systems and in comparing different algorithms along with an extensive set of experiments. As a point of criticism, a lot of (interesting) material was pushed to the Appendix. Resolving the references makes reading the paper harder. Moreover, given that this paper has more than 35 pages appendix material, it seems this work would better be suited for a journal as for a conference. There is a reason for papers to have a page limit and this work circumvents this limit by presenting a lot of additional material. Therefore, I am not willing to strongly support this work.
Specific Comments:
- Section 3.1: It is not clear how the initial system state is established. In Section 3.1. the text states that 'parameters are randomly sampled and explored' before the process starts, but it is not clear why a random sampling is used and what this means for the subsequent sampling. Later in the text (3.3) it becomes more clear, but here this appears too unclear.
- Section 3.1: ""distribution over a hypercube in \mathcal{T} chosen to be large enough to bias exploration towards the frontiers of known goals to incentivize diversity."" This sentence is not clear and needs more details. How is the distribution chosen exactly?
- Section 3.2 appears a bit repetitive and could be more concise. I don't think it is necessary here to contrast manual vs learned features of the goal space.
- Section 3.2 (P3): the last sentence of this paragraph reads as if there exists no approaches for VEAs in online settings. This should be toned down or backed up by a reference.
- Section 3.2: (last sentence): it is not clear how the history is used exactly to train the network. Which strategy is used to sample from the history of observations?
- Section 3.3: What is meant by ""The CPPNs are used of the parameters \{theta}""? The details provided after this sentence are not clear and need more details.
- Section 4.2: Please provide more details what ""very large"" dataset means.
- Section 4.2: 'HGS algorithm' is not defined.
- Section 5: It seems unnecessary to explain what t-SNE does as a method. | Review: The paper uses the continuous Game of Life as a testing ground for algorithms that discover diverse behaviors. The problem is interesting, under-explored, and rich. The combines a variety of interesting ideas including compositional pattern producing networks (CPPNs) to learn structured primitives. Although the authors do propose formal measures of behavioral diversity and so show performance improvements, at the end of the day this work, like much empirical work on generative adversarial networks, is drifting towards art -- where performance is ultimately judged by human eyes rather than quantiative metrics.
Comments:
The paper refers to hand-designed goal spaces and talks, on p28, about “the statistical measures used to define the goal space”. At the same time, the analytic behavior space is also defined in terms of statistical measures, but it is *not* referred to as hand-designed. At this point, the profusion of spaces and measures means that I am no longer sure what counts as hand-crafted or not. Please clarify.
The hypothesis on p34, sec E.4.2 that the VAE’s 8-dim bottleneck helps focus on animals rather than non-animals (which are differentiated more in terms of textures and details) is important and should be checked.
Some of the decisions about what to check and vary are unclear. For example, section E.1 considers the effect of different initializations (“pytorch”, “xavier” and “kaiming”). The choice of initialization is important mostly to do with improving gradients to improve the rate of convergence (or convergence at all) in deep nets. It’s not clear why initializations are an parameter to vary when considering diversity of solutions. Or, rather, why initializations are more interesting to consider various other architectural considerations. More broadly, looking at Fig 17, the x-axis doesn’t make much sense. The experiments along the x-axis vary according to initialization, but also according to the nature of the goal space and other features. It seems a bit incoherent.
Overall I think this is a good paper. The results are novel and even better, they are fun. However, the paper is extremely long, and it feels as though the authors have to some extent lost control of the material. I could add more comments but TL;DR it needs a lot of editing and pruning.",5.5
26,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,"Keywords: visually-grounded speech, self-supervised learning, discrete representation learning, vision and language, vision and speech, hierarchical representation learning","Decision: Accept (Talk) | Review: Pretty interesting paper attempting to learn discrete linguistic units via vector quantization of visually grounded, speech related features. I think this is a worthwhile contribution. My main complaint is that the exposition is a bit diffuse and fails to crystallize the essence of the work. In particular, the claim is that the novelty is from the use of a ""discriminative, multi-modal grounding objective"". Reading the work, this seems to be the triplet loss described in Section 3.5. Is that the novel objective? In my my the really interesting aspect that should be stressed is the visual grounding -- I encourage the authors to highlight that aspect more directly. I fear that essential and interesting point is somewhat diluted in the detailed exposition of results. | Review: This paper attempts to learn discrete speech units in a hierarchical (phone and word) fashion by incorporating multiple vector quantization layers into the audio encoder branch of a model that visually grounds speech segments with accompanying images.
The model has been tested and compared against two algorithms and implementations that set the SOTA on the Zero Speech 2019 challenge (further improving one of them in the process, it seems), and outperforms these significantly using the ABX metric, so the proposed method seems to perform well (the model is using additional supervision, though). In addition, this is an interesting and timely research problem with implications far beyond the core machine learning setup. The hierarchical setup, and the finding that successful learning here depends on the curriculum, is intriguing indeed. The paper is a pleasure to read and provides a rich set of results and analyses.
A few remarks:
- It is probably worth explaining how the ABX test is performed, i.e. that features are extracted from some layer of a model, and then a time alignment is performed to get the score - this is in the text somehow, but i had to read it multiple times.
- Did you try other architectures like 5 layers (rather than 4) in Figure 2
- Figure 2 is a bit hard to interpret. Maybe plot log of ABX error rate or something, to pull apart the different layers?
- Could you explain the difference between cold-start and warm-start? One is adding the discretization to a pre-trained model, the other is training from the start?
- When you measure ABX at layer 2 and 3, in a model trained with quantization, do you measure ABX on the features before or after quantization? does it make a difference?
- Table 7: some of the top word hypothesis pairs make sense acoustically (building-buildings, red-bed, ...), some could be neighboring words (large-car, ...), but some are just weird (people-computer) - any intuition as to what is going on? | Review: Overview:
The paper proposes a method to learn discrete linguistic units in a low-resource setting using speech paired with images (no labels). The visual grounding signal is different from other recent work, where a reconstruction objective was used to learn discrete representations in unsupervised neural networks. In contrast to other work, a hierarchy of discretization layers are also considered, and the paper shows that, with appropriate initialization, higher discrete layers capture word-like units while lower layers capture phoneme-like units.
Strengths:
The paper is extremely well-written with a clear motivation (Section 1). The approach is novel. But I think the paper's biggest strength is in its very thorough experimental investigation. Their approach is compared to other very recent speech discretization methods on the same data using the same (ABX) evaluation metric. But the work goes further in that it systematically attempts to actually understand what types of structures are captured in the intermediate discrete layers, and it is able to answer this question convincingly. Finally, very good results on standard benchmarks are achieved.
Weaknesses:
Although I think the paper is very well-motivated, my first criticism is that discretization itself is not motivated: why is it necessary to have a model with discrete intermediate layers? Does this give us something other than interpretability (which we obtain due to the sparse bottleneck)? In the detailed questions below, I also specifically ask whether, for instance, the downstream speech-image task actually benefits from including discrete layers.
My second point is that it is unclear why word-like units only appear when the higher-level discrete layers are trained from scratch; as soon as warm-starting is used, the higher level layers capture phoneme-like units (Table 1). Is it possible to answer/speculate why this is the case?
Overall assessment:
The paper presents a new approach with a thorough experimental investigation. I therefore assign an ""accept"". The weaknesses above asks for additional motivation and some speculation.
Questions, suggestions, typos, grammar and style:
- Section 3.3: It maybe makes less sense for the end-task, but did the authors consider discretization on the image side of the network? This could maybe lead to parts of objects being composed to form larger objects (in analogy to the speech network).
- Section 3.3, par. 3: ""with the intention that they should capture discrete word-like and sub-word-like units"" -> ""with the intention that they should capture discrete *sub-word-like and word-like units*"" (easier to read with first part of sentence)
- Section 3.3: The more standard VQ-VAE adds a commitment loss and a loss for updating the embeddings; was this used or considered at all, or is this all captured through the exponential moving average method?
- Section 3.4: ""with same VQ layers"" -> ""with *the* same VQ layers""
- Section 3.5: Can you briefly outline the motivation for adding the two losses (so that it is not required to read the previous work).
- Section 4.1: Following from the first weakness listed above, the caption under Figure 2 states that the non-discrete model achieves a speech-image retrieval R@10 of 0.735. This is lower than some of the best scores achieved in Table 1. Can this be taken as evidence that discretization actually improves the downstream task? If so, it would be worth highlighting the point more; if there is some other reason, that would also be worth knowing.
- Figure 1: Did the authors ever consider putting discrete layers right at the top of the speech component, just before the pooling layer? Would this more consistently lead to word-like units?",5.75
27,Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks,"Abstract: While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on two realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework.","Decision: Accept (Talk) | Review: Summary
========
This paper introduces a mechanism for gradient-based meta-learning models for few-shot classification to be able to adapt to diverse tasks that are imbalanced and heterogeneous. In particular, each encountered task may have varying numbers of shots (task imbalance) and even within each task, different classes may have different numbers of shots (class imbalance). Further, test tasks might come from a different distribution than the training tasks. They propose to handle this scenario by introducing three new types of variables which control different facets of the degree and type of task adaptation, allowing to decide how much to reuse meta-learned knowledge versus new knowledge acquired from the training set of the given task.
Specifically, their newly introduced variables are: 1) the factor for learning rate decay (a scalar) for the inner-loop task adaptation optimization which allows to not deviate too much from the global initialization when insufficient data is available, 2) a class-specific learning rate (one scalar per class) that allows to tune more for under-represented classes of the training set, 3) a set of weights on the global initialization (one scalar per dimension) that can down-weigh each component if it’s not useful for the task at hand (e.g. if test tasks have significantly different statistics than training tasks did).
The values of these variables are predicted based on the training set of the task: the support set is encoded via a hierarchical variant of a set encoding (where pooling is done using higher order statistics too instead of simply averaging). The resulting encoded support set is the input to the network that produces the values for the three sets of variables discussed above. Each new variable is treated in a Bayesian fashion: a prior is defined over it (Normal(0,1)), which is updated by conditioning on the training set to form a posterior for each given task. Specifically, each of the above variables is represented by a Gaussian whose mean and variance are the learnable parameters that are produced by the network described above.
Experimentally, this method outperforms others on a setting of imbalanced tasks (the shot is sampled uniformly at random from a designated range). The gain over other methods is large in particular when evaluated on out-of-distribution tasks (coming from a different dataset) and when the imbalance is large.
Comments (in decreasing order of importance)
========================================
A) The Bayesian framework helps because it offers an elegant way to use a prior. In the deterministic version, was any effort made to resemble the effect of that prior? For example, one can define a regularizer that penalizes behaviors that ignore the meta-knowledge too much (e.g. too large values for \gamma, or for the class-specific learning rates etc). Albeit more ‘hacky’, if these regularization coefficients are tuned properly, they might result in a similar effect to that of having a prior. A fair comparison to the deterministic variant should include this.
B) I think that \gamma and z can be merged into a single set of parameters? In particular, imagine a per-dimension-of-\theta learning rate. This would then be large for a dimension when there is a larger need for adapting that dimension of \theta. In the case of large training sets, this can be large for all dimensions, recovering the behavior of a large \gamma. For the case of diverse datasets, this would behave as the current z (updates a lot the dimensions of \theta that are irrelevant for the given task due to the dataset shift).
C) Meta-Dataset (https://arxiv.org/abs/1903.03096) is a recent benchmark for few-shot classification that introduces both of what is referred to here as task imbalance and class imbalance and also is comprised of heterogeneous datasets and evaluates performance on some held-out datasets too. The current state-of-the-art on it (as far as I know) is CNAPs [1] which employs a flexible adaptation mechanism on a per-task basis but is fully amortized (performs no gradient-based adaptation to each task) and makes no explicit effort to tackle imbalanced tasks as is done here. I’m curious how this method would compete in that setup. It definitely seems to be a strong candidate for that benchmark!
Less important
=============
D) which dataset is used in Tables 4 and 5? I assume it’s Omniglot (due to the numbers being in the 90s) but it would be good to say this explicitly.
E) In section 5.2, expressions such as x5 and x15 are used to characterize the degree of imbalance of a task. How exactly are these computed? Does x5 mean that the largest shot is 5 times larger than the smallest shot? It would be good to explicitly state this.
F) In the Related Work section, in the Meta-learning paragraph there is a sentence that’s not accurate: “Metric-based approaches learn a shared metric space [...] such that the instances are closer to their correct prototypes than to others”. This sentence does not describe all metric based approaches. It describes Prototypical Networks (Snell et al) but not, for example, Matching Networks (Vinyals et al) nor many others that like Matching Networks perform example-based comparisons and don’t aggregate a class’ examples into a prototype.
In a nutshell
===========
I think this work is a useful contribution for moving towards a more realistic setting in few-shot classification. It captures some desiderata of models that can operate in more realistic settings and outperforms previous models in those scenarios. My comments above are mostly suggesting improvements and clarifications but I am inclined to recommend acceptance of this paper.
References
=========
[1] Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes. Requeima et al. NeurIPS 2019. | Review: The paper proposes a Bayesian approach for meta learning in settings were the tasks might be OOD or have imbalanced class distribution. The proposed approach has 3 task-specific balancing variables with a prior and an inference network. Using an amortized inference scheme, the model unifies the meta-learning objective loss with the lower bound of probabilistic model marginal likelihood.
The paper is well-written and well-motivated. I only have some minor comments and questions:
- Can you add the standard errors for the results in Section 5.2 (maybe at least in the Appendix)?
- Specifically it would be interesting to see if the results for analyzing the class imbalance variable are statistically significant; specially in light of the recent work on the effects of importance weighting in DL (see “What is the Effect of Importance Weighting in Deep Learning?” by Byrd and Lipton) which essentially question the value of importance weighting for handling class imbalance in various DL settings.
- For the experiments in Section 5.1 can you also report the values for the three task-specific balancing variables? (Maybe in the Appendix).
Minor:
“obtains significantly improves over” -> ""significantly improves over""
Overall, I found the paper interesting and practically useful, although I believe some additions to the empirical evaluation can improve the impact of the paper. | Review: Summary
-------------
This paper proposed to improve existing meta learning algorithms in the presence of task imbalance, class imbalance, and out-of-distribution tasks. Starting from the model-agnostic meta-learning (MAML) algorithm (Finn et al. 2017), to tackle task imbalance, where the number of training examples of varies across different tasks, a task-dependent learning rate decaying factor was learned to be large for large tasks and small for small tasks. In this way, the small task can benefit more from the meta-knowledge and the large task can benefit more from task-specific training. To tackle class imbalance, a class-specific scaling factor was applied to the class-specific gradient. The scaling factor was large for small class and small for large class so that different classes can be treated equally. To tackle the out-of-distribution tasks, a task-dependent variables was learned to emphasize meta-knowledge for the test task similar to training tasks. Additional model parameters are learned through variational inference. Experimental results on benchmark datasets demonstrate the proposed approach outperformed its competing alternatives. Analysis of each component confirm they work as expected.
Comments
---------------
This paper is well motivated and clearly written. The empirical evaluation also support major claims in the paper.
Can the author provide more details on the inference of the model? In the likelihood term in Eq. (7), the task specific parameters \theta^{\tau} was parameterized by Eq. (3), which contains K iterative gradient updates. How was the gradient w.r.t. \theta was computed in this setting?
The task-specific learning rate decaying factor was constrained to be between 0 and 1 using the function f(). The class-specific scaling factor made use of the SoftPlus() function, for the same purpose of scaling learning rate, why do these two different options of functions were applied?
For the scaling vector of the initial parameters g(z^{\tau}), for its zero entries, the initialization of the corresponding entries in task-specific parameter \theta would be zero. Would it be better to apply a linear interpolation between \theta and a randomly-initialized vector in Eq (2)?
Edits after reading the author's rebuttal
==================================
The author's reply well addressed my questions. After reading other reviewers' positive comments and the author's thorough reply, I decide to increase my rating to 8: Accept.",7.0
28,Mathematical Reasoning in Latent Space,AddPublic Comment,"Review: The paper proposes a technique to perform reasoning on mathematical formulas in a latent space. The model is trained to predict whether a rewrite rule can be applied to a formula given its latent representation. When the rewrite is possible, the model also predicts the embedding of the resulting formula. Experiments show that the network can be applied multiple steps in a row, while operating only in the embedding space.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega. Did you try to have a single network? This seems a much more natural approach to me, and I'm surprised that you did not start with that. From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension. The role of \sigma seems very redundant given \omega.
2. If you consider \sigma, why do you also predict the rewrite success with \omega? Couldn't it be simply a function from S x S -> L ?
3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given. It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.
5. To train \sigma and \omega, the negative instances are selected randomly. You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma'). I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)). This could be a possibility to remove the noise you have when doing multi-step operations (and potentially go way beyond 4 steps). Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.
Overall, the model is a bit complicated (e.g. question 1.), but the results are promising, the paper is well written, and the ability to manipulate formula embeddings is probably going to be useful in the context of theorem proving. | Review: The paper proposes a method to do math reasoning purely using formula embeddings. The proposed method employs a graph neural network to embed math formulas to a latent space. The formula embeddings are then combined with theorem embeddings (also formulas, computed in the same way as formula embeddings) to predict whether one can do one step of math reasoning using the corresponding theorem, and also to predict the embeddings of the resulting formula. Empirically the authors demonstrate that the method can be chained end-to-end to do multiple steps of reasoning purely in the latent space.
I tend to accept this paper, (but also OK if it gets rejected), for the following reasons: (1) the idea is novel and interesting; (2) the writing of the paper is below conference standard and very hard to read, especially the method and the experiment sections.
===========================================================================
Novelty and significance
I really like the idea of doing math reasoning in latent space. The idea is definitely novel and interesting. It is related to existing works such as neural logic induction[1] and planning in latent space[2]. It is amazing that one can do multiple steps of math reasoning after only training the model using data from one single step. It would be interesting to see how it can improve existing learning-based theorem provers.
My question is if we want to integrate the proposed method into theorem provers, after multiple steps of math reasoning, how would us know the goal has been proved? Is it possible that we can train a decoder that maps back from the latent space to the formula space? Also can it work with theorems that decompose the current goal into several sub-goals? I know these are not the concerns of this paper, but I would be really grateful if you could provide some intuitive answers!
===========================================================================
Writing
The paper is not well-organized and not written in a consistent way. For the method and the experiment sections, I need to jump back and forth several times in order to understand what the authors are trying to say.
1. Typo: Third paragraph in section 1, ""...which is makes use of ..."".
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?
3. Mentioning ""merging \sigma and \omega, is left for future work"" is confusing before formally introducing \sigma and \omega.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.
6. The rationale of the two tower design (why not combine two) is not clearly explained.
7. Typo: Page 5 last paragraph, ""... negative instances for for each ..."".
8. The itemized part in 5.3, ""...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx"". However, both 3 and 4 are not baselines!
9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.
10. Reading the baselines before the experiments is very confusing. For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a ""random baseline"".
11. Baseline 2 is actually referred to as ""usage baseline"" but this name is not introduced in the itemized part.
[1] Rocktäschel, Tim, and Sebastian Riedel. ""End-to-end differentiable proving."" Advances in Neural Information Processing Systems. 2017.
[2] Srinivas, Aravind, et al. ""Universal planning networks."" arXiv preprint arXiv:1804.00645 (2018). | Review: = Summary
Embeddings of mathematical theorems and rewrite rules are presented. An in-depth analysis of the resulting embeddings is presented, showing that a network can learn to ""apply"" embedded rewrite rules to embedded theorems, yielding results that are similar to the embedding of the rewritten theorem. [i.e., app'(emb(thm), emb(rule)) is near to emb(app(thm, rule))] This is an interesting property for the application of deep learning to automated theorem proving, though not directly a breakthrough result.
= Strong/Weak Points
+ Simply a cute result, showing that proof search can remain in embedding space for a limited time horizon without having to switch back into the theorem prover environment.
+ Nicely designed experiments testing this (somewhat surprising) property empirically
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail
- Writing sometimes a bit overcomplicated (e.g., Sect. 4.5 could just be a figure of a commuting diagram and two sentences...)
- Architecture choice unclear: Why are
σ
and
ω
separate networks. This is discussed on p4, but it's unclear to me how keeping
σ
separate is benefitial for the analysis, and this is not picked up again explicitly again?
= Recommendation
Overall, this is a nice, somewhat surprising result. The writing and experiments could use some improvement, but I believe that the majority of the ICLR audience would enjoy seeing this result (even though it would have no impact on most people's research)
= Detailed Comments
- page 4, Sect. 4.4: Architecture of
α
would be nice (more than a linear layer?)
- page 5, paragraph 3: ""we from some"" -> ""we start from some""
- p6par1: ""much cheaper then computing"" -> than
- p6par6: ""on formulas that with"" -> no that
- p6par7: ""measure how rate"" -> ""measure the rate""
- p8par1: ""approximate embedding
α(e(γ′(...)))
-
e
is undefined and should probably be
e′
(this is also the case in the caption of Fig. 5), and
c′
should probably be included as well. However, I don't understand the use of
α
here. If Fig. 4 is following Fig. 3 in considering
p(c(γ(T),π(P)))
, then Fig. 4 should plot the performance of, e.g.,
p(c(e′(c′(γ′(Ti−1),π′(Pi−1))),π(Pi)))
(i.e.,
p
applied to approximate embedding of
Ti
and (""true"") embedding of
Pi
). I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation in Sect. 6.",8.0
29,Meta-Learning with Warped Gradient Descent,"Keywords: meta-learning, transfer learning","Review: Summary:
The current paper deals with meta-learning and essentially proposes a generalization of MAML (a popular gradient-based meta-learning algorithm) that mostly builds upon two main recent advances in meta-learning: 1) an architectural one (see e.g. T-Nets), which consists in optimizing the parameters of additional layers during the meta-learning outer loop (as opposed to only optimizing the initial conditions of the original parameters like in MAML), and 2) a theoretical one (see e.g. Meta-SGD, Meta-curvature), which is based on the geometrical observation that one set of parameters can precondition a second set of parameters that are consequently being optimized in a ""warped"" geometry, possibly speeding up learning.
The authors provide a great and thorough overview of the literature, in particular for gradient-based meta-learning methods, which helps putting all this in perspective.
The way they obtain the mentioned ""warped"" geometry in practice is by adding additional so-called warp-layers to an architecture that is being trained with meta-learning. Such warp-layer are generic deep learning modules (such as convolutions followed by BatchNorm, or LSTM layers), which are being trained in the outer-loop of the meta-learning optimization. In this sense, WarpGrad extend T-Nets, which only allowed for linear layers.
The second main innovation of WarpGrad is the proposal of a new meta-learning objective, which incorporates a meta-learning internal loop of only one step of (preconditioned) SGD, meaning that, as the authors notes, ""in contrast to MAML-based approaches (Eq. 1), [...] avoids backpropagation through learning processes"".
The authors test their algorithm on several meta-learning benchmarks, including few- and multi-shot learning tasks demonstrating very competitive performance when their algorithm is combined with MAML or Leap. They then deploy WarpGrad on a maze navigation reinforcemente learning task to demonstrate training of recurrent architectures, and on a continual learning toy dataset to show that their objective can be adapted to mitigate catastrophic forgetting.
Decision:
This is a good paper which proposes an interesting generalization of previous gradient-based meta-learning methods like MAML and T-Net, with an impressive number of experiments. However, some of the statements regarding the advantages of WarpGrad over previous algorithms seem a little bit misleading, in particular in situations where WarpGrad needs to be combined with these same algorithms. For instance (and I might have completely misunderstood things here), it seems that when the WarpGrad objective is being combined with MAML (which requires backpropagation through multiple-step gradient descent trajectories), then also the resulting combined objective will necessarily need to backprop through the same multi-step trajectory, defeating the stated advantage of the WarpGrad algorithm (i.e. that its objective avoids backpropagating through the learning processes).
In general, even if one only considers the WarpGrad objective eq. (10), that comprises a meta-learning inner loop which consists of one step of (preconditioned) gradient descent. However, it seems like an arbitrary (and limiting) choice of the authors to only perform one step, as opposed to multiple ones. As a matter of fact, even very sophisticated second order gradient descent methods like natural gradient descent typically require more than one step to reach a local minimum. That is to say, that the main advantage showcased by the authors (the fact that the WarpGrad objective avoids backprop through a whole learning trajectory) seems like a limitation, rather than the result of a principled derivation.
It would be beneficial if the authors could clarify this points. In particular, whether combining WarpGrad with MAML does not indeed negate the stated advantages of WarpGrad over MAML, and whether there is a principled way of demonstrating that executing only one step in the inner loop of the WarpGrad objective is completely general (i.e., additional steps do not help the inner loop).
Minor:
- The authors use the wrong citation key when referring to the T-net paper: it should be Lee et al 2018, instead of Lee et al. 2017
- I believe that when the authors mention Fast and slow weights, they are being described in the opposite way: slow weights should be in charge of meta-learning information, while fast ones are in charge of task-specific information.
- Line 3 and 4 of Algorithm 1 and 2: shouldn't it say ""mini-batch of tasks"" (plural), instead of ""mini-batch of task"", since several tasks are being sampled? Otherwise, it might be erroneously interpreted as ""mini-batch of (samples belonging to) task T"".
- The comment that ""learning to precondition gradients can be seen as a Markov Process of order 1"" is never clearly elucidated or developed. It would help to develop this. | Review: The authors propose warped gradient descent (WarpGrad) an optimisation framework for facilitating gradient-based meta-learning. WarpGrad interleaves within the learner meta-learned warp-layers that implicitly precondition the gradients of the task-specific parameters during backpropagation. In contrast to the linear projection layers employed in T-Nets, warp-layers are unrestricted in form and induce a full Jacobian preconditioning matrix. The warp layers are meta-learned in a trajectory-agnostic fashion, thus obviating the need to backpropagate through the gradient steps to compute the updates of their parameters. The framework is readily applicable to standard gradient-based meta-learners, and is shown to yield a significant boost in performance on both few-shot and multi-shot learning tasks, as well as to have promising applications to continual learning.
The paper is well-structured and well-motivated: the problem statement is clearly laid out from the outset, with appropriate context, and explanations supported well diagramatically. The idea, and perhaps more so the applications thereof, is seemingly novel and its explanation is given straightforwardly while avoiding getting bogged down in technical details. Clear comparisons and distinctions with previous work are drawn - for instance with the update rules for several gradient-based methods - MAML and its derivatives - being laid out in standard form (though it might also be nice to echo this with the WarpGrad update rule).
The experiments are logically ordered with the initial set covering the standard few-shot learning benchmarks with appropriate baselines (though the results for few-shot tieredImageNet are lacking in this respect), with most essential details given in the main text and full details, including those related to the datasets in question and hyperparameter selection, documented in Appendix H. Meta-learning does seem uniquely well-positioned for tackling the task of continual learning and it's heartening to see this being explored here with a degree of success - it would be interested to see how its performance compares with standard continual learning methods (such as EWC) on the same task. Particularly impressive is the depth into which the Appendices regarding the experiments, both elaborating on the details given in the main text as well as additional ablation studies.
Minor errors:
- Page 7: ""a neural network that dynamically **adapt** the parameters..."" - should be ""adapts""
- Page 22: ""where
I
is the **identify** matrix"" - should be ""identity""
- Page 27: ""The task target function
gτ
is **partition** into 5 sets of **sub-task**"" - should be ""partitioned"" and ""sub-tasks"", respectively | Review: This paper proposes a learning strategy to precondition gradients for meta-learning. I really enjoyed reading the paper though I admit that I couldn't fully grasp all the details yet (paper is dense). My comments below are mostly to improve the readability of the paper for readers like me (knowing a thing or two in optimization and meta-learning)
1- The authors emphasize on the method being trajectory-agnostic. Can you explain why this is very important? What methods are not trajectory-agnostic?
2 - Also in various places, the authors claim the method does not suffer from vanishing/exploding gradients and credit-assignment problem. This needs to be properly verified (and explained as I do not see the connections clearly)
3- Some claims are based on the Omniglot experiments (eg., the effect of the stop-gradient). It would be good if this can be done on Mini-imagenet instead.
4- I am not sure I understand the stop-gradient operator, can you be more explicit there?
5- I read the conversation regarding linear units on openreview and I disagree with your statement. A cascade of linear layers does not necessarily match one linear layer unless some constraints on the rank of layers are envisaged, a bottleneck in the middle ruin everything.",8.0
30,Meta-Q-Learning,TL;DR: MQL is a simple off-policy meta-RL algorithm that recycles data from the meta-training replay buffer to adapt to new tasks.,"Decision: Accept (Talk) | Review: This paper proposes Meta Q-Learning (MQL), an algorithm for efficient off-policy meta-learning. The method relies on a simple multi-task objective which provides initial parameter values for the adaptation phase. Adaptation is performed by gradient descent, minimizing TD-error on the new validation task (regularizing towards initial parameter values). To make adaptation data efficient, the method makes heavy use of off-policy data generated during meta-training, by minimizing its importance weighted TD-error. Importance weights are estimated via a likelihood ratio estimator, and are also used to derive the effective sample size of the meta-training batch, which is used to adaptively weight the regularization term. Intuitively, this has the effect of turning off regularization when meta-training trajectories are “close” to validation trajectories. One important but somewhat orthogonal contribution of the paper is to highlight the importance of context in meta-learning and fast adaptation. Concretely, the authors show that a simple actor-critic algorithm (TD3), whose policy and value are conditioned on a context variable derived from a recurrent network performs surprisingly well in comparison to SoTA meta-learning algorithms like PEARL. MQL is evaluated on benchmark meta-RL environments from continuous control tasks and is shown to perform competitively with PEARL.
I have mixed opinions on this paper. On the positive side, and subject to further clarifications (see below), the paper seems to confirm that multi-task learning is almost sufficient to solve current meta-RL benchmarks in continuous control, without adaptation, as long as policy and critic are conditioned on a recurrent task context. This either highlights the strength of multi-task learning, or the inadequacies of current meta-RL benchmarks: either of which will be of interest to the community. On the other hand, the proposed MQL algorithm is only shown to significantly outperform this new baseline TD3-context agent on 1 of 6 tasks (Ant-Goal-2D), and furthermore the ablative analysis seems to suggest that the importance weighting and adaptive weighting of trust region are not very effective, and do not significantly change the performance of the method. The take-away seems to be that while context is crucial to generalization on these validation tasks, adaptation is not but can indeed be improved in a data-efficient manner with fine-tuning. MQL is only then a second thread to this story.
Clarifying Questions:
* The text and figures could be much clearer with respect to what is being measured/presented. Can you confirm that you report average validation returns as a function of meta-training steps, where validation performance is estimated after training on (at most) 2x200 transitions from the validation task (|D_new|=400)? This would match the protocol from PEARL. Fig 4(a) would then imply that one can get close to SoTA results on Ant-Fwd-Back + Half-Cheetah-Fwd-Back with no adaptation whatsoever (using TD3-context).
* Could the authors provide more details about how context is generated, in particular whether the GRU is reset on episode boundaries or not? If the recurrent context does span episode boundaries, are rewards being maximizing over a horizon greater than the episode length (similar to RL2)?
* How do you reconcile your result that a deterministic encoder is sufficient, compared to Figure 7 of PEARL which shows stochasticity is paramount for performing structured exploration during adaptation?
* Ablative analysis seems to suggest that off-policy learning plays a very minimal role during adaptation (beta=0). Can you confirm this interpretation? Would this not suggest that regularization via fine-tuning (regularized to multi-task prior) and context are sufficient to solve these meta-RL tasks? This would be a sufficient contribution by itself but unfortunately does little to validate the proposed method.
* Could you repeat the ablative analysis on all of the 6 tasks? Currently, this is performed on the two tasks for which TD3-context does best which leaves little room for improvement.
* Section 4.4. Propensity Score Estimation. “Use this model to sample transitions from the replay buffer that are similar to the new task”. Is this done via rejection sampling? This should be described more prominently in Section 3, along with a detailed description of the proposed MQL algorithm.
Detailed Comments:
* Paper desperately needs an algorithmic box, which clear and concise description of algorithm (how losses are interleaved, etc). Importantly, do the authors pretrain \theta_meta using the multi-task loss before doing adapation?
* Please add more informative labels to axes for all your figures: timesteps for [validation,training] ? Same for return. Please also augment captions to make figures as stand-alone as possible.
* MQL encompasses technique for off-policy training using a discriminator to estimate a likelihood ratio. It would be nice to evaluate this in standard off-policy learning setting, instead of it being limited to meta-learning. | Review: Summary
-------------
The authors propose meta Q-learning, an algorithm for off-policy meta RL. The idea is to meta-train a context-dependent policy to maximize the expected return averaged over all training tasks, and then adapt this policy to any new task by leveraging both novel and past experience using importance sampling corrections. The proposed approach is evaluated on standard Mujoco benchmarks and compared to other relevant meta-rl algorithms.
Comments
--------------
Meta-rl is a relevant direction for mitigating the sample-complexity of rl agents and allowing them to scale to larger domains. This work proposes interesting ideas and overall it constitutes an nice contribution. In particular, I found interesting (and at the same time worrying) that a simple q-learning algorithm with hidden contexts compares favorably to state-of-art meta-rl approaches in standard benchmarks. The paper is well-organized and easy to read. Some comments/questions follow.
1. (15) is probably miss-written (theta is trained to maximize the TD error)
2. In the adaptation phase, are (18) and (19) performed one after the other? Could they be done at the same time by setting to 1 the importance weights of the new trajectories and sampling from the whole experience (new and old)?
3. Note that the ESS estimator (13) diverges to infinity when all weights are close to zero. Is it clipped to [0,1] in the experiments? See e.g. [1] or [2] for more robust estimators that are bounded.
4. Since many recent works try to improve the generalization capabilities of meta-rl algorithms, I was wondering how the proposed approach generalizes to out-of-distribution tasks (i.e., tasks that are unlikely to occur at meta-training). Though it is never mentioned in the paper, I believe the proposed method has the potential to be robust to negative transfer since the importance weights (which would be very small for very different tasks) should automatically discard old data and focus on new data alone. This is in contrast to many existing methods where the meta-trained model might negatively bias the learning of very different tasks. I think an experiment of this kind would be valuable to improve the paper.
[1] Elvira, V., Martino, L., & Robert, C. P. (2018). Rethinking the effective sample size. arXiv preprint arXiv:1809.04129.
[2] Tirinzoni, A., Salvini, M., & Restelli, M. (2019, May). Transfer of Samples in Policy Search via Multiple Importance Sampling. In International Conference on Machine Learning (pp. 6264-6274). | Review: The authors investigate meta-learning in reinforcement learning with respect to sample efficiency and the necessity of meta-learning an adaptation scheme. Based on their findings, they propose a new algorithm 'MQL' (Meta-Q-Learning) that is off-policy and has a fixed adaptation scheme but is still competitive on meta-RL benchmarks (a distribution of environments that differ slightly in their reward functions).
They motivate the paper by data-inefficiency of current meta-learning approaches and empirical results suggesting that meta-learning the adaptation scheme is less important than feature reuse.
On the other hand, their introduction section would benefit from additional references to the kind of meta-learning they describe. In particular, their so-called ""definition of meta-learning"" is mostly about domain randomization (e.g. Tobin et al. 2017 https://arxiv.org/abs/1703.06907) and not about the broader 'learning to learn' RL methodology (in particular Schmidhuber 1994 ""On learning how to learn learning strategies"").
**The authors make the following contributions:**
1. They show that Q-Learning trained on multiple tasks with a context variable as an input (an RNN state summarizing previous transitions) is competitive to related work when evaluated on a test task even though no adaptation is performed
2. Based on these observations, they introduce a new method for off-policy RL that does not directly optimize for adaptation but instead uses a fixed adaptation scheme
3. The new method leverages data during meta-testing that was collected during meta-training using importance weights for increased sample efficiency
**Overall, we believe the contributions are significant and sufficiently empirically justified.**
There are strong similarities, however, to parallel work on analyzing whether MAML relies on feature reuse or rapid learning (Raghu et al. 2019 https://arxiv.org/abs/1909.09157).
This work and the present submission conclude that feature reuse is much more significant than meta-learning an adaptation scheme when evaluated on current meta-RL benchmarks. This is a significant result and supports the new method developed in this paper.
During meta-training, their proposed method maximizes only the average return across tasks, not the ability to adapt from the resulting parameters.
Their method introduces a fixed (non-learned) adaptation scheme that performs favorably compared to certain methods from the existing meta-learning literature and demonstrates that even dropping this adaptation still does well.
There are strong similarities to Nichol et al. 2018 (https://arxiv.org/abs/1803.02999). We encourage the authors to relate this work to Raghu et al. 2019 and Nichol et al. 2018.
**Despite these interesting results, we strongly disagree with the meta-learning narrative of their new method.**
Because the adaptation scheme is no longer optimized directly, instead a fixed adaptation scheme is assumed, hence the approach in this paper is no longer a meta-learning algorithm.
Instead, this method has strong similarities with transfer-learning and domain adaptation (first training on one distribution of tasks, then fine-tuning on another task).
The authors should discuss the links to these fields of research, and clarify what's really novel, already in the abstract.
For example, on page 2 the authors claim that optimizing the multi-task objective (the mean error across tasks) is the simplest form of meta-learning. This objective, however, is NOT meta-learning.
**Decision.**
The submission contains strong empirical results emphasizing the significance of feature reuse and the insignificance of learned adaptation on the tested meta-RL benchmarks.
The reuse of experience from meta-training during meta-testing by employing importance weights is also an interesting contribution.
In contrast, we are not satisfied with the presentation of their new approach as a meta-learning approach. This method should be introduced along the lines of: 'Transfer Learning / Feature Reuse in RL is competitive to meta-learning across similar tasks'.
In its current form, we tend to reject the paper because it further obscures what the term meta-learning refers to. The authors are confusing it with more limited transfer learning.
Additionally, it was not clear to us whether the quadratic penalty they add to their adaptation scheme is only empirically valid or whether there is a theoretical reason.
For now, we'd lean towards rejecting this submission, but we might change our minds, provided the comments above were addressed in a satisfactory way - let us wait for the rebuttal.
Edit after rebuttal: score increased!",5.75
31,Mirror-Generative Neural Machine Translation,...,"Review: In this paper, the authors propose MGNMT (Mirror Generative NMT) which aims to integrate s2t, t2s, source and target language models in a single framework. They lay out the details of their framework and motivate the need for leveraging monolingual data in both source and target directions. They also talk about related work in this space. Finally, they perform experiments on low and high resource tasks. They also investigate certain specific phenomena like effect of non-parallel data, effect of target LM during decoding, and effect of adding one side monolingual data.
Pros:
- Overall, the paper was clearly written and well motivated. The authors clearly lay out their new framework and establish it for the reader.
- The set of experiments are very detailed and the authors make sure to compare against all semi-supervised works like BT, JBT and Dual learning.
- The set of analyses at the end was also interesting and tried to dig deeper in certain phenomena.
- All training details and hyperparameters have been laid it in the paper.
Cons:
- For all the additional complexity, this newly proposed method only slightly outperforms other semi-supervised methods like BT, JBT & Dual learning as seen in Tables 3 and 4.
- The authors could have been more upfront about training and inference costs of their proposed framework and compared it to the other setups. For example, decoding costs 2.7x more than a vanilla transformer. A comparison of decoding and training costs of all methods would have provided the right balance between complexity and quality. This additional complexity might outweigh the gains obtained in some cases.
Rating Justification:
Despite the con of added complexity, I like the formulation of the new joint framework and I think this will serve as a good starting point for others to push in this direction further. Hence, I want to see this paper accepted.
Minor comments:
last para of section 1: first line is too big. Please break into multiple lines.
""Exploiting non-parallel data for NMT"" - second para, please cite Dong et. al and Johnson et. al who also share al parameters and vocab in a single model.
Page 5, section 3.2, second para - line 1 please rephrase. | Review: This work proposes a new translation model that combines translation models in two directions and language modelss in two languages by sharing a latent semantic representation. The basic idea to joint modeling of translations conditioning on the latent representations and the parameters are learned by generating pseudo translations in two directions. Decoding is also carefully designed by interchanging sampling in two directions in a greedy fashion. Empirical results show consistent gains when compared with heuristic methods to generate pseudo data, e.g., back translation.
It is an interesting work on proposing a unified framework to translation by conditioning on a shared latent space in four models. It is not only rivaling heuristic methods to generate pseudo data, but surpassing competitive Transformer baselines.
Other comment:
- It is a bit confusing that MGNMT was not experimented with Transformer, though the paper and appendix describe that it is easy to use the Transformer in the MGNMT setting. | Review: This paper proposes an approach to neural MT in which the joint (source, target) distribution is modeled as an average over two different factorizations: target given source and source given target. This gives rise to four distributions - two language models and two translation models - which are parameterized separately but conditioned on a common variational latent variable. The model is trained on parallel data using a standard VAE approach. It can additionally be trained on non-parallel data in an approach similar to iterated back-translation at sentence-level granularity, but with language and translation model probabilities for observed sentences coupled by the latent variable. Inference iterates between sampling a latent variable given the current best hypothesis, and using beam search plus rescoring to find a new best hypothesis given the current latent variable. The approach is evaluated in several different scenarios (low- and high-resource, domain adaptation - trained on parallel data only or parallel plus monolingual data) and found to generally outperform previous work on generative NMT and iterated back-translation.
Strengths: clearly written, well motivated, very comprehensive experiments comparing to relevant baselines.
Weaknesses: somewhat incremental relative to Shah and Barber (Neurips 2018), results are only marginally positive, framework is probably too cumbersome to justify widespread adoption based on the results.
I think the paper should be accepted. Although it’s not highly original, it ties together three strands of work in a principled way: joint models, variational approaches, and back-translation / dual learning. The increment over Shah and Barber is bolstered by the addition of back-translation, which gives substantial improvements when using non-parallel data; and to a lesser extent by the argument about the advantage of separate models for distant language pairs. Using all possible LMs and TMs coupled with a latent variable feels like an area that was inevitably going to get explored, and this paper does a good job of it. Although the gains over the baselines are not overly compelling, they are quite systematic, indicating that the advantage is probably real, albeit slight. The authors are also to be commended on their use of not just the Shah and Barber baseline, but also the back-translation-based techniques, which are generally stronger competitors when monolingual data is incorporated.
Further comments/questions:
Why are there no results for Transformer+Dual in table 4? This omission looks odd, since Transformer+Dual was the strongest baseline in table 3.
Please add implementation details for the Transformer+{BT,JBT,Dual} baselines.
It was surprising not to see robustness experiments like Shah and Barber’s dropped source words, since robustness to source noise could be one of the advantages of having an explicit model of the source sentence.
A few additional suggestions for related work: noisy channel approaches (eg, The Neural Noisy Channel, Yu et al, ICLR 2017); decipherment (eg, Beyond parallel data: Joint word alignment and decipherment improves machine translation, EMNLP 2014 - yes, from SMT days, but still); other joint modeling work (KERMIT: Generative Insertion-Based Modeling for Sequences, Chan et al, 2019).
Consider dropping the “1+1 > 2” metaphor. It’s not clear to me exactly what it means, or what it adds to the paper.
“deviation” is used in a couple places where you probably meant “derivation”?
Line 6 in algorithm 2 should use both forward and backward scores for rescoring.",8.0
32,Mogrifier LSTM,TL;DR: An LSTM extension with state-of-the-art language modelling results.,"Review: I have read the authors' response. Their points regarding baseline comparisons are sensible in that there isn't a reason to expect the observations to *not* generalization to other datasets. It is odd that mLSTM is outperformed by LSTM in Table 3, but as the authors note in section 4.2 this may be due to instability of mLSTM during training. The results in the paper demonstrate significant improvement over LSTM, and while there are not as many baseline comparison to similar models as I would have liked to see, the quality of this work is sufficiently high that this is not a fatal flaw. In light of the author response and other reviews, I am revising my rating to 6: Weak Accept.
=====
This paper proposes a modification of LSTM networks in the context of language modeling called Mogrifier LSTM. Ordinary LSTMs are defined as recurrent operations on the current input, previous hidden state, and previous cell state. The proposed Mogrifier LSTM utilizes the same recurrent unit as the LSTM, but the input and previous hidden state are updated with several rounds of mutual gating. In each round, the input is multiplied elementwise by a gate computed as a function of the hidden state (or vice versa). The authors experiment on word-level and character-level modeling and compare their Mogrifier LSTMs to several state-of-the-art approaches. They also conduct an ablation study to show the effect of various design choices and hyperparameters and experiments on a reverse copy task.
Specific contributions include:
* Proposal of a novel approach for modulating inputs to a recurrent unit by mutual gating.
* Experiments demonstrating strong performance on a number of language modeling tasks.
The paper in its current state is borderline, leaning towards weak reject. Points in favor of acceptance include the high clarity of writing, good experiments of the proposed model, and a discussion of possible reasons for why the mogrification operation works well. The main shortcoming of the paper is experimental comparison to baselines.
The authors were able to train baseline LSTMs to high levels of performance (presumably due to tuning of hyperparameters) and then demonstrate that Mogrifier LSTMs improve upon LSTMs significantly. This is perhaps not entirely surprising, because the hyperparameter range of the Mogrifier LSTM includes zero rounds of updates, which would render it identical to the baseline LSTM. Therefore, if the hyperparameters are tuned sufficiently well, the performance of the Mogrifier LSTM should be at least as good as the LSTM. What the experiments do not show is that the proposed mogrification outperforms other forms of multiplicative interaction and/or gating. The closest that the authors come to this is the single validation perplexity of the Multiplicative LSTM in Table 3. If thorough hyperparameter tuning is applied to the Multiplicative LSTM or the approaches of Wu et al. (2016) and/or Sutskever et al. (2011), does the Mogrifier LSTM still outperform them?
Other than this critical issue of baseline comparison, the experiments are quite informative. The ablation study showing the effect of different design decisions and the hyperparameter visualiztion in Appendix B are particularly useful. The mogrification operation is described precisely enough for other researchers to implement and the arguments made in 4.4 are compelling.
Question for the authors:
* Some qualitative analysis of the learned mogrification operation would be helpful for understanding the nature of the modulation. For example, how do the predictions change depending on the modulation? If x is modulated by different hidden states, is there a noticeable effect on the output?
* Did you experiment with other forms of modulation before arriving upon the mogrification formulation? There are some naive approaches such as concatenating the hidden state to the input and applying a nonlinear layer, or predicting affine parameters for the input as a function of the hidden state in the style of FiLM [1]. Are there obvious shortcomings in these naive approaches that mogrification handles gracefully?
[1] Perez, E., Strub, F., De Vries, H., Dumoulin, V. and Courville, A., 2018, April. Film: Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on Artificial Intelligence. | Review: Summary:
This paper tackles the problem of context modelling within recurrent neural networks (RNNs). The authors propose an interdependent gating mechanism that enriches the coupling between inputs and hidden states. For an input x_0 and hidden state h_0; h_0 gates x_0 to create x_1; x_1 then gates h_0 to create h_1; this cyclical gating operation is applied for several rounds and it's output is fed into a recurrent neural network. For the next time-step, this process is repeated, with h_0 as the final h obtained after the final round of gating in the previous time-step. This results in the RNN processing a more contextualized version of the input tokens x.
Main Contributions:
1. A simple pre-processing step that contextualizes inputs for recurrent neural networks and significantly improves performance.
2. An extensive evaluation of the proposed technique against previous works and on all relevant datasets.
Pros:
The paper is very well-written and clear. It motivates and explores the questions and issues surrounding this topic very well.
Cons:
It would be good to see how this performance translates to other RNN architectures such as GRUs.
Final notes:
This paper raises many interesting question:
- What is the really going on with the gating mechanism?
The authors explore this question but the jury is still out on exactly what is going on here.
- ""Mogrification"" as a general preprocessing step: could it also improve performance for transformer models?
- Are there better ways to preprocess and gate the RNN inputs?
--------
Review Decision:
It is clear, well motivated, well written and represents a concrete contribution to the language modelling literature. Furthermore, most claims made are substantiated via thorough experimentation. Lastly, this work demonstrates that rather than relying on data and model scaling to improve performance; there is alot left to be done in tackling language modelling on smaller scale datasets. | Review: Summary:
The paper proposes a novel LSTM architecture that adds several gating mechanism that gates the hidden state and inputs in between the LSTM update. The proposed model shows superior performance on smallish datasets including PTB, Enwick8 and NWC.
Comments on the paper:
1. The paper proposes an interesting architecture and it seems to show significant improvement in terms of performance for some language datasets.
2. The paper is very well written, the motivation and formulation is clear. There are many analysis to understand the model (the strength and weaknesses).
3.. One thing is that since this could take into account more context, it seems that this model could potentially generate language / tokens with longer time dependencies. I wonder if the authors have performed any experiments on this and if they have seen any improvements on that front.
4. Also, I am curious about the generalization ability of the model. Could the authors train the model on shorter sequences and test for generation with longer sequences and see how this compares with baseline models.
5. The model seems to be related to Adaptive Computation Time (ACT) from Gaves et al. it would be nice to compare to the ACT.
6. Another slight improvement in writing could be to hightlight the intuition (conclusion in page 8) at the beginning of the paper, this could help in better understanding the motivation of the paper.
Minor comments on the paper,
1. The link and the self-citations on page 4 are does not seem to be valid links and citations.
Overall, a well-written paper, extensive analysis and good experimental result.",7.333333333333333
33,Neural Network Branching for Neural Network Verification,-''-,"Review: Summary:
This paper deals with complete formal verification of Neural Network, based on the Branch and Bound framework. The authors focus on branching strategies, which have been shown to be a critical design decision in order to obtain good performance. The tactic employed here is to learn a Graph Neural Network (which allows to transfer the heuristic from small networks to large networks), using supervised training to imitate strong branching. The authors also discuss fallback mechanism to prevent bad failures case, as well as an online fine-tuning strategy that provide better performance.
Experiments are performed on the CIFAR dataset and show convincing improvements compared to the baselines.
Comments:
* ""This allows us to harness both the effectiveness of strong branching strategies and the efficiency of GPU computing power"". Most other hand crafted heuristics also benefit from GPU computing, as they are based on gradients, or on the K&W dual, which all have GPU implementations.
* The description of the Nodes indicates that all hidden activation have a representative node in the GNN. Does it make sense to have it for non-ambiguous hidden activations?
* ""Since intermediate lower and upper bounds of a node xˆi[j] are completely decided by the layers prior to it"" -> That's not necessarily true depending on the Relaxation used. In the context of the full LP relaxation of Ehlers and branching on the ReLUs, constraint on following nodes can have an impact on earlier bounds. The authors make the same point later in the paragraph, so it's just a matter of being precise in the writing.
* ""underlying data distribution, features and bounding methods are assumed to be same when the trained model is applied to different networks"" -> This is a very reasonable assumption to make. Is there some intuition on which features are the most important? Given the features chosen, a strong relaxation needs to be used to obtain all the required features. Do the authors have any insights or experiments on how looser relaxations, which would lead to less feature available would fare?
* With regards to the improvement measure (8), I'm slightly confused by the definition. It essentially measures independently and averages the improvement for each of the subdomain resulting of the split. In this case, if we go from one subdomain with a lower bound of -5, to a pair of subdomain with respective lower bounds (0, -5). (essentially we have split across a useless dimension), this metric will grant a certain amount of improvements, while the global lower bounds held by the BaB process will not have changed. Did the authors give a try to other metrics?
* I'm happy to see some discussion of the failures case of following a learned policy, leading to a series of bad decisions, which in my experience is a real problem. Am I correct in understanding the explanation that after a split is done, if it provides poor improvement, the split is undone and a back-up heuristic is applied? Or is it just that for the resulting subdomains of the low improvement split, the back-up heuristic is used?
* I'm wondering if some hand crafted heuristics could be learned by the model? As in, is the model expressible enough that it could encode the heuristics of Bunel, Royo or Wang? This would be an interesting analysis and show that following the learning approach is essentially a ""free win"". From what I can see, it wouldn't be able to as it is missing some information (the GNN doesn't have access to the bias of the network for example?), but I might be wrong.
* For the upper bound computations, ""For the output upper bound, we compute it by directly evaluating the network value at the input provided by the LP solution"". Is there some reference on how effective of a scheme that is, compared to more expected things like adversarial attacks?
* I know that they are not directly comparable but Gurobi provides the information about the numbers of branches that it performed internally. This would have been beneficial to obtain for the results of Table 1 and 2
* Am I correct in assuming that MIPplanet is the same method as in Bunel et al., where all intermediates bounds are computed with the method of Ehlers et al.? Given that solving LP on large networks can be quite slow, is this method penalized by using tight but very expensive bounds? Would a MIP with bounds based on the linear relaxation of (1b) be faster and provide a stronger baseline?
* One aspect that is missing from this paper is the discussion of the cost of generation of the training dataset, and of the training of the GNN? How many properties do you need to have to verify for it to make sense to learn a heuristic rather than just using a handcrafted one?
There might also be some more interest if the network was shown to generalize to other settings. We can already observe that there is at least some transfer between architectures and across ""hardness of problems"", but it would be great to see if it generalizes further (learn a GNN on MNIST, use it to verify CIFAR?)
Opinion:
The paper is quite interesting and outperform its baseline by a significant amount. I have some question about whether the MIP baseline is the best one but even if it could have been improved, I still think there is interest in methods that are more specialized and go beyond trusting a MIP solver. | Review: This paper proposes to use graph neural networks (GNNs) to replace the
splitting heuristic in branch and bound (BaB) based neural network verification
algorithms. The paper follows the general BaB framework by Bunel et al., but
considers only splitting ReLU neurons, not input domains. The GNN is built by
replacing each neuron in network to be verified as a vertex, and the
connections between neurons as edges. Each vertex has a feature vector combining
information like pre-activation bounds and primal/dual LP solutions. A
specialized GNN training procedure is developed to exploit the structure of the
problem, and the weights of GNN are updated in a forward and backward manner.
Overall the paper proposes a novel idea of using GNN for accelerating
verification and it is demonstrated to be effective on one MNIST network as
well as its wider and deeper variants. I feel the main weakness is that the
empirical evidence provided are not thorough and sufficient (only 1 base model
on 1 dataset). Since this paper is 10-page, I evaluate it at a higher standard
and expect more convincing empirical results.
Questions and suggestions for improvements:
1. How much time does it take to generate training examples? It seems to me
that it is a very costly process because obtaining the relative improvement (8)
of splitting at each node can be quite expensive - basically, we need to split
almost every ambiguous neuron to get their improvement values, and in normal
BaB we only need to split one each time. The paper mentioned it ""minimum 5%
coverage per layer"" but does not provide more details.
2. Also how much time does it take for training the GNN? It seems the GNN has
many vertices - the same number as the number of neurons in a network, which
can be quite large. If the dataset generation and training time are much
longer comparing to the BaB time, the usefulness of the proposed method can be
limited especially it does not necessarily generalize to foreign networks
(networks with significantly different structure, or trained using different
methods).
3, An ablation study for the fail-safe strategy is needed. Without the
fail-safe strategy, is the GNN learned split better than other strong
heuristics? If the fail-safe strategy is too strong, the improvement we see can
probably come from the fail-safe strategy mostly, and GNN might not do too much
useful things. This is an important study that should be part of this paper.
4. It seems all networks in this work are trained using a single training
method, Wong & Kolter, 2018. Does the split heuristic learned by GNN works for
networks trained using a different training strategy? For example, interval
bound propagation (IBP) based methods [1][2] which achieve the state-of-the-art
results. Also, adversarial training with L1 regularization is also verifiable,
as demonstrated in [3]. Running a few pretrained models by these methods should
be an easy experiment to add.
5. There have been a few strong baselines in this field that the authors do not
discuss and compare against, including [4][5][6]. They solve similar problems
as in this paper and also provide promising results. At least, the authors should
discuss them in related works, and it is strongly encouraged to add at least one
of them as a stronger baseline.
6. This paper claims that Neurify is theoretically incorrect (in Appendix D.2,
page 17). I am quite surprised and not sure if this claim is true. I am not
aware of any firm evidence that Neurify is theoretically incorrect. It is
better to communicate with the authors of Wang et al., 2018 and make sure this
paper is making a correct claim.
Given that the idea proposed by this paper is novel and interesting, I tend to
accept this paper *under the condition* that the authors can conduct an
ablation study of the fail-safe strategy, provide generalization results on
models trained using different robust training strategies, and provide results
on at least one more dataset (like ACAS Xu, or CIFAR). Adding at least one more
baseline is also strongly encouraged.
** After author discussion, I have increased my score based on the new results
provided. The authors should make sure to include the ablation study results, and
a detailed discussion on training data generation time and training time in the
final version of the paper.
[1] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. ""On the effectiveness of interval bound propagation for training verifiably robust models."" arXiv preprint arXiv:1810.12715 (2018).
[2] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh, ""Towards Stable and Efficient Training of Verifiably Robust Neural Networks"" (https://arxiv.org/abs/1906.06316)
[3] Xiao, K. Y., Tjeng, V., Shafiullah, N. M., & Madry, A. (2018). Training for faster adversarial robustness verification via inducing relu stability. arXiv preprint arXiv:1809.03008.
[4] Katz, Guy, et al. ""The marabou framework for verification and analysis of deep neural networks."" International Conference on Computer Aided Verification. Springer, Cham, 2019.
[5] Singh, G., Gehr, T., Püschel, M., & Vechev, M. (2018). Boosting Robustness Certification of Neural Networks.
[6] Anderson, G., Pailoor, S., Dillig, I., & Chaudhuri, S. (2019, June). Optimization and abstraction: a synergistic approach for analyzing neural network robustness. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (pp. 731-744). ACM. | Review: The paper proposes learning a branching heuristic to be used inside a branch-and-bound algorithm used for solving integer programming problems corresponding to neural network verification. The heuristic is parameterized as a neural network and trained to imitate an existing heuristic called Strong Branching which is computationally expensive but produces smaller branch-and-bound trees than other heuristics. A graph neural network architecture is used to take the neural network being verified as input, and a message passing schedule that follows a forward pass and a backward pass along the network being verified is used. An online learning variant is also considered that fine tunes the learned heuristic at test time as a problem instance is being solved. Results for verifying large convolutional neural networks on CIFAR-10 show approximately 2x improvement in average running time of the branch-and-bound algorithm.
Pros:
- Significant reductions in average running time across the various datasets.
- Well-written paper with clear figures (especially figure 2) and explanations. I enjoyed reading it.
Cons:
- Novelty is somewhat low, as it is a straightforward application of existing ideas like Gasse et al. NeurIPS’19 to the problem of verification. The idea of forward and backward message passing schedule is similar to the idea considered in Amizadeh et al., ICLR’19 (/pdf?id=BJxgz2R9t7).
- It would be useful to present results on other datasets like MNIST. Even if they are not as impressive, it would be good to know when the proposed approach works and when it doesn’t.
Additional comments:
- Reporting average running time and number of branches can be sensitive to outliers. Shifted geometric mean will be less sensitive, please include these metrics.
- It would be good to compare against using a mixed integer program input representation (as done in Gasse et al., NeurIPS’19) of the verification problem to see what the difference in performance is. This can indicate how much benefit is obtained by conditioning on the neural network graph as the input representation and the associated forward-backward message passing schedule.
- How accurate is the learned heuristic in imitating strong branching? Is it necessary to get high accuracy on the imitation task to achieve an improvement in the final solve task?
- As a baseline it would be good to include the results for branch-and-bound using strong branching. Even if this is much slower, it would still help to know how much slower.
- I’m surprised that the reduction in the number of branches closely follows the reduction in the running time. This seems to suggest that the overhead of running graph neural network inference within branch-and-bound is negligible. Is this the case? If so, why -- is it because the LP solve time is much higher than the graph net inference time?",7.333333333333333
34,On the Convergence of FedAvg on Non-IID Data,AddPublic Comment,"Review: Federated learning is distinguished from the standard distributed learning in the following sense:
1) training is distributed over a huge number (say N) of devices and communication between the central server and devices are slow.
2) The central server has no control of individual devices, and there are inactive devices that does not respond to the server; full participation of all devices is unrealistic.
3) The local data distribution at each device is different from each other; i.e., the data is non-iid.
Due to property 1), communication-efficient algorithms such as Federated Averaging (FedAvg) have been proposed and studied. FedAvg runs SGD in parallel on K (≤N) local devices using their local datasets, and updates the global parameter after E local iterations by aggregating the updates from the local devices.
Properties 2) and 3) makes analysis of FedAvg difficult, and previous results have proven convergence of FedAvg assuming that the data is iid and/or all devices are active. In contrast, this paper studies FedAvg on the non-iid data and inactive devices setting and shows that, with adequately chosen aggregation schemes and decaying learning rate, FedAvg on strongly convex and smooth functions converges with a rate of O(1/T).
Overall, I enjoyed reading this paper and I would like to recommend acceptance. This is the first result showing convergence rate analysis of FedAvg under presence of properties 2) and 3), which is a nontrivial, important, and timely problem. The paper is well-written and reads smoothly, except for some minor typos. The convergence bounds provide insights of practical relevance, e.g., the optimal choice of E, the effect of K in convergence rate, etc. The authors also provide empirical results supporting their theoretical analysis.
Some questions I have in mind:
- What is ""transformed Scheme II""? Is it the scaling trick described at the end of Section 3.3? The name appears in the experiment section before being defined.
- What happens if we choose \eta_t that is decaying but slower than O(1/t), say O(1/\sqrt t)? Can convergence be proved? If so, in what rate?
Minor typos:
- Footnote 3: know -> known
- Assumptions 1 & 2: f in $f(w)$ is math-bold
- Choice of sampling schemes: ""If the system can choose to active..."" -> activate
- mnist balanced and mnist unbalanced: the description after them suggests they should be switched
- Apdx D.1: widely -> wide, summary -> summarize | Review: This paper analyzes the convergence of FedAvg, the most popular algorithm for federated learning. The highlight of the paper is removing the following two assumptions: (i) the data are iid across devices, and (ii) all the devices are active. For smooth and strongly convex problems, the paper proves an O(1/T) convergence rate to global optimum for learning rate decaying like 1/t with time. It is also shown that with constant learning rate eta, the solution found can be necessarily Omega(eta) away from the optimum (for a specific problem instance), thus justifying the decaying learning rate used in the positive result.
Federated learning has been an important and popular research area since it models a highly distributed and heterogeneous learning system in real world. Previous theoretical analysis of FedAvg was quite scarce and either made the iid data assumption or required averaging all the devices. This work is the first to prove a convergence guarantee without these two assumptions. In particular, it only requires averaging a (random) subset of devices each round, which is much more realistic than averaging all.
I don't quite have an intuition for why you need strong convexity. I hope the authors could explain this in words and maybe comment on what are the challenges of removing this assumption.
------
Thanks to the authors for their response. | Review: This paper presents convergence rates for straggler-aware averaged SGD for non-identically but independent distributed data. The paper is well-written and motivated with good discussions of the algorithm and the related works. The proof techniques involve bounding how much worse can the algorithm do because of non-identical distribution and introduction of stragglers into the standard analysis of SGD-like algorithms. The presented theory is useful, and also provides new insights such as a new sampling scheme and an inherent bias for the case of non-decaying step size. The empirical evaluation is adequate and well-presented. I think this paper is a strong contribution and should spark further discussions in the community.",7.333333333333333
35,Optimal Strategies Against Generative Attacks,...,"Review: This paper addresses the issue of malicious use of generative models to fool authentication/anomaly detection systems that rely on sensor data. The authors formulate the scenario as a maxmin game between an authenticator and an attacker, with limitations on the number of samples available to the authenticator to fix a decision rule, the number of samples required at test time for the authenticator to take a decision and the number of leaked samples the attacker has access to. The authors prove that the game admits a Nash equilibrium and derive a closed form solution for the case of multivariate Gaussian data. Finally, the authors propose an algorithm called ""GAN In the Middle"" and perform experiments to show consistency with the theoretical results, better authentication performance than state of the art methods and usability for data augmentation.
This pager should be accepted. Overall, it addresses crucial problems with the recent advances of generative models and provides significant theoretical results. The experiments would benefit from some clarification.
For the experiments, the following should be addressed:
* Confidence intervals for all results (specifically in Figure 1a and in Table 1)
* Figure 1a: it would be interesting to see a similar analysis also for other values of m and k.
* Table 1: This result would also be more supporting if experiments were performed for varying values of m, n and k. The description of the RS attack could be made more precise: does it mean that the attacker samples images at random? Intuitively, it feels confusing that the GIM authenticator would perform worse on this setting.
* The experiments on handwritten data would be more similar to what I imagine being a real-world authentication scenario if performed on a task where a class is a single person writing multiple characters, as opposed to characters being classes.
Minor comments:
* Page 6, second to last row: there is a""the"" repeated twice. | Review: ""Optimal Strategies Against Generative Attacks"" describes just what the title implies - various dimensions of the problem of defending against a generative adversary, with theoretical discussion under limited settings, as well as practical experiments extending on the intuitions gained using the theoretical exploration under limited conditions.
Particularly, one of the key stated goals of the paper is to ""construct a theorectical framework for studying the security risk arising from generative models, and explore its practical implications"". Given this goal, the paper performs admirably.
The appendix is extensive, and gives a lot more insight into the core paper itself.
I am unclear on how the data augmentation experiment fits into the overall picture - perhaps a more detailed explanation of how and why this would be used to form an ""attack"" would help. The other experiments are sensible, and demonstrate reasonable and expected results.
This is a solid paper, and most of my critiques are ""out of scope"" and revolve around experiments that would be nice to see. Though GAN-for-text is not simple, showing this type of setup for text would be interesting for a number of reasons, same for audio.
Continual passes through the text, with a focus on clarity could also be helpful - the topic is dense, and the text does a good job describing what is happening, but it is always possible to further distill these complex topics, and relegate some useful-but-not-critical pieces to the appendix.
These are minor quibbles, and overall this paper was an interesting and useful read, on a relatively underexplored topic. It shows theorectical results, and practical experimental demonstrations of the theories proposed. Given the stated goals of the paper, it performs admirably. | Review: # Summary
The authors investigate an attack-defense problem in which an attacker attempts to pass authentication by generating a faked input, while an authenticator attempts to detect the fraud. They formulate this problem as a zero-sum game and reveal the closed form of the optimal strategies. Furthermore, they reveal a more insightful closed form of the optimal strategies in the Gaussian case. This result clarifies the relationship between the success rate of the attacker and the numbers of the source, registration, and leaked observations. The analysis for the Gaussian case also gives an interesting insight that the optimal attacker’s strategy is to generate fake inputs so that its sufficient statistics are matched to that of the leaked observations. Based on this insight, the authors propose a new learning algorithm for the authenticator and demonstrate by some empirical evaluations that the proposed algorithm is robust against the faked input.
# Detailed comments
This is an interesting and well-written paper. I recommend acceptance of this paper.
The authors investigate an attack of generating a faked input for passing the authenticator under which the attacker can only observe partial information about the source input. This is an interesting point of view and allows us to analyze a more practical situation. Furthermore, based on the theoretical analyses, they reveal an interesting insight of the optimal attacker’s strategy that the optimal strategy generates a faked input so that its sufficient statistics are matched to that of the leaked observations. This insight introduces the new robust learning algorithm which outperforms the existing robust learning algorithm demonstrated as in the empirical evaluations.
Some minor refinements would improve the paper:
- \bar{x} and \bar{a} in Theorem 4.2 should be clearly defined.
- It seems to me that the authors use the term “ML attacker” to denote some different attacking algorithms. | Review: This paper proposes a new threat model for generative impersonation attacks: The attacker has access to several leaked images of a person; the authenticator knows several registration images per person and decides a person's identify by comparing some newly-sampled images from that person with corresponding registration images. The authors formulate this threat model as a minimax game and analyzed its Nash equilibrium. In the simplified case that observations are multivariate Gaussian, the authors are able to characterize the optimal strategies of the attacker and authenticator explicitly, which gives a nice intuition on how the theoretical optimum changes with respect to data dimension, number of leaked images, etc. Additionally, the authors implemented this attack (named Gan-in-the-middle attack) with an objective similar to GANs, empirically verified the theoretical results, and demonstrated the success of their approach on VoxCeleb2 and
As far as I know, this formulation of generative impersonation attacks is novel. The threat model nicely captures the most important aspects of impersonation attacks and is relatively realistic.
The theoretical analysis is insightful. I especially like that the authors can prove no defense is possible when n <= m, which nicely matches the intuition. The results on Gaussian case not only provide intuition, but also provide motivation for the design of attacker and defender architectures in GIM attacks.
The experiments are well-designed. The model architectures are well-motivated from Theorem 4.2. It is great to see that results of toy experiments match the theoretical analysis in Figure 1(a). The GIM attack on the Voxceleb2 images generates very realistic and reasonable portraits in Figure 2(a). The data augmentation experiment can be naturally fit into the framework of impersonation attacks and the application of their techniques in this direction is very exciting.
I only have two minor suggestions:
1. In Theorem 4.1, the symbol g_{X | Y} was introduced previously, whereas g_{X | A} was never introduced. I have to go to the appendix to understand the definition of g_{X | A}.
2. There is a minor issue in the proof of Lemma D.2 in page 16. The authors seem to miss a 1/2 factor in the second to last row in equation (D.3).",8.0
36,Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information,...,"Review: Review for ""Posterior Sampling for Multi-Agent Reinforcement Learning"".
The paper proposes a sample-efficient way to compute a Nash equilibrium of an extensive form game. The algorithm works by maintaining a probability distribution over the chance player / reward pair (i.e. an environment model).
I give a weak recommendation to accept the paper. Although I haven't checked the proofs in detail, the premise seems to be sound - the authors extend model-based exploration results from MDPs to games. The essence of the argument seems to be that the model of the chance player becomes close to d^\star quickly enough to get a sub-linear bound.
The main complaints I have about the paper concern clarity.
1. The paper is very densely written. This isn't necessarily bad, but it makes the paper a bit hard to understand. It would benefit the manuscript greatly to provide a figure which shows how the algorithm works for a small toy game. There is space left in the paper, so even a one-page figure would fit in. The figure should show all the major quantities: d, \sigma, u.
2. The meaning of the quantity \mathcal{G}_T^i should be more thoroughly described, given it is important in the proof.
3. You define a game with N players, but the algorithm works with 2.
4. Do you really need all the notations in section 2.1? Why not just define the ones used in the algorithm?
5. Can you discuss how large the constants \xi can become in practice? The definition of \xi^i seems to be different on page 10 and in Theorem 1 - please disambiguate.
I ask the authors to add a figure and address the issues above.
I am not an expert in this sub-field so I may have missed aspects of the paper.
Minor points:
- In Figure 1, please say that ""default"" is your algorithm.
- ""optimal in the face of uncertainty"" => ""optimism in the face of uncertainty"" | Review: PSRL
------
This work considers the task of finding a Nash equilibrium in a two-player zero-sum imperfect information game, where some aspects of the game are not known to the agents (specifically, the chance node probabilities, and the reward function).
The authors propose a method based on PSRL, i.e. at each iteration a set of game parameters are sampled from the posterior of the distribution. Then, CFR is applied in the inner loop; but instead of finding the NE strategy, one player finds the strategy that basically maximizes the reward deviation between two games sampled from the posterior, given that the opponent is playing Nash in the first game.
The authors prove convergence bounds for their algorithm, and demonstrate its performance on Leduc Hold'em with game parameters randomly chosen from a Dirichlet distribution.
-------------------------
I agree with the authors that standard CFR suffers from the requirement that the full game is known, so it doesn't work well in its standard form when the environment is not known. There *are* other regret minimizers that do work in the model-free setting ([1], [2], [3]), none of which are discussed by the authors.
I also find the proposed setting somewhat unconvincing. The authors are considering a situation where the environment is unknown, but it's not the RL setting because you still must control both (opposing!) agents. Of course, you can't actually find a Nash Equilibrium if you can't control the other player (because they might just never explore part of the game tree). But you would want your algorithm to be a regret minimizer regardless of your partner's strategy. Is this true of the proposed algorithm?
The proposed interaction strategy described in Eq. 5 and 6 is clever: basically, each player explores a part of the tree that maximizes the difference in payoffs between the two sampled games. This seems a bit inefficient thouggh, why doesn't the agent just find the BR to \sigma_{-i} under \tilde{d} given that the opponent plays the NE under d? I don't see why you would want to explore parameters that have a high reward uncertainty if there's a different strategy that does better than the whole confidence interval. It's like UCB: you should play a strategy not with the highest uncertainty, but with the highest optimistic payoff.
I think the work could be substantially improved by comparing against model-free baselines, e.g. fictitious self-play [2], CFR with outcome sampling [1]. The current work deosn't provide any evidence of what benefits the Bayesian approach provides over model-free regret minimization. Especially since the Bayesian approach presumably does not scale as well due to the requirement of maintaining beliefs over all possible games, and the requirement that a correct prior is provided. I would be curious to see Bayesian and model-free appraoches compared in games of different sizes to see how the different methods scale.
Nits:
- such as the private pokers in poker games (private cards)
- h_1, h_1 \in H^C, are independent
- The reference to Neil 2018 should be ""Neil Burch"", not ""Burch Neil""
- And if you're going to say ""we can directly apply the technique from [100-page PhD thesis]"", please mention the page number.
[1] Lanctot, Marc, et al. ""Monte Carlo sampling for regret minimization in extensive games."" Advances in neural information processing systems. 2009.
[2] Heinrich, Johannes, Marc Lanctot, and David Silver. ""Fictitious self-play in extensive-form games."" International Conference on Machine Learning. 2015.
[3] Srinivasan, Sriram, et al. ""Actor-critic policy optimization in partially observable multiagent environments."" Advances in Neural Information Processing Systems. 2018. | Review: Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information
================================================================
This paper investigates the use of Thompson sampling in multi-agent reinforcement learning.
They present a natural extension of the PSRL algorithm paired with counterfactual regret minimization, rather than expected reward maximization.
They provide support for this algorithm's efficacy through a theorem that proves polynomial learning rates, together with empirical evaluation where this approach is competitive with state of the art.
There are several things to like about this paper:
- This paper is definitely ""groundbreaking"" in that it makes a true extension to the existing literature: PSRL has been relatively well-studied in single-agent RL but never (to my knowledge) in the multi-agent setting.
- The extensions from single agent to multi-agent are natural, but also non-trivial, and it seems like this is a genuinely novel piece of work that can be interesting to both side (exploration and multi-agent).
- The general structure of the paper and presentation is good.
- The support from the theorem is great, and also the empirical evaluation is convincing.
There are a few places where the paper might be improved:
- It might be helpful to draw the connection to Thompson sampling more explicitly at the start. PSRL is really an application of Thompson sampling principle, but it is important that it doesn't happen every step but instead on a longer timescale. It might be helpful to cite ""a tutorial on thompson sampling"" Russo et al.
- Do you think there are promising avenues towards PSRL with generalization (rather than tabular)? It feels like actually this should carry over naturally... so maybe you should mention this?
- I'm not really an expert on the novelty / impressiveness of this algorithm in the multi-agent setting so cannot fully comment on that.
Overall I think this is a really interesting paper that should be of interest to the ICLR community.
I can't say this with full confidence (especially with respect to the multi-agent side) but I do think it's something that probably would add value to the conference!",6.666666666666667
37,Principled Weight Initialization for Hypernetworks,...,"Review: Principled Weight Initialization for Hypernetworks
Summary:
This paper investigates initialization schemes for hypernets, models where the weights are generated by another neural network, which takes as input either a learned embedding or the activations of the model itself. Standard initialization schemes for deep networks (Glorot, He, Fixup) are based on variance analyses that aim to keep the scale of activations/gradients well-behaved through forward-backward propagation, but using this approach is ill-founded for hypernets where the output is a set of weights rather than e.g. a softmax’d classification output. This paper extends the standard variance analysis to consider the hypernet case by investigating what the choice of hypernet initialization should be if one still wishes to maintain the well-behaved activations/gradients in the main model. The authors present results showing the evolution of hypernet outputs with their scheme are better-behaved, demonstrate that their modification leads to improved stability for hypernet-based convnets on CIFAR (over a model which, for the standard choice of He init, basically does not train) and improved performance on a continual learning task.
My take:
This is an ""aha!"" or an “obvious in retrospect” paper: a simple idea based on noticing something being done wrong in practice with a fairly straightforward fix, coupled with a decent empirical evaluation and analysis. The paper is well-written and reasonably easy to follow (although I am not familiar with Ricci calculus I did not feel that I was flummoxed at any point during the maths), and the potential impact is decent: any future work which employs a hypernetwork would likely do well to consider the methods in this paper. While I would like to see the empirical evaluations taken a bit further, I think this paper is a solid accept (7/10) and would make a good addition to ICLR2020.
Notes:
While this is a good paper, I think the impact of the paper could be magnified if the authors were a bit more ambitious with their empirical evaluations. This method seems to enable training hypernets in settings which would previously have been unstable; it would be good to more thoroughly characterize robustness to hyperparameters or otherwise demonstrate additional practical value. Showing results on ImageNet would also be helpful (I’m well aware this is not always in the realm of compute possibility) or just showing progress on a more challenging task outside of CIFAR or a small continual learning problem would, I think, greatly increase the chances that this paper catches on. As this is a “unleash your potential” note, I have not taken this sentiment into account in my review (as should hopefully be evident from my accept score).
Minor:
-Caption for figure 2 should indicate what kind of magnitudes are represented—are these the average weight norms in each layer vs epochs? The axes should be labeled. | Review: The paper presents an extension of Glorot/He weight variance initiazation formula for the hypernetworks. Hypernetworks are the class of neural networks where one (hyper) model generates the weights for the another (main) network, introduced by Ha et.al in 2016.
Authors argue and show via experiments that standard weight init formulas do not work for hypernetworks, resulting in vanishing or exploding activations and re-derive the formula for convolutional/fully-connected networks + ReLU.
They show that proposed method allows hypernet training when the standard ways don`t.
The technical contribution seems as logical and straightforward yet necessary step for hypernetwork-related research.
Questions:
- In standard NNs, initialization issues are mostly solved after introduction of BatchNorm. Wouldn`t it be the case for hypernetwork as well: to just add BN layers between main net layers?
- Figure 2. What are is the y axis of the figure? Norm? Variance? Mean? The same question for the most of Figures in appendix
- It would be nice to see how proposed method stands vs mentioned heuristics like M3 and M4.
Overall I like the paper.
Minor comments:
> ""This fundamental difference suggests that conventional knowledge about neural networks may not
apply directly to hypernetworks and radically new ways of thinking about weight initialization,
optimization dynamics and architecture design for hypernetworks are sorely needed.""
I don`t see anything ""radically new"" in re-derivation of Xavier formula to the new type of network.
======
Revision:
Revised version addressed my concerns and the batchnorm-related experiment is indeed surprising.
Overall, I like the paper and increase evaluation to strong accept. | Review: Review of “Principled Weight Initialization for Hypernetworks”
There has been a lot of existing work on neural network initialization, and much of this work has made large impact in making deep learning models easier to train in practice. There has also been a line of work on indirect encoding of neural works (i.e. HyperNEAT work of Stanley, and more recent Hypernetworks proposed by Ha et al) which showed promising results of training very large networks (in the case of Stanley), or have network weights that can adapt to the training data (in the case of Hypernetworks), and these approaches have been shown to be useful in applications such as meta-learning or few-shot learning (i.e. [1]). However, as far as I know, there hasn't been any work that looks at a principled way of initializing the weights of a weight-generating network, which this work tries to explore.
Making the observation (and claim) that traditional init methods don't init hypernetworks properly, they propose a few techniques to initialize hypernetworks (""Hyperfan""-family), which are justified in a similar way as original classical init techniques (i.e. preserving variance like in Xavier init), and they demonstrate that their method works well for feed forward networks on MNIST, CIFAR-10 tasks compared to traditional classical init methods, as well for a continual learning task.
I liked the paper as they identified a problem that hasn't been studied, and proposed a reasonable method to solve it. Their method may be able to make Hypernetworks accessible to many more researchers and practitioners, the way classifical init techniques have made neural net training more accessible.
There are a few things that could improve the paper (and get an improvement score from me). The authors don't have to do all of these, but just a few suggestions:
1) The experiments, to my understanding, are all feed forward networks. How about RNNs or LSTMs?
2) Are there any (interesting) tasks that use Hypernetworks that are not trainable with existing methods, but made trainable using this proposed scheme?
3) Would this method also work with HyperNEAT [2] or Compressed Network Search [3]? (probably should cite that line of work too). In [3], a research group at IDSIA used DCT compression to compress millions of weights into a few dozen parameters, so would be interesting if the approach will work on similar ""learn-from-pixels"" RL experiments.
I'm assigning a score of 6 (it's currently like a ""really good"" workshop paper, but a normal conf paper IMO), but I like this paper and would like to see the authors make an attempt to improve it, so I can improve the score to see it get accepted with a higher certainty.
Good luck!
[1] i.e. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6519722/ https://arxiv.org/pdf/1710.03641.pdf https://arxiv.org/pdf/1703.03400.pdf
[2] http://eplex.cs.ucf.edu/hyperNEATpage/
[3] http://people.idsia.ch/~juergen/compressednetworksearch.html
*** Revised Score ***
Nov20: Upon reading the other reviews, and looking at the changes to the paper with the extra citations, I'm improving the score to 8. (For the record, if this was a 1-10 scale, I would have liked my score to be a 7).",8.0
38,RNA Secondary Structure Prediction By Learning Unrolled Algorithms,"Keywords: RNA secondary structure prediction, learning algorithm, deep architecture design, computational biology","Review: RNA Secondary Structure Prediction by Learning Unrolled Algorithms
This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. Previous methods rely on dynamic programming (which does not work for molecular configurations that do not factorize) or rely on energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima). The former does not work for all molecules and the latter can be difficult to optimize. The method presented here is novel, shows strong SOTA performance, and would be of interest to the wider deep learning community.
The method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding. These constraints limit the wide RNA search space. The first component of the method is a “Deep Score Network” which uses a stack of Transformer encoders (with relative and exact positional embeddings) followed by 2D convolutional layers to output a L x L symmetric matrix describing the “scores” of base pairing. As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the “Deep Score Network” to enforce constraints. This network starts with a transformation that symmetrizes the matrix and applies a constraint-enforcing mask. The problem is transformed into an unconstrained problem by using Lagrange multipliers; it is then solved using a proximal gradient. Finally, a recurrent cell is defined that implements this algorithm in a deep learning framework. This method is creative, could be applied to other tasks with constraints, and would be interesting to the wider deep learning community.
In addition to developing the deep score network and post-processing network, the authors also develop a differentiable F1 loss, so that the network can directly optimize for precision and recall on the task. The performance of this method significantly outperforms previous methods. There was a fruitful discussion on OpenReview regarding whether this was a result of overfitting on the task. Indeed, it is critical in deep learning applications to carefully construct train/test sets to avoid high performance by memorization alone. To address this, the authors train on RNAStralign and test on ArchiveII. As the original ArchiveII dataset contains subsequences of other RNA sequences, which can result in overfitting, the authors re-ran their experiment with that removed, and similar results were achieved. To support the hypothesis that ArchiveII and RNAStralign capture different distributions, they perform a permutation test on the unbiased empirical Maximum Mean Discrepancy estimator, finding that the distributions are different. I do wonder why they did not check if P(ArchiveII) = P(ArchiveII) as they do check if P(RNAStr_train) = P(RNAStr_train). On the specific task of pseudoknot prediction, the method also performs well (F1 is >0.23 over the baseline). On sequence length-weighted F1, the model does even better.
The paper is rich with ablations. The analysis of the number of unrolling iterations T helps support the use of an unrolled method and builds intuition for its importance - it would be useful to include this in the appendix of the paper. I also appreciated the visualizations, which are a good sanity check that the model correctly handles pseudoknots. The performance of the method is broken down by RNA family, which is also quite interesting -- the method outperforms LinearFold on all classes, besides 5S RNA, SRP, and Group I intron. Further analysis is required to better understand why the method is weaker on those datasets. Additionally, further work should explore training on one set of families and testing on a held-out set of families. This was pointed out by public comments on this paper. This is potentially a limitation of E2EFold (the authors do not seem to have tried this suggested experiment) and further exploration is required. Exploring this limitation (even if it is not overcome) would make this paper even more rich.
That said, I recommend acceptance of this paper due to the extensive experiments, polished writing, novel method, and strong results, which can inspire future research. | Review: This paper introduces an end-to-end method to predict the secondary structure of RNA, by mapping the nucleotide sequence to a binary affinity matrix. The authors decompose this problem into two part: (i) predicting an affinity score between each base pair in the input sequence, using a combination of a transformer sequence encoder network and a convolutional decoder, and (ii) a post-processing step that ensures that structural local and global constraints are enforced. An innovation is to express this post-processing as an unrolled sequence of proximal gradient descent steps, which are fully differentiable, and allow the full combination of (i)+(ii) to be trained end-to-end. A thorough set of experiments validate the approach.
Overall, the paper is well written and easy to follow. The approach of unrolling structural constraints as shown in the paper is interesting and applicable to much wider domains than secondary structure prediction. The proposed approach appears to provide a novel, convincing and non-obvious solution to RNA secondary structure prediction, and subject to suggestions below, would represent a valuable contribution to ICLR.
The principal area for improvement would be to include additional detail (perhaps in appendix) on the model hyperparameter configurations that were used in the experiments. Moreover, more details on the set of \psi functions, and the MLP details for P_i (e.g. number of hidden units, activation function, the use of dropout, batch normalization, etc) should be given, as well as more information on the specifics how how the “pairwise concatenation” is carried out in the output layer. What unrolling constant T is used? Finally, in the ablation study (p. 8) details on how U_\theta is trained by itself (without the post-processing step) should be given.
Detailed comments:
* Overall, the whole paper should be thoroughly reviewed for English grammar and writing style; a subset of suggested changes follow.
* p. 1: structure a result ==> structure is a result
* p. 2: energy based methods ==> energy-based methods
* p. 2: energy function based approaches ==> energy function-based approaches
* p. 2: view point ==> viewpoint
* p. 2: E2Efold is flexible ==> E2Efold are flexible
* p. 2: nearly efficient ==> nearly efficiently
* p. 3: typically scale ==> typically scale as
* p. 3: few hundred. ==> few hundreds.
* p. 4: all binary matrix ==> all binary matrices
* p. 4: output space can help ==> output space could help
* p. 5: formulation are the ==> formulation are that the
* p. 6: eq. (7) should contain quantities indexed by
t
in the RHS
* p. 8: pesudoknotted ==> pseudoknotted
* p. 9 ff: in the bibliography, all lowercase rna should be uppercase RNA. Use {RNA} in bibtex entries. | Review: The authors proposed an end-to-end method (E2Efold) to predict RNA secondary structure. The method consists of a Deep Score Network and a Post-Process Network (PPN). The two networks are trained jointly. The score network is a deep learning model with transformer and convolution layers, and the post-process network is solving a constrained optimization problem with an T-step unrolled algorithm. Experimental results demonstrate that the proposed approach outperforms other RNA secondary structure estimation approaches.
Overall I found the paper interesting. Although the writing can be improved and some important details are missing.
Major comments
As the authors point out, several existing approaches for unrolling optimization problems have been proposed. It would be helpful to clarify the methodological novelty of the proposed algorithm compared to those.
Training details and implementation details are missing; these hinder the reproducibility of the proposed approach. The author stated pre-training of the score network, how is the PPN and score network updated during the joint training? Does the model always converge? The authors vaguely mentioned add additional logistic regression loss to Eq9 for regularization. What is a typical number of T? How does varying T affect the performance, both in terms of training time (and convergence) and in terms of accuracy/F1?
Minor comments
The 29.7% improvement of F1 score overstates the improvements compared to non-learning approaches.. This performance was computed on the dataset (RNAStralign) on which E2Efold was trained. A fair comparison, as the authors also stated, is on the independent ArchiveII data. On this data, E2Efold has F1 score 0.686 versus 0.638 for CONTRAfold. The author should report performance improvement under this line.
It would be helpful to report performance per RNA category, both for RNAstralign data and ArchiveII data, while the ArchiveII data should still remain independent. Different models may have their strengths and weaknesses on different RNA types.
It is not obvious to me how the proximal gradient was derived to (3)-(5). It would be helpful if the authors show some details in the supplements.
Why is there a need to introduce an l_1 penalty term to make A sparse?
On which data is Table 6?
Typos, etc.
The references are not consistently formatted
“structure a result” -> “structure is a result”
“a few hundred.” -> “a few hundred base pairs.”
“objective measure the” -> “objective measures the”
“section 5” -> “Section 5” (in several places)
In the equation above Equation 2, should it be -\rho||\hat{A}||_{1} instead of plus? Otherwise, the “max” could be made arbitrarily large. | Review: *Summary*
The authors perform RNA secondary prediction using deep learning. The outputs are subject to hard constraints on which nucleotides can be in contact with others. They unroll a sophisticated optimization algorithm for a relaxation of the task of finding the optimal contact map subject to these constraints. This work is in a long line of work demonstrating that end-to-end training of models that incorporate application-specific optimization routines as sub-modules is very useful. In particular, it outperforms an approach where the inputs to this optimization problem come from a network that was trained using a simple loss that ignores the fact that it will feed into this structured optimizer. The paper also considers an application domain that will be unfamiliar to many ICLR readers interested in deep structured prediction, and may serve as a call to arms for the community engaging with additional problems in this field.
*Overall Assessment*
The paper is well written, well executed, and part of a general research thread that ICLR readers care about. There are a number of technical details, such as the loss function in (8) that will be of general interest. I advocate for acceptance.
*Comments*
The actual specification of the output constraints doesn't occur until late in the paper. Before then, the discussion of them is very abstract. Given that the constraints are easy to describe, the exposition would be improved notably if you described the specific constraints earlier on. This would help me understand the problem domain better.
Fyi, the idea of nested structures vs. non-nested structures appears in NLP in terms of projective parsing vs. non-projective parsing. There may be some relevant reading for you to do there. Your specific work (minus the unrolled constraint enforcement) is similar to ""Dozat et al. 2017. Deep biaffine attention for neural dependency parsing.""
The idea of backpropping through some constraint-enforcing process is reminiscent of backpropping through belief propagation. See, for example, Domke's ""Learning Graphical Model Parameters with Approximate Marginals Inference."" Or Hershey et al. ""Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures."" You should also cite work using unrolled ISTA to learn sparse coding dictionaries. They have terms similar to (5).
What exactly was your motivation for the setup in ""Test On ArchiveII Without Re-training?""
How sensitive is performance to the number of optimizer iterations? Does it work to train with a fixed number of unrolled iters, but at test time run the optimizer until convergence?
(8) is cool!",7.5
39,Reformer: The Efficient Transformer,-''-,"Review: This manuscript presents a number of algorithmic techniques to reduce the computational and space complexity of Transformer, a powerful and very popular deep learning model for natural language processing (NLP). Although Transformer has revolutionized the field of NLP, many small groups cannot make a full use of it due to lack of necessary computational resources. As such, it is very important to improve the space and computational complexity of this popular deep model. The techniques presented in this manuscript seem to be very reasonable and the experimental results also indicate that they are effective. My major concern is that the authors shall present more detailed experimental results. In addition to bits per dim, it will also better if the authors can evaluate the performance in terms of other metrics. | Review: This paper presents a method to make Transformer models more efficient in time and memory. The proposed approach consists mainly of three main operations:
- Using reversible layers (inspired from RevNets) in order to prevent the need of storing the activations of all layers to be reused for back propagation;
- Using locality sensitive hashing to approximate the costly softmax(QK^T) computation in the full dot-product attention;
- Chunking the feed-forward layers computations to reduce their cost.
This approach is first applied to a toy dataset to analyze its complexity, then tested on enwik8 language modelling task and imagenet-64 image generation task for ablation study and performance assessment.
The problem approached by the paper is interesting and the proposed approach is novel to the best of my knowledge. The paper is well structured and clearly written a part from some small typos (see minor comments below).
While the analysis of complexity is sound and convincing, and the fact of being able to train larger Reformers is very interesting, I have some questions and concerns about the approach and experiments.
- Effect of reversible layers: It is clear for the experiment of Imagenet64 that the effect is negligible, but the experiment on enwik8 in the paper seems unfinished. Did the authors manage to finish the training, and does it confirm the observation?
- Sharing QK: I am a bit confused about the effect and usefulness of this operation. Can the authors comment on why it is needed for LSH attention? It seems to me that the same operations can be achieved with different Q and K. Indeed, doing so, the authors slightly reduce the capacity of the model. The observed non-significantly decreased performance can be an effect of using only 3-layers. This may explain why the results reported for larger models in figure 5 show higher bpc than similar size state of the art models.
- Time per iterations: Can the authors report the time per iteration for the larger hash rounds (8 and 16) that are closer to full attention? For the highest reported number (4), from a quick and not precise look at figure 4, it seems that the performance achieved by the proposed method after 140k iterations is achieved by the full attention after ~40k iterations. The gain in time per iteration for this particular number of hash rounds can be lost by the loss in performance.
- Can the authors detail how they chose the hyperparameters of their approach? e.g. the size of hash buckets, the distribution used to generate the random matrix R ..
- The reported results can be made stronger by reporting average/error bars across several trial to show consistency.
Minor: typos:
Dimension of matrix R [d_k, d_b/2] -> [d_k, b/2]
Last paragraph of page 6: state of these art -> state of the art
———————————————
After rebuttal:
I have read the authors answer, and found they addressed my concerns. I'm therefore increasing my score. | Review: This paper presents an attempt to reduce the memory complexity of Transformers. The authors call their model the Reformer. It presents a LSH based self-attention mechanism, along with reversible adaptation of Transformers. The Locality sensitive hashing scheme reduces complexity from L^2 to L which is pretty neat.
Tackling the quadratic complexity of self-attention is indeed an important and nice direction. I think the LSH based attention quite novel and is a natural solution to reducing the complexity of the self-attention module. However, I think the technical description could be improved as the current form is quite confusing and difficult to parse.
The experiments are a little on the weaker side. Authors presented results on imagenet, enwiki and a synthetic task. I am mainly concerned if the Reformer works on tasks such as machine translation or other NLP tasks. The paper does not present much evidence that the effectiveness of LSH is broad and versatile.
My current vote is a weak accept, based on some preliminary understanding and the general novelty of the idea.
I do have some questions/issues/comments:
1) Given that there is some form of QK sorting, how is it possible to mask the future? Is this because tokens are sorted within buckets?
2) Can the authors clarify what ""Causal masking on the Transformer is typically implemented to allow a position i to attend to itself."" mean?
3) I'm a little confused about how the sorting is being done. Can this be done in an end-to-end differentiable manner?
4) Can the authors present some results on other tasks? While neat, I think other tasks (e.g., MT or QA) can be investigated to further ascertain that the LSH attention works well. Current experimental results are not too convincing.",7.333333333333333
40,Restricting the Flow: Information Bottlenecks for Attribution,"Keywords: Attribution, Informational Bottleneck, Interpretable Machine Learning, Explainable AI","Review:
Summary
---
(motivation)
Lots of methods produce attribution maps (heat maps, saliency maps, visual explantions) that aim to highlight input regions with respect to a given CNN.
These methods produce scores that highlight regions that are in a vague sense ""important.""
While that's useful (relative importance is interesting), the scores don't mean anything by themselves.
This paper introduces another new attribution method that measures the amount of information (in bits!) each input region contains, calibrating this score by providing a reference point at 0 bits.
Non-highlighted regions contribute 0 bits of information to the task, so they are clearly irrelevant in the common sense that they have 0 mutual information with the correct output.
(approach - attribution methods)
An information bottleneck is introduced by replacing a layer's (e.g., conv2) output X with a noisy version Z of that output.
In particular, Z is a convex combination of the feature map (e.g., conv2) with Gaussian noise with the same mean and variance as that feature map.
The weights of the combination are found so they minimize the information shared between the input and Z and maxmimize information shared between Z and the task output Y.
These weights are either optimized on
1) a per-image basis (Per-Sample) or
2) predicted by a model trained on the entire dataset (Readout).
(approach - evaluation)
The paper uses 3 metrics with differing degrees of novelty:
1) The bbox metric rewards attribution methods that put a lot of mass in ground truth bounding boxes.
2) The original Sensitivity-n metric from (Ancona et al. 2017) is reported with a version that uses 8x8 occlusions.
3) Least relevant image degredation is compared to most relevant image degredation (e.g., from (Ancona et al. 2017)) to form a new occlusion style metric.
(experiments)
Experiments consider many of the most popular baselines, including Occlusion, Gradients, SmoothGrad, Integrated Gradients, GuidedBP, LRP, Grad-CAM, and Pattern Attribution. They show:
1) Qualitatively, the visualizations highlight only regions that seem relevant.
2) Both Per-Sample and Readout approaches put higher confidence into ground truth bounding boxes than all other baselines.
3) Both Per-Sample and Readout approaches outperform all baselines almost all the time according to the new image degredation metric.
Strengths
---
The idea makes a lot of sense. I think heat maps are often thought of in terms of the colloquial sense of information, so it makes sense to formalize that intuition.
The related work section is very well done. The first paragraph is particularly good because it gives not just a fairly comprehensive view of attribution methods, but also because it efficiently describes how they all work.
The results show that proposed approaches clearly outperform many strong baselines across different metrics most of the time.
Weaknesses
---
* I'm not sure why the new degredation metric is a useful addition. What does it add that MoRF and LeRF don't capture on their own independently?
* I think [1] would be a nice addition to the evaluation section as it tests for something qualitatively different than the various metrics from section 4. It would also be a good addition to the related work.
Missing Details / Points of Confusion
---
* I think there's an extra p(x) in eq. 11 in appendix D.
* I think the variable X is overloaded. In eq. 1 it refers to the input (e.g., the pixels of an image) while in eq. 2 it refers to an intermediate feature map (e.g., conv2) even though it later seems to refer to the input again (e.g., eq. 3). Different notation should be used for intermediate feature maps and inputs.
Presentation Weaknesses
---
* In section 3.1 is lambda meant to be constrained in the range [0, 1]? This is only mentioned later (section 3.2) and should probably be mentioned when lambda is introduced.
* ""indicating that all negative evidence was removed."" I think this should read ""indicating that only negative evidence was removed.""
Suggestions
---
""The bottleneck is inserted into an early layer to ensure that the information in the network is still local""
I'd like this to be explored a bit more. Though deeper feature maps are certainly more spatially coarse they still might be somewhat ""local"". To what degree to they loose localization information? My equally vague alternative intuition goes a bit differently: The amount of relevant information flowing through any spatial location seems like it shouldn't change that much, only the way its represented should change. If the proposed visualizations were the same for every choice of layer then it would confirm this intuition. That would also be an interesting result because most if not all of the cited baseline approaches (where applicable) produce qualitatively different attributions at different layers (e.g., see Grad-CAM).
[1]: Adebayo, Julius et al. “Sanity Checks for Saliency Maps.” NeurIPS (2018).
Preliminary Evaluation
---
Clarity: The paper is clearly written.
Originality: The idea of using the formal notion of information in attribution maps is novel, as is the bbox metric.
Significance: This method could be quite significant. I can see it becoming an important method to compare to.
Quality: The idea is sound and the evaluation is strong.
This is a very nice paper in all the ways listed above and it should be accepted!
Post-rebuttal comments
---
The author responses and other reviews have only increased my confidence that this paper should be accepted. | Review: This paper presents an information-bottleneck-based approach to infer the regions/pixels that are most relevant to the output. For all the metrics listed in the paper, the proposed approaches all achieve very good performance. It turns out, the proposed two architectures are better (at least alternative) choices to the other existing attribution methods.
I do agree that the proposed two models (Per-Sample and Readout) can be used to roughly infer regions of interest, which has been strongly supported by the comprehensive experiments. To minimize equation (6), we need to make beta*L_I small. Minimizing L_{CE} in (6) tries to maximize the mutual information between Z and output (labels); while minimizing L_I with respect to weight beta would try to inject noise to each dimension of Z. However, L_{CE} needs to ensure it can get enough information for prediction, and thus would prevent the noise injection process for “the key regions”. By choosing reasonable beta (similar to variational information bottleneck), the proposed approaches are capable to highlight key regions used for prediction.
Overall, I think the method is elegant for approximately estimating the relevance score map.
Below are some of my (minor) questions/concerns:
1. What we learned = What we want?
The proposed approach seeks a sort of “sparse heatmap”.
The larger the beta, the more regions/pixels would be suppressed while smaller beta might fail to suppress non-important regions in the image.
In the paper, the beta used for calculating the per-sample bottleneck is among [100/k , 10/k, 1/k].
The beta for ReadOut bottleneck is 10/k.
However, according to Table 1, only when beta is smaller than 1/k, the accuracy of the model does not degrade too much.
When using beta=10/k to get the ""heat map"" (where 10/k is the best choice of per-smaple bottleneck for degradation task), how close is the ""heat map in beta=10/k"" to the ""ground-truth heatmap""?
To better understand the proposed methods, I have a small suggestion:
------ Try betas in a broader range including very small betas, e.g. [0.0001/k, 0.001/k,....,1/k,10/k], for both Table one and visualization.
Fix a few images and visualize the heatmap given different betas.
We might better see how the growth of beta changes the heatmap.
2. About zero-valued attributions.
I agree with you that equation (5) is an upper bound of MI (eq (4)).
However, I am not sure if I totally agree with the claim ""If L_1 is zero for an area, we can guarantee that no information from this area is used for prediction.""
----- Given L_1=0 really implies that no information of the corresponding region is used for the certain beta, but is this true for the original model (beta=0)? Table one shows that different beta would lead to very different downstream task accuracy.
3. Specific to the two approaches you proposed, can you explain/motivate in what situations per-sample bottle would be better and in what cases we should prefer ReadOut bottleneck? | Review: Summary
The paper proposes a novel perturbation-based method for computing attribution/saliency maps for deep neural network based image classifiers. In contrast to most previous work on perturbation-based attribution, the paper proposes to inject carefully crafted noise into an early layer of the network. Importantly, the noise is chosen such that it optimizes an information-theoretically motivated objective (rate-distortion/info bottleneck) that ensures that decision-relevant signal is flowing while constraining the overall channel-capacity, such that decision-irrelevant signal is blocked from flowing. The flow of signal is controlled by the amount of noise injected, which translates into a certain amount of mutual information between input image regions and noisy activations/features. This mutual information can be visualized in the input image, but it also has a clear, quantitative meaning that is readily interpretable. The paper introduces two ways to construct the injected noise, based on the information bottleneck. Resulting attribution maps are computed and evaluated on VGG-16 and ResNet-50 (on ImageNet), and are compared against an impressive number of previously proposed attribution methods. Importantly, the paper uses three different quantitative measures to compare the quality of attribution maps. The proposed method performs well on all three measures.
Contributions
i) Derivation of a novel method for constructing attribution maps. Importantly, the method is grounded on solid theoretical footing for extracting minimal relevant information (rate-distortion theory / information bottleneck method).
ii) Proposal of a novel quantitative measure to compare quality of pixel-level attribution maps in image classification, and extension of a previously reported method.
iii) Evaluation and comparison against a large body of state-of-the-art attribution methods.
Quality, Clarity, Novelty, Impact
The paper is clear and well written, with a nice introduction to the information bottleneck method. Experiments are well described and hyper-parameter settings are given in the appendix. To the best of my knowledge, the proposed method is sufficiently novel and the application of the information bottleneck framework to pixel-level attribution has not been reported before. Some of the design- and implementation-choices needed to render the intractable info bottleneck objective tractable could perhaps be discussed and potentially even improved in light of recent results in other fields (Bayesian DL, deep latent-variable generative models, and variational methods for deep neural network compression), but I currently don’t consider this a major issue. To me personally the work in convincing and mature enough to vote for acceptance - perhaps most importantly it lays important groundwork for important connections to the theory of relevant information and puts a lot of much needed emphasis on objective evaluation of attribution methods (i.e. without subjective visual judgement of saliency maps). My suggestions below are aimed at helping improve the paper even further.
Improvements
I) A short section of current shortcomings/limitations could be added to the discussion.
II) Perturbation-based approaches that inject noise (into the input image directly) have been proposed previously. Most notably: Visualizing and Understanding Atari Agents, Greydanus et al. 2018 and potentially follow-up citations. It would be interesting to compare both works empirically, but perhaps also theoretically/conceptually. Could the Greydanus work be related to applying the noise directly to the input image along with some additional constraints?
Minor Comments
a) Is there a particular reason for this choice of colormap? While it seems to be roughly perceptually uniform (which is of course good), why not choose a simple sequential colormap (instead of a rainbow-like one)? At least the use of red and green at the same time should rather be avoided to maximize colormap readability under the most common forms of color vision deficiencies.
b) Just a pointer - no need to act on this for the current paper. Large parts of the field of neural network compression are concerned with a similar kind of attribution - the question is which weights/neurons/filters are relevant and which ones are not and can thus be removed from the network without loss in accuracy. Information-bottleneck style objectives (or the closely related ELBO / variational free energy) in conjunction with sparsity inducing priors have been proven to be quite fruitful. See e.g. Variational Dropout Sparsifies Deep Neural Networks, Molchanov et al. 2017 for interesting work, that aims at learning the variance of Gaussian noise that is injected into neural network weights using a similar construction and variational objective as shown in this paper. Perhaps some ideas can be borrowed/translated for future, improved versions of the method from that body of literature (Molchanov 2017, but also more sophisticated follow-up work).",8.0
41,Rotation-invariant clustering of neuronal responses in primary visual cortex,...,"Review: The paper proposes an original approach to predict the function of groups of neurons in the V1 cortex based on their invariance to well designed rotation invariant CNN filters. The design of these features is funded by the observation that specific ganglion cell types have rotation and scale invariant responses to visual stimuli.
The method is very clearly explained and the evaluation on an publicly available dataset looks promising. The clustering Figure 6 in particular is very insightful.
The paper could have been more impactful if a comparison with a ground truth was built. The issue is clearly that ground truth is hard to establish for this type of problems but biological observations and annotations of cell types can be available (unfortunately not public as far as I know).
I would also be curious to know how such a method can be applied to a blind patient whose retina does not react to visual stimuli. Is there a biological function that will still preserve such invariance properties which allow to find structure in the data? | Review: In this study, the authors develop a method to cluster cells in primary visual cortex (V1) based on the cells' responses to natural images. The method consists in three steps:
- fit a rotation-equivariant convolutional neural network model to V1 cells (previously described in Ecker et al. 2019)
- align all cells by choosing the rotation for each cell that minimizes overall distance between cells in feature space, so that the clustering is mostly blind to the orientation of the filters.
- cluster the cells using a Gaussian mixture model (GMM).
Although I find this article mostly well-written and the topic important, I cannot recommend acceptance because (1) the study does not make a significant contribution to our understanding of V1, (2) the main innovation in ML presented (alignment method) is quite specific and will thus not likely be of interest for the general audience of ICLR:
(1) An important question in visual neuroscience is whether V1 cells form discrete functional clusters as opposed to a continuum. Another related question is whether these functional clusters correspond to distinct cell types characterized by specific wiring patterns, gene expression and/or morphology.
The analyses performed do not answer any of these two questions:
- the clustering model (GMM model) is not compared statistically to other models that would assume a continuous structure in the data (e.g. cells form a sparse continuous manifold in feature space). Although clusters do appear in the t-SNE visualization, this visualization does not provide statistical evidence that cells indeed form distinct clusters.
- The correspondence of the proposed clusters to cell types with specific wiring patterns, gene expression and/or morphology is not established. To establish this correspondence would require further experiments, as acknowledged by the authors: ""To systematically classify the V1 functional cell types, these proposals need to be subsequently examined based on a variety of biological criteria reflecting the different properties of the neurons and the prior knowledge about the experiment"".
(2) The alignment method, which consists in rotating the cells in feature space so that orientation is not a factor for subsequent clustering, is quite specific to the problem studied and likely not of interest for the general ICLR audience.
Additional feedback:
- Title: ROTATION-INVARIANT CLUSTERING OF FUNCTIONAL CELL TYPES IN PRIMARY VISUAL CORTEX
=> ""functional cell types"" is not adequate here, since the article does not establish the existence of functional cell types. Could be replaced with ""cell responses"".
- Abstract: We apply this method to a dataset of 6000 neurons and provide evidence that discrete functional cell types may exist in V1.
=> this sentence is misleading, since no evidence for functional clusters is provided.
- ""Thus, the network has learned an internal representation that allows constructing very similar functions in multiple ways""
=> To avoid the caveat of redundant features, the authors could try to add a dimensionality bottleneck on feature space before readout.
- ""Small values of β incur a small cost for poor reconstructions resulting in small optimised values of T and over-smoothed aligned readouts.""
=> A simulated annealing procedure (progressive increase of T during learning) could potentially allow the use of larger β values here (i.e. less distortion of the filter).
- The alignment procedure could lead to the emergence of spurious structure in the clustering. It would be important to control for this potential artifact by running the procedure on an unstructured synthetic dataset.
- It is possible that the MEIs within clusters look more similar than they actually are, since the cells are fitted from the same common bank of features. It would be useful but maybe difficult to control for this.
- It would be interesting to test the clustering procedure on a shuffled version of the readout weights (shuffle across features and V1 cells), so as to keep sparsity but not any other structure. Does the t-SNE map look less clustered? Is the GMM fit qualitatively different?
- Fig1(2): add legend/caption. what are the ellipses? | Review: The authors present a rotation-invariant representation of a CNN modeling the V1 neurons and a pipeline to cluster these neurons to find cell types that are rotation-invariant. Experimental validation is performed on a 6K neuron dataset with promising results.
The paper is well postulated.
Below are comments about the work:
1. In Figure 2, what does 1 x feature + 2 x another_feature mean?
2. In Equation 3, why was the ‘square’ of error differences not used?
3. In the clustering approach, how is the number of mixtures set for the GMM? How stable is the model to different number of mixtures?
4. In Figure 6: are Blocks 5 and 13 the same clusters (since they are of the same color) or is it that the colourmap use did not have 100 colors?
5. In the ‘network learned redundant features’, Sentence 1: why do the authors say ‘similar MEIs’. The 16 neurons rendered in both blocks look different.
6. It will be informative to know how the number of clusters vary based on the correlation threshold used to collapse 100 clusters to a lower number. Are the clusters still functionally distinct for varying thresholds? Further why is MEI confusion matrix only shown for 13 groups?",8.0
42,SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference,"Keywords: machine learning, reinforcement learning, scalability, distributed, DeepMind Lab, ALE, Atari-57, Google Research Football","Review: This paper presents a scalable reinforcement learning training architecture which combines a number of modern engineering advances to address the inefficiencies of prior methods. The proposed architecture shows good performance on a wide variety of benchmarks from ALE to DeepMind Lab and Google Research Football. Important to the community, authors also open source their code and provide an estimate which shows that the proposed framework is cheaper to run on cloud platforms.
Pros:
1. This work is solid from the engineering perspective. It effectively addresses the problems with prior architectures and the accompanying source code is clear and well structured. It is also extensively tested on several RL benchmarks.
2. The proposed framework is especially suited for training large models as the model parameters are not transferred between actors and learners.
3. The paper is well written and organized.
Cons:
1. The gain of the main algorithmic improvement (SEED architecture) over the baseline (IMPALA architecture) is obscured by the usage of different hardware. TPUv3 has different characteristics than Nvidia P100/V100 GPU chips which also might contribute to the speed up.
Questions:
1. Is it possible to provide more “apple-to-apple” comparison by running SEED and IMPALA on the same hardware (TPUv3 or Nvidia P100/V100 GPU)? | Review: The paper proposes a new reinforcement learning agent architecture which is significantly faster and way less costly than previously distributed architectures. To this end, the paper proposes a new architecture that utilizes modern accelerators more efficiently. The paper reads very well and the experimental results indeed demonstrate improvement. Nevertheless, even though working in deep learning for years and have also some experience with Reinforcement learning I am not in the position to provide an expert judgment on the novelty of the work. I do not know if ICLR is the right place of the paper (I would probably suggest a system architectures conference for better assessment of the work). | Review: The paper presents SEED RL, which is a scalable reinforcement learning agent. The approach restructure the interface / division of functionality between the actors (environments) and the learner as compared to the distributed approach in IMPALA (a state-of-the-art distributed RL framework). Most importantly, the model is only in the learner in SEED while it is distributed in IMPALA.
The architectural change from to IMPALA to SEED feels reasonable, and the results support the choices in a positive way.
SEED is evaluated using a large number of benchmarks using three environments, and the performance is compared to IMPALA. The results are very good, shows good scalability, and significantly reduced training times.
The paper is well written, easy to read, and I enjoyed it.
The code for SEED is released open source, which enables future research to build upon SEED.",7.333333333333333
43,Target-Embedding Autoencoders for Supervised Representation Learning,...,"Review: This paper examines target-embedding autoencoders (TEAs) in theory and practice. TEAs autoencode the output (rather than input) space and find a mapping from the input to the latent representation of the output. The forward pass of the decoder (for the output space) is shared by the input-to-output computation.
Target-embedding autoencoders (TEAs) have previously been proposed and used in practice (though not necessarily by the ""TEA"" name). The paper's presentation is confusing on this matter, at it claims to be the first to ""motivate and formalize"" TEAs; I do not believe it is appropriate to claim such a contribution in light of prior work. [Girdhar et al.] clearly utilizes a target-embedding autoencoder (see [Girdhar et al.] Figure 2). In addition, more recent published work clearly utilizes TEAs (though not named as such) as the centerpiece of their approaches. See, for example:
[A] Adrian V. Dalca, John Guttag, Mert R. Sabuncu. Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation. CVPR, 2018.
[B] Mohammadreza Mostajabi, Michael Maire, Gregory Shakhnarovich. Regularizing Deep Networks by Modeling and Predicting Label Structure. CVPR, 2018.
Figure 2 of [A] and Figure 1 of [B] both clearly depict applying target-embedding autoencoders on semantic image segmentation problems. [B] operates in the same supervised representation-learning setting proposed here. Notably, [B] utilizes staged training -- learning the autoencoder first -- as discussed in Section 2 of the submitted paper, and finds that to be important for achieving a regularization effect.
The real applications explored by [A] and [B] are perhaps more challenging than the datasets used in experiments here. The concluding sentence of the paper,""Target-representation learning is potentially applicable to any high-dimensional prediction task, and exploring its utility for specific domain-architectures may be a practical direction for future research"" should be changed -- prior work has already successfully utilized TEAs in the specific domain of image segmentation.
Given that the paper has missed (not cited) highly related published work that applies TEAs in practice, a rewrite of Section 4 is required. In the appendix, Table 6, Table 7 and Section B.1 also need significant updates. The proposed approach is no longer a unique entry in Table 6 or 7 -- e.g. [B] already contributed ""autoencoder component as regularization for learning predictor"" (Table 7). Additionally, toy experiments in Section 5 appear less significant a contribution when multiple full-scale systems already employ TEAs.
This paper's theoretical analysis does appear to set it apart from prior work. However, theorems are developed for an extremely limited context (linear TEAs) and it is unclear whether or how they might extend to practical use cases (i.e. TEAs that are nonlinear, deep neural networks).
---
The extensive author response and updated paper address many of my original concerns. I have updated my overall rating. | Review: This is an extremely well-written and well-motivated paper. The idea of target-embedding autoencoders is extremely relevant for problems where the dimension of the label space is as large (or larger) than the dimension of the input features. The experiments are thorough, the theoretical guarantees are extremely well thought of and derived. The applications to modelling the progression of cystic fibrosis and Alzheimer's are extremely useful and timely. I vote for a strong accept for this paper.
I would like to see some references to the extreme multi-label classification problems (http://manikvarma.org/downloads/XC/XMLRepository.html) and some of the other probabilistic approaches attempted in this domain (please see https://papers.nips.cc/paper/5770-large-scale-bayesian-multi-label-learning-via-topic-based-label-embeddings and the references and citations). | Review: 1. Summary: In this paper, the authors proposed a Target-Embedding Autoendocer (TEA) model for supervised representation learning. Different from the traditional feature embedding autoencoder model, TEA tries to learn a compact latent representation that can reconstruct the target vector. Hypothetically, this model should be especially useful when the target vector has a much higher dimension than the feature vector. The authors analyzed the proved some characteristics of this framework and conducted empirical experiments on three datasets to prove its effectiveness.
2. Overall assessment: The motivation of this paper is well justified. It's easy to follow and fun to read, even for a person who is not an expert in this area, like me. However, there still exist some problems in this paper. It needs more improvement to get published in a competitive conference like ICLR.
3. Comments:
3.1 Datasets used in this paper cannot fully prove the effectiveness of this framework. These datasets are all from very similar domains. The dimension of target vectors is comparable to that of feature vectors. In my view, it's necessary to test on more different types of datasets to prove the usefulness of a model, especially if it is a general framework like TEA.
3.2 Models used in this paper are relatively simple. Demonstrate the performance of TEA on more advanced models and more difficult tasks can deliver more insights to the community.
3.3 No state-of-the-art models are used in experiments. It's very likely that some existing work has already adopted the idea of target embedding. There also exist much other work on dealing with high dimensional target vector problem. How are the performances of these models? What is the advantage of the proposed framework over these existing work?
3.4 The source of gain part on page 8 should contain more explanations and analysis. This part is one of the most important parts of this paper. It can provide quite valuable insights to readers. I hope the author can expand it.
3.5 More details about training and inference are needed. The authors only use a few sentences to describe their three staged training process. I still have some questions left after reading it, such as how do you train the shared parts in TEA? Do you update its parameters in all stages? What the effect of the order of training? What will happen if I change it? | Review: This work introduces the idea of target embedding autoencoders for supervised prediction, designed to learn intermediate latent representations jointly optimized to be both predictable from features and predictive of targets. This is meant to help with generalization and has certain theoretical guarantees.
It is an interesting problem setting to consider where Y is high dimensional instead of X. More examples of this would be useful to provide in the intro. I think this is crucial to understand where this method might be useful.
Figure 1 is super informative and very nice!
Section 2:
Why do we expect that this paradigm of autoencoder based regularization “generalizes” better?
I like the explicit and honest discussion of prior work in this section.
One question is how important is the choice of reconstruction loss function - L2, vs max likelihood gaussian, vs L1, vs cross entropy, etc for performance?
Another question: how bad is performance if the learning is done stagewise - first the Y-Z-Y^ representation is learned and then the X->Z predictor is learned.
If something is out of distribution, how easy are TEA based learners to finetune?
Overall the idea seems reasonable - if the targets have some common set of factors, just predict those instead of predicting the full target value which might be harder to get right. It’s just a question of whether this holds true in many domains and how well this reconstruction loss generalizes across problems?
Section 3:
“We havenoted that TEA components can in principle be instantiated by any architecture. Does its benefit extend beyond the commonly-studied domain of static classification?” -> not clear what this means? Does this mean this algorithm has been proposed before or is it that it can ALSO work on non static classification tasks? Not clear how to situate this claim
The theoretical section seems to follow largely from Le et al, but with important distinctions on dimensionalities of various spaces involved. I wonder if the authors can comment on how often Assumption 1 and 2 are actually satisified?
Related Work:
Is the main difference between Yu 2014 and this just in the norm based regularization? I don’t think so, can this be made more clear. This seems also fairly related to Yeh, is it just a generalization of that paradigm? Or is there more to it? In light of the contribution of Yeh, this seems like slightly more marginal of a contribution? Is the main points of contribution the theoretical analysis and the extended experiments to sequence data rather than static classification?
The results do seem to show a signficant benefit as compared to FEA or base models. It also seems like this is applicable across multiple disease datasets. Do the authors think that this could be applicable to other domains altogether? Would it be quick to run a comparison on these?
Generally seems like a well grounded and meaningful contribution with many improvements. Would be curious to see applications to other datasets and also some improvements/clarifications noted above?",7.0
44,Understanding and Robustifying Differentiable Architecture Search,-''-,"Review: This paper studies the causes of why DARTS often results in models that do not generalize to the test set. The paper finds that DARTS models do not generalize due to sharp minima, partially caused by the discretizing step in DARTS. The paper presents many experiments and studies on multiple search spaces, showing that this problem is general. To address this problem, the paper proposes several different ways to address this, e.g., an early stopping criteria and regularization methods.
Overall, the paper is well written, thorough experiments (various tasks and search spaces) show the benefit of the approach. The final experiments using the full DARTS space also show an improvement over standard DARTS.
The final method is fairly simple, running the search with different regularization parameters and keeping the best model, which suggests it could be widely used for DARTS-based approaches.
Two (very) minor comments:
- There's a missing space before ""Similarly"" in section 2.1
- Extra "")"" in last paragraph of section 2.2 (after the argmin eq). | Review: This paper seeks to understand why Differential Architecture Search (DAS) might fail to find neural net architectures that perform well. The authors perform a series of experiments using different kinds of search spaces and datasets, and concluded that a major culprit is the discretization/pruning step at the end of DARTS. To avoid this, the authors propose early stopping based on measuring the eigenvalue of the Hessian of the validation loss. The results look promising (though as someone who is not familiar with the datasets, I don't have a sense of the significance of improvements.)
In general, this is a strong paper. I enjoyed reading it. It describes the problem clearly and performs a set of convincing experiments to support the claims. I especially like how different constrained search spaces are investigated, as this makes the results easier to interpret. I think the analysis in this paper will benefit researchers who work on similar problems. | Review: ----- Updated after rebuttal period ---
The author's detailed response effectively addressed my concerns. I am moving my score to Accept. This paper proposes an interesting systematic study of differentiable approach in NAS.
------ Original Review ----
Summary
This paper presents a systematic evaluation on top of differentiable architecture search (DARTS) algorithm and shows it usually searched an architecture with all skip-connection. It empirically reveals that the largest eigenvalue of the Hessian matrix (\lambda) of loss w.r.t. architecture parameters has a strong correlation with the generalization ability (via loss of test dataset), and shows this \lambda will first decrease but then drastically increase after a certain epoch number on 4 different search spaces. It then proposes an early-stop scheme (DARTS-ES) to stop the search before this phenomenon occurs. In addition, it proposes to use data-augmentation, path-dropping and tuning L2 regularization during the search, namely Robust-DARTS(R-DARTS), and yield constantly better results over original DARTS on 3 datasets.
Overall, the observation that the largest eigenvalue of the Hessian matrix is novel and intriguing, and the experiments are extensive and meaningful. The idea to use more search spaces for comparison is fair and performance increase demonstrates the proposed R-DARTS and DARTS-ES are effective. Although I still have some questions regarding the detail settings, I think this paper provides a novel angle to understand the search phase of DARTS, and proposed simple but effective regularization can be beneficial to the research community using DARTS.
Main concerns:
- Problem of DARTS as a motivation
The claims of local smoothness/sharpness and generalization are related to network generalization is quite intriguing, however, only using largest eigenvalue of Hessian matrix as an indicator of this local shape does not seem to be enough. A recent paper on loss-landscape visualization [1] provides means to examine this hypothesis directly, and could the author try to provide additional visualization to support their claim? Otherwise, the paper's claim does not generalize to the local shape of the loss function, and should stays with the largest eigenvalue. It is totally okay in my perspective, but just indicates some revision to the main text and analysis of Section 4.2.
- Questions about Figure 3 experiments
How is test error computed? Is it on a batch of test-split, or the entire one? Also, which architecture is used to compute this test error? Paper mentioned, in Section 4.1, the word ""final architecture"", but does this refer to the super-net (the one-shot model in paper's definition), or the stand-alone model obtained via binarized architecture alphas? If latter, is this generalization error obtained from training from scratch? Or simply using the super-net parameters during the search? Since the conclusion of this plot serves as the foundation of designing R-DARTS and DARTS-ES, if the experiments are only conducted over a small set of images or the binarized model with super-net parameters, it undercuts the credibility of the conclusion, largest architectural eigenvalue, and generalization ability.
- More independent runs of experiments.
In Figure 3, validation of original DARTS, and Table 1, DARTS vs DARTS-ES, paper runs the experiment for 3 times and take the average, but for the proposed R-DARTS, it is not. Is there a reason why not scaling the experiments? I suggest the author provide results over 5 runs, like the one in Table 4 for PTB and show if the R-DARTS truly surpasses DARTS constantly. This question also applies to Figure 1, when paper claims the original DARTS found poor cell type, could this be repetitive over multiple runs?
I guess for the experiments in Table 3, it is already done since paper mentioned the reported results are the best model of searching 4 times.
- R-DARTS failed to out-perform DARTS in the original space on CIFAR-10
This is confusing, will this suggest, if tuning well, DARTS will surpass R-DARTS(L2) in other cases as well? Since this is the only setting that DARTS is built upon.
Minor comments and questions
- Using test data during search
After showing the strong correlation between the largest eigenvalue of Hessian and the network generalization error, the early-stop is natural, however, does this mean the model selection is using the test data? Or the actual test-data is never seen during the search phase of Section 4.3.
- L2 stabilizes max eigenvalue
Paper uses L2 coefficient up to 0.0243, showing constant improvement of test error while validation error drops in CIFAR-10 of Figure 11. Could the author try larger coefficients to determine when this trend will stop?
- Question about section 4.2
Performance drop due to the binarized operation (pruning step) in DARTS analysis is very interesting, I am curious how many architectures does the paper evaluate in Figure 5, when the dominant eigenvalue is smaller than 0.25? Since the conclusion is ""low curvature never led to large performance drops"" if the number of points is too few, it is not that convincing, especially from the plot, we see at eigenvalue = 0.5, there exists 2 architecture with >20% drop. In addition, what does each point in Figure 5 refer to? The best model (and the binarized one according to the argmax of \alpha) of one independent DARTS run or some binarized models sampled from a distribution on top of the same DARTS run (meaning only one super-net)? Is this experiment follows the setting in Figure 4?
- Figure 6, C10 S2, DARTS-ES is worse than DARTS when Drop probability = 0.6, whereas all other cases, DARTS-ES outperforms DARTS, why does this happen? Could the author comment on it?
- ScheduledDropPath in section 5.1
Does Drop-path belongs to data-augmentation techniques? It is more like a regularization in my perspective and should be grouped with 5.2.
- S1 S2... in Table 3
Does this refer to the search space? Or different random seed (mentioned )
- one-shot v.s. weight sharing model
One-shot in NAS domain is firstly introduced by Bender et al., while Pham et al. use parameter sharing. The reason to use one-shot is that all the sub-paths will have a fair chance to be trained.
- Typos
1. In section 2.1, line 4 ""better.Similarly"" should have space.
--- Reference ---
[1] Li et al., Visualizing the Loss Landscape of Neural Nets, arxiv'17.",8.0
45,Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search,AddPublic Comment,"Review: This paper introduces a new algorithm for parallelizing Monte-Carlo Tree Search (MCTS). Specifically, when expanding a new node in the search tree, the algorithm updates the parent nodes’ statistics of the visit counts but not their values; it is only when the expansion and simulation steps are complete that the values are updated as well. This has the effect of shrinking the UCT exploration term, and making other workers less likely to explore that part of the tree even before the simulation is complete. This algorithm is evaluated in two domains, a mobile game called “Joy City” as well as on Atari. The proposed algorithm results in large speedups compared to serial MCTS with seemingly little impact in performance, and also results in higher scores on Atari than existing parallelization methods.
Scaling up algorithms like MCTS is an important aspect of machine learning research. While significant effort has been made by the RL community to scale up distributed model-free algorithms, less effort has been made for model-based algorithms, so it is exciting to see that emphasis here. Overall I thought the main ideas in paper were clear, the proposed method for how to effectively parallelize MCTS was compelling, and the experimental results were impressive. Thus, I tend to lean towards accept. However, there were three aspects of the paper that I thought could be improved. (1) It was unclear to me how much the parallelization method differs from previous approaches (called “TreeP” in the paper) which adjust both the visit counts and the value estimate. (2) The paper is missing experiments showing the decrease in performance compared to a serial version of the algorithm. (3) The paper did not always provide enough detail and in some cases used confusing terminology. If these three things can be addressed then I would be willing to increase my score.
Note that while I am quite familiar with MCTS, I am less familiar with methods for parallelizing it, though based on a cursory Google Scholar search it seems that the paper is thorough in discussing related approaches.
1. When performing TreeP, does the traversed node also get an increased visit count (in addition to the loss which is added to the value estimate)? In particular, [1] and [2] adjust both the visit counts and the values, which makes them quite similar to the present method (which just adjusts visit counts). It’s not clear from the appendix whether TreeP means that just the values are adjusted, or both the values and nodes. If it is the former, then I would like to see experiments done where TreeP adjusts the visit counts as well, to be more consistent with prior work. (Relatedly, I thought the baselines could be described in significantly more detail than they currently are—-pseudocode would in the appendix would be great!)
2. I appreciate the discussion in Section 4 of how much one would expect the proposed parallelization method to suffer compared to perfect parallelization. However, this argument would be much more convincing if there were experiments to back it up: I want to know empirically how much worse the parallel version of MCTS does in comparison to the serial version of MCTS, controlling for the same number of simulations.
3. While the main ideas in the paper were clear, I thought certain descriptions/terminology were confusing and that some details were missing. Here are some specifics that I would like to see addressed, roughly in order of importance:
- I strongly recommend that the authors choose a different name for their algorithm than P-UCT, which is almost identical (and pronounced the same) as PUCT, which is a frequently used MCTS exploration strategy that incorporates prior knowledge (see e.g. [1] and [2]). P-UCT is also not that descriptive, given that there are other existing algorithms for parallelizing MCTS.
- Generally speaking, it was not clear to me for all the experiments whether they were run for a fixed amount of wallclock time or a fixed number of simulations, and what the fixed values were in either of those cases. The fact that these details were missing made it somewhat more difficult for me to evaluate the experiments. I would appreciate if this could be clarified in the main text for all the experiments.
- The “master-slave” phrasing is a bit jarring due to the association with slavery. I’d recommend using a more inclusive set of terms like “master-worker” or “manager-worker” instead (this shouldn’t be too much to change, since “worker” is actually used in several places throughout the paper already).
- Figure 7c-d: What are game steps? Is this the number of steps taken to pass the level? Why not indicate pass rate instead, which seems to be the main quantity of interest?
- Page 9: are these p-values adjusted for multiple comparisons? If not, please perform this adjustment and update the results in the text. Either way, please also report in the text what adjustment method is used.
- Figure 7: 3D bar charts tend to be hard to interpret (and in some cases can be visually misleading). I’d recommend turning these into heatmaps with a visually uniform colormap instead.
- Page 1, bottom: the first time I read through the paper I did not know what a “user pass-rate” was (until I got to the experiments part of the paper which actually explained this term). I would recommend phrasing this in clearer way, such as “estimating the rate at which users pass levels of the mobile game…”
- One suggestion just to improve the readability of the paper for readers who are not as familiar with MCTS is to reduce the number of technical terms in the first paragraph of the introduction. Readers unfamiliar with MCTS may not know what the expansion/simulation/rollout steps are, or why it’s necessary to keep the correct statistics of the search tree. I would recommend explaining the problem with parallelizing MCTS without using these specific terms, until they are later introduced when MCTS is explained.
- Page 2: states correspond to nodes, so why introduce additional notation (n) to refer to nodes? It would be easier to follow if the same variable (s) was used for both.
Some additional comments:
- Section 5: I’m not sure it’s necessary to explain so much of the detail of the user-pass rate prediction system in the main text. It’s neat that comparing the results of different search budgets of MCTS allow predicting user behavior, but this seems to be a secondary point besides the main point of the paper (which is demonstrating that the proposed parallelization method is effective). I think the right part of Figure 5, as well as Table 1 and Figure 6, could probably go in the supplemental material. As someone with a background in cognitive modeling, I think these results are interesting, but that they are not the main focus of the paper. I was actually confused during my first read through as it was unclear to me initially why the focus had shifted from demonstrating that parallel MCTS works to
- The authors may be interested in [3], which also uses a form of tree search to model human decisions in a game.
- Page 9: the citation to [2] does not seem appropriate here since AlphaGo Zero did not use a pretrained search policy, I think [1] would be correct instead.
[1] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484.
[2] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Chen, Y. (2017). Mastering the game of go without human knowledge. Nature, 550(7676), 354.
[3] van Opheusden, B., Bnaya, Z., Galbiati, G., & Ma, W. J. (2016, June). Do people think like computers?. In International conference on computers and games (pp. 212-224). Springer, Cham. | Review: The paper introduces a new algorithm for parallelizing monte carlo tree search (MCTS). MCTS is hard to parallelize as we have to keep track of the statistics of the node of the tree, which are typically not up-to-date in a parallel execution. The paper introduces a new algorithm that updates the visitation counts before evaluating the rollout (which takes long), and therefore allows other workers to explore different parts of the tree as the exploration bonus is decreased for this node. The algorithm is evaluated on the atari games as well on a proprietary game and compared to other parallelized MCTS variants.
The makes intuitively a lot of sense, albeit it is very simple and it is a surprise that this has not been tried yet. Anyhow, simplicity is not a disadvantage. The algorithm seems to be effective and the evaluations are promising and the paper is also well written. I have only 2 main concerns with the paper:
- The paper is very long (10 pages), and given that, we reviewers should use stricter reviewing rules. As the introduced algorithm is very simple, I do not think that 10 pages are justified. The paper should be considerably shortened (e.g. The ""user pass rate prediction system"" does not add much to the paper, could be skipped. Moreover, the exact architecture is maybe also not that important).
- The focus of the paper is planning, not learning. Planning conferences such as ICAPS would maybe be a better fit than ICLR.
Given the stricter reviewing guidelines, I am leaning more towards rejects as the algorithmic contribution is small and I do not think 10 pages are justified. | Review: This paper introduces a novel approach to parallelizing Monte Carlo Tree Search
which achieves speedups roughly linear in the number of parallel workers while
avoiding significant loss in performance. The key idea to the
approach is to keep additional statistics about the number of
on-going simulations from each of the nodes in the tree. The approach is
evaluated in terms of speed and performance on the Atari benchmark and in a
user pass-rate prediction task in a mobile game.
I recommend that this paper be accepted. The approach is well motivated and clearly
explained, and is supported by the experimental results. The experiments are reasonably thorough and
demonstrate the claims made in the paper. The paper itself is very well-written, and all-around
felt very polished. Overall I am enthusiastic about the paper and have only a few concerns, detailed below.
- I suggest doing more runs of the Atari experiment. Three runs of the experiment does not
seem large enough to make valid claims about statistical significance. This is especially
concerning because claims of statistical significance are made via t-testing, which assumes
that the data is normally distributed. Three runs is simply too few to be making conclusions
about statistical significance using t-testing. I think that this is a fair request to make and
could reasonably be done before the camera-ready deadline, if the paper is accepted.
- The experiments in Atari compare against a model-free Reinforcement Learning baseline, PPO.
Was there a set clock time that all methods had to adhere to? Or alternatively, was it verified that
PPO and the MCTS methods are afforded approximately equal computation time? If not, it seems
like the MCTS methods could have an unfair advantage against PPO, especially if they are
allowed to take as long as necessary to complete their rollouts. This computational bias
could potentially be remedied by allowing PPO to use sufficiently complex function
approximators, or by setting the number of simulations used by the MCTS methods
such that their computation time is roughly equal to that of PPO.
- I would be careful about stating that PPO is a state-of-the-art baseline. State-of-the-art is a big claim, and I'm not quite sure that it's true for PPO. PPO's performance is typically only compared to other policy-based RL methods; it's hard to say that it's a state-of-the-art method when there's a lack of published work comparing it against the well-known value-based approaches, like Rainbow. I suggest softening the language there unless you're confident that PPO truly is considered a state-of-the-art baseline.",7.333333333333333
46,Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity,AddPublic Comment,"Review: This paper applies new assumption on smoothness that assume the norm of Hessian is bounded by a scalar plus norm of gradient. The traditional smoothness is only bounded with a scalar, the proposed assumption is more relaxed because now the norm of Hessian can grow with the norm of gradient. Under this assumption, the authors show clipped gradient converges faster than gradient for general nonconvex problems. The authors provide insights on why the proposed assumption is good for describing neural networks, and empirically verify the assumption with ResNet on CIFAR and LSTM on PTB.
I like the paper in general. It is well written and easy to follow. The contributions are clearly described and the techniques seem to be solid.
I want a little bit more discussion to help me better understand the paper.
1) Intuitively, try best in plain English, why does clipped gradient convergence does not have dependency on L_1 M? Could the authors provide more discussion on the learning rate (hyperparameters) used for clipped gradient and gradient?
2) The convergence rates under the proposed assumption are slower than traditional smoothness assumption. Could you verify the slow convergence rate under proposed assumption aligns better with practical training? Could you provide a toy example, for example x^3, to show the advantage of the proposed assumption and the convergence under the assumption? What is the gap between clipped gradient and gradient, for experiments in figure 4, and possible toy problem? Could the authors elaborate the difference of clipped gradient and gradient with the details of theory, instead of simply claiming clipped gradient is faster?
Typo, page 5, (0, L) -> (L, 0)
============ after rebuttal ===================
I am happy to see the paper in the conference. The paper is intuitive and well written. My remaining comments are still on the slower convergence rate comparing to previous assumption. Even before, researchers suggest overparameterized network can converge faster than expected. This paper suggests a generally slower convergence rate if the proposed assumption does fit practice better. | Review: In this paper, the authors relax the generally used Lipschitz smoothness condition in optimization, to a more general smoothness condition that may depend on norm of the gradient. The authors proved that, with this relaxed condition, under such cases, both GD and clipped GD can converge within O(1/\epsilon^2) time, but when their exists x where the gradient norm can be very large, the clipped GD will converge provably faster than GD with a proved GD convergence rate lower bound. The authors also generalize their results to SGD. Experiments show that in deep neural network local gradient Lipschitz constant is scale with gradient norm, and use clipped gradient can accelerate convergence while keep good generalization performance as expected, which is well observed by other researchers.
Detailed comments:
1. The f^* In theorem 3 is not defined. I think it’s not the f^* in Assumption 1? Should be more clear. Also, in non-convex optimization, can the deterministic GD find the global optimum? If f^* is the stationary point the algorithm find, then the Assumption 1 is a little weird. The authors should better clarify this.
2. Feel the claim of the Assumption can be relaxed to only hold on \mathcal{S} is a little confusing. Which Assumption can be relaxed? It seems that in the proof of Lemma 9, we need the Assumption 3 holds for the set \|x^+-x\| \leq min{1/L_1, 1}. I think maybe the other Assumption will also be needed globally, unless the authors can prove that the optimization is only on a compact set in S.
3. Maybe a discussion connected the existing Lipschitz constant based results to the new results in this paper can make the readers more aware of the contribution in this paper. For example, the Lipschitz constant of neural network can be of order O(L_1 M) due to the definition, so if L_1 M is large, for clipped GD we can still have expected decreasing of O(1/L_0) each turn while vanilla GD with the best O(1/L) step size can still only have O(1/L) where L = L_0 + L_1 M due to the traditional results based on Lipschitz smoothness.
I don’t totally go through the whole proof, but I think the results are reasonable and the most of the techniques are standard to the whole community. So there may be no fatal error that violates the conclusion. I find the whole paper interesting and match the intuition from the practice. However, I think it can be polished to make the whole idea more clear and more easy to accept by the potential theorists and practitioner audience. | Review: The authors provide the definition of a (L_0, L_1)-smooth function.
They motivate this class of function based on theoretical arguments and
empirical observations in the context of neural network training loss.
The authors go on to show that for (L_0, L_1)-smooth loss constant
step-size gradient descent can perform arbitrarily bad with poor initialization.
Specifically, the largest constant step size that can be used without divergence
depends on the upper bound of the gradient norm.
If the function is steep (high gradient norm) at some point during the
optimization process the steps need to become arbitrarily small
(especially relevant in nonconvex loss, where gradient norm does not always decrease).
The small constant step size slows down convergence everywhere else.
The described phenomenon is an argument for step sizes adaptivity in general
Clipping the gradient based on the known (L_0, L_1)-smoothness is a form of
adaptivity that makes steps small enough to avoid divergence only where the
function is steep and can thus be more efficient.
Overall, the authors give a theoretical justification for the use of clipped
gradient descent in the context of training neural networks.
The analysis of clipped gradient descent is performed rigorously (proofs are
provided in the appendix) and extended to the stochastic gradient descent setting.
The motivation for using (L_0, L_1)-smoothness and gradient clipping is also
shown to be relevant in practical experiments with language and image
classification models.
I recommend to accept the paper.
The paper is a clearly stated contribution to optimization theory as motivated
by machine learning applications.
The authors give a clean theoretical analysis of a previously empirically
observed phenomenon and an algorithm that is already used in practice.
Notes:
- Page 1, first paragraph:
If f(x) = E_xi [f(x, xi)] then f(x) the expectation is generally not stochastic,
like you write before. It is clear what you mean but Maybe use a different symbol.
- Page 2, ...we observe the function smoothness has a strong correlation with gradient norm 2:
Perhaps write something like ""(see Figure 2)"" instead of just ""2"" here.
- Page 2, after definition of Lipschitz continuous:
Nitpick: Formal definition based on Hessian norm upper bound should also have, for all x \in \mathbb R^d
- General: Some equations that are not referenced seem to be numbered, some are not.",8.0
47,Your classifier is secretly an energy based model and you should treat it like one,-''-,"Review: This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects. The idea of reinterpreting the traditional classifier is very interesting, and the experiments show some good results of the proposed method.
Here are my main concerns of the current paper:
1. The training procedure seems to be very sensitive, and the SGLD may take a long time at each iteration to converge. This may be a big limitation of the proposed method.
2. According to equation (8), the proposed method is having a trade-off between classification and generation, and this seems to be the key to improve the performance of the model in generation by sacrificing some classification accuracy. I think author should emphasize this instead of energy based model.
3. The presentation is not very clear in section 5. What is the task of calibration, and what is the definition of ECE?
4. The robustness guarantee seems too good to be true. Although the authors claim that they allow the attacker to have access to the gradient of SGLD, the SGLD will add noise during the forward process, this will obfuscate the gradient. In this sense, I don’t think the proposed method will have the strong robustness as they claimed.
----------------
Post-Rebuttal Comments:
Thanks for addressing my concerns. Although I think the proposed method is not comprehensive to check obfuscated gradients, I do think the current version is a good fit for ICLR, and I decide to increase my score. | Review: The paper uses energy-based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out-of-distribution detection, etc.
Although there has been previous work attempting to bridge discriminative classifiers with generative modeling, this work proves to be competitive with both specialized models on discriminative/generative tasks as well as in many downstream tasks such as out-of-distribution detection, calibration, and adversarial robustness. The paper provides a clear exposition of the method, succeeds to discuss related work it bases on, conducts a thorough experimental study providing convincing explanations for results and does not hide the limitations of the work (high computational requirements, optimization difficulties connected with training energy-based model and the method used, limited approximation of the true energy). Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative - generative setting. However, there are points I would like the paper to address for better exposition.
1. It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood.
2. On CIFAR100 the accuracy drop from the reference value is larger than for datasets with 10 classes, could it be due the logits dimension is higher and challenges optimization?
3. It would also be helpful to clarify whether application of the proposed method is primarily restricted by the computational complexity or is there any property inherent to energy-based models that makes treating high-dimensional data challenging?
Minor remark
- Although the paper doesn't state on which dataset results shown in Table 1 were obtained, I suspect its CIFAR10, please specify this. | Review: This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold-out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc.
While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). The advantage is an additional degree of freedom in the scale of the logit vector, which is would have been otherwise normalized by the softmax layer and now can now model the data distribution. The downside is the loss in ease of training. Whereas (discriminative) deep networks can be easily trained by gradient descent on a cross-entropy objective, the partition function in the energy model makes this un tractable. This is addressed through sampling, similar to (Welling & Teh, 2011).
One of the biggest achievements reported by the authors is that the performance on discriminative tasks is not hurt (much) by adding the generative model. There is only a 3 point gap between Wide-ResNet and the proposed model (92.9% vs. 95.8%) … but on what dataset? 3 datasets are mentioned in the experimental section, but table 1 does not mention on which datasets the accuracy is reported. My guess is that this is a mean or mixture, since GEM performances of 96.7% and 72.2% are reported for SVHN and CIFAR10, respectively, but this should be made clearer.
On out of distribution detection, could the authors comment on the histograms in table 2, in particular the difference between the new measure (AM JEM) compared to JEM log p(x) on CelebA? The proposed measure does not seem to fare well here.
Although the method does not outperform the gold standard of adversarial training, I found the models robustness to adversarial examples quite appealing, given that it was not trained for this objective (which also means that it does not require an adaptation to a norm).
I was very impressed by Figure 6 showing distal adversarial initialized from random images, showing pretty clear images of the modelled class. The modelled variations require more investigation to verify whether we have a collapse for each class, but the results look very promising.
The paper is well written and easy to understand. A couple of details on the training procedure are missing in the experimental part. It is stated that, both, p(y|x) and the generative part p(x), are optimized, but how are these exactly integrated? Given the difficult in training this model reported in the paper, this seems to be particularly important.
I also appreciated the description of the limitations of the algorithm, and the details in the appendix (ICLR should go back to unlimited paper lengths, btw.).
More information on complexity (training times etc.) should also be helpful.",7.333333333333333
