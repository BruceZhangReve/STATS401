"Papers_Title","Papers_Keywords","Papers_OfficialReview_list1_StrengthandWeakness","Papers_OfficialReview_list1_Rate","Papers_OfficialReview_list1_Confidence"
"On the mapping between Hopfield networks and Restricted Boltzmann Machines","Keywords: Hopfield Networks, Restricted Boltzmann Machines, Statistical Physics","Two knowledgeable reviewers were positive 7 and very positive 10 about this paper, considering it an important contribution that illuminates previously unknown aspects of two classic models, namely RBMs and Hopfield networks. They considered the work very well developed, theoretically interesting and also of potential practical relevance. A third reviewer initially expressed some reservations in regard to the inverse map from RBMs to HNs and the experiments. Following the authors' responses, which the reviewer found detailed and informative, he/she significantly raised his/her score to 7, also emphasizing that he/she hoped to see the paper accepted. With the unanimously positive feedback, I am recommending the paper to be accepted.","Two knowledgeable reviewers were positive 7 and very positive 10 about this paper, considering it an important contribution that illuminates previously unknown aspects of two classic models, namely RBMs and Hopfield networks. They considered the work very well developed, theoretically interesting and also of potential practical relevance. A third reviewer initially expressed some reservations in regard to the inverse map from RBMs to HNs and the experiments. Following the authors' responses, which the reviewer found detailed and informative, he/she significantly raised his/her score to 7, also emphasizing that he/she hoped to see the paper accepted. With the unanimously positive feedback, I am recommending the paper to be accepted.",""
"On the mapping between Hopfield networks and Restricted Boltzmann Machines","Keywords: Hopfield Networks, Restricted Boltzmann Machines, Statistical Physics","The paper demonstrates a mathematical equivalence between Hopfield nets and RBMs, and it shows how this connection can be leveraged for better training of RBMs.
What a great paper - well written, an enlightening mathematical connection between two well-known models that to my knowledge was not previously known. Hopfield nets and RBM's have been around for decades, and I don't think we've been aware of this connection, so it seems like a pretty important finding. The paper explores the utility of this connection by applying to an MNIST task. Interestingly, the connection yields important insights in both directions: stochastic sampling in an RBM is faster than Hopfield due to a smaller matrix and parallel layer wise updates, whereas initializing an RBM with the projection rule from Hopfield allows it to find a better solution faster.
I really enjoyed reading the paper, I learned something new, and I think others will too! It is an important advance in our understanding of Hopfield nets and RBMs.","10: Top 5% of accepted papers, seminal paper","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"On the mapping between Hopfield networks and Restricted Boltzmann Machines","Keywords: Hopfield Networks, Restricted Boltzmann Machines, Statistical Physics","This paper considers a mapping between the well known Hopfield Neural Networks and Restricted Boltzmann Machines. In contrast with previous literature that consider the case where the patterns / data features to memorize were uncorrelated, the authors extend the mapping to arbitrarily correlated patterns, which allows to consider much more realistic settings. The mapping is computationally speaking relatively cheap. This mapping is shown to allow for significantly better initialization (than random) of the weights of a RBM, in the sense that the training is then much faster to reach comparable generative and/or generalization performance. In this sense the mapping is not only interesting from a theoretical point of view, but also practically. This paper should be considered as an applied one, as there is no real analytic theory of why this mapping helps the learning, but the experiments are well carried: the boost in learning is demonstrated through experiments in MNIST data, and the results are well explained and convincing. The appendices are also well written and are a good addition to the main part. Overall the paper is well written (the paper can be used by non-specialists also as introduction to Hopfield NNs and RBMs), the results are interesting and relevant to the ML community, the paper can be read without much effort. Even if RBM are not anymore state-ot-the art generative models, the results are encouraging and might lead to future improvements in more modern architectures. I have no specific concern. The paper is overall very well written. The paper is slightly incremental as similar mappings were known, but it remains a relevant contribution, and the aspect of using this mapping as a way to boost learning in RBM seems new, and interesting. I recommend publication after slight corrections, see below.
Typos and corrections: _text below (1): J=1/N Xi^T Xi^T ->1/N Xi Xi^T _(3): please detail the last equality _(8) is true only for the lambda that verify the fixed point / saddle point equations: please mention it _below (11): the p the columns of -> the p columns of _(12): please explain what is GL_p(R) _""At the other end, 0 ≪ 10k/N < 1, ..."" : Any x > 0 is >> 0, so please be more precise _Above Fig 3: ""appear qualitatively similar"" : this is not obvious...","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"On the mapping between Hopfield networks and Restricted Boltzmann Machines","Keywords: Hopfield Networks, Restricted Boltzmann Machines, Statistical Physics","Summary
This paper shows a relationship between the project rule weights of a Hopfield network (HN) and the interaction weights in a corresponding restricted Boltzmann machine (RBM). The mapping from HN to RBM is facilitated by realising that the partition function of BN can be seen as the partition function of a binary-continuous (Bernoulli-Gaussian) RBM. The authors comments on the mapping from RBM to BN. The experiments show the advantages of training RBM with weights initialised from BN projection weights in generation and classification.
Strong points:
I am not familiar with the literature, but the results seem new to me.
The experiments show advantages of BN initialisation, pointing to new directions of improving RBM training.
The paper is fairly clearly written.
Weak points
The HN -> RBM mapping is quite clear, but the reverse RBM -> HN mapping is not very well established, and there are no experiments showing how effective the approximate reverse mapping works on associative memory tasks typical for HNs. I also believe this lowers the impact of this paper, given that the forward mapping is based on a simple revelation.
The authors' description of the experimental results are not accurate enough. and the results raise several questions to be addressed.
Recommendation
I'm in favour of rejection, but some concerns can be addressed fairly easily (with experiments) so I'm open to raising my score if questions are well-addressed.
Issues and questions to address
The authors should provide experiments on the reverse mapping as suggested above.
I do not agree that figure 3 shows that RBM training ""simply 'fine tunes'"" the weights -- the difference is quite stark. How about increasing the batch size so that there is little SGD noise?
Figure 4a: traces are cut-off just when random initialization is catching up with HN initialization. This also applies to Figure 5.
There are a few descriptions suggesting ""HN init. appears to train much faster than random init"". However, the rate of increase in of likelihood in Figure 4 is shallower for HN than for rand init. Is the advantage only at the 0'th RBM epoch?
The author only compared with purely random initialisation, which is perhaps the most naive baseline. I would suggest comparing to a (slightly) more clever initialisation, perhaps PCA or something better (those mappings in previous work the authors cited and in Appendix B). Or, the authors could also initialise the RBM by first training it on the within-class cluster centres (using a very large number of sleep samples for the sleep-phase) which may also be a more fair comparison?
In the classification objective, if I understand correctly, the feature function is essentially quadratic in the input patterns. Should there be an ideal test error that is computed by a quadratic neural network trained with supervision by backpropagation? If the HN classifier (blue in Figure 5) is approaching this idealization, then this will strengthen the claim.
The discussion on the extension to more generic, deep architectures is not well supported, and I do not see the extension to be so straightforward given the content of the current paper. In generation, supervised labels and clustering are used to simplify learning. Is the network able to learn just on the MNIST digits, even for the real images within a single class (e.g. ""7"")?
Can the authors try to characterise whether the HN initialisation is related to log-likelihood training? I wonder if there is any interesting theory; otherwise, measuring model performance by log-likelihood seems a bit arbitrary (though it makes the comparison to contrastive divergence easier).
Detailed suggestions (not to affect decision)
Reference to RBMs should include more historic ones from Hinton (e.g. 2006)
I do not see the purpose of (7) and (8), and they are only referred to in the Appendix (the review content in the Appendix is informative by itself though).
Eqn (11), should it be
wμwμT
in the sum?
Third line above (16), should
H
and
Z
be indexed by
μ
?
Line above (D.4),
WWT=…BpT
?
==== update ====
I thank the authors for providing such detailed response. All my concerns are addressed and reflected in the revision (though some are much better done than the rest). I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score. I hope to see this paper accepted.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","I haven't read through your paper in detail yet - it looks very interesting! - but our NeurIPS paper https://arxiv.org/abs/2004.03658 is quite related - we also compared to Query2Box, and we also were able to get good performance by training only on simple queries.","",""
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","I am a researcher that is interested in the area of complex query answering. However, the descriptions of the experiments are so vague that it is hard to reproduce the experiment according to the paper only. So I have to ask some questions here about the experimental settings and evaluation details.
Generally, the paper said only the atomic queries are used to train the model. What are atomic queries? Are they 1-projection queries? If the model is only trained on 1-projection queries, it is basically training a link predictor. Then why you describe two optimization methods for complex queries? This is really confusing. Or actually, the model was trained on multiple complex query types?
What is your inference method for your model? It seems that your model can be optimized in two ways as mentioned in the paper. But during the inference time, only the beam search method can be used to do inference. My question is whether both CQD-CO and CQD-Beam models are evaluated by the Beam search method.
Why not just use a pre-trained link predictor? It seems that a pre-trained link predictor can be also used to do inference by using beam search. Also, I think this might be the most appropriate baseline to evaluate the effectiveness of both optimization methods.
Although I am really confused by some descriptions in the paper, I really think this is a great paper. Because it points out an important new direction of solving complex query answering.
Will you release the code for the experiments?","",""
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","The reviewers unanimously agree that this paper is a strong accept; it makes important progress in developing our ability to query relational embedding models.","The reviewers unanimously agree that this paper is a strong accept; it makes important progress in developing our ability to query relational embedding models.",""
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","Summary: This paper proposes Continuous Query Decomposition (CQD) a novel method for evaluating complex queries over incomplete KGs. Each variable of a logical query (involving existential quantifiers, conjunctions and disjunctions) is mapped to an embedding. A link predictor, trained on single edge prediction, is used to score the atomic query involving the variable. The full query is evaluated using continuous versions of the logical operators and gradient-based or combinatorial optimization. Evaluating complex logical queries on (necessarily incomplete) KGs and other graph-structured data is an important problem for data mining purposes. The paper proposes an elegant and effective method.
Strong points Elegant, efficient solution. SOTA results. Provides aspects of explainability, although this could be discussed and illustrated better.
Detailed comments
What is particularly important/challenging about EPFO queries, beyond existential and conjunctive ones? Obviously it is an extension that covers more FOL, but a qualitative discussion would help the reader, particularly with respect to applications to KGs.
Could you talk more, give more insights about the 8 complex queries types? Why are they important?
The query “What international organisations contain the country of nationality of Thomas Aquinas?” sounds really artificial. Maybe there is a better example involving entities and relations, similar to the drugs one?
Could you say a bit more with respect to how the KG incompleteness is accounted for in the evaluation?
The paper mentions “.. in many complex domains, an open challenge is developing techniques for answering complex queries involving multiple and potentially unobserved edges, entities, and variables, rather than just single edges.” It would be great to articulate this more for sake of providing context and motivation.","9: Top 15% of accepted papers, strong accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","The paper aims to answer complex queries on knowledge graphs. Different from previous methods that aim to embed the queries, the method views the query answering problem as an optimization / search problem where the goal is to find the most plausible entities on the reasoning path. The merits are that the method only needs to train on 1 hop path queries (link prediction), saving the effort of training on complex queries as in previous work, and proposes two solutions, which both achieve nice results on standard multi-hop reasoning benchmarks. It also demonstrates interpretability of the model by showing some examples of the intermediate entities found in the reasoning path when answering a complex multi-hop query.
I think the paper is clear and easy to follow. I have some questions regarding the two methods. For the first method, continuous optimization in sec 3.1, what is the difference between this method and the previous works GQE, Q2B, etc. apart from different neural link predictors? Especially for path queries, e.g., given a two hop query,
(Obama,BornIn,V1)∧(V1,CapitalOf,V2)
, then the optimal
eV1
will be
eObama+eBornIn
, because the distance will be 0, and
ϕp
will be 1 (here it assumes TransE model, and of course it can be generalized to DisMult, ComplEx, etc.). Then the first formulation is in essence very much similar to GQE, because GQE/Q2B also models
eV1
in the exact same way and the difference only lies in (1) you use ComplEx (2) t-norm modeling of conjunction? However, it seems that t-norm demonstrates less expressiveness for modeling conjunction because both GQE/Q2B models conjunction using a MLP with additional learnable parameters, which can also approximate t-norm and even be more adaptive depending on the training queries/KG. For the second method, the time complexity seems exponential with respect to the number of hops. For a m hop query, and each step you keep the top-k, then do you end up with
km
entities?
Additional questions:
How did you calibrate the output of ComplEx so that
ϕp(es,eo)
is in [0,1]? Better to add more details on neural link predictors.
Some ablation studies that use different t-norm and t-conorm other than the Godel and product may make the argument stronger.
There exists a tradeoff between the inference time and training queries. For GQE/Q2B, they can leverage complex queries to train the conjunction operator (MLP), so that during inference, there is no need to do any optimization. But for the proposed method, it saves the effort of training on complex queries, however, during inference, the method needs an online optimization process to instantiate the variables on the path. Especially for CQD-CO, the authors mention that they need to optimize online for 1000 iterations, which is too expensive for answering a query. Can you list the inference time of both models (continuous, combinatorial) and compare it with GQE/Query2box?
For 1p performance, it is equivalent to the performance of ComplEx on link prediction right?
The table 3 is confusing, why are the numbers (e.g., 5.5, 46.76) larger than 1, I think the model normalized the output of
ϕ
to [0,1]?
The proposed two optimization methods are independent of neural link predictors. Can you use the same neural link predictor for your models and GQE for fair comparison? You can train a TransE model for the neural link predictor, and accordingly define
ϕp
, then it will be clear to show whether the gain comes from a different neural link predictor (TransE vs ComplEx), or comes from the t-norm and the two optimization methods. And of course another choice is the other way around, e.g., use the ComplEx version of GQE and make the same comparison.
Minor points to fix:
In the method section, bold
e
denotes vector embedding while normal
e
denotes a logic formula, which is subtle and confusing. Authors can change the notation of one of them.
Also, the notation
eij
is abused, in Eq. 2, it represents a logic formula, however, in Eq. 3, it represents the output of
ϕp
, which is a scalar.","6: Marginally above acceptance threshold","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","The proposed two optimization methods are independent of neural link predictors. Can you use the same neural link predictor for your models and GQE for fair comparison?
We just updated our submission with additional experiments on adopting DistMult rather than ComplEx as the underlying neural link prediction model - results are available in the appendix (Sect. C). We find that results with DistMult are slightly less accurate than with Complex, as we expected, while still more accurate than the GQE and Q2B baselines.
A bilinear interaction model similar to DistMult was also considered as a projection operator by GQE, with significantly less accurate results. We believe that our improvements in terms of accuracy can be attributed not just to the use of a competitive neural link predictor, but also to the compositional nature of our model, which allows it to generalise from atomic to complex queries thanks to the use of t-norms and t-conorms.","",""
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","The paper attempt to answer conjunctive queries that are in the form of a chain of facts bound together with unobserved variables. The authors suggest that you can use any relational learning method to embed entities and relations in a k-dimensional space and then use the t-norm in order to create a loss function that will be used in order to find the result of the query. The paper investigates continuous optimization through stochastic gradient descent and a greedy method for combinatorial optimization. The results demonstrate that the greedy optimization method performs better. In addition, they claim that their method outperforms other methods with the advantage of using less training data.
Here are some comments
I think the authors cover the relevant work sufficiently
The idea is very simple and builds upon other work that is well studied and well understood by the community
I think that there is an excessive mathematical formalism that is a bit unnecessary. There is no reason for that, the fact that the idea is very simple does not mean that we have to add extra formalism.
In terms of generalization, I think it is very interesting that the users train only on 1-hop queries and evaluate up to 5-hop. In the introduction, the authors claim they use less data than the other methods, but they don’t make it very clear in the experimental section. I think they need to be more explicit about that. They need to clarify that less data means just the 1-hop queries
I have two reservations about the paper. I suspect that part of the success of their approach is the ComplEx embeddings. I would appreciate an ablation study with at least one more method for relational learning, let’s say TransE to see how sensitive it is on the embeddings. To be fair the authors study in depth the performance of their algorithm in other variations, such as the length of the chain
The other concern is about timing results. It would help to know how the whole algorithm compares in terms of time to answer the query compared to the others. I think it is of particular interest the difference between the two optimization techniques. I suspect that the greedy one might be two slow for longer chains. From the preliminary analysis, it seems to grow as k^d where k is the width of the beam per relation and d is the length of the chain.
In general, I think it is a very practical paper","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","The paper proposes Continuous Query Decomposition (CQD), an approach for answering Existential Positive First-Order (EPFO) queries over incomplete knowledge graphs exploiting a neural link predictor for 1-hop-only queries. Entities are embedded in a low dimensional space and entity vectors are used to compute the score of query atoms that are then combined using a t-norm for conjunction and t-conorm for disjunction. Answers to queries are found either with continuous optimisation by gradient descent to find embeddings for query variables or combinatorial optimisation where top-k entities for query variables are looked for yielding a beam search. CQD is compared with Graph Query Embedding (GQE) and Query2Box over three datasets on a large number of queries. The result show that CQD outperforms the baselines on Hit@3 on average. CQD also offers the possibility of explaining the results of queries by showing the top scoring entities for query variables and the score of atoms.
CQD tackles the difficult problem of answering queries that are beyond simple 1-hop completion queries. It improves over previous work which need to train the model over a large number of queries (Hamilton et al., 2018;Daza & Cochez, 2020; Ren et al., 2020) and do not consider disjunctive queries (Hamilton et al., 2018; Daza & Cochez, 2020). These advantages are obtained by not embedding the query into a low dimensional space but using continuous or combinatorial optimization to answer queries, considering the query as a formula in fuzzy logic and applying t-norms and t-conorms. While the use of fuzzy logic in query answering is not new, they way in which it is combined with entity embeddings and neural link predictors is original to the best of my knowledge.
The fact that queries are not embedded (and so learning does not need large numbers of queries) is a strong point of CQD, with competing methods (Hamilton et al., 2018;Daza & Cochez, 2020; Ren et al., 2020) requiring many queries for tuning the query embeddings. Since queries are not embedded, the results of CQD are also easier to explain.
The experiments are sufficiently extensive to support the claim of the paper that CQD is also outperforming competitors in terms of the quality of solutions. However, the authors should justify why they used embedding size 500 for their methods and 400 for the baselines.
From a technical point of view the article seems sound but the authors say that ""Then, after we identified the optimal representation for variables
A,V1,…Vm
, we replace the query target embedding
eA
with the embedding representations
ec∈Rk
of all entities
c∈E
, and use the resulting complex query score to compute the likelihood that such entities answer the query."" In this way the authors throw away vector
eA
that may have information about the problems, isn't there a method to exploit the information in
eA
?
I have a few remarks about the presentation: Citation Raedt, 2008 should be De Raedt, 2008. In Figure 1 the edges of the graphs have the opposite direction with respect to the caption and main text. Page 6: ""we only make use of type 1-chain queries to train the neural link predictor"": do the authors mean 1-hop queries? 1-chain appears here for the first time. ""In all cases, we use a rank of 500."": for rank do the authors mean the embedding size? This should be clarified Page 7: ""Since a query can have multiple answers, we implement a filtered setting, whereby for a given answer, we filter out other correct answers from the ranking before computing H@3."": this sentence is not clear. Does it mean that answers that follow from the KG without completion are removed from the ranking?
----After reading the other reviews and the authors' comments, I still think the paper is excellent and should be accepted.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Complex Query Answering with Neural Link Predictors","Keywords: neural link prediction, complex query answering","Your comment cleared my doubts","",""
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","This paper proposes a conditional language-specific routing (CLSR) mechanism for multilingual NMT, which also considers the trade-off between language specificity and generality.
All of the reviewers think this paper is interesting for both idea and empirical findings. Therefore, it is a clear acceptance.","This paper proposes a conditional language-specific routing (CLSR) mechanism for multilingual NMT, which also considers the trade-off between language specificity and generality.
All of the reviewers think this paper is interesting for both idea and empirical findings. Therefore, it is a clear acceptance.",""
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","Dear Reviewers:
Thanks for your insightful reviews! Now the discussion stage is open and the authors have posted their responses. We will appreciate that the following things-to-do can be done by Tues, Nov 24.
1 Acknowledge explicitly that you have read the responses.
2 Modify your review if necessary.
3 Communicate with the authors/reviewers/AC by adding/responding to the comments if necessary.
Thanks a lot!","",""
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","In this work, the authors present a conditional language-specific routing (CLSR) scheme for transformer-based multilingual NMT systems. They introduce a CLSR layer after every transformer encoder and decoder layer; each such layer is made up of hard gating functions conditioned on token representations that will either select a language-specific projection layer or a shared projection layer. Further, a budget is imposed on the language-specific capacity measured by aggregating the number of gates that allow for language-specific computations; this budget constraint forces the network to identify the sub-layers that will benefit most from being language-specific.
This is nice work. The proposed technique has been described clearly, the idea is intuitive and the experiments are pretty compelling. I have a couple of minor comments/suggestions for the authors.
The authors show heat-maps of LSScore distribution in Figure 6 (Appendix B) which suggest that the LS capacity schedule might have little to do with linguistic characteristics. However, this might have to do with the multilingual model being trained on as many as 94 different languages. It seems plausible that linguistic similarities might govern LS capacity scheduling when there are fewer training languages to learn from. To check for this, it might be interesting to redo this experiment with the medium resource and low resource buckets containing 26-28 languages each.
There are two (among many other) interesting things that stand out from the results in Tables 1 and 2. (1) From Table 1, the only setting where CLSR* (as well as ""Top-Bottom"" and ""Dedicated"") underperforms compared to the baseline is M2O for low-resource languages. It seems like the use of language-specific layers here has a strong adverse effect on performance (-4.56 with CLSR-L) which is largely offset by CLSR*. Some more insights based on the individual BLEU scores for each test language in the ""Low"" bin and whether there were certain languages that were largely responsible for the drop in performance would be interesting to the reader. (2) From M2O in Table 2, the win ratios of Top-Bottom are much lower when compared with Dedicated and CLSR* (61.54 vs. 84.62 vs. 84.62; 30.77 vs. 84.62 vs. 100). Could the authors share their thoughts on why this drop might be appearing?","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","The work proposes a hybrid architecture that has: (1) language-specific (LS) components; (2) as well as the components that are shared across all the languages -- a trade-off between specificity and generality. A key conclusion of the work is that the best architectures typically are. the ones that have ~10-30% language-specific capacity.
In terms of experimental work, the work uses WMT-14 and OPUS-100 datasets to show the proposed trade-off.
In terms of exposition of the ideas, it's a well-written paper for the most part.
One issue that the authors could improve on is clarifying how ""the amount of LS computation"" is measured. You have mentioned it several times in the abstract/intro and it's neither clear nor referenced (it could be the number of parameters, it could be the number of basic computations, etc). For a new reader, it takes quite a while to find that
p
is defined in eq. 6 and defined as a budget contains.
One other quibble is that all the trade-off figures are shown based BLEU/automatic metrics, which are known to be inaccurate. It would be nice to repeat one of the included evaluation with human judgments.
Overall, I view this as a good contribution to pave the way towards stronger, but reasonably-sized multilingual models. This is partially assuming that the authors will stay true to their promise that ""Source code and models will be released.""","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","Manual parameter sharing schemes are generally costly to come up with and when they are obtained for certain language pairs they do not necessarily generalize well to arbitrary language pairs in multilingual NMT. The idea of learning which parameters to share across languages in multilingual transformer models is original and potentially useful for designing and analyzing multilingual models in the context of NMT.
Strengths:
The paper is well-written and easy to follow. The idea was (reasonably) well-positioned with respect to prior work and clearly presented.
The technical merit is essentially in coming up with the budget constraint term in the loss function that forces the multilingual encoder-decoder ""super-network"" to use the desired percentage of language-specific computation using gating.
A significant part of the contribution was in the analysis of the results, obtained by this learning-based parameter sharing approach, which was quite informative and revealed some interesting insights about where and when a language-specific computation is required. The takeaways should be of interest to researchers and practitioners interested in designing and analyzing multilingual NMT systems.
Weaknesses:
(1) Even though it is the first time such a method is applied in the context of NMT, the idea is not as much novel in the broader context of deep learning. Prior work has explored ""learning-to-share"" strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions.
(2) Another weakness is that the comparison with the vanilla and LS baselines does not seem to be properly controlled in terms of parameters. I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively. How important is this difference?
(3) In the experiment about linguistic similarity, it appears that the capacity schedule is the same across languages and the authors conclude from this that the schedule has little to do with linguistic characteristics. However, the main driving force in the choice of the language-specific computation is currently a single hyper-parameter p which is the same across languages; so, this will lead to choices that are good on average for all language pairs involved for a given universal budget. Do you think the conclusion would be still the same if a language-specific hyper-parameter p_l was used instead?","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","Thanks for taking the time to answer my questions.
(1) Yes, these are the references I was talking about.
(2) The evidence presented in the paper is not sufficient to support that parameters do not play a role. The CLSR-L is only one out of the four baselines and it still has 9M and 10M fewer parameters than the CLSR* method for OPUS-100 and WMT14 datasets respectively. This is a somewhat stretched notion of ""roughly the same amount of parameters"". My recommendation for this issue would be to tone down the claims regarding improvements or simply compare against baselines that have a comparable number of parameters (e.g. within ~0.5-1M range).
(3) Correct me if I am wrong but, apart from budget p, the gate parameters in Eq. 5 are also the same across languages and layers which creates an inductive bias towards gating z^l across languages in a unified way. It is also evident from all subfigures in Figure 6 that the gates function in (almost) the exact same way consistently across languages which seems like a hard constraint to achieve without enforcing it explicitly. One more simple explanation of the observed behavior in Fig. 6 is that the parameters W_1, b, and w_2 do not provide enough flexibility for controlling the gate in different ways across languages and layers. Simply put, there is no language-specific behavior because there is no language-specific gating taking place. In other words, the capacity schedule has little to do with linguistic characteristics because it is designed to do so.","",""
"Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation","Keywords: language-specific modeling, conditional computation, multilingual translation, multilingual transformer","In this paper, the authors present a study of different aspects of language-specific model capacity for massively multilingual machine translation. To this end, language-specific behaviour is achieved via a combination of conditional computation to decide whether to use language-specific parameters or not and statically assigning experts for each languages. The language specific sub-layers are incorporated throughout the network. The training objective allow budgetary constraints on the amount of language-specific parameters. The paper does a systematic analysis on the role of language specific parameters using the proposed architecture. Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally. The study sheds light on the amount of language specific parameter sharing, their distribution in the network, impact of language, etc.
I find the analysis presented in the paper very interesting and insightful - and distinguishes it from previous work in this area. The hypothesis are clearly stated and the experiments are well designed. The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT.
In terms of modelling, the work follows in the line of recent work on language-specific parameters for multilingual NMT. The deviation from existing work is mixing elements of conditional computation with language specific computation. I see this work more as an analysis on language-specific parameters for a particular LS-model rather than a novel architecture. It is not clear how this model would compare to other models using language specific parameters (sparsely gated mixture of experts (Lepikhin et al 2020), light-weight adapters (Bapna et al 2019) ).
Questions:
Previous work has tried to combine both language-specific and shared parameters (Wang et al 2018), rather than making a binary choice between these. Did the authors compare with such an approach?
Since a major part of the model contains shared parameters, was there a need for new set of shared parameters along with the language-specific parameters. The gating decision could have been to bypass the language-specific sublayer or not.
References Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong. Three strategies to improve one-to-many multilingual translation. EMNLP. 2018.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end-to-end fashion. The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder. A number of techniques including adversarial training and soft DTW are applied to improve the training. The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors. After the rebuttal and discussion, all reviewers are supportive on accepting the paper.","This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end-to-end fashion. The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder. A number of techniques including adversarial training and soft DTW are applied to improve the training. The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors. After the rebuttal and discussion, all reviewers are supportive on accepting the paper.",""
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","This paper presents an approach for end-to-end speech synthesis, where every step is learned jointly with the others. Specifically, the proposed model takes a character sequence as input and outputs an audio signal directly. The model is trained using an combination of losses, including an adversarial loss. In the experimental section, the proposed approach is compared against strong baselines, and ablation studies are presented.
Pros:
The proposed approach is novel and a significant step toward competitive end-to-end models.
The related work is thorough as far as I can tell.
The experiments are insightful, showing the impact of each part of the system.
Cons:
The performance are promising, but still below the baselines.
The end-to-end claim is a bit misleading as the character-based model is not performing well, and the phoneme-based model is not really end-to-end, as the g2p part is not trained jointly.
The paper is sometime not easy to follow.
Detailed comments:
The reason behind using an adversarial loss is not really explained in the paper. A few lines before section 2.1 would help clarify that.
The order of the sub-sections (2.1-2.7) in Section 2 is not intuitive and seems a bit random, making the section slightly hard to follow.
Section 2.7: ""inconsistent spelling rules of the English language"" -> It's not about inconsistent rules: every language has inconsistent rules, several dialects and variations in the pronunciation of words. Please rephrase.
It's not clear which dataset was used in the experiment. If it is a private dataset, please state it clearly.
Overall, despite the paper's two main weaknesses (not fully end-to-end and lower performance), I think it is a significant step towards fully end-to-end model and should be accepted.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","This paper proposes a fully end-to-end TTS system with adversarial training. The proposed method has three main advantages: 1) a simple, end-to-end pipeline with only two submodules, with a fully differentiable and efficient architecture; 2) a flexible dynamic time warping to compute the loss between the predicted (generated) and true spectrograms; 3) a good MOS performance compared with the other state-of-the-art systems with more supervision. The evaluation is done very thoroughly with a variety of ablation studies.
Overall, I vote for accepting the paper. First of all, the paper is very well organized and easy to follow. Related works are well summarized, while focusing on the main differences on the proposed method. Method is explained in great detail with easy-to-follow descriptions, proper equations, and very appropriate figures. Solid evaluation is performed, and experimental results and speech samples are convincing.
Pros:
The structure of the paper allows an easy read.
The main contributions are clearly stated and supported by the experiments.
Major works on the similar topic are widely covered and referenced.
Evaluation is thorough enough to support the arguments with in-depth ablation studies.
Appendices provide useful, supplementary information.
Cons:
No comparison over the computational cost nor model size is presented. It is of particular interest because the proposed model is non-autoregressive, and thus may be capable of a causal, real-time inference.
No use of widely accepted benchmark datasets. More direct comparison would be of interest.
Minor comments/questions:
If I understood correctly, the training sample has no phoneme-level alignment. Instead, only a sentence-speech pair is provided. If so, how do you select the corresponding text snippet from a sentence when you randomly sample 2 seconds of audio from the training examples whose length varies from 1 to 20 seconds?
Section 2.5 on DTW is rather lengthy. DTW is a quite well-known algorithm for alignment between the two sequences, the detailed explanation on the algorithm may be omitted without the loss of readability, in my opinion.
In Section 2.1, T is used to denote the total number of output times steps of the aligner, while T in Section 2.4 denotes the number of mel-frequency frames. Are these T's identical?
The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): very small improvement from 3.551 to 3.559 MOS. Can you provide more explanations?","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","We wanted to let you know we've just updated the manuscript again to update our TPU benchmark and add a GPU benchmark. When improving our benchmarking setup we found there were large IO bottlenecks that made our original TPU benchmark artificially slow, and resolving those bottlenecks improved the measured speed by a factor of more than 5.
In our updated benchmarks, with TPU v3 the batched inference speed is 157x realtime per chip (79x realtime per core); with a single V100 GPU, batched inference runs at 207x realtime. Please see the updated Appendix A and newly added Table 3 for additional details.","",""
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","This paper proposes a novel TTS system that 1) relies on almost no intermediate representation; and 2) is entirely feedforward instead of autoregressive. There are two major strengths in the paper. First, several modules in the proposed system are novel and smart, including the aligner and the dynamic programming loss, and it is, to my knowledge, the first feedforward text-to-speech system that does not rely on the intermediate representation. Second, the experiment result is convincing. There are, however, some room for improvement and questions for the author to clarify.
First, the elegance in the architecture is overshadowed by the complicated training algorithm. The training loss is like the superposition of common loss terms in the speech synthesis community, making the method look a bit heuristic. The complicated training algorithm also makes the proposed method harder to reproduce. It would be helpful if the authors can provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.
Second, notice that EATS performs slightly worse than GAN-TTS, which does not quite show the benefit of end-to-end training (unlike ClariNet). I understand that there are a lot of challenges in training EATS, and the authors have briefly discussed this in Section 5. However, it is worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end-to-end training.
Finally, although end-to-end comes with the (potential) merits of improved data efficiency and improved quality, it also has its downsides. Without a clearly interpretable hidden representation, it is harder to have direct control over prosody. How would prosody control be possible under the end-to-end framework?
Despite the weaknesses, this paper makes sufficiently novel contributions in TTS, making it above the acceptance threshold. I would look forward to further justifications of the EATS paradigm.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","Summary
The authors propose EATS, a method for TTS from unaligned audio and text data, directly to the waveform. Previous work either use aligned phonetic features, or output spectrograms that are later converted to a waveform by a deep vocoder model.
In order to achieve this, the authors had to use several tricks, some already existing, for instance taken from the GAN TTS architecture, and some novel. The three key novelties are:
differentiable monotonic attention with gaussian kernel and length prediction.
dynamic time warping for the spectrogram loss.
using both spectrogram and waveform domain discriminators
The authors provide a comprehensive ablation study with MOS score, although their model is under the state of the art by a significant margin.
Review
This paper builds on GAN TTS, and tries to make it trainable end-to-end without aligned features.
The two main contributions, namely dynamic time warping and monotonic attention with gaussian kernel are both elegant, and can likely be used for many other applications related to time series with heterogeneous time scales. In particular, the time warping loss allows to accomodate both for the natural irregularities in spoken speech, as well as providing sufficient signal for the monotonic attention to work.
The rest of the architecture is very similar to GAN TTS except for the spectrogram domain discriminator that was added.
While the model is under the state of the art for TTS, the samples are already quite convincing. The authors conduct a thorough ablation study, both with MOS and audio samples.
Overall I think this is a really good paper, that is likely to prove quite useful for the development of end to end speech synthesis solution. As I already mentioned, I also believe that the approach of using dynamic time warping and monotonic attention can be used for other kind of time series.
Remarks and questions to the authors
Table 1, MOS for Tacotron 2 would be very informative. All the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data. The point of the authors is that their methods is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, I'm not entirely sold on the simplicity argument. The tacotron 2 paper reports a MOS of 4.5 but on a private dataset.
Section 3, [1] used the same simple L1 + log spectrogram loss as used here.
I was surprised by the bad performance of the transformer attention, in particular in the audio samples, the output for this model is garbage towards the end of the signal. Any clue on why this would happen?
It would be interesting to have a benchmark, in particular, can the model generate speech in real time on GPU and on CPU?
[1] SING: Symbol-to-Instrument Neural Generator, Defossez et al. Neurips 2018.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","We wanted to let you know we've just updated the manuscript again to update our TPU benchmark and add a GPU benchmark. When improving our benchmarking setup we found there were large IO bottlenecks that made our original TPU benchmark artificially slow, and resolving those bottlenecks improved the measured speed by a factor of more than 5.
In our updated benchmarks, with TPU v3 the batched inference speed is 157x realtime per chip (79x realtime per core); with a single V100 GPU, batched inference runs at 207x realtime. Please see the updated Appendix A and newly added Table 3 for additional details.","",""
"End-to-end Adversarial Text-to-Speech","Keywords: text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model","Thanks for the interest! We just ran a CPU benchmark; we get 8.5x realtime inference on a 6 core Intel Xeon E5-1650 v4. We've updated Appendix A again. (It's potentially an imperfect benchmark as it's running on a desktop machine sharing the CPU with other background processes etc., but we're not easily able to better isolate this.)","",""
"Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity","Keywords: Data Augmentation, Deep Learning, Supervised Learning, Discrete Optimization","This paper proposes a type of Mixup-style data augmentation that works at the batch level rather than simply between pairs of examples. Each generated example accumulates salient images from potentially many other examples while ensuring diversity across the generated examples. This is achieved through a 4-part objective with submodular and supermodular components. The paper demonstrates the method using extensive experiments, including generalization performance on CIFAR-100, Tiny ImageNet, ImageNet and GoogleCommands. It also explores weakly supervised object localization, expected calibration error, and robustness to random replacement and Gaussian noise.
Reviewer 1 thought the approach was interesting but raised some concerns with clarity, thoroughness of experiments and whether the approach was computationally prohibitive to be used in practice. I was surprised myself that a discussion on the trade-off between computational expense and accuracy gain was not discussed in the submission. The authors responded to the review, adding a comparison to the BP algorithm (Narshiman and Bilmes 2005). The empirical result seems to back up the claim that the proposed algorithm finds a better solution and with less variance. It also appears to run much faster. The authors also responded to minor issues raised with respect to clarity and organization. In their response, the authors provided considerable detail with respect to running time and time complexity, and show that models trained with co-mixup are practical, though they do come with a significant added cost. The authors added the requested comparisons to non-mixup baselines and enhanced the ablation study. In my opinion, this is a comprehensive and satisfying response, and the paper has improved in many respects since submission.
The review from R2 was largely positive, though limited in its scope. They also expressed concerns with training time (addressed in the response to R1). Clearly the approach extends to an arbitrary (m) number of images; this was explicit in the paper/formulation and clarified by the authors. I have some concern that R2 may have skimmed the paper if they missed this point.
Reviewer 4 thought the paper was interesting and asked several clarifying questions. They expressed concern with the significance of the reported gains. Similar to R1, they asked about non-mixup baselines (VAT specifically). This was addressed in the response to R1. The authors responded to the clarifying questions and addressed the issue of significance.
Like the reviewers, I think that this is an intriguing, fresh, and elegant way to perform data augmentation. I appreciate that it has been evaluated just not from the pure generalization setting, but from other angles like robustness and calibration. There are still some outstanding concerns regarding the computational effort required to use Co-Mixup, so this would be nice to see in follow-up work.","This paper proposes a type of Mixup-style data augmentation that works at the batch level rather than simply between pairs of examples. Each generated example accumulates salient images from potentially many other examples while ensuring diversity across the generated examples. This is achieved through a 4-part objective with submodular and supermodular components. The paper demonstrates the method using extensive experiments, including generalization performance on CIFAR-100, Tiny ImageNet, ImageNet and GoogleCommands. It also explores weakly supervised object localization, expected calibration error, and robustness to random replacement and Gaussian noise.
Reviewer 1 thought the approach was interesting but raised some concerns with clarity, thoroughness of experiments and whether the approach was computationally prohibitive to be used in practice. I was surprised myself that a discussion on the trade-off between computational expense and accuracy gain was not discussed in the submission. The authors responded to the review, adding a comparison to the BP algorithm (Narshiman and Bilmes 2005). The empirical result seems to back up the claim that the proposed algorithm finds a better solution and with less variance. It also appears to run much faster. The authors also responded to minor issues raised with respect to clarity and organization. In their response, the authors provided considerable detail with respect to running time and time complexity, and show that models trained with co-mixup are practical, though they do come with a significant added cost. The authors added the requested comparisons to non-mixup baselines and enhanced the ablation study. In my opinion, this is a comprehensive and satisfying response, and the paper has improved in many respects since submission.
The review from R2 was largely positive, though limited in its scope. They also expressed concerns with training time (addressed in the response to R1). Clearly the approach extends to an arbitrary (m) number of images; this was explicit in the paper/formulation and clarified by the authors. I have some concern that R2 may have skimmed the paper if they missed this point.
Reviewer 4 thought the paper was interesting and asked several clarifying questions. They expressed concern with the significance of the reported gains. Similar to R1, they asked about non-mixup baselines (VAT specifically). This was addressed in the response to R1. The authors responded to the clarifying questions and addressed the issue of significance.
Like the reviewers, I think that this is an intriguing, fresh, and elegant way to perform data augmentation. I appreciate that it has been evaluated just not from the pure generalization setting, but from other angles like robustness and calibration. There are still some outstanding concerns regarding the computational effort required to use Co-Mixup, so this would be nice to see in follow-up work.",""
"Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity","Keywords: Data Augmentation, Deep Learning, Supervised Learning, Discrete Optimization","Thank you for your time and the effort spent providing feedback. We appreciate the encouraging comments [R1] “The paper presents interesting ideas with impressive accuracy results”. [R2] “The authors proposed the co-mixup technique which is novel”. “approach was formulated well”. [R4] “The optimization problem formulated seems to be very reasonable”, “promising empirical performance is reported on several tasks”, “this is an interesting paper”.
We address your comments and questions in the replies, and revised our submission.
Best,
Co-Mixup authors","",""
"Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity","Keywords: Data Augmentation, Deep Learning, Supervised Learning, Discrete Optimization","This paper proposes a new batch mixup method, co-mixup, to improve the networks’ generalization performance and robustness. It formulates the construction of a batch of mixup data by maximizing the data saliency measure of each individual mixup data and the supermodular diversity among the constructed mixup data. An iterative submodular minimization algorithm is used to solve the proposed problem through approximation. Promising empirical performance is reported on several tasks.
All previous mixup methods are limited to mixup example generation between a pair of input examples. This paper can be viewed as an extension of the Puzzle mixup, and it generates a mixup example over multiple input examples. The optimization problem formulated seems to be very reasonable by maximizing the saliency and diversity of the mixup data. The proposed method also demonstrates slightly better performance than other mixup methods. Overall, this is an interesting paper.
There are however several issues that are not clear. The authors can clarify the following questions:
How are the labels of the mixup examples determined? Will each generated example become a multi-label example? How to perform training with the generated data?
As the proposed method requires additional process to produce information such as the saliency information and compatibility information (the matrix A), won’t this induce additional computational cost?
As stated in the paper, the A_c matrix measures the distances between locations of salient objects in the input examples. Does this require object localization? Or it simply computes the distances between the feature locations?
How much does the \omega value (to compute A) affect the performance of the proposed approach?
In Section 4.2, two criteria are described and then an approximation is proposed based on the two criteria. Although there is nothing wrong about the criteria, it is unclear how good can an approximation be by simply satisfying these two criteria? That is, what is the quality of the approximation to the original problem? Can the approximation guarantee an optimal solution to the original problem?
In the experiments, although the proposed approach demonstrates promising results, the differences between the proposed method and the comparison method, Puzzle Mix, are very small. The paper claims Co-Mixup significantly outperforms all other baselines. How significant are the differences in Table 1?
The paper limits the comparison to mixup methods. How is the performance level of the co-mixup by comparing with other regularization based methods? For example, VAT regularizaiton based methods.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity","Keywords: Data Augmentation, Deep Learning, Supervised Learning, Discrete Optimization","Summary
Mixup is one of the representative data augmentation techniques to improve the generalization of the network.
The authors proposed the co-mixup technique which is novel.
Especially, the technique can mix several images with z, and the z is found by optimizing their objective function.
It is novel, and the experimental results are convincing.
Strong points
co-mixup approach was formulated well
clearly outperforms the other mixup-like technique such as CutMix and Puzzle Mix
Weak points
Training is slower than others, even if computing z is fast.
Can we mix more than three images? How about 1000 images?
This paper is a good one, and I look forward to acceptance.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity","Keywords: Data Augmentation, Deep Learning, Supervised Learning, Discrete Optimization","This paper proposes a new mixup method that encourages diversity among the samples mixed from a minibatch of data in addition to saliency of each mixed sample. The authors formulate two objectives: 1. a BP set function (submodular + supermodular), and 2. a submodular relaxation obtained by modularizing the supermodular component. Then they solve this problem approximately with coordinate descent by modularizing wrt the update coordinate at each step. This approach outperforms mixup baselines on image classification and several other tasks (calibration, object localization, and robustness).
The paper presents interesting ideas with impressive accuracy results. My biggest concerns with this work are clarity, thoroughness of experiments, and whether it is too computationally expensive to be used in practice.
Significance:
The proposed algorithm's running time may be prohibitive for some applications. In the appendix the authors mention partitioning each minibatch and running the algorithm on each partition to make running time feasible, which suggests that accuracy improves at the expense of running time. Section 4.2 presents the algorithm as having linear running time, the exponential dependence on the number of labels
|L|
should be mentioned here
Experiments:
The results sections compare against good baselines across several tasks, but this would be stronger if it compared to non-mixup baselines.
A more thorough ablation study would analyze each term in the proposed objective function, compare to the mixup baselines applied to m>2 inputs, and compare to the original set modularization method.
The paper proposes an alternative heuristic to the set modularization method of Narshiman and Bilmes 2005. The claim that Algorithm 1 is faster and produces better solutions would be stronger with a thorough empirical comparison to set modularization.
Clarity:
Saliency is described at a high level several times, but it's unclear how these values are actually computed in experiments
Notation for
z
is somewhat difficult to parse in the statements/proofs of Propositions 1 and 2.
f(z)
is pairwise supermodular when considering the same column index across 2 different matrices of output coefficients
(zj1,k,zj2,k)
(Proposition 1), but
f(z)
is modular when considering the full output label matrix
zj
(Proposition 2). What does this mean when the optimization is over all matrices
z
? Do the ground set and constraints change in each case?
typo: In section 4.2, ""proposition 1"" should be ""Proposition 2""
Organization:
Sections 5.4 (5.5) on robustness (sensitivity) as written in the main text are not informative. They should either contain more concrete statements about results or be moved entirely to the supplementary material.
Also, discussion of the ground set would be much clearer if it appeared before the bottom of page 6
Pros
Improves over previous strong mixup baselines
The proposed method outperforms baselines even when m=2 (it's worth mentioning this in the main text even if there is not enough room for a full ablation)
Cons
Running time of the algorithm may be prohibitive in certain applications, this should be analyzed an discussed
Several issues with clarity and organization (see above)
Questions:
Does larger m always improve accuracy, or does this eventually decrease?
Exactly how is saliency computed in the experiments? Is it the same pretrained map or does it vary with architecture?
How much time does co-mixup take compared to training the network?
Are Algorithm 1 and applying mixup ideas to m>2 samples novel contribution?
EDIT: The author response addressed all my concerns and answered all my questions, in particular that the exponential running time is a worst-case bound that is indeed loose in practice. I believe that the revised version will be much clearer and am therefore increasing my score","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions","Keywords: Explainable AI, Deep Reinforcement Learning","This paper tackles the important problem of endowing deep RL agents with added interpretability. Action values are decomposed as the combination of GVFs learned on externally-specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities. Reviewers praised the approach, as well as the level of detail for reproducibility purposes. R3 had concerns about the generality of the method but follow-up experiments have allayed these concerns. Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance.","This paper tackles the important problem of endowing deep RL agents with added interpretability. Action values are decomposed as the combination of GVFs learned on externally-specified features, offering action explanations in terms of discounted future returns in the space of interpretable quantities. Reviewers praised the approach, as well as the level of detail for reproducibility purposes. R3 had concerns about the generality of the method but follow-up experiments have allayed these concerns. Given the reviewer response and the central importance of the problem considered to the field, I can wholeheartedly recommend acceptance.",""
"Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions","Keywords: Explainable AI, Deep Reinforcement Learning","Thanks for all of the suggestions for improving the paper. We uploaded a revised paper that includes improvements based on those suggestions. Below is a change log and we have also referenced specific changes in the responses to each reviewer.
Change Log:
Updated the Cart Pole learning curves and GVF loss curves (Figure 2) by adding a result of the agent with continuous “deltas” features.
Added a new paragraph which is called ""Schema for Selecting GVF Features"" at the start of Section 6.1 to describe the schema we used and that could be used in other domains.
Rewrote the description of each environment so that description of features aligns with (2) above.
Added a diagram of the ESP model (Figure 1)
Updated the text in the introduction and second paragraph of Section 2 to clarify that the greedy policy is the ‘Q-function maximizing greedy policy’.
Added a short extended discussion of the influence of feature choice at the end of Section 2 (last paragraph).
Added an expanded discussion of the activation functions used in the GVF network architecture (last paragraph Appendix D).","",""
"Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions","Keywords: Explainable AI, Deep Reinforcement Learning","This paper proposes a method that offers explanation of action preference in a deep RL agent based on given features by the human. In other words, the model explains why action A is preferred to action B based on some given features. This is done through the embedded self-prediction (ESP) model. The authors also proposed a method for evaluation of importance of features in the learned policy. While the paper benefits from extended experimental result and interesting theoretical analysis in the tabular RL, I think its readability could be improved. For example, I believe generalized value functions should be explained more extensively as (I think) it is less known to the community compared to concepts like MDP and DQN. Also, an analysis (or at least some discussion) on the effect of the number of features on learning QFs would be helpful. Another question I have, is about the dependence of features. What would happen to the evaluation if some given features are dependent? Some minor points:
It would be helpful if the authors add a figure of their networks similar to what is common in the field, specifying input and output of the networks.
The phrase of ""greedy policy"" was a little confusing for me, especially because ""greedy algorithm"" is usually one-step look ahead search. Is it just argmax Q(s, a) (as suggested in page 3)?
I think the defense on manually-designed features could be transferred to conclusion since the importance of interpretability has been already mentioned in the first paragraph.","7: Good paper, accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions","Keywords: Explainable AI, Deep Reinforcement Learning","Summary
The paper attempts to improve the interpretability of RL agents' action selection process by (a) proposing embedded self-prediction (ESP), a model that embeds generalised value functions (GVFs) in the action-value function of the agent with a ""combining"" function to and (b) ESP-DQN, an extension of DQN that augments experience replay tuples with a GVF feature vector and that decomposes the model into separate combining and GVF parameters. These enable to define action-values with respect to predefined feature maps, thus providing more ""resolution"" into the behaviour of the policy.
Good stuff
The idea of decomposing the policy into GVFs as a way to force explanations wrt. some features is brilliant, and it is well executed when combined with the contrastive explanation system.
Sections 2-4 provide both a clear introduction to GVFs as well as a detailed and sound description of the overall framework.
The related work section is fairly tight, but actually covers a good amount of necessary and relevant related work, which makes it easy to scan through the literature.
The experimental section does employ mostly fairly similar environments, but it is clear in the hypotheses being tested, and it is fairly satisfactory considering what it is attempting to evaluate.
Uncategorised notes
This is more of a meta-comment, but I enjoyed reading the paragraph about manually-designed features in Section 1. I understand why the authors felt the need to write it, and it is a sad state of affairs that it is now often a requirement to argue what to many people is just reality.
I wonder if it'd be worth it to test the method on Atari, through possibly the use of MinAtar: https://github.com/kenjyoung/MinAtar -- It feels like there's a gap in difficulty (of learning and analysis) between the ToW setting and the rest of the environments, and something aking to a middle ground would probably be a good setting to add. Complex gridworlds such as BabyAI and the NetHack Learning Environment are probably also good options.
Final comments
Overall, I'm extremely happy to strongly recommend this paper towards acceptance. It is well written, it introduces a method that attempts to move forward towards solving an important problem in Reinforcement Learning, and there's a significant amount of details in the paper that would make it fairly straightforward to reproduce.
Typos
Sec 1
RL agents explain its... -> their
directly ""embed""... -> embeds
train those... -> trains
Sec 3
Combination function update -> Combining? The manuscript is at times a little inconsistent.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions","Keywords: Explainable AI, Deep Reinforcement Learning","Edit:
I have read the authors' response as well as the other reviews. Based on the additional results and added feature selection details, I now agree that ESP is generally applicable.
Summary:
The authors present ESP, an RL system that can then explain action choices in terms of future feature values. Generalized value functions (GVFs) are used to learn an estimate of total future discounted feature values. These estimated total feature values are then used to estimate each actions Q-value. Since Q-values are based on GVF outputs, these intermediate values can be used as an explanation. The authors select a subset of these values to form a Minimal Sufficient Explanation (MSX). The proposed system is evaluated using three domains. The authors show that performance is comparable to non-GVF agents.
Reasons for score:
Though the authors present a novel explanation format, the applicability of the method is uncertain. The results appear to rely on specific GVF feature choices. Non-general methods are still of interest, but the limited information on feature construction prevents a fair comparison to other approaches. Additionally, the explanations are not evaluated quantitatively.
Pros:
-The use of GVFs for explanations in terms of future feature values is a novel line of work. MSXs are a natural way to then produce more concise explanations, and the authors extend MSX to their non-linear use case in a well-justified way.
-The analysis of the Tug of War explanations was thorough. It clearly showed how ESP explanations would be used to investigate agent behavior.
Cons:
-ESP is built upon the GVF features, but the choice of GVF features is suspect. Each environment uses a different style for its features. Lunar Lander and CartPole both have continuous features, yet the authors use ""deltas"" for Lunar Lander and region discretization for Cart Pole. Tug of War uses a bunch of features, including information about feature values when a game ends, and non-linearities are applied to the outputs of the GVF features. Note that these non-linearities are used only for Tug of War, and different non-linearities are used for different features (Table 2 in Appendix D). Effectively, each environment appears to use carefully engineered features. Given that DQN-Full performs very poorly for specific settings (i.e., in some cases, the agent cannot learn without the GVF features), the choice of features seems to be important. The authors should indicate the process used for selecting them and how these features should be chosen for other environments. ESP may not be robust to GVF feature choice, but this is insufficiently addressed in the paper.
-In Section 6.3, the authors present potential conclusions that can be drawn from an ESP agent. These conclusions can be evaluated to determine whether valid conclusions can be drawn from the explanations. Such an evaluation would allow the hypotheses of the authors to be tested.
-A substantial reorganization of the paper would improve clarity. Various definitions and descriptions are provided a few sections after the terms/methods are first used. Terms are unnecessarily over-loaded (such as ""sound"").
Questions During Rebuttal Period:
-Please address and clarify the ""Cons"" above.
-In particular:
a) How were the GVF features chosen? Why does each environment use different features?
b) What happens when Lunar Lander is given ""CartPole-style"" (region discretization) features? What happens when CartPole is given ""Lunar-Lander-style"" (change in features) features?
Minor comments:
-This work would benefit from another editing pass for tense/plurality matching between subject and verb.
-The proofs in Appendix B would benefit from a re-write; currently, they are hard to parse.","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?","Keywords: sample complexity separation, equivariance, convolutional neural networks, fully-connected","The paper analyzes the sample complexity of convolutional architectures, proving a gap between it and that of fully connected (fc) networks. The approach builds on certain invariances of fc nets. The reviewers appreciated the technical content and its contribution to understanding the relative advantages of different architecture, as well as the role of invariance.","The paper analyzes the sample complexity of convolutional architectures, proving a gap between it and that of fully connected (fc) networks. The approach builds on certain invariances of fc nets. The reviewers appreciated the technical content and its contribution to understanding the relative advantages of different architecture, as well as the role of invariance.",""
"Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?","Keywords: sample complexity separation, equivariance, convolutional neural networks, fully-connected","We thank all the reviewers sincerely for their constructive and positive feedback. We have incorporated the suggestions in our updated manuscript. Below we provide an overview of the revisions. Please also see our individual responses to each reviewer.
Major Updates:
We added a new figure (Figure 2) in appendix section E, where we compare the test accuracy of networks with a broader class of architectures, including Resnet, ReLu activation, and BatchNorm, on the same hard instance used in Figure 1. We found that ConvNets still always outperform FC nets experimentally.
In Figure 2, we also reported the performance of a hybrid architecture of FC and Conv nets --- we add an FC layer before the ConvNet with quadratic activation, as suggested by Reviewer 1. Not surprisingly, as predicted by our lower bounds, the performance of the hybrid net is as low as FC nets.","",""
"Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?","Keywords: sample complexity separation, equivariance, convolutional neural networks, fully-connected","This paper studies an interesting theoretical question: are there any natural tasks that provably separate fc-nets from convnets. The main contribution of this paper is an Omega(d^2) vs O(1) separation.
To prove the hardness result, the authors use (and generalize) the notion of orthogonal-equivariance introduced by Ng (2004). The current submission improves the hardness results of Ng (2004) in the following aspects:
Ng (2004) proved an Omega(d) vs O(1) separation, while this paper provides an Omega(d^2) vs O(1) separation. This is interesting not only from a theoretical perspective, but could also be relevant to practice. In practice, the dimensionality d is always moderately large. Moreover, the labeling function employed in the hard case is natural and could indeed capture practical scenarios.
The hardness result by Ng (2004) does not use a fixed hard distribution, while this paper shows that there exists a universal (and in fact, natural and simple) hard distribution that is hard for any orthogonal-equivariant algorithm. Personally I find such an improvement important: in order to demonstrate the intrinsic superiority of convnets over fc-nets, it is crucial to obtain distributions that are hard for all training algorithms.
The authors generalize the notion of orthogonal-equivariance and propose permutation-invariance, which allows them to prove hardness results for a wider class of algorithms. In particular, separation between fc-nets and convnets trained by Adam, which is a corollary of the hardness result in this paper, is not implied by previous results. Generalizing hardness results to a larger class of algorithms is definitely interesting for a broad class of audience in the deep learning community.
The lower bound is proved by using Benedek-Itai’s approach and carefully bounding certain covering numbers.
Overall, this paper presents a set of interesting results which rigorously explain why covnets could be more sample-efficient than fc-nets. This paper is generally well-written and provides lots of intuition on why the hardness results hold. On the other hand, there are a few typos that need to be fixed (see below). Given the great importance of the topic and results in this paper, I would recommend acceptance.
Minor Comments:
For random variable X and Y => random variables
for function class F,G => function classes. also missing space before G
semi-definite positive matrix => positive semi-definite matrix
ConvNets(CNN): missing space
( which may depend on S) : extra space
models below , FC-NN: extra space
But as noted in the introduction: remove but
namely, the standard Gaussian, => the standard Gaussian distribution","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?","Keywords: sample complexity separation, equivariance, convolutional neural networks, fully-connected","The paper studies simple distributional settings in which convolutional neural networks give a provable sample complexity advantage over fully connected networks. This perspective is a valuable complement to prior work in statistical learning theory that often focuses on distribution-free results, which make it harder to study interactions between the training distribution and learning algorithm.
Overall I find this research direction very interesting and the paper is a good contribution. My two main concerns are the following:
It is unclear how the proposed theory relates to real data distributions. Concretely:
The authors conduct two experiments, one of them with real examples (CIFAR-10 images) and synthetic labels (Figure 1). Did the authors explore a range of architectures and optimization hyperparameters for the fully-connected networks in Figure 1? Performance of neural networks can vary a lot under hyperparameter changes, so the separation would be more convincing if the authors performed a search through a space of hyperparameters.
In addition, it would be interesting if the authors related their theory to properties of real datasets (without synthetic labels).
The theoretical results (upper bounds) study the case where only the last layer of a neural network is trained. Is this also the case in Figure 1?
The presentation of the theoretical results could be improved. Concretely:
The first eight pages contain about one page of definitions. While it is certainly important to be technically precise, some of the definitions could be moved to the appendix so that there is more space for conveying the core ideas in the main text (e.g., the next point).
The upper bounds for gradient descent on CNNs are for the convex case where only the second layer is trained. It would be good to state this in the main text so that the reader understands the flavor of the results more easily.
Some parts of the proofs in the appendix are only sketched or omitted, e.g., Lemma C.4 or the convergence guarantee for gradient descent in Theorem 4.1
Given these limitations, I am currently hesitant about accepting the paper even though I find the overall research questions very interesting. Addressing the points above could substantially improve the paper.
Additional comments:
Page 21: the step from the second to last line to the last line is unclear to me. We have conditioned on the event B, which presumably means d(x, x_i) >= 3 (the event B is not clearly defined?). Hence we should have tau_i(s_i) = s_i, but the last line replaces tau_i(s_i) with t_i?
Definition 3.3 contains ""n \in ?"" - what does this symbol stand for?
Page 5: typo ""bewteen""
Theorem 4.2: typo ""samples from a fixed ,""
The appendix contains many proofs without restating the theorems from the main text, which makes it hard to read the paper thoroughly. The thm-restate package is helpful for this, see https://tex.stackexchange.com/questions/51286/recalling-a-theorem
Thank you for addressing my comments, I have updated my score accordingly.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?","Keywords: sample complexity separation, equivariance, convolutional neural networks, fully-connected","The paper presents an interesting analysis of MLP and convnets, where they show a gap between the number of required training examples to generalize well. They show that due to orthogonality invariance in MLP training, then more examples are required compare to convnet, where one example is needed. This approach, which relies on an older result, provides an intuition as to the success of resnet.
While the work is interesting I have one main concern: Is the distribution analyzed related to real problems? I think making such a relationship is important as at the moment I don't see any connection between the models analyzed and the structure of real data. The reason this question is important is that in a similar way to the analysis performed, one may find a data distribution that cannot be learned with convnet but can be applied with MLP. Then convnet will get very bad error, while MLP will be able to generalize. So, it is important to explain why the distribution used in the analysis is related to realistic data.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?","Keywords: sample complexity separation, equivariance, convolutional neural networks, fully-connected","This paper proves that, for the learning problem where the input distribution is standard Gaussian, and the ground-truth label is given by the difference between the sum of squares of the first half coordinates and second half coordinates, any orthogonal-equivariant algorithm (e.g., a fully-connected network with SGD) needs \Omega(d^2) samples to achieve a constant test error with a constant probability, while there exists a ConvNet which only needs O(1) samples. Similar results on l_2 regression and adaptive training algorithms are also given.
I think this paper attacks the important problem of sample efficiency of ConvNets. The notion of equivariance between algorithms, including orthogonal and permutation equivariance, is clean and inspiring: it can help us understand and improve various algorithms and architectures, and may also be extended to other settings. Various proof ideas are also interesting, such as Theorem 5.1, the use of Benedek-Itai's lower bound, etc. The writing is clean in my opinion.
Here is one question that is interesting to me: Suppose we insert a Gaussian-initialized fully-connected layer before a ConvNet, i.e., let the fully-connected layer be the first layer, followed by the original ConvNet. Now I think SGD on this new architecture becomes orthogonal equivariant, even if we don't train the first fully-connected layer. Does generalization deteriorate in this setting?
On the weakness of the paper, here are my thoughts:
For the proofs to work, it seems that the feature distribution needs to be rotational invariant. Can we relax this condition? On the other hand, it is mentioned that the target function is still easier for ConvNets to learn on CIFAR inputs, which partly answers this question.
The ConvNet used to learn the target function (given on page 15) is special: it is a nearly-minimal function class that can represent the target function. What if we use ReLU activation, max-pooling, etc.?
Here are some minor comments:
In the last inequality of Definition 3.3, the middle N^* should be N.
In the definition of ConvNets, the subscript d'(r-1)+1:d'r should be d'(i-1)+1:d'i.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions. In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure. The paper shows good performance in inducing compositional structure in two datasets.
Summarizing the reviewers' doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up. The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see. If anything, this is probably the most original paper in my pool.
Concerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time.
Finally, the authors added new text and new experiments strenghtening their conclusion during the discussion.
I am strongly in favour of accepting this paper.","This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions. In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure. The paper shows good performance in inducing compositional structure in two datasets.
Summarizing the reviewers' doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up. The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see. If anything, this is probably the most original paper in my pool.
Concerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time.
Finally, the authors added new text and new experiments strenghtening their conclusion during the discussion.
I am strongly in favour of accepting this paper.",""
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","We highlight our main contributions here and will update the paper to reflect these more clearly. Additionally, we provide a note on some updated results and remark on the choice of SHAPES-SyGeT and CLEVR/CLOSURE for our work.
Main contributions:
We present iterated learning (IL) as a more fundamental way of recovering structure in machine learning. To the best of our knowledge, IL has not been applied outside language emergence or preservation. We believe this is an important message to share with the machine learning community, where there is still a need and a lack of understanding of the emergence of compositional structure.
We propose an IL method to achieve higher systematic generalization (generalization to novel combinations of known concepts) in neural module networks (NMNs). Previously, NMNs have been shown to exhibit superior systematic generalization ([1], [2]) but only with gold-standard layouts. Subsequently, prior work has required supervision with a large number of ground-truth programs for CLEVR (18000 for [3], 1000 for [4]) to get NMNs to generalize well. In contrast, our method is significantly more data-efficient (only 100 ground-truth programs for supervision) by using IL to learn the program language.
We introduce the SHAPES-SyGeT dataset as a new split of the existing SHAPES dataset to evaluate systematic generalization as a lightweight alternative to CLEVR/CLOSURE.
Minor contribution: Tensor-FiLM-NMN module architecture to combine the parameter-efficient Vector-NMN with the spatial representational power of Tensor-NMN.
Updates to CLEVR results:
While we already presented clear advantages in recovering the correct programs, the CLEVR models in the first version of our paper did not completely converge. Thus, we have updated the paper with experiments that run twice as long and with 8 seeds instead of 3. With this setup, IL outperforms the baselines on all but one CLOSURE category for both Tensor-NMN and Vector-NMN, illustrating the superior systematic generalization of IL.
Note on our choice of datasets:
We use SHAPES-SyGeT and CLEVR/CLOSURE for this work to clearly demonstrate the advantages of IL for systematic generalization. These diagnostic VQA datasets make it easy to analyze and split questions into templates, which can then be divided into training and testing templates. We train on questions generated from the train templates and evaluate on questions generated from templates never seen during training (SHAPES-SyGeT's Val-OOD and CLOSURE).
However, we agree with the reviewers that it is important to understand the scaling limitations of methods such as ours. Although not necessary for our paper's goals, we are trying our method on the GQA dataset ([5]) to analyze the gains of our method against the baselines on a large-scale non-synthetic VQA dataset. This dataset also provides programs for its questions, similar to the datasets we have used. We plan to include these results in the appendix before the camera-ready deadline.
References:
[1] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019.
[2] Dzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783, 2019.
[3] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989–2998, 2017.
[4] Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Probabilistic neural-symbolic models for interpretable visual question answering. arXiv preprint arXiv:1902.07864, 2019.
[5] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.","",""
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","Review: The authors address methods to encourage the emergence of the layout expression structures on the frameworks of neural module networks (NMN) for the visual QA problems. The methods are motivated from the works on language emergence for communication between multi-agents and the language acquisition of new-born babies from parents, which achieved with limited data. The methods, ‘iterative learning’ (IL) are designed as forming two agents (program generators and execution engines) to play VQA games. Basic architectures and learning methods seem to be very similar to the approach of semi-supervised learning introduced in [ICCV17]. This paper deals with one of very interesting topics, the language emergence among cooperative multi-agent environments and the compositionality of human language as recent related studies are well-surveyed in related work. In particular, the main idea of problem formation, layout expressions in NMN as emergent languages is very fresh and interesting. The main claims are as follows: (1) the proposed approach of IL improves generalization performance for visual QA, and it is shown experimentally by comparing the ablation results of IL. (2) the language structures in the ground-truth data are recovered with only limited supervisions and the superiority is validated on two datasets – SHAPES-SyGeT and CLOSURE. However, I believe that the evidences for their claims are insufficient. Specifically, the authors do not provide enough information of language structure such as the superiority compared to other methods and the structure similarity of recovery levels. I recommend 'ok, but not good enough – reject’ for this paper.
Pros:
The authors propose novel interesting problem and their solutions. Arguably, it seems potentially to be on one of important research flows to make influence to lots of works for academia in the future.
They find and report good performance for out-of-distribution accuracies for visual QA datasets.
Concerns:
It is not clear which parts in the proposed methods are novel with respect to previous works. Those make vague which parts are the authors’ contributions.
I think IL should be clarified from semi-supervised learning approaches on them for visual QA. Also, for reproducibility, it should be specified which parts are with/without IL. What is ‘learning bottleneck’ on this approach? Also, it is not enough how program generators and execution engines are specified, even though some explanations are in appendix. I think it needs more links for reference or explanation.
As mentioned above, the supports for main claims are not appropriate or unclear. It needs to theoretically or experimentally show the results of comparison with other methods and the similarity of the recovery level of language structures.
Table 1 reports the result of comparative methods such as MAC and FiLM without program supervision. How are they configured in the experiments?
I think that it would be better understandable to show usability and superiority with the experimental results on realistic visual QA such as VQA and GQA.
Minors:
In Section 3.1, γ. -> γ, β. -> β
It needs the description for operators and symbols for the formulae in Section 3.1
[ICCV17] Johnson et al., Inferring and executing programs for visual reasoning, ICCV 2017.
After rebuttal
From the revised version of this manuscript, the authors resolve my major concerns such as clarity/reproducibility of the method, differentiation from the previous works including semi-supervised learning, and scalability. So, I've raised my score to 6. Thank you for the contributions!","6: Marginally above acceptance threshold","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","Recovery level of language structures:
Our understanding of the reviewer's concern regarding the ""recovery level of language structures"" is that they would like to see some measure of how much of the structure is recovered by our methods and baselines beyond just the program accuracy. We would appreciate it if the reviewer could clarify this. However, we respond here with our current understanding to argue that the program accuracy indeed largely gets to what we want for this work.
In the context of NMNs, the program determines the computation to be performed on the input images. Small differences in the serialized representation of the computation graph, which the PG outputs, can result in trees that differ significantly in structure. Metrics that use n-gram statistics like the BLEU score can thus be misleading since they may provide high scores for programs that have similar serialized structure but vastly different tree representations once parsed. Since we have some amount of program supervision, there indeed is a notion of a ""correct"" program structure, which is one that agrees with the supervision programs. Finally, we note a very significant and clear difference in the program accuracy exhibited by IL and baselines without IL that correlates with the models' ability to systematically generalize, which supports our choice of discussing the recovery of structure in terms of program accuracy.
Theory:
Finally, while we do believe that a theoretical understanding of IL is necessary, we consider it a broad research problem that is out of scope for our study. A mathematical model from cognitive science is presented in [8], that relies on a Bayesian learner assumption. However, this theory does not sufficiently explain IL in deep learning or in the presence of a grounding task. Our paper offers contributions to encourage the research community to study IL in more detail, which will hopefully offer further theoretical insights.
References:
[1] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019.
[2] Dzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783, 2019.
[3] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989–2998, 2017.
[4] Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent communication. In Advances in Neural Information Processing Systems, pp. 15851–15861, 2019.
[5] Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, and Simon Kirby. Compositional languages emerge in a neural iterated learning model. In International Conference on Learning Representations, 2020.
[6] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[7] Drew Arad Hudson and Christopher D. Manning. Compositional attention networks for machine reasoning. In International Conference on Learning Representations, 2018.
[8] Thomas L. Griffiths and Michael L. Kalish. Language evolution by iterated learning with Bayesian agents. In Cognitive science 31.3, 2007.","",""
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","We realize that due to the lack of a formal algorithm in our paper that there could be a misunderstanding regarding what is ""iterated"" in iterated learning (IL). Just in case, we clarify it here.
Our IL approach follows four phases in a single iteration, and these phases are repeated many times during training. For brevity, we denote the interacting phase as P1, transmitting phase as P2, PG learning phase as P3, and EE learning phase as P4. Then, the training of an IL model goes as follows:
P1 -> P2 -> P3 -> P4 -> P1 -> P2 -> P3 -> P4 -> P1 -> ...
In contrast, the training of a model without IL proceeds through one long interacting phase. Viewed another way, it can be seen as:
P1 -> P1 -> P1 -> ...
Every time a PG learning phase is executed, a new program generator (PG) is initialized to represent a new generation. Similarly, a new execution engine (EE) is created in EE learning phases (it can either be initialized from scratch or be a clone of a previous EE).
We will add a formal algorithm describing our approach in the appendix.","",""
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","We thank the reviewer for their response and thoughtful questions. For the convenience of the reviewer and other readers, we start by giving a short overview of our method. We will refer to this overview later when addressing the reviewer’s specific concerns.
Our iterated learning (IL) approach involves the following phases:
Interacting phase: This is a standard self-play setting where the program generator (PG) and the execution engine (EE) are trained end-to-end to solve the VQA classification task. We use REINFORCE to estimate the gradients of the PG, and also provide direct supervision to it with a small subset of labeled programs. This phase is carried out for a fixed number of gradient updates specified by
Ti
. In our experiments without IL, this is the only phase executed.
Transmitting phase: We collect samples from the PG to serve as targets for an imitation learning task in the next phase. We sample training questions
q
and do a forward pass through the PG to produce predicted programs
z^
. We add tuples
(q,z^)
to the transmitting dataset
D
to contain
Tt
samples.
PG learning phase: A new PG is initialized and trained to imitate the previous PG, using the transmitting dataset
D
. The new PG is trained to match
z^
when given
q
as input, where
(q,z^)
is sampled from
D
. This phase is carried out for a fixed number of gradient updates specified by
Tp
. The value of
Tp
controls the learning bottleneck by altering the new PG’s tendency to underfit or overfit the previous PG’s language.
EE learning phase: We initialize a new EE and train it to adapt to the new PG. We train the new EE to solve the VQA classification task using programs generated by the new PG, without modifying the new PG. Adapting the EE stabilizes training during the interacting phase. This phase is carried out for a fixed number of gradient updates specified by
Te
.
We now address the reviewer's specific concerns.
Learning bottleneck:
Iterated learning creates a new PG and a new EE for every generation, and a learning bottleneck controls the amount of information that passes from one generation to the next. We impose this learning bottleneck only by limiting the number of gradient updates
Tp
performed during the PG learning phase. Based on the hypothesis that structured language is easier to fit for neural networks ([4], [5]), we expect systematic global linguistic rules to be acquired before any specific idiosyncrasies or non-compositional rules. A well-tuned
Tp
would ideally stop the PG learning phase just before any idiosyncrasies are learned (for example, see Table 1 in [5]). Thus, the goal of the learning bottleneck is to allow structured rules to survive to the new generation, while eliminating non-compositional idiomatic rules from the layout language.
By repeatedly applying this learning bottleneck, our IL algorithm pushes the language towards one that can solve the task while maintaining a structure that is easy-to-learn. It is this combination that aids systematic generalization. Our algorithm therefore explores regularization that emerges from modifications to the learning process rather than structural constraints.
As we specify in Table 3 in the appendix,
Tp
is 2000 in our experiments. If we initialize the new EE in the EE learning phase from scratch,
Te
is 250; if it is initialized as the EE after the previous EE learning phase,
Te
is 200; and if it is initialized as the EE after the previous interacting phase,
Te
is 50. We find
Tp
to be the more important hyperparameter since it controls the learning bottleneck, while
Te
primarily affects the stability of training.
Parts with/without IL:
In our runs without IL, the model is trained entirely in one long interacting phase, roughly analogous to previous works that have used NMNs on these tasks. In this case, there are no transmitting or learning phases, and the PG and the EE are never reset.
In our plots, the x-axis denotes the number of gradient steps. This is straightforward to track in the runs without IL. In the case of IL, we maintain a global counter for gradient steps across all phases. This ensures that our plots fairly reflect the computational requirements of IL. The training task and program accuracies are measured only during the interacting phases, so one can see dips in performance between the end of one interacting phase and the start of the next interacting phase. Validation accuracies are measured at the end of interacting phases in the case of IL, and at regular intervals for the baselines without IL.
We will upload a revision of the paper shortly, updated to address some of the reviewer's concerns. As a reminder, we will not have the GQA results until the camera-ready version since these experiments are time-consuming and not necessary to convey the core message of our paper.","",""
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","This paper proposes to combine iterated learning (the process of repeated language transmission from a ‘parent’ agent to a ‘child’ agent) with neural module networks (NMNs), in order to emerge NMN layouts that perform better at systematic generalization. The paper evaluates on their new variant of the SHAPES dataset that tests systematic generalization, and on CLEVR / CLOSURE, showing improved systematic generalization performance while requiring a small amount of ground-truth layout supervision.
Pros:
I think the idea of combining IL with NMNs is really clever (heh). Treating the program generator and execution engine as two agents that need to coordinate through a shared language (NMN layouts) is really interesting. If it were to work without layout supervision, it could open up new doors to applying IL + NMNs to many other tasks
The paper is quite well-written, and easy to follow
The related work section is thorough
The experiments on SHAPES-SyGeT, show that IL helps significantly for generalization of NMN models
I appreciate the ablations in the Appendix.
Cons:
One of the main questions I have about this approach is whether it will provide any benefit on more complex problems (e.g. large-scale VQA). There are a few reasons to think it might not be able to do so:
As alluded to in the paper, the IL procedure requires a lot of compute, which could be used to train larger models on more pre-training data
The improvement in validation performance on the more complex CLEVR dataset is fairly modest (though the program accuracy increase is large). While CLEVR is more complex than SHAPES, it is still a fairly artificial dataset targeted ‘compositional’ in nature, compared to general VQA. This suggests that it might be hard to get IL+NMN to work well on harder problems.
The method still requires (a small amount of) ground-truth layout supervision, which is not obtainable in general VQA or most other tasks.
I would also like to have seen a bit more analysis / description of why the method currently fails without any ground-truth layout supervision. I think this would improve the paper a lot, as it would help other researchers improve upon the method to address this problem.
Overall: I think the ideas in this paper are interesting enough, and the execution good enough, to warrant acceptance. While I have some concerns about whether this approach will scale, these questions will have to be answered in subsequent works and with further research.
Small typos: “each new-born child need” -> needs
“Recently machine learning community also show” -> the machine learning community also shows
“Minimzing” -> minimizing","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","The authors apply iterated learning - a procedure originating in CogSci analyses of how human languages might develop - to the training of neural module networks. The goal is for iterated learning to encourage these networks to develop compositional structures that support systematic generalization without requiring explicit pressures for compositional structures (in past work, such explicit pressures have generally been necessary). The proposed approach brings substantial improvements in systematic generalization across two datasets, SHAPES and CLEVR.
Strengths:
The approach is well-motivated, including impressive coverage of prior literature in both ML and CogSci.
The approach brings impressive gains in an area that is one of the major weaknesses of current ML systems, namely systematic generalization.
In addition to the gains in accuracy, one particularly impressive benefit of this approach is the decreased amount of supervision that it requires compared to past approaches.
The paper is generally well-written and easy to follow.
Weaknesses:
One of the motivations is to expand the use of iterated learning beyond toy datasets. While SHAPES and CLEVR may be not as toy-ish as datasets used in the past, they still are pretty toy-ish, so I’m not sure if this paper can reasonably claim that one of its contributions is to expand iterated learning to realistic domains.
Though the paper in general was very clear, I found Section 3.2 to be a bit hard to follow, and that section is important as it is the part that describes the structure of the iterated learning framing. I think this section would benefit from starting each subpart with a more high-level, intuitive description of what that stage accomplished, before diving into the details.
Minor comments:
Fodor et al only has 2 authors - Fodor and Pylyshyn
Page 3: “Although, the Gumbel straight-through estimator”: this use of “although” is usually frowned upon - better to use “However”
Page 5: typo: “minimzing”
In general, for the bibliography, check to see if a paper has been published at a conference or journal; if so, cite that version instead of the arXiv version. E.g., “Neural machine translation by jointly learning to align and translate.” was published in ICLR 2015, and “Systematic generalization: what is required and can it be learned?” was published at ICLR 2019.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Iterated learning for emergent systematicity in VQA","Keywords: iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality","Thank you for the responses! I think it would be nice for the paper to use different language than ""toy"", purely because I worry that some readers will view the paper negatively on the grounds that SHAPES and CLEVR still are toy-ish (due to the subjective nature of the definition of ""toy"", as you noted). So I would recommend using different terms that are more objective, to prevent readers from misreading the paper in that way. (E.g., you could say that you have applied IL to datasets that are substantially more complex than those used in prior work - ""substantially more complex"" seems a fair statement to me, as it makes not claim that SHAPES and CLEVR are not toy datasets).
By the way, I very much support the use of synthetic datasets, as they are more controlled and therefore allow for more rigorous experiments. The only concern I had was that the current phrasing might make the paper seem like it is overclaiming.
Anyway, this is a small enough issue that it has not affected the score I assigned to the paper.","",""
"When Do Curricula Work?","Keywords: Curriculum Learning, Understanding Deep Learning, Empirical Investigation","Dear Xiaoxia Wu, Ethan Dyer, Behnam Neyshabur,
This is a great work to read. Congratulations!
BTW, we had two pieces of work which are highly related (In other words, curricula learning and example weighting are highly related). Our work studied the implicit example weighting introduced by a loss's derivative and back-propagation when optimising a model using gradient descent.
IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters https://arxiv.org/abs/1903.12141
Derivative Manipulation for General Example Weighting https://arxiv.org/abs/1905.11233
In a nutshell, a Curricula (i.e., learning order of data points), regardless of being implicit or explicit, will be influenced by the implicit example weighting introduced by a loss function, especially when the derative is non-monotonic with respect to the loss value.
I think it will be very interesting to discuss them in the paper or study them together in the future work. I am looking forward to your ideas, or further discussion if you are available.
Many thanks and kind regards.","",""
"When Do Curricula Work?","Keywords: Curriculum Learning, Understanding Deep Learning, Empirical Investigation","This nice paper gives a better understanding of how Curriculum Learning (CL) affects image classification. In particular, it gives insight into cases such as noisy training data and limited training time. It shows that examples can be rated by difficulty to some extent, in that the order in which examples are learned seems to be consistent across runs. The paper is thorough and well-written.","This nice paper gives a better understanding of how Curriculum Learning (CL) affects image classification. In particular, it gives insight into cases such as noisy training data and limited training time. It shows that examples can be rated by difficulty to some extent, in that the order in which examples are learned seems to be consistent across runs. The paper is thorough and well-written.",""
"When Do Curricula Work?","Keywords: Curriculum Learning, Understanding Deep Learning, Empirical Investigation","Summary: The paper conducts a large-scale evaluation of the impact of curriculum learning (CL) in image classification. The paper progresses nicely through a sequence of well-thought research questions and experiments, with the key findings stated up front. In particular, the notion of ""implicit curriculum"" is shown to exist. Prior findings around when CL is helpful (limited training, label noise) are confirmed, which is nice. Overall, this methodical empirical evaluation comes away with a clear set of takeaways, empirically ""summarizing"" a lot of prior work on CL and the training of deep models. Some discussion about why CL helps when training is limited or data has label noise (or next steps) would strengthen the paper a bit more.
Strengths:
Extremely well-written and easy to read. Key findings are summarized and visualized up front.
Well-designed large-scale empirical investigation into important open questions for training image classifiers.
Areas for improvement
Can't think of too many. I suppose the paper could have included a bit more discussion into why the reduced training or label noise benefits from curriculum learning. I'm also curious to see how these findings compare with a similar study on sequence (especially text) data but as the paper mentions, it's outside the scope of this paper.
A pointer up front directing the reader to the appendix where ""standard training"" is defined would have been nice to have.
Questions:
On Page 5, I didn't follow the sentence ""Given these pacing functions, we can now ask if the explicit curricula enforced by them can change the order in which examples are learned."". I agree that Fig 3-right shows that the difficulty ordering (e2d, rand, d2e) can change the order in which examples are learned when using the step pacing function. What other pacing functions are used in Fig 3-right? Is there a different interpretation of the sentence involving the pacing functions?
Minor comments
A few typos
Fig 2's caption (""ReseNet50"", ""EfficeientNet"")
Page 6 (""CIAFR10"")
UPDATE: I thank the authors for their detailed response and updated paper. I'm now more inclined to accept.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"When Do Curricula Work?","Keywords: Curriculum Learning, Understanding Deep Learning, Empirical Investigation","########################################################################## Summary:
The paper provides a comprehensive analysis of the benefits of curriculum learning in different application scenarios. This includes investigating the phenomenon of implicit curricula, showing if the examples are learned in a consistent order across different architectures, and exploring the influences of explicit curricula in the standard and emulation settings. The paper empirically shows that curriculum learning has marginal benefits for standard training, but is helpful when the training time is limited or the training data is noisy.
##########################################################################
Reasons for score:
I vote for accepting the paper. I believe the analyses presented in the paper can be valuable for the community. I like the implicit curricula experiments, showing that the difficulty of an example is somewhat independent of the training method. Other empirical observations are also interesting. In general, I think the paper provides a satisfactory answer to the question raised in the paper title (when do curricula work?)
##########################################################################
Pros:
This paper has extremely comprehensive evaluations, examining the influence of curriculum learning (curriculum/anti-curriculum/random-curriculum) in diverse settings (standard/limited training time budget/noisy data). The methodology for the evaluations is technically sound;
The findings presented in the paper can be valuable for the community: (1) the difficulty of an example is somewhat independent of the training method; (2) curriculum learning provides little benefit for standard training, but help for limited time and noisy training;
The paper is well written. It is a thoroughly enjoyable experience to read the paper.
##########################################################################
Cons:
I found few weaknesses in the paper. I include a question below which I hope could be clarified:
The learned iteration of a sample is defined by the first epoch from which the prediction remains correct for all subsequent epochs. I wonder if there is any sample that is predicted correctly in earlier steps but incorrectly in later steps (e.g. the forgettable examples). How to handle them in the implicit curricula experiment?
##########################################################################
Minor comments:
Page 16: two data loader → two data loaders
#########################################################################
Final recommendation:
I have read the authors' responses as well as the comments from my fellow reviewers. I would like to keep my rating of the paper (8).","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"When Do Curricula Work?","Keywords: Curriculum Learning, Understanding Deep Learning, Empirical Investigation","This paper provides a comprehensive empirical study on the effects of the ordering of training samples in curriculum learning. The authors designed extensive experiments and obtained some interesting results: 1. the models with a similar architecture learn samples in a consistent order; 2. with enough iterations curriculum learning has no gain on performance comparing with random ordering; 3. curriculum learning outperforms others when the training time is limited; 4. curriculum learning is more robust with noisy samples. In general, I think this is a quite practical work that could be beneficial to the community. The experiments are carefully designed and the results are sound, and the paper is well structured.
There are just some minor issues that may need some clarification:
As the authors mentioned in Sec.3, the random ordering is not as the same as i.i.d. training because it corresponds to dynamic training size. Isn't it more similar with bootstrapping? It would be interesting if the authors can have some discussion in this point of view.
The parameters of pacing function a, b should be introduced with the pacing function in the beginning of Sec.3 , as they were mentioned before Sec. 3.2 without explanation.
Subfigures in Fig.5 should have subcaptions.
In the experiments with noisy labels, the best pacing functions ignore all noisy data, does it mean their gain of performance is simply from filtering out the noisy samples? Is there any other influence from the formulation of the pacing function itself?
In the paragraph under Fig.8, the last sentence 'a trend to start and maintain ...' is a bit confusing to understand, could authors clarify it a bit?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","This paper proposes an approach of generating mathematical expressions with a recurrent neural network, which is trained with risk-seeking policy gradient to maximize the quality of best examples rather than average examples. The proposed approach also enables easily incorporating domain knowledge or constraints to avoid illegal or redundant expressions. In extensive experiments, the proposed method is shown to significantly outperform strong baselines, including commercial software. All of the reviewers find the work interesting and relevant, and there are no major concerns or issues after discussion. The topic is also of interest to a wide range of audience in the ICLR community.","This paper proposes an approach of generating mathematical expressions with a recurrent neural network, which is trained with risk-seeking policy gradient to maximize the quality of best examples rather than average examples. The proposed approach also enables easily incorporating domain knowledge or constraints to avoid illegal or redundant expressions. In extensive experiments, the proposed method is shown to significantly outperform strong baselines, including commercial software. All of the reviewers find the work interesting and relevant, and there are no major concerns or issues after discussion. The topic is also of interest to a wide range of audience in the ICLR community.",""
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","This paper presents a novel approach to the problem of symbolic regression, where the goal is to learn relationships between variables in the form of mathematical expressions. This is clearly a very relevant task towards constructing explainable AI systems.
The proposed approach in this paper is based on the generation of mathematical expression with recurrent neural networks, by exploiting background knowledge about the form of the expressions to impose constraints on the generated examples.
The RNN is trained by maximizing a risk-seeking policy gradient that aims to increase best-case performance. The key idea here is to increase the reward of the top-epsilon fraction of samples from the distribution, without taking into account the samples that fall below such threshold.
The technique is sound and novel, and it provides a significant contribution to this research area. The positioning of the paper with respect to the state-of-the-art in the field is highly accurate.
A very solid experimental evaluation is carried out on several benchmarks, comparing the proposed approach with state-of-the-art systems for the same task, including commercial software such as Eureqa and Wolfram. The analysis includes an ablation study and experiments conducted with different amounts of training data. The proposed methodology is shown to perform better than all the competitors.
Overall, I consider the paper to be a strong contribution for ICLR.
In the experiments with different levels of noise in data, why was Gaussian noise added to the dependent variable only, and not also to the independent variables?
Pag. 4, ""is not allowed.While"" -> ""is not allowed. While""
Pag. 5, ""but in practice has high variance"" -> ""but in practice it has high variance""","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","Thank you very much for the clarification.","",""
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","Summary
This paper presents a symbolic regression algorithm which uses policy gradient to learn a distribution over the space of mathematical expression structures. The distribution is represented by an RNN, and the on-policy sampling is also realised by this RNN. To prune the massive sample space, the authors include several constraints according to some prior domain knowledge. Furthermore, instead of optimising the RNN directly with policy gradient, the proposed algorithm also makes use of the risk-constrained method that emphasises the risks above a given percentile criteria. The proposed approach has achieved the best performance on several benchmark symbolic regression datasets.
Pros
This paper is motivated by an interesting problem. I like the idea of using domain knowledge to introduce hierarchical constraints, which seems to be effective in this task.
The authors have done extensive experiments, the presented algorithm performs well on the benchmark dataset.
This paper has covered a wide range of related work.
Cons
The main contribution of this paper is introducing the heuristic constraints to prune the expression search space and applying existing reinforcement learning techniques (e.g., using RNN to represent the policy of sampling a grammar; the risk-constrained reinforcement learning) to the symbolic regression task. It looks more incremental rather than a novel contribution.
The number of experiments in the main article is small, and many experimental details are hidden in the supplementary, I think the authors should re-organise the paper and prompt some of the results in the appendix to the main article.
One of the biggest problem for the on-policy reinforcement learning is its time complexity since it requires a large sample for estimating the expectation of risk. Considering that symbolic regression is a hard problem with highly complicated underlying distribution over the expression structures, I wonder how efficient is the proposed approach comparing to the other baseline methods. I think the authors should include the results of running time of all the compared approaches.
The experimental setting is questionable for some compared methods. In Appendix E, the authors report the results of several SOTA symbolic regression approaches. However, as the authors stated, the experimental results are different because the choice of datasets and primitive functions are different. I have noticed that this is a re-submission from ICLR 2020, and reviewers were pointing out the problem of lacking comparison one year ago. I think one year should be enough for carrying out experiments of all these SOTA approaches under the same experimental setting. Still, there is no such result in the current submission.
Recommendation
I think this paper proposes a nice approach to tackle the symbolic regression problem. Although the presented algorithm seems to be a combination of existing approaches, its performance on benchmark data sets looks quite good. However, the experiments are still weak because this paper lacks a fair comparison (e.g., using same library and the same set of problems), and my biggest concern of the proposed approach is its time complexity. I think the authors should address these problems to improve this work.
Update after rebuttal
The authors have explained the reasons for the comparing experiments in Appendix E and updated the result of time complexity, which I think worth including in this paper.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","Dear AC,
The authors have answered my questions and reported the result on time complexity, which looks good. I'll update my review and rating.","",""
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","Summary
The paper uses RNN trained by risk seeking RL objective to predict mathematical expression that generated the target dataset. In the second step placeholders for constants in sampled expression are optimized again by gradient optimizer to maximize the reward function. The decoder RNN uses heuristics that make it easier to generate valid math expressions.
Strong points
The paper is clearly written and potentially relevant to larger audience.
The proposed method is relatively easy to implement and it leads to good empirical results.
Recommendation
I recommend acceptance of this paper since the proposed technique is highly competitive, easy to implement and well presented.
Questions
How does compute used by GP to get results in Tab 1 compare to policy gradient based methods? My perspective is that all the techniques should find the correct solution in the limit with enough computational resources (under assumption that they can escape local optima). Therefore making sure that all the methods had roughly similar budgets is important.
In figure 2E mean reward for standard PG is going up even after 2M training steps, would it be possible to run the experiment longer to see where the max would be once mean reward converges? (Similarly in figure 8 in tasks Nguyen 3,5 and 8 mean of standard GP is still going up.) It is great to see that risk seeking PG converges much faster on these tasks, however knowing what is the limit for standard PG would be also interesting.
In section 3.1 you say that generated expressions had to be between 4 and 30 symbols long, however tasks 8 and 11 (sqrt(x) and x^y) have shorter descriptions. What am I missing?
Possible improvements
Using the set of problems from AI Feynman would make empirical evaluation stronger. However the same can be said for AI Feynman system and Nguyen dataset.
Beyond scope of this paper: At the moment a new sequential model is learned for each task and it learns about that task only through the reward function. What if the model can ""see"" the dataset first by reading it through n-dimensional CNN (or RNN) that would produce ""dataset embedding"" that will be later used to condition the sampling RNN. In the same way that image captioning model is conditioned on image embedding. Generating infinite training dataset (with random expressions) should be trivial, that is one advantage over image captioning with limited data.
Typos
standard standard -> standard","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","(continued)
[Token sequences for $\sqrt{x}$ and $x^y$] This is an excellent question and is clarified in the original submission, although it was somewhat buried (and thus easy to miss) in Appendix D and so we expand upon it here. The square root and power operators are not part of the function set as specified by the Nguyen benchmarks; this is precisely what makes these benchmark tasks challenging. However, these benchmarks can still be recovered with the given function set via alternate algebraic manipulation. In particular, Nguyen-8 can still be exactly recovered, e.g. via the sequence $[\exp, \times, \div, x, +, x, x, \log, x]$, which translates to $\exp(\frac{x}{x+x}\log(x))$ and simplifies to $\sqrt{x}$ (noting $x>0$). Similarly, Nguyen-11 can still be recovered via the sequence $[\exp, \times, y, \log, x]$, which translates to $\exp(y\log(x))$ and simplifies to $x^y$.
[AI Feynman benchmarks] Designing benchmarks for symbolic regression is a challenging activity and known struggle within the field (McDermott et al., 2012). In particular, it is often quite unintuitive what makes a particular expression a challenging (or not) symbolic regression problem. Thus, we chose to stick with well-established, community-vetted benchmarks like the Nguyen benchmark set that have many existing works to establish a baseline for difficulty.
While we do appreciate that one contribution of AI Feynman is introducing a new symbolic regression benchmark suite, we are also wary of adopting it due to some peculiarities. For example, the datasets treat physical constants (like the universal gravitational constant $G$) as variables with arbitrary domains. Also, many of the problems can be solved with dimensional analysis alone; in practice, a real-world practitioner of symbolic regression would apply dimensional analysis as a pre-processing step to simplify the problem before employing a symbolic regression algorithm.
[Suggestion for future work] Thank you for the valuable suggestion! This would be very reminiscent of image-to-caption systems (Vinyals et al., 2015), but instead mapping datasets to expression traversals. A similar research direction we are pursuing is using supervised learning on input datasets (and their corresponding expression traversals) to learn a prior over which tokens may exist in that dataset. Even just learning a prior over the first token (i.e. outermost operator) of a dataset's expression could be a powerful, data-driven way to reduce the search space for symbolic regression. Such priors can be incorporated into our framework by adding logits directly to the logits emitted by the RNN (just like how we impose in situ constraints by adding $-\infty$ to the logits of tokens that would violate a constraint).","",""
"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients","Keywords: symbolic regression, reinforcement learning, automated machine learning","The authors introduce a novel method for inferring a simple algebraic expression for an output in terms of an input. The method outputs a sequence tokens of the algebraic expression as represented by the pre-order traversal of an expression tree. For each token of the sequence, a recurrent neural network outputs a probability distribution on possible tokens, from which the token is sampled. Some notable contributions:
The authors introduce a novel reinforcement learning objective that optimizes the quality of the best examples instead of optimizing the average quality.
At each point in the sequence, their recurrent neural network takes as input the sibling and parent nodes of the expression tree, instead of the previous token in the sequence.
Simple constraints can be introduced in their approach by zeroing out the sampling probability for tokens that don't meet the constraints. (For example, they disallow redundant expressions like log(exp(x))).
The authors do a comprehensive comparison with other methods (in which their work performs very favorably) as well as an ablation analysis to confirm that each of their innovations is helpful (which I was very pleased to see).
Well done.","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"Improved Autoregressive Modeling with Distribution Smoothing","Keywords: generative models, autoregressive models","Thanks for your insightful paper.
However, I have a small question about facts presented in Section 4.2 2-D synthetic datasets. The paper estimates the density through modeling p(noise x) p(x | noise x), then how can we calculate the exact negative log-likelihoods shown in Table 1? To calculate p(x), we need to marginalize noise x, which is intractable by integrating p(noise x, x).
Update: The p(x) is an approximation. There is q(noise |x) which could be a good proposal distribution for important sampling and also can be approximated by ELBO.","",""
"Improved Autoregressive Modeling with Distribution Smoothing","Keywords: generative models, autoregressive models","All reviewers recommend acceptance. Some concerns were raised about the precision of theorem 2 (now renamed to proposition 1), as well as the analysis of hyperparameter choices and quantitative evaluation, which I believe the authors have adequately addressed. Based on a suggestion of reviewer 1, experiments with flow-based models were also added, which demonstrates that the method is not strictly tied to autoregressive models. Personally, I was also curious about the connection between noise injection and quantisation, which the authors responded to by adding a paragraph discussing this connection in the manuscript.
I would recommend that the authors also add the kernel inception distance (KID) results reported in the comments to the manuscript.
This work stands out to me in that it combines a relatively simple, easy to understand idea with nice results, which is a trait of many impactful papers. I will therefore join the reviewers in recommending acceptance.","All reviewers recommend acceptance. Some concerns were raised about the precision of theorem 2 (now renamed to proposition 1), as well as the analysis of hyperparameter choices and quantitative evaluation, which I believe the authors have adequately addressed. Based on a suggestion of reviewer 1, experiments with flow-based models were also added, which demonstrates that the method is not strictly tied to autoregressive models. Personally, I was also curious about the connection between noise injection and quantisation, which the authors responded to by adding a paragraph discussing this connection in the manuscript.
I would recommend that the authors also add the kernel inception distance (KID) results reported in the comments to the manuscript.
This work stands out to me in that it combines a relatively simple, easy to understand idea with nice results, which is a trait of many impactful papers. I will therefore join the reviewers in recommending acceptance.",""
"Improved Autoregressive Modeling with Distribution Smoothing","Keywords: generative models, autoregressive models","Summary : This paper proposes an approach to modeling distributions based on a two-step process which involves sampling a noisy x first and then applying a denoising function. The theory is grounded in other works in the literature that use the connection between denoising and the gradient of log p(x) with respect to x.
I enjoyed reading the paper and I think that the authors are definitely working in an exciting area. The authors do a good job in section 3.3 ""Tradeoff in modeling"" to explain clearly that everything about this ""two step"" method relies on striking the right balance between the tasks of modeling p(noisy x) and p(x | noisy x). When one is trivial, the other is very hard.
Everything about this paper hinges on the fact that the authors are learning p(noisy x). If they didn't, this whole paper would be trivial and a rather useless exercise. It took me a few passes to realize that they were indeed learning p(noisy x). This is such an important thing, and I think it might be worth insisting a bit more on this. Otherwise it's easy to look at the pictures and to conclude that they are taking x from the training distribution, adding a small amount of noise, and then showing that they can remove the noise. The authors are doing more than that, and this is why I like this paper.
Some more specific comments about the text.
Section 3.3 has ""q(noisy x| x) is simply zero"" when it's in fact a distribution with all its mass around x; not around zero. We understand what the authors meant by the context, but I recommend rephrasing this.
I like figure 1-2 for how they illustrate the concepts well.
I like that the authors took the time to write Theorem 1 instead of hand waving to appeal to how reasonable result feels.
Theorem 2 contains a typo in its statement. The left side of the equation should be ""log p"" instead of ""p"".
I could argue that ""Theorem 2"" might be more appropriately called simply a ""Proposition"" instead of a ""Theorem"", but I will leave it up to the authors to decide that. However, using the \approx notation without clarifying its precise meaning is unbecoming for a theorem. Here the authors intend to communicate something that there is an equality, but with an extra term o(epsilon^2) on the right-hand side, as epsilon->0, with that epsilon having some role in the definition of q(noisy x | x). I think this deserves to be part of the statement of this theorem. Plenty of things are ""almost equal"" to something else, so the precise meaning matters when stating a theorem.
Figure 5 refers to columns in a way that doesn't feel natural to me. To my eye, column 2 looks bad, column 3 is the best, and column 4 looks the worse. I suspect that the description of the figure either treats the first column as ""Column 0"", or it starts counting ""column 1"" after skipping the leftmost column, or we just don't have the same visual appreciation of thumbnails (especially when they contain wild pixels like column 2 does).
Inpainting at the bottom of page 7 doesn't really say what regions of the image are cut out to be inpainted.
Figure 9 says ""unconditioned"", but it's about p(x | noisy x). Wouldn't that be the opposite of ""unconditioned""?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Improved Autoregressive Modeling with Distribution Smoothing","Keywords: generative models, autoregressive models","SHORT DESCRIPTION
This paper proposes a two-stage generative modeling approach, first learning a distribution over noised data, then learning the original data distribution conditioned on this noised data. The paper demonstrates that this leads to improved sample quality compared to fitting the data distribution directly.
DISCUSSION
Overall, I like this paper: it's a straightforward idea, decently motivated and fairly well described, and has good supporting empirical evaluation. I didn't expect the sampling performance to improve substantially by adding just a single denoising step, and I think demonstrating this is a good contribution. However, I think the paper could be improved by some more careful discussion, and a better placement in the literature.
""Theorem 2 shows that our smoothing process provides a regularization effect on the original objective... This regularization effect can, intuitively, increase the generalization capability of the model."" How does the extra term in the theorem lead to a regularization effect? Why does this 'intuitively' increase the generalization capability of the model? Unless I'm mistaken, the added term is (up to a constant) the Laplacian of the log-likelihood w.r.t. data. The objective maximizes this on average across observed data, which intuitively minimizes the 'curvature' or 'steepness' of the log-likelihood at observed data, thus presumably smoothing the maximum likelihood solution. This Laplacian term also appears in the score matching objective presented in Theorem 1 of 'Estimation of Non-Normalized Statistical Models by Score Matching, Hyvarinen 2005', where it is minimized instead of maximized. There are also known connections between score matching and denoising methods e.g. 'Optimal Approximation of Signal Priors, Hyvarinen 2006', and 'A Connection Between Score Matching and Denoising Autoencoders, Vincent 2011', which you've cited in passing later. Much of this material and how it relates to the objective in Theorem 2 might be discussed in more depth rather than passing over it as simply a 'regularization term'.
""Our approach is related to two-stage VAE (Dai & Wipf, 2019) which introduces a second VAE to correct the errors made by the first VAE."" I'm not sure I agree with this. The idea of the two-stage VAE in that paper is to clean up mismatch between the aggregate posterior q(z) and the prior p(z). On the other hand, your variational model is identical to the canonical VAE setup: x is data, z is noised data, the 'posterior' q(z | x) is fixed and adds Gaussian noise, the 'prior' p(z) is a powerful autoregressive model, and the observation likelihood p(x | z) is another powerful autoregressive model (the canonical VAE would have learned q(z | x), fixed simple p(z), and learned but simple p(x | z)). This is one of the reasons 'VAE' can a confusing term when used to describe latent variable generative models in general: assuming something should be 'encoded' and 'decoded' can sometimes obfuscate the actual probabilistic model. What you propose in this paper might generically be called a 'denoising VAE', but again that's maybe not the most accurate description. I think the most closely related work is probably the denoising diffusion and denoising score-matching approaches which have received attention recently and which you've mentioned, but you could also think of it as turning a denoising autoencoder into a generative model. In any case, I think a more careful discussion of these points would be beneficial for the paper.
Finally, the approach isn't really tied to autoregressive models, apart from the motivation given in terms of smoothing 1D distributions. It's fairly likely that the same idea could readily be applied to e.g. normalizing flows and that it would work well there also, so it would have been nice to see experiments featuring flows included here. This would be especially useful since your best-performing two-stage method takes autoregressive models, which are already slow samplers, and effectively doubles their sampling time.
EXTRA NOTES
Maybe be careful with the word 'spurious' in the intro - I know what you mean, but samples from a model are by definition typical samples from that model, and there's nothing spurious about them. They're only questionable when compared to data. Similarly: ""The “erroneous” sample xˆ, in some sense, resembles an adversarial example, i.e., an input that causes the model to make mistakes."" This seems to be implying that samples generated by a model are somehow pathological. By virtue of the fact that they are generated by the model, they are by definition typical samples from the model. There is nothing pathological whatsoever about them. Why the model specification and fitting procedure have resulted in such samples, and whether the samples resemble training data or not, is another issue entirely.
'However, this approach bounds the capacity of the model by limiting the number of modes for each conditional.' All models have limited capacity -- what is particularly bad about the capacity of an autoregressive model being limited in this way? Do we have reason to believe inability to cover multiple nodes is a common bottleneck?
Figure 2 & Figure 3: Axis ticks are too small (and there probably too many), whole figure could be made bigger (this would also help with legends cutting off a lot of the plots).
'Proofs' for theorems 1 and 2 should be referred to in the main text. Theorem 2 should also have log p(x) on the LHS?
Figure 5 caption: ""All the samples are not conditioned on class labels."" -> None of the samples are conditioned on class labels.
What exactly is being inpainted in Figures 7 (a) and (b)?
Since a central claim of the paper is that the method results in improved sample quality, it might be good to add the Kernel Inception Distance (""Demystifying MMD GANs"" Binkowski et al 2018) which has many favourable properties over FID, and is really no more difficult to compute.
CONCLUSION
Overall, I think this paper is a nice submission, and would like to see it accepted given a few tweaks.
UPDATE: I've upped my score to a 7, and would like to see the paper accepted.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Improved Autoregressive Modeling with Distribution Smoothing","Keywords: generative models, autoregressive models","Summary. Autoregressive models have demonstrate their potential utility for modeling images and other types of complex data with high flexibility (particularly in density estimation). However, its sampling ability is not that good as explained in the paper. Authors show that one of the main weaknesses of autoregressive models comes from the propagation of mistakes due to the mismatch of conditionals. Inspired in the promising results of randomized smoothing in adversarial models (Cohen et al. 2019), authors propose a similar strategy. The addition of Gaussian noise and posterior modeling of the smoother data makes easier to the autoregressive density to capture the true data distribution. The benefits of this strategy are empirically proved and shown in the experiments.
Strengths. The quality of writing is high and the presentation of the paper facilitates the process of reading. I have to say that I enjoyed while reviewing it. The analysis and description of problems for sampling from autoregressive models is completely understandable to me and I agree with the manifold hypothesis held.
Results with the “sharp” multimodal data looks reliable to me and I believe that the smoother process can also reduce the lipschitz constant as stated in Theorem 1. Until pp. 5, nothing is said about the data denoising process, so one could initially think that there is no way to recover the target density without noise, but authors also did an effort on this. Good point. It is important to remark that the randomized smoothing process can be reverted once learning finishes.
Additionally, I particularly like how authors first present the idea on 1-d examples, later in the experiments, the method is validated with 2-d rings and finally, as stated in the introduction, with different image datasets.
Finally, I did not find any similar work that mixes the idea of smoothing for improving autoregressive modelling.
Weaknesses, Questions & Recommendations. To me, there are 3 main points of weakness: [W1]. A lack of analysis about the optimal noise for randomized smoothing. [W2]. Why just Gaussian noise, what if data is discrete, could we do this with another type of noise? [W3]. Comments about denoising are included a bit late in the manuscript. I think that authors should remark that this is a reversible process.
My main questions are: [Q1]. In section 2.2, I do not see why data closer to the manifold should have larger first order derivatives or even infinity. Is this a bit counter intuitive, or not? Like, better positioned, worse gradient values? [Q2]. Is the 1/N term in the global likelihood expression of 1st paragraph of section 2 correct? [Q3]. If I do not appropriately choose the \sigma parameter for smoothing, do I have the risk of not capturing some modes of the original data? I have the opinion that adding too much or too less noise to data could “mask” modes and something could be lost. Am I correct? Did authors empirically analyzed this in the experiments? [Q4]. How could we assess that conditionals are now better fitted than before?
A few recommendations for improvement: [Rec1]. I would explain a bit more the manifold hypothesis of section 2.2, maybe a diagram or figure would help for quicker comprehension of the problem. [Rec2]. Some acronym for “randomized smoothing” would help in the 1st paragraph of section 3.1. To avoid repetitive expressions.
Reasons for score. I liked the idea, think that the paper is well written and I trust the results presented by the authors. Despite the randomized smoothing strategy is rather simple, it seems to work particularly well. For this reason I tend to vote for accept. If I not set a higher score, it is because a bit more of analysis on the optimal sigma, distribution for smoothing and lipschitz constant could have been included.
Post-rebuttal update. Thanks to the authors for their response to all my questions and comments. I also read the updated version of the manuscript, which is clearly improved and the rest of reviews and comments by the AC. Looking to that, I agree with the rest of reviewers about the quality of the paper, so I raised my score and I recommend to accept it.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Improved Autoregressive Modeling with Distribution Smoothing","Keywords: generative models, autoregressive models","Summary of the paper :
The authors propose to improve the sample quality of autoregressive models. The authors propose to (1) - smooth the input data distribution leveraging methods that have shown success in adversarial defense, (2) recover input distribution by learning to reverse the smoothing process. The authors first demonstrate the efficiency of their method on 1d toy-problem, and extend the demonstration to more complex datasets such as MNIST, CIFAR-10 and CelebA with application such as image generation, inpaintting and denoising.
Pros :
The idea to leverage a method previously used for adversarial defense to density estimation is interesting and novel.
The paper is well motivated through the manifold hypothesis approximation (which results in densities with high Lipschitz constants) and compounding errors.
The theory is strong
Cons :
The experiments on denoising and inpainting are only qualitative and suffer from a lack of quantitative evaluation.
Recommandation :
The article is clear, well motivated, and have a strong theoretical grounding. Therefore I would tend to accept the article.
Detailed comments :
The experiment on 2d synthetic datasets (especially the olympic dataset) should be discussed more thoroughly. First, it is not clear that the proposed model is generating better sample than the MADE baseline on this specific dataset. Second, the intersection between rings, in the olympic dataset, seems to be much poorly modeled with the proposed approach compared to the MADE baseline. What is the reason ?
In the section 3.2 the authors are introducing 2 different debiasing methods (either a denoising step or another autoregressive model). In the rest of the article it is not clear which of the two methods the authors are using. In addition, in the 2d toy-problem (i.e. ring and olympic) as the authors are choosing a gaussian smoothing both debiasing methods are usable. Therefore it would be interesting to show both methods and to describe thoroughly the differences (in addition, it might provide an answer to my previous point).
The authors should not mention denoising and inpainting applications if there is no quantitative assessment (at least in appendix)… For the inpainting part, the corrupted input are not even shown (which part of the image has been predicted). The denoising and inpainting experiments sounds like it’s been rushed…
Typos and suggestions to improve the paper :
Minor : Both theorems are provided with nice demonstrations, then the authors should refer to the demonstration in the core text of the article (e.g. see Appendix A).
Minor : Add small arrows in Table 2 to indicate that Inception score is better when lower, and opposite for FID
Typo : page 5, section 3.3, paragraph 2 : relative —> relatively
Figure3 : Right panel : What are the 3 shaded curves ? This should be shown in the legend or at least in the caption
Figure3 : Right panel: In the x-axis it should be specified ‘Variance of q(x^{\tilde} \mid x)
Page 7 : paragraph 1 : 'Thus, it is hard to conclusively determine what is the best way of choosing q(x ̃|x).’ —> I think the authors actually give the key to properly choose the noise level (i.e. variance). It seems to depend on the task : if one wants to generate good samples, then the variance has to be set by heuristic. If one needs a good likelihood (e.g. for subsequent downstream tasks) then the variance could be optimized.
Figure 6 : On my understanding, the part ‘denoising’ is redundant with the section image generation. It is interesting to mention the denoising application, but I am not convinced of the utility of the figure 6.
Figure 7 : What is the corrupted input ? Which part of the input has been masked ??","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","Thanks for your nice work. I have some questions about Algorithm 5 in the paper. I can not get why the step size
ϵ
of the corrector in the VP SDE setting need to be multiplied by the
αi
, which does not appear in the VE SED setting.","",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","Our code and checkpoints are released at GitHub:
JAX + FLAX codebase (recommended): https://github.com/yang-song/score_sde
PyTorch codebase: https://github.com/yang-song/score_sde_pytorch
We additionally provide several colab notebooks to help people use our codebase and understand the basic ideas of this paper:
JAX + FLAX: Load our pretrained checkpoints and play with sampling, likelihood computation, and controllable synthesis. link
PyTorch: Load our pretrained checkpoints and play with sampling, likelihood computation, and controllable synthesis. link
Tutorial of score-based generative models in JAX + FLAX. link
Tutorial of score-based generative models in PyTorch. link","",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","Nice work! When can we expect the code release?","",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","All reviewers agree that this is a well-written and interesting paper that will be of interest to the ICLR and broader ML community.","All reviewers agree that this is a well-written and interesting paper that will be of interest to the ICLR and broader ML community.",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","We would like to thank all reviewers for providing high quality reviews and constructive feedback that have improved the paper. We are encouraged that reviewers think our paper “makes significant technical and empirical contributions to the emerging area of score-based generative models” (R3); our proposed model and sampling algorithms “offer substantial conceptual improvements to the existing models” (R4); our empirical evaluation is “extensive” (R4), “particularly well-done” (R3), “a nice evaluation of theoretical claims” (R2); and our writeup “well-written” (R1, R4), “well-structured” (R4) and “was able to follow all without having neural SDEs or Langevin Samplers as a core competence” (R1).
We have updated our draft to further improve the writing and incorporate suggestions from reviewers, extended the appendix with more details for reproducibility, and will be releasing code and model checkpoints. Below, we summarize changes made in the updated submission.
A. New toy example figure
We have added a new figure (Fig. 2 to Section 3) as suggested by R2. It depicts the SDE and ODE trajectories for transforming a one-dimensional toy data distribution to a standard Gaussian. We believe it provides a good overview of our score-based generative modeling framework with SDEs, and contrasts the trajectories of the SDE and ODE.
B. Clarifying how our general SDE framework encapsulates VE/VP SDE.
In response to R1 and R4 on how our framework unifies VE and VP SDEs, we have improved the text in Section 3. We now present our general SDE framework before the introduction of VE and VP SDEs, and make it more clear that they are two particular instantiations.
C. Improved comparison of different samplers
We have improved the clarity of Table 1, provided additional results, and highlighted the computational cost of each sampler. These results show that adding 1 corrector step for each predictor step (PC1000) doubles computation but always improves performance (against P1000). Moreover, it is typically better than doubling the number of predictor steps (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for SMLD/DDPM models. Depending on the interpolation method, predictor-corrector methods may uniformly outperform predictor-only samplers with the same computation (newly-added results in Table 4, PC1000 vs. P2000).
We also improved the results of corrector-only samplers (C2000) by using 2 corrector steps per noise scale, instead of 1 corrector step with interpolated noise scales. Predictor-corrector samplers (PC1000) still perform uniformly better than corrector-only ones.
D. Additional details on probability flow ODEs
We have polished the writing of Section 4.3. In addition, we added Appendix D, where we provide a detailed derivation of the probability flow ODE (Appendix D.1), full description of likelihood computation (Appendix D.2), detailed description of sampling with probability flows (Appendix D.3 and D.4), as well as additional experimental results for uniquely identifiable encoding (Appendix D.5, Figs. 8 and 9).
E. Updating results of previous work in Table 2 and 3.
StyleGAN2-ADA authors updated their FID and inception scores after our paper submission, and we have incorporated these new results into Table 3. Our results are still state-of-the-art, though the gap is smaller. We also added results from NCSNv2 to Table 2, and results of RealNVP and iResNet to Table 3.
F. Inpainting and colorization on
256×256
images.
We have included inpainting and colorization results on
256×256
images from LSUN bedroom and church_outdoor. We updated Section 5, Fig. 5 and provided extended samples in Figs. 13-16 in Appendix I. These results present the same qualitative findings as the original submission with CIFAR-10 figures, but highlight that the method works well on higher resolution images too!","",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","This paper generalizes a family of score-based generative models that rely on sequences of noise scalings of the data and extends them to the continuous domain, which leads to an SDE-based framework. By using score-matching, the forward SDE, which transforms data into a tractable noise distribution, can be reversed and thus used as a generative model. This is then improved by employing a two-phase algorithm with a prediction step, followed by a tunable number of correction steps. Further, reformulating the problem as a neural ODE allows for exact likelihood computations and reduces the number of required function evaluations. The framework enables unconditional, as well as conditional samples.
I find the paper to be very well written and straightforward. As someone who does not have neural SDEs or Langevin Samplers as a core competence, I was able to follow all of the writeup, which is remarkable. I think the framework is nice and there is substantial novel innovation to justify accepting this paper. The experiments are convincing.
A few questions and remarks:
You claim that you unify current methods into a common framework. While I see that you attempt to do this (i.e. putting the algorithms side-by-side, etc.), but in essence, you still handle VE and VP SDEs separately throughout. My suggestion would be to either really try to unify them into a single formulation, or alternatively, tone down the claims of unification, maybe just say that you show commonalities.
In Figure 2, you claim that the results are best when computation is ""split"" between the predictor and corrector. However, this is a very imprecise statement. An equal split would mean just 1 step of corrector, but I don't see clear evidence that that's best. Do you have numerical evidence that a 1-to-1 split is best, or what do you mean by ""split""? Otherwise, you could just say that M is a tunable hyperparameter.
Also in Figure 2, it seems that there is a clear shift at some point where the samples go from low to high quality. Do you have any numerical indication (without looking at a test set, FID, etc.) of how a practicioner could notice that running for more steps would or wouldn't help?
In Table 1, what is the last row? I guess it's just employing the corrector, but maybe a label for the row would be nice.
Also in Table 1, it looks like the corrector helps for VE SDEs, but hurts for VP SDEs. Do you have an explanation for this? Maybe it's somewhere in the text, but if it is, I've missed it, so maybe point me to it.
Given that you can compute exact likelihoods, is it possible to compute the exact NLL for any real dataset, like a test dataset?","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","Summary: This paper presents a generative model based on stochastic differential equations (SDEs), which generalizes two other score-based generative models score matching with Langevin dynamics (SMLD) and denoising diffusion probabilistic modeling (DDPM). The recipe to sample from the data distribution is based on (i) the observation that both SMLD and DDPM can be formulated as the discretization of an SDE, (ii) the finding from Anderson (1982) about the reverse of an Ito process, (iii) a score model. A novel aspect of the presented technique is the use of the score model as ""predictor"", which gives the initial sample from the MCMC sampler that serves as ""corrector"". Finally, the Ito process induced by the reverse SDE is formulated in a deterministic manner, leading to a neural-ODE based generative model.
Pros:
The authors did a good job at showing connections between the previous score-based generative models and their model. I believe on its own this is a nice contribution.
The method is thoroughly analyzed. I went through the derivations and didn't find any errors.
Experiments show that combining the predictor and corrector routines leads to better performance, a nice validation of the theoretical claims.
As promised, the model achieves SOTA on several tasks.
Cons:
I'm having difficulty seeing the transformation of the reverse SDE into an ODE (from eq.10 to eq.12). Is it as simple as multiplying the second term with 1/2 and discarding the Brownian motion? Also, eq.12 is a simple ODE system, which has nothing to with a process as far as I understand. Maybe more explanation or pointers in Maoutsa et al., 2020 would be nice.
The paper lacks the discussion on the benefits/downsides of different SDE solvers, discretization time steps, etc.
As such, the paper lacks a ""toy example"" experiment, for example, on a simple 2D dataset like half-moons. A visual demonstration of the SDEs and probability flow (maybe corresponding vector fields and Brownian motion over time) would be interesting.
Additional comments:
A typo (intead) right below eq.9
Best performing rows can be bold in Table 1.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","The paper looks even better at the moment. I also liked Figure-2 very much in the sense that both stochastic/deterministic forward/backward flows are visualized. Great contribution!","",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","SUMMARY
The submission proposes a score-based generative model, which uses an SDE to map the data distribution to a simple noise distribution and the corresponding reverse-time SDE to generate samples by mapping the noise to the data space. The proposed model builds upon and generalises two existing models (SMLD and DDPM) by transforming the data using a continuous SDE dynamics as opposed to perturbing the data with a finite number of noise distributions utilized by these models.
##################################################################
REASON FOR SCORE
The paper provides a clear motivation for the proposed modifications to SMLD and DDPM, as well as a clear technical description of these modifications, their analysis and discussion. I think the proposed model and sampling algorithms offer substantial conceptual improvements to the existing models and should be of interest to the community. The paper is well written and structured.
##################################################################
PROS
Clear motivation for the work.
Detailed technical description of the proposed models.
Interesting discussion of similarities and differences between SMLD, DDPM, and the SDE based model, as well as corresponding sampling algorithms.
Extensive experiments.
##################################################################
CONS
I found the discussion of equivalent neural ODE and its differences to reverse SDE a bit short, especially given that it is used in multiple experiments.
The case of using general SDEs (not only those derived from SMLD and DDPM) is mentioned only briefly, leaving it unclear if using a general SDE would require relatively simple changes, or if the proposed model is limited to SDEs derived from SMLD and DDPM.
##################################################################
QUESTIONS and COMMENTS
Is it correct that the function \sigma(t) in Eq. (6) is assumed to be monotonic and bounded by \sigma_max, while \beta(t) in Eq. (8) is bounded by 1, but doesn't have to be monotonic?
In the case of general SDE for noise perturbations in Eq. (9), are there any assumptions on drift and diffusion function (such as monotonicity or boundedness)?
In the case of SDEs (6) and (8), \nabla_x p_{0t} (x(t)) is available in closed-form. Is it always necessary to have such a closed form expression in order to compute the objective (11), or can it be estimated somehow without it? (I guess for a general SDE, there is typically no closed-form \nabla_x p_{0t} (x(t)) available)
Why do you think the FID values for the PC sampler with corrector are higher than without it for VP SDE? (Table 1b)
In section 4.2: ""[...] PC samplers significantly outperform the corrector-only method, and can improve over predictor-only approaches for most cases without extra computation."" Why does full PC sampler (with predictor and corrector) not incur extra computation in comparison to prediction-only approaches. Don't we need to evaluate the approximate score function s_\theta(x, i) for each of the M steps in the corrector sampler?
In section 4.3: ""[...] deterministic process whose trajectories induce the same evolution of densities"". Does it mean that ODE (12) and reverse SDE (10) map the same noise distribution p(x(T)) to the same distribution in the data space? If so, are there any advantages of using a reverse SDE instead of equivalent ODE if the latter is easier to solve numerically and admits an exact computation of likelihoods?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","Q: Why do you think the FID values for the PC sampler with corrector are higher than without it for VP SDE? (Table 1) Why does a full PC sampler (with predictor and corrector) not incur extra computation in comparison to prediction-only approaches?
A: We have updated Table 1 with new results and better presentation (see C in this response). For both VP and VE SDEs, adding one corrector step always helps (see PC1000 vs. P1000) but also doubles computation.
To match the computation, predictor-only methods need to evaluate the score-based model at more noise scales than those seen at training, which can be accomplished by interpolating between noise scales in an ad hoc way. The performance of predictor-corrector vs. predictor-only with doubled noise scales (PC1000 vs. P2000) depends on many factors, such as the predictor, SDE, and interpolation methods. We experimented with two interpolation strategies, the first leads to results in Table 1 and the latter leads to the newly-added Table 4. For VE SDE, predictor-corrector uniformly outperforms predictor-only with doubled noise scales. For VP SDE results in Table 1, even though predictor-corrector has slightly worse performance than predictor-only for some predictors, it can achieve better performance than all other samplers when using the probability flow predictor; while for VP SDE results in Table 4, predictor-corrector samplers are uniformly better.
These results indicate that predictor-corrector methods (PC1000) can be beneficial for both SDEs to various extent, even compared to predictor-only methods with doubled noise scales and matched computation (P2000).
Q: Does it mean that ODE (12) and reverse SDE (10) (now Eq. (6)) map the same noise distribution
pT(x(T))
to the same distribution in the data space? What’s the advantage of reverse SDE?
A: Yes. Theoretically they map the same noise distribution to the same data distribution. In practice solving the reverse SDE for sampling may lead to better performance. For example, in Table 2, the black-box ODE sampler obtained an FID of 3.77 (DDPM (probability flow)), whereas the ancestral sampling sampler (a predictor-only sampler that solves the reverse VP SDE) obtained an FID of 3.17 (DDPM (
Lsimple
)). In general, we empirically find that more accurately solving the reverse SDE or ODE does not always lead to improvements in sample quality and different approximate sampling schemes (such as including a corrector) could potentially improve performance.","",""
"Score-Based Generative Modeling through Stochastic Differential Equations","Keywords: generative models, score-based generative models, stochastic differential equations, score matching, diffusion","Summary and contributions
This paper proposes a generalized framework for score-based generative modeling (SBGM). The proposed method subsumes previous SBGM techniques of score matching with Langevin dynamics (SMLD aka NCSN) and denoising diffusion probabilistic modeling (DDPM) and shows how they correspond to different discretizations of Stochastic Differential Equations (SDEs). The continuous-time SDE generalizes the idea of a finite number of perturbation kernels used by previous methods to a continuum of them. The authors propose a forward SDE that transforms the data distribution into a known noise distribution and the corresponding reverse-time SDE that converts samples from this noise distribution to the data distribution. A predictor-corrector sampling framework is studied that leads to improved performance of both NCSN and DDPM frameworks. The paper also shows the equivalence of the proposed SDE to Neural ODEs which allows exact computation of the log-likelihood using the continuous change of variables formula. Quantitative experiments on the CIFAR10 dataset show that the proposed framework leads to significant improvements over previous SBGMs. Qualitative results on the CelebA-HQ dataset demonstrate the ability of the method to scale to high resolution images.
Strengths
This paper makes significant technical and empirical contributions to the emerging area of score-based generative models. The generalized SDE framework subsumes recent works in this area and is also connected to Neural ODEs, enjoying exact likelihood calculation, which may be relevant to the normalizing flows and generative modeling community is general. The empirical evaluation is particularly well-done. It bridges the gap between the performance of NCSN and DDPM models leading to state-of-the-art performance. The authors also demonstrate the ability of the method to generate high quality images of human faces when trained on CelebA-HQ dataset. Preliminary experiments on class conditional generation, imputation, and image colorization demonstrate the wide applicability of the proposed method.
Weaknesses
The paper does not suffer from any obvious weaknesses. The quantitive experiments could be strengthened by the addition of results on another dataset but the empirical evaluation is sufficient in its current state.
Additional feedback
Questions:
In equation 11, how is the weighting function
λ
chosen?
In equation 11, apart from being able to sample from the transition kernel, it should also have a closed-form density for the evaluation of the score. Is my understanding correct?
In equation 21, should there be a discretization step-size corresponding to
Δt
?
In table 1 (a), why does SMLD with corrector only perform so poorly? As far as I understand it is equivalent to NCSN. Can the authors clarify if I misunderstood something?
Post Rebuttal: I thank the authors for clarifying on my questions and updating the manuscript.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Global Convergence of Three-layer Neural Networks in the Mean Field Regime","Keywords: deep learning theory","This paper provides a global convergence guarantee for feedforward three-layer networks trained with SGD in the MF regime. By introducing the novel concept of neuronal embedding of a random initialization procedure, SGD trajectories of large-width networks are shown to be well approximated by the MF limit, a continuous-time infinite-width limit (Theorem 3). Furthermore, under some additional assumptions the MF limit is shown to converge to the global optimum when the loss is convex (Theorem 8, case 1) and for a generic loss when
y=y(x)
is a deterministic function of input
x
(Theorem 8, case 2). The global convergence guarantee presented in this paper is based on less restrictive assumptions compared with existing studies. All the reviewers rated this paper quite positively, with less confidence however, seemingly because of mathematical thickness of the proofs. Although the reviewers did not manage to check every detail of the proofs, they agreed that the reasoning seems mathematically sound as far as they can tell. The authors response adequately addressed minor concerns raised by the reviewers. I am thus glad to recommend acceptance of this paper.
Pros:
Introduces the idea of a neuronal embedding, which allows establishing relation between SGD on large-width three-layer networks and its MF limit in a quantitative way with a less restrictive setting.
Provides a global convergence guarantee under the iid initialization, in the sense that if the MF limit converges it attains the global optimum.
Shows that the global convergence guarantee does not require convexity of the loss when a deterministic function is to be learned.
In particular, the uniform approximation property, rather than the convexity of the loss, plays a crucial role in proving the global convergence guarantee (it allows translation of the vanishing gradient in expectation at convergence into the almost-sure vanishing gradient), which is a quite original contribution of this paper.","This paper provides a global convergence guarantee for feedforward three-layer networks trained with SGD in the MF regime. By introducing the novel concept of neuronal embedding of a random initialization procedure, SGD trajectories of large-width networks are shown to be well approximated by the MF limit, a continuous-time infinite-width limit (Theorem 3). Furthermore, under some additional assumptions the MF limit is shown to converge to the global optimum when the loss is convex (Theorem 8, case 1) and for a generic loss when
y=y(x)
is a deterministic function of input
x
(Theorem 8, case 2). The global convergence guarantee presented in this paper is based on less restrictive assumptions compared with existing studies. All the reviewers rated this paper quite positively, with less confidence however, seemingly because of mathematical thickness of the proofs. Although the reviewers did not manage to check every detail of the proofs, they agreed that the reasoning seems mathematically sound as far as they can tell. The authors response adequately addressed minor concerns raised by the reviewers. I am thus glad to recommend acceptance of this paper.
Pros:
Introduces the idea of a neuronal embedding, which allows establishing relation between SGD on large-width three-layer networks and its MF limit in a quantitative way with a less restrictive setting.
Provides a global convergence guarantee under the iid initialization, in the sense that if the MF limit converges it attains the global optimum.
Shows that the global convergence guarantee does not require convexity of the loss when a deterministic function is to be learned.
In particular, the uniform approximation property, rather than the convexity of the loss, plays a crucial role in proving the global convergence guarantee (it allows translation of the vanishing gradient in expectation at convergence into the almost-sure vanishing gradient), which is a quite original contribution of this paper.",""
"Global Convergence of Three-layer Neural Networks in the Mean Field Regime","Keywords: deep learning theory","This article is concerned with convergence guarantees of online stochastic gradient descent for a rather generic class of three layers neural networks (instead of similar analyses that treated two layers). The main results state that in a proper limit of infinite width + vanishing learning rate, the dynamics of online SGD is proven to be tracked thanks a mean-field description in the form of coupled ordinary differential equations. Once this mean-field description at disposal, the main result is obtained: in the infinite width + vanishing learning rate + infinite time (= number of training samples), the generalization error tends to it minimal value for a broad class of models and losses (not necessarily convex, which is a novelty of the work) as well as generic data distribution.
Overall this paper is very well written, enjoyable to read despite the technicality of the results, and understandable even for non-specialists of this line of works (like myself). I did not check the appendices and proofs. In the main part there are no typos, and I have no main concerns to bring about. Yet: I would find interesting to know more details about the differences with the refs Nguyen (2019); Araújo et al. (2019); Sirignano & Spiliopoulos (2019); that is not clear. Also I would find useful to have some hints about the meaning of the (trained third layer) hypothesis in Theorem 8. Finally I find a bit surprising that there are nor restrictions whatsoever on the data distribution (or I missed that). The authors may comment on that in the final version.
I recommend publication. Even if I'm not a specialist, it is obvious that the authors made a big effort of redaction, that the results are very solid, the proof technique seems original and requires less assumptions than previous works (I liked very much the ""idea of proof"" part). I have very few doubts about the quality of the paper despite I did not read the proof details, and the fact that I'm not aware of the literature in this specific field.","9: Top 15% of accepted papers, strong accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Global Convergence of Three-layer Neural Networks in the Mean Field Regime","Keywords: deep learning theory","Summary: Analysis of neural networks in the mean field regime gains more and more attention as it helps to study the dynamics in the wide regime. The paper extends the recent studies and provides global convergence guarantees for an unregularized feedforward three-layer NN. This is the first time global convergence is established for neural networks of more than two layers in the mean-field regime.
I find the writing a bit chaotic and overdosed with notations. But, overall, I think the results are significant and will help to extend the line of research in the mean-field regime applied to neural networks.
Questions:
In the paper, it is said several times that the convergence result “does not rely critically on convexity”. What do you mean by “critically”? You still assume the convexity. Do you mean that it can be easily relaxed in future works? I think it should be better stated throughout the paper.
Minor suggestions:
Mean field -> mean-field
Section 2.1 “k” is introduced at the very end. Maybe it would be better to add in the beginning, e.g. “the following network at time k”?
I would say “W(k) consist of the weights …” instead of “W(k) is the weight with ….”
It perturbs me a bit that the difference between NN notations and MF is its boldness. Boldness usually means a vector while in authors’ notations
w2
is an element. But I guess with superscripts or symbols like
w^
it would be heavier in notations...
Section 2.2. I would like to see the description of \Omega, F and P in the beginning. Like “Given a NN \Omega_i would be a space of…”. Just to connect it from the start and to easify the following read.
Definition 2. “The following hold” -> holds
Section 4.2 “where V a set…” -> is
“Helps avoiding” -> helps to avoid","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Global Convergence of Three-layer Neural Networks in the Mean Field Regime","Keywords: deep learning theory","This paper studies the behavior of a 3-layer fully connected network when the width of the network is large. The authors define a mean field regime and prove that the behavior of the network under stochastic gradient descent converges to this mean field regime (for any finite time horizon). This result complements nicely previous works like (Nguyen 2019) that contain an informal derivation of the mean field regime. This transient regime is complemented by a long-term analysis under quite restrictive assumptions (which imply essentially than the mean field regime always converge to the minimizer of the loss function).
I did not check all details of the proof but the approach seems mathematically sound. Once the model is defined, the proof for the finite regime is relatively classical: it relies on Martingale concentration plus Gronwall's lemma. Yet, as always, the devil being in the details and defining the right model and using the right notations is a difficult task. The result of the stationnary regime seems also reasonable but I must admit that the proof of the infinite horizon is not really clear to me. I would have appreciated more pedagogical effort from the authors.
To summarize, the paper seems a nice theoretical contribution. Yet, to me one thing that this paper is missing is an explanation or illustration of how useful is their result to understand the behavior of deep neural networks.
That being said, one major concern that I have about the paper is the link with https://arxiv.org/pdf/2001.11443.pdf The arXiv paper considers a very similar model (but more general as it considers L layers instead of 3). It uses almost the exact same notations and the same structure (overall paper and proofs). I think that the authors should clarify the link between the two papers. Also, if I can admit that the present paper is a resubmission of the arXiv paper, I do not understand why does the current paper focus on 3 layers and not the more general model of the arXiv paper.","7: Good paper, accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Global Convergence of Three-layer Neural Networks in the Mean Field Regime","Keywords: deep learning theory","This paper studies some theoretical properties of three-layer neural networks (NNs) under the mean-field (MF) regime. The authors proposed neuronal embedding in order to study large-width neural networks. Then, the quantitative relation between finite-width NN and the MF limit was clarified. The global convergence of the continuous limit of the stochastic gradient descent (SGD) was proved without assuming the convexity of the loss function. The global convergence of the MF limit is used to establish the optimization efficiency of the neural network with SGD.
The problem considered in this paper is important. The definitions and the statement of theorems are clearly described. In order to prove the global convergence, the authors assumed the uniform approximation property rather than the convexity of the loss function. This approach is interesting. Though I'm not very familiar with the MF regime, the high-level idea to prove the theorem is well-written. I read some proofs in the appendix, and I found that the description is accessible to a wide range of audiences. Overall, this paper is thought to provide a promising idea to analyze not only three-layer NNs but deep NNs.
Some comments are shown below:
Neuronal ensemble is introduced to analyze the dynamics of the MF limit. Apart from the MF regime, is there any similar idea of the neuronal ensemble? Showing some references would be beneficial to readers.
The constant K appears in the upper bound in Theorem 3. Showing a more concrete expression of K is informative. What is the typical K in this case?
In Theorem 8 and Corollary 10, the global convergence was proved. If the convexity was also assumed in addition to the assumptions in the theorems, is it possible to derive the convergence rate?
In the paper, the global convergence of three-layer NNs was analyzed. What is the main obstacle to investigating the global convergence of multi-layer NNs or deep NNs according to the idea proposed in this paper?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Rethinking Architecture Selection in Differentiable NAS","Abstract: Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efficiency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of architecture parameters reflect the operation strength. While much has been discussed about the supernet's optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. We propose an alternative perturbation-based architecture selection that directly measures each operation's influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet.","Hi everyone,
The code for this paper can be found at https://github.com/ruocwang/darts-pt (not indexed by the Google search engine for some reason).","",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","Dear authors,
I have difficulties understanding how you have evaluated the operation strength when there are more than 2 operations in the search space. In the case of DARTS search space where there are 8 operations, based on your proposed algorithm, you select an edge randomly, and remove one operation each time from the operation set, then how you evaluate the network accuracy with 7 remaining operations? Do you perform similar to DARTS where they choose the operation with the maximum value of alpha, in this case, the maximum among 7 remaining operations?
Thanks in advance,","",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","Thank you Authors for your reply.
I have read your paper quite a few times but it was not very clear to me how the process of computing validation accuracy is done. From your description here, I would conclude that you ignore all alpha values, and for all edges on the graph, you select every possible operation one by one, train the graph from scratch for each operation, and compute validation accuracy. If that's true, then what is the advantage of using darts to compute alpha values? Isn't it an extensive training/ validation process using all possible combinations of edge connections and operations?
One other request, when do you expect to share your code publicly?
Thanks again for further clarification,","",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","This paper proposes a new selection paradigm for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one-shot model and that deploy gradient-based methods for the search. Basically, the paper focuses on examining the max selection very closely and found the magnitude of architecture weights are misleading. Instead, the paper proposes much more intuitive finalization step, pick the operator that has the largest drop in validation if the edge is removed. All reviewers agreed that the idea is interesting, the paper is well-written, and the results found in the paper are interesting. In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score. Therefore, I recommend acceptance.","This paper proposes a new selection paradigm for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one-shot model and that deploy gradient-based methods for the search. Basically, the paper focuses on examining the max selection very closely and found the magnitude of architecture weights are misleading. Instead, the paper proposes much more intuitive finalization step, pick the operator that has the largest drop in validation if the edge is removed. All reviewers agreed that the idea is interesting, the paper is well-written, and the results found in the paper are interesting. In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score. Therefore, I recommend acceptance.",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","We thank all four reviewers for the detailed reviews and suggestions. We have made the following changes to the paper in our revision, following the comments from our reviewers. All major revisions are colored blue.
Moving the experiments on Darts+PT (fix $\alpha$) from discussion to section 6.3 (suggested by reviewer 1)
Extra figures like Figure 1 in Appendix A.5 (suggested by reviewer 2)
The trajectory of Darts+PT (fix $\alpha$) on NAS-Bench-201 in Appendix A.6 (following comments by reviewer 2)
Ablation study on the number of fine-tuning epochs in Appendix A.7. (suggested by reviewer 1)
ImageNet experiment in Appendix A.8 (following comments by reviewer 3)
Minor fixes (suggested by reviewer 1 and reviewer 2)
We also attached the appendix to the pdf submission for ease of reference (was in the supplementary material folder).","",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","-- Short Summary --
This paper proposes a new policy for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one-shot model and that deploy gradient-based methods for the search. The proposed algorithm sequentially prunes and fine-tunes the one-shot (aka supernet or weigh-sharing) model until the operations that contribute the most in the one-shot validation performance remain at each edge of the cell (represented as a DAG).
-- Detailed comments --
Positive points:
The structuring of the sections.
The paper investigates an interesting aspect of gradient-based NAS algorithms.
I liked the theoretical justification on the issue with the high \alpha values for skip connections in many DARTS [1] and the experimental backup for that in table 1.
Issues and concerns:
The proposed algorithm does not seem that elegant and still uses another proxy for true performances of stand-alone architectures, even though indirectly in a lower level, i.e. the drop in performance of the one-shot model after removing one operation at a time in each edge is used to assess the optimal operations operation that will be present in the final architecture. Furthermore, this procedure seems to induce quite some variance by randomly sampling the edges in the cell and then executing the operation drop & fine-tuning steps. Did the authors investigate this by running multiple times their proposed procedure on the same one-shot model?
Another issue with the algorithms seems to be that it does not scale well with the number of operation and edges in the cell.
I am not fully convinced by the experiment conducted in 3.1. It seems the authors discretize only one edge of the supernet and fine-tune it further with the discretized edge. In this case the other edges (not discretized) contribute to the network performance together with the single discretized edge and this might be misleading when assessing the importance of an operation. Ultimately, we care about the final network performance, and how this single operation in the discretized edge would perform when combined with all other possible choices in the other edges (still intractable to compute of course). Another potential issue is that the fine-tuned supernet might not correlate well with the discretized stand-alone architecture performance. Can the authors please elaborate on this experiment further in more details?
Do the theoretical and experimental backup for that hold for other sampling-based methods, e.g. GDAS [2] or SNAS [3]?
I think the reported search costs in Table 2 are inaccurate. The authors should also add on top of their method the costs of the base algorithm (e.g. 1GPU day for DARTS 2nd order on one GTX 1080Ti).
Minor:
There are a lot of grammatical errors throughout the text. I would recommend to do a detailed proof read of the paper.
section 3.2, second paragraph: The acronym VGG does not stand for vanilla networks
The data in Fig. 1 would look better as a scatter plot, which would also more nicely show the miscorrelation between the 2 quantities that are being compared.
I like the structuring of the paper, however I think the experiment in the conclusion should not be there, but in the benchmark tables.
In conclusions: why do you fix \alpha = 0? Did you mean \alpha = 1?
-- References --
[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR 2019
[2] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In CVPR 2019
[3] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR 2019","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Rethinking Architecture Selection in Differentiable NAS","-''-","I thank the authors for their detailed reply and for updating their paper with the proposed suggestions. This paper shows a fundamental flaw in the designing of one-shot NAS algorithms and a simple algorithm on how to circumvent that. I am increasing my score to Accept.
Further suggestions (minor):
""Darts"" --> ""DARTS""
It would be useful to include the cost analysis somewhere in the paper.","",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","Summary:
In one-shot differentiable NAS, a supergraph is usually trained (via bilevel optimization as in DARTS, or other approximations to bilevel such as gumbel softmax, etc). After supergraph training, a final architecture is obtained by taking the operator at each edge which has the highest architecture weight magnitude. This step is usually termed as the 'finalization' step. (In DARTS the finalization step actually orders incoming edges by the max of the architecture weight magnitudes at each edge and selects the top two edges and the corresponding maximum architecture weight in them as the final operators.). This paper examines this quite ad hoc step very closely. It finds that the magnitude of architecture weights (alphas commonly in this niche literature) are misleading. It shows by careful ablation experiments that alpha magnitudes are very much not useful in selecting good operators.
By taking inspiration from the ""unrolled estimation"" viewpoint of ResNet prior work it shows that DARTS converging to degenerate architectures where alphas over parameters operators like skipconnect is actually to be expected when finalization step relies on the magnitude of alpha.
The paper proposes a much more intuitive finalization step which just picks the operator at each edge which if removed from the supergraph results in the largest drop in validation accuracy. To bring back the supergraph to convergence a few epochs of further training is carried out between operator selection.
Experiments show that just by carefully thinking about the finalization step in differentiable one-shot NAS, one can obtain much better performance. In fact, one does not even need architecture weights at all! Don't worry about complicated bilevel optimization, gumbel softmax approximation, etc. Just train a supergraph and pick operators progressively.
Comments:
The paper is wonderfully written! Thanks!
As I read a paper I try to think without looking at the experiments, what set of experiments I would try to run to prove/disprove the hypotheses proposed. Afterwards I go through the experiments and see if those experiments were actually run (or if they differed why). In this case, every experiment and more were already run. Particularly towards the end I was thinking what if we just got rid of all the alphas and just trained a supergraph as usual and did the PT finalization as proposed. And lo and behold, it actually works better!
This paper is actually throwing a big wrench in one-shot differentiable NAS literature. Many papers are being written which try to improve/fix DARTS and DARTS-like methods. If I were to believe the experiments, I don't actually need to do any of that. I have some questions I hope to discuss with the authors:
Is all the complicated bilevel optimization (often popular as 'metalearning' currently) not useful in the case of NAS? (This is not really the authors' burden to answer but I am just hoping to see if they have any insights.)
Can we view the PT finalization step as a progressive pruning step? So if I were to turn this into a method which produces a pareto-frontier of models (e.g. accuracy vs. memory/flops/latency etc), we first train a big supergraph and then progressively prune out operators one at a time as proposed here and take a snapshot of the supergraph and plot it on the x-y plot (where say x is latency and y is accuracy) and pick the ones clearly on the pareto-frontier and train them from scratch? (Again not really authors' burden but curious if they have any insights)
Figure 4 suggests that training the supergraph anymore than 20 epochs only hurts performance (no matter which finalization procedure is used, of course PT has far less of a drop). Does bilevel optimization actually hurt with weight sharing?","10: Top 5% of accepted papers, seminal paper","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Rethinking Architecture Selection in Differentiable NAS","-''-","I agree with most of the above statements and thanks for the clarification on Figure 4.","",""
"Rethinking Architecture Selection in Differentiable NAS","-''-","post rebuttal
I have no further concerns and increase the rate to accept.
Summary
This paper identifies an interesting phenomenon that on DARTS based method, the operation can not be simply chosen based on the maximum value of trained weights. The authors propose a new selection paradigm. For each operation, one should first discretize the soft weights encoding into a one-hot, then fine-tune the network for some iterations, and use the performance as a metric to select the operation. It provides an interesting and novel analysis on the question of why DARTS always converges to skip-connections. The experiments are conducted on CIFAR-10, 100, and SVHN on DARTS space and NASBench-201 over three datasets.
Strength
I enjoyed reading this paper although there are some flaws in terms of presentation. It tackles one of the most critical problems in the DARTS domain, why the skip-connection dominates after the super-net training converges. All earlier works propose some solutions in an ad-hoc manner, by early stopping. This is the first time I have seen a simple and reasonable explanation of this phenomenon, and by itself is a great contribution to the NAS community. The toy example constructed in section 3 Figure 3 also evidences this explanation, though the theoretical part only discussed a simple case with two operations.
Weakness
I am looking forwards to hearing back from the authors and improve my scores if these issues are fixed/explained.
Effectiveness of the method
The proposed solution is simple yet effective, though there is not another analysis of why discretization and fine-tune will triumph except comparison with baselines. On page 6 we can see the perturbation based method, test accuracy drops for both your method and the baseline after 20 epochs. Does it mean the current approach is still not working well? Training for longer simply destroys the search and without proper training, the network is essentially in a random search state, i.e. it goes back to the game that DARTS do not out-perform random search. This trend is consistent over three datasets.
Other questions
Ablation on the finetuning. In the algorithm part, you mentioned to train for a few epochs, but do you have some rule for that? I saw you doing this on DARTS space (Figure 6), but will this be more reasonable on NASBench-201?
Table 1: Will it because the swapped edges belong to the same node? For example, you could control the swap that it only happens, e.g. edge 0->2 and 2->4?
Discretization accuracy is not clearly defined. Say for the operation weights in DARTS, [0.2, 0.4, 0.2, 0.2], does this mean you will directly evaluate the accuracy over [0, 1, 0, 0] given the supernet? Or first, make it one-hot and train to convergence while using the original super-net as a warmup? (I think I found it on page 3 after reading it again but please also help me to confirm.)
Figure 1: NAS methods are not stable over one set of training, could authors provide another visualization, say training the darts super-net for multiple times, and shows the average of these training the alpha and its discretization accuracy? Showing three selected edges is not convincing.
suggestions on the presentation
Section 2.1, 2.2 and 2.3 can be a \paragraph instead of subsections. They are too short.
Grammar issues
Abstract: one of the most ... methods (s)","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking Architecture Selection in Differentiable NAS","-''-","Pros
The work analyzes the differential NAS methods from a new perspective that the value alpha is not suitable for selecting edges. Based on this observation, the author proposes a new method to evaluate the edge strength.
The motivation is clear and the finding of the residual path is interesting.
The proposed method is effective and shows good results.
Cons
All experiments are based on the Cifar10 dataset, the author may consider extending the proposed method to a larger dataset such as ImageNet.
The efficiency of the proposed method, what is the computational cost?
Some descriptions and figures are not very clear. E.g., in Figure 1, what do the three figures stand for respectively?
Except for the residual connection, can the same rules be found in other layers?
The paper is interesting and has found some values for the NAS community by rethinking the representation ability of the important factor $\alpha$. The paper may need proof-reading and do some experiments on a more widely used large-scale dataset. I tend to accept this paper.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Evolving Reinforcement Learning Algorithms","Keywords: reinforcement learning, evolutionary algorithms, meta-learning, genetic programming","This paper proposes a meta-learning algorithm for reinforcement learning. The work is very interesting for the RL community, it is clear and well-organized. The work is impressive and it contributes to the state-of-the-art.","This paper proposes a meta-learning algorithm for reinforcement learning. The work is very interesting for the RL community, it is clear and well-organized. The work is impressive and it contributes to the state-of-the-art.",""
"Evolving Reinforcement Learning Algorithms","Keywords: reinforcement learning, evolutionary algorithms, meta-learning, genetic programming","We thank the reviewers for their time and helpful feedback. We summarize the changes we made in response to the reviewers’ feedback, including newly added dataset with learning algorithms:
-- We have released the data for the top 500 algorithms for both learning from scratch and learning from bootstrapping experiments to enable further theoretical analysis of the discovered algorithms and justify the computational cost of obtaining them. This data contains the score for the algorithm and an image of the computational graph. The data contained in the supplementary zip file along with a README for how to parse it.
-- We have added results on Atari games, showing that one of the learned algorithms, DQNReg, outperforms baselines on all the Atari games that we tested. Table 1 summarizes the results.
-- We have revised the writing to acknowledge that Regularized Evolution falls under GP.
-- We have updated Table 3 in the appendix with more loss functions. The distribution of loss functions show that the top found algorithms are consistent with each other.
-- We have updated the writing to address various questions including the use of training return to score the algorithms and how CPUs are allocated during training.","",""
"Evolving Reinforcement Learning Algorithms","Keywords: reinforcement learning, evolutionary algorithms, meta-learning, genetic programming","This paper proposes a method of discovering, through evolutionary search, programatically defined RL algorithms.
Pros:
-- Novel (to my knowledge) in the search language it uses, which renders the resulting learned algorithms very interpretable and generalizable (as compared to, say, using a meta-learned loss function parameterized by a neural network)
-- Interesting analyses of particular meta-learned algorithms
-- Useful discussion of key implementation details (e.g. early hurdles, functional equivalence check)
Cons
-- It would be nice to see more analysis of the algorithms obtained ""from scratch"" rather than starting from DQN. Are any other (variants of) standard RL algorithms discovered? Are there any new but interpretable algorithms that perform well?
-- Comparison to other approaches to meta-learned loss functions (e.g. Kirsch et al., or Oh et al.) would be helpful
-- I realize it may be computationally infeasible to meta-train on many more environments, or more difficult tasks, but it would at least be good to see how the learned algorithms generalize to environments considerably unlike any seen during meta-training (e.g. not gridworld and not ""classical control"" -- perhaps Atari?)","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Evolving Reinforcement Learning Algorithms","Keywords: reinforcement learning, evolutionary algorithms, meta-learning, genetic programming","##########################################################################
Summary:
The paper introduces learning of RL algorithms via evolution. Over here, RL algorithms are represented as computational graphs where the graph outputs the objective of the algorithm to be minimized. Also, the graph comprises pre-defined symbols and operations. Given these computational graphs or RL algorithms, they use Regularized Evolution(RE) to evolve a RL algo. For evolution, each RL algo is evaluated with a set of training envs. Also, there are various checks introduced to avoid incorrect algos or re-evaluation of the same algo. For the mutation, the algo which performs best across all training envs is selected and it’s mutations are reintroduced in the population. These computational graphs can be initialized randomly or existing RL algos could be induced into them for bootstrapping.
##########################################################################
Reasons for score:
I vote for marginal acceptance. I believe the notions introduced in this work could be useful for the community and their results seem to indicate a promising direction.
##########################################################################
Pros:
They learn two new RL algos “DQN clipped” & “DQNreg” which happen to be better sample efficient and have better final performance than DQN. As per authors, these algorithms also share resemblance to recent CQL and M-DQN.
Promise of better algorithms than researchers can design given the coverage of all reasonable operators/symbols in the computational graphs.
They performed ablation study of with/ without bootstrapping of RL algorithms.
#########################################################
Cons:
Large number of CPUs required.
#########################################################
Questions:
How to make a choice of environments for meta-training?
Why are the authors using training return for performance of the Algorithm? After training for M episodes, they can simply evaluate the greedy policy for “k” episodes and that metric should be used for performance measure. Otherwise, we are using an epsilon-greedy metric for comparison which could be really low as that’s dependent on epsilon and environment.
It’s not clear if they are using constant epsilon or decaying epsilon?
Other comments:
Section 4.2, 2nd para: “The two-environment training setup is learns the known … “ => remove “is”","6: Marginally above acceptance threshold","3: The reviewer is fairly confident that the evaluation is correct"
"Evolving Reinforcement Learning Algorithms","Keywords: reinforcement learning, evolutionary algorithms, meta-learning, genetic programming","The paper proposes an approach to develop new Reinforcement Learning (RL) algorithms through a population-based method very reminiscent of Genetic Programming (GP). The authors ""evolve"" loss functions that can be used across different RL scenarios and providing good generalization.
Pros:
The paper is well written, well structured and clear at all times. There are few typos here and there but nothing that affects the readability of the paper.
The results are relevant: the fact that the method re-discovers recently proposed learning models is remarkable, as well as it finds new models indicates that it is a line of research worthy of further exploration.
The other minor contributions are also noteworthy, specifically the Functional equivalence check, which IMO is a brilliant idea, as well as the early hurdles approach.
The interpretation of the approach is also correct: the fact that authors make a clear distinction between learning from scratch and bootstrapping tells me that they truly understand the overall framework they based their method on.
Cons:
The main idea behind this type of meta-learning is always very interesting to revisit. Nevertheless, and truth to be told, the idea of using GP to find new loss (or ""objective/fitness"", as known in evolutionary computation (EC)) functions is quite old [1,2]. At some points the work presented here feels somewhat ""old-fashioned"", or a mere re-edition or adaptation of those early seminal works.
The results can be easily rebutted; if I understand correctly, the authors are presenting the results of a SINGLE run for every scenario they tested their method on (different no. of environments) , which tell us nothing about the average behavior of their proposed approach. Although this fact is understandable given such long training times (3 days with 300 cpus for a single run), many people will not accept the presented results arguing that they could have been the result of lucky runs. On the bright side, such practice is somewhat usual in the deep learning community, so many people may overlook it for now. I'd suggest the authors make a statement arguing why they feel confident that they approach may present a low variance, or why they still consider these results relevant, even if they require twenty or 30 runs to find results this good. They could argue that even if their method is currently statistically unreliable, it could probably be stabilized with many methods proposed in the EC community (such as spatially distributed populations [3], etc.)
The thing that bugs me the most of this work is that the proposed method is clearly a flavor of GP; however the authors never really acknowledge it by its name; instead they claim it to be something called ""Regularized Evolution"" which is supposedly introduced in a previous paper; nevertheless, it is really just GP: they are evolving tree-shaped programs, which is the hallmark of GP (there exist variants of GP that are not even EC-based [4]), so I don't see a reason for not calling it for what it is. I wish the authors clearly state why their approach cannot be considered part of the GP framework, or if it indeed is, then make such statement clearly.
In this vein, it is also interesting to note that they do not use a crossover operation. In the GP framework, crossover is generally regarded as a more powerful operator than mutation, and the combination of both operators can give the best results. My guess is that crossover is difficult to implement given the ""search language"" (or 'primitives', as known in GP), and then crossover would result in many invalid programs. Still, it would be nice if authors could explain a little bit on this issue.
Other comments: It draw to my attention that you use populations of size 300 as well as a 300 cpu system. It would seem that you intend to have one cpu for each individual evaluation; however, given the advances presented by the functional equivalence check and the early hurdles technique, I wonder what happens when a individual is no longer being evaluated.. is that cpu left idle? or is it reassigned to contribute to the evaluation of another individual (which sound complex to do, given the RL nature of the problems being treated)?
References:
Bengio, S., Bengio, Y., & Cloutier, J. (1994, June). Use of genetic programming for the search of a new learning rule for neural networks. In Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence (pp. 324-327). IEEE.
Trujillo, L., & Olague, G. (2006, July). Synthesis of interest point detectors through genetic programming. In Proceedings of the 8th annual conference on Genetic and evolutionary computation (pp. 887-894).
Tomassini, M. (2006). Spatially structured evolutionary algorithms: Artificial evolution in space and time. Springer Science & Business Media.
Hooper, D. C., Flann, N. S., & Fuller, S. R. (1997). Recombinative hill-climbing: A stronger search method for genetic programming. Genetic Programming, 174-179.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","Keywords: Differentiable rendering, inverse graphics, GANs","The paper proposes to bring together a GAN, a differentiable renderer, and an inverse graphics model. This combined model learns 3D-aware image analysis and synthesis with very limited annotation effort (order of minutes). The results look impressive, even compared to training on a labeled dataset annotation of which took several orders of magnitude more time.
The reviewers point out the novelty of the proposed system and the very high quality of the results. On the downside, R2 mentions that the model appears over-engineered and some important experimental results are missing. The authors’ response addresses these concerns quite well.
Overall, this is a really strong work with compelling results, taking an important step towards employing generative models and neural renderers “in the wild”. I think it can make for a good oral.","The paper proposes to bring together a GAN, a differentiable renderer, and an inverse graphics model. This combined model learns 3D-aware image analysis and synthesis with very limited annotation effort (order of minutes). The results look impressive, even compared to training on a labeled dataset annotation of which took several orders of magnitude more time.
The reviewers point out the novelty of the proposed system and the very high quality of the results. On the downside, R2 mentions that the model appears over-engineered and some important experimental results are missing. The authors’ response addresses these concerns quite well.
Overall, this is a really strong work with compelling results, taking an important step towards employing generative models and neural renderers “in the wild”. I think it can make for a good oral.",""
"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","Keywords: Differentiable rendering, inverse graphics, GANs","The authors provided a framework for inverse graphics, i.e., infer 3D mesh, light, and texture from a 2D image. They first use StyleGAN to generate realistic multi-view images, and then trained their model with a differentiable graphics renderer.
Pros
Training on data generated by StyleGAN is novel. It is easy to control the view of rendered images, but they usually do not look real. Real images look real, but we usually do not know the viewpoint (most annotated datasets are either small or have limited annotation quality). The authors proposed a method to generate images of the same category of objects with the same viewpoint (Figure 2), which addresses this issue -- the viewpoints are known, and the images look real.
The experimental part is impressive. 3D reconstructions look realistic, and the authors demonstrated the effectiveness to train on StyleGAN by comparing their model to a neural network trained on PASCAL3D+.
The way the authors generate data by StyleGAN is also novel. They empirically realized some latent code in StyleGAN controls the camera viewpoint, so that they only need to choose some latent codes that well spanned over the space. It is a wise and elegant way to control the viewpoints of an object in StyleGAN.
In summary, compared to previous methods, generating multi-view images from StyleGAN solves the unrealistic and unknown viewpoint issue, and the results look impressive.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","Keywords: Differentiable rendering, inverse graphics, GANs","This paper proposes to couple a GAN, an inverse graphics network, and a differentiable renderer. The authors base their work on StyleGAN, and use the observation that a specific part of the latent code corresponds to camera view-point to rapidly annotate a large amount of synthetic images with approximate camera pose. They then use these images and rough annotations to train the inverse graphics network to provide 3D and texture data. The differentiable renderer is used to synthesize 2D images from 3D, which can be compared to the input for consistency. In a second step, the authors use the inferred 3D data to disentangle the latent space of StyleGAN to allow to use it as a controllable renderer.
--- Strengths ---
The presented results look great. The inverse graphics network, seems to get both shape and textures mostly right. This is especially remarkable, given the small amount of supervision that was used. The controllable StyleGAN provides very plausible results. Most importantly, results on real images are shown, and indicate the the presented approach does make progress in brining inverse graphics networks to real images.
While the individual parts of this work are taken from other prior works, the specific combination of different components is novel and quite creative. Using a GAN to train an inverse graphics network is to the best of my knowledge novel. So is using an inverse graphics network to turn a GAN into a controllable renderer. There are also various key-insights that make the approach work, such as the that StyleGAN is partially disentangled with respect to view-point, or the use of multi-view consistency when training the inverse graphics network.
--- Weaknesses ---
The paper is overall well written, but leaves out some technical descriptions which make it not self-contained and hard to reproduce. Specifically, I'd like to ask the authors to elaborate more on the individual loss terms in Equation (1). Especially, the 3D-related losses
Llap
and
Lmov
are unclear.
While many results are shown in the supplement, I would have loved to see a video that shows more results. For example, rotating the obtained 3D models, or showing interpolations of various factors for the controllable renderer. Even if the results are not perfect, a video would help to judge the overall quality of the results.
--- Summary ---
This paper shows an interesting pipeline with good results. It presents some non-trivial observations and effectively leverages them into a complete system that looks quite impressive.
Typos:
Page 3, last sentence: ""conten""
--- Post rebuttal --- After reading the other reviewers comments and the rebuttal, I'm keeping my initial score.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","Keywords: Differentiable rendering, inverse graphics, GANs","This paper proposes to use StyleGAN images to train a differential renderer, then in turn to use this differential renderer to train a more controllable GAN (dubbed StyleGAN-R) which allows to perform independent manipulation of shape, background, and texture. There are many different things in this paper, and I am struggling a bit to decide what's the main contribution, here are the ones that seem the main one to me (note I am neither a GAN nor a neural renderer expert):
training a neural renderer with GAN images: to the best of my knowledge, that has never been demonstrated and is nice in itself. However, the way this is achieved is not completely satisfying: the entire idea is based on the empirical observation that the first part of styleGAN latent codes correspond to viewpoint; the training requires manual selection of ""good"" latent codes and annotation of the viewpoints (once, this is fast, but remains very unsatisfying and strongly limits the number of viewpoints); further image filtering is required using a trained MaskRCNN and masks are necessary for training. Additionally, while this is nice to see, there seem to be very little/no technical contribution in this part which is mainly engineering and empirical results
Another way to see the first contribution is to say that the paper demonstrate that using GAN generated images allows to improve over using datasets of manually annotated images. This has been an important hope as an application of GAN, and the beginning of the results section (4.1) seems to emphasize this aspect: less annotation time and higher quality than training a differential renderer on Pascal3D. Again, while this would make a very nice story, this is not related to any technical contribution. Additionally, it's hard to really trust the results on this, since the training of the networks are complicated and the results are hard to evaluate, it might be that the hyperparameters are simply better adapted to training with the StyleGAN training set than with Pascal3D (the presented AMT experiment is good for evaluation, but it is too expensive to do to use it to choose training hyperparameters)
StyleGAN-R for decorelated image manipulations. This is again very appealing, but done in an unsatisfying way and with limited results (only on cars). There are many tricks involved in making this work, from the choice of 144 (!?) dimension among the 2048 for the latent vector to the complicated stage-wise training. The results of this step look good, but are only shown for cars, since reconstruction was also done for horses and birds, I can only assume that this doesn't work for other categories.
To summarize, I think there are several cool stories in this paper and some appealing results. While the methods remain unsatisfying, the paper shows interesting proof of concepts, I would thus tend to accept it, but I could be convinced otherwise especially if other reviewers who know the field better than I do point to other papers demonstrating similar points.
I list bellow more specific issues/requests:
a. whether it works or not, I want to see the equivalent of figures 8-9-10 with other categories. This should also be discussed in limitations (or the paper should demonstrate it works). I am annoyed that this is not discussed more visibly in the paper and kind of hidden. No clear answer on this point would lead me to recommend rejection (but I could still recommend accept even if the method doesn't work very well on other categories)
b. I would like to see a comparison between StyleGAN (using the annotated viewpoints) and styleGAN-R for camera controler (figure 8, but using the code predicted by the approach before styleGAN finetuning - not the optimized code as in fig 7), that seems the natural baseline
c. I don't agree at all with the claim in 4.1 that the approach works with articulated classes: horses are indeed articulated, but looking at the horse dataset used (figure C in appendix) reveals that the selected images from styleGAN do not present any articulation (on the contrary to most real horse datasets). Thus the approach works simply because styleGAN do not present the same diversity as natural images, and would not work for actual articulated horses, this is more a limitation that a strength.
d. fig 7 comparison with optimization: to me the natural approach/baseline would be to use a much simpler CNN to predict the latent code (and potentially do some local optimization afterward). This type of learning for optimization approach often regularize the problem well, and lead to good results. This would also be more similar in spirit to the proposed approach.","6: Marginally above acceptance threshold","3: The reviewer is fairly confident that the evaluation is correct"
"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","Keywords: Differentiable rendering, inverse graphics, GANs","Experiments:
StyleGAN-R manipulation on other classes: Our method is general and it can be applied to other classes as well. We further validate our method on the bird dataset and get similar results -- please see a revised manuscript (Fig. P,Q in appendix) for these results. Due to a logistics issue we cannot show results on horse in time for the rebuttal, which we will add to the final manuscript.
StyleGAN v.s. StyleGAN-R: We have shown such examples in Fig.6, where we refer to dual renders. Given StyleGAN images (Col.1,4), we predict mesh and texture, and render them with the graphics renderer (Col. 2,5), and finally map them back to latent codes and adopt StyleGAN-R to render them (Col. 3.6).
The main difference in camera manipulation of StyleGAN-R is that StyleGAN-R accepts explicit camera parameters as input, and thus viewpoints are more easily controllable. Original StyleGAN does offer control over viewpoints, but in an implicit way, by sampling viewpoint codes and labeling the camera. We clarified this in the revised manuscript in Fig.8, and thank the reviewer for bringing this point up.
The reviewer is right in that we can sample the exact same cameras as those corresponding to the “viewpoint” latent codes we discovered in StyleGAN, to verify that the synthesized views look equally good. These results can be found in Fig. R in appendix.
There seems to be a misunderstanding here. We do NOT claim that our approach handles articulated objects anywhere in our paper. In fact, we currently only handle articulated objects such as birds and horses by treating them as rigid objects that always adopt the same articulation. We specifically choose an articulation, and try to keep it fixed across the different viewpoints. In fact, articulation is not perfectly disentangled in StyleGAN, and thus there are slight variations in articulation across the different viewpoints. We discussed this point in the supplementary, section C: “Note that for articulated objects such as horse and bird, StyleGAN does not perfectly preserve the articulation in different viewpoints, which leads to challenges in training high accuracy models using multi-view consistency loss. We leave further investigation in articulated objects to future work.” We have further emphasized limitations with respect to object articulations in the revised manuscript. Handling articulated objects is a fruitful direction for future work.
We followed Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space? to create Fig 7. We agree that using an encoder to predict the latent code can potentially lead to better results for StyleGAN and we add this experiment in Fig. S. StyleGAN’s synthesis in this case is improved significantly. The texture is also more faithful than for StyleGAN-R, which means that further improvements in texture map prediction (which are input to StyleGAN-R) for the inverse graphics network are possible. We emphasize that StyleGAN is not a baseline here, as this is really the meat of StyleGAN-R -- our main point is that we turn StyleGAN into a neural renderer that can accept explicit geometry texture and background and ``render” a photo-realistic image (just as a graphics renderer does).","",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","Thanks for your submission to ICLR.
When the initial reviews were written, three of the four reviewers were positive about the paper. Everyone felt it was overall a solid contribution, but there were some concerns about the clarity and presentation, as well as some suggestions for additional experiments. During the rebuttal/response period, the authors did a very nice job in responding to the concerns of the reviewers. Ultimately, all of the reviews were in agreement after discussion that the paper is strong and ready for publication. I also like this paper a lot, and find it to be a nice way to combine LSH with NN training. I am happy to recommend this paper for publication.","Thanks for your submission to ICLR.
When the initial reviews were written, three of the four reviewers were positive about the paper. Everyone felt it was overall a solid contribution, but there were some concerns about the clarity and presentation, as well as some suggestions for additional experiments. During the rebuttal/response period, the authors did a very nice job in responding to the concerns of the reviewers. Ultimately, all of the reviews were in agreement after discussion that the paper is strong and ready for publication. I also like this paper a lot, and find it to be a nice way to combine LSH with NN training. I am happy to recommend this paper for publication.",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","We thank all the reviewers for the time and effort in helping us improve the quality of the paper. We were glad that the reviewers found the problem interesting, necessary and critical ( R2, R4), the observation smart, inspiring and impressive (R1, R2, R3, R4), and the approach or algorithm principle, novel and clever (R1, R2, R3, R4). The reviewers also agreed that the theoretical analysis was solid and believable (R1, R3) and the experiments were ample and effective (R1, R4).
We have updated the paper to incorporate constructive suggestions. We summarize the major changes:
[R4] an analysis of the speedup and memory savings of the linear layer where MONGOOSE is applied during training in Section 4.1.1 and Table 2.
[R4] the memory usage in Table 1 and Section 4.1.2 along with more detailed comparisons and discussions in Appendix D.
[R4] an ablation study of parameters of the scheduler in Section 4.2 and also learnable LSH in Section 4.3.
[R2] a comparison between the updating time of HNSW (a graph-based ANNS data structure) and our learnable LSH in Section 4.3.
[R2] updated our main result of the extreme classification task with an additional larger dataset in Section 4.1.1 and the language modeling task with a 10-layer model in Section 4.1.2.
[R1, R2, R3, R4] a notation section in Section 3.2.1 and addressed all other notation concerns in Section 3.
[R4] a discussion on the connection between the MONGOOSE scheduler and a former work (Cohen et al., 2019) in Appendix B.
[R2] a broader impact discussion in Appendix G.
[R1] illustrations for Algorithm 1 in Figure 8 (Appendix B.1) and Algorithm 2 in Figure 9 (Appendix C.1) along with descriptions.","",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","Summary of the paper:
This paper introduces a framework that uses an LSH sketches to improve time an memory bottlenecks neural network training. Specifically, an LSH sketch is used to approximate the matrix multiplications involved in training. It is observed that networks' weights get stable after small number of epochs therefore frequent updates to LSH sketches (which is expensive) are not required. This paper uses data dependent/learnable LSH methods that better adapt to data in order to improve the performance (query and update) of the sketches. Ample experiments are provided to validate the results.
Quality:
Needs improvements in notations. For example, in assumption 3.1, sum is over what? If the indexing is over
i
, then does that means over rows of
w
s (since
w∈Rn×d
)? Then should the quantities be L2 norms since they are vectors?
Clarity and the presentation can be improved significantly. For example, ""Rehash"", ""Rebuild"" functions in algorithm 2 needs to be defined or explained. In general it is better to explain the intuition behind both algorithms 1 and 2. I believe the figure 1 should clarify the LSH update scheduling, but it is not very clear.
Originality and significance:
I believe that the ideas presented in this paper adds nice contributions to the ICLR community. The idea of using learnable LSH that adapts to the data together with the observation that the weights stabilize after a few epochs is a clever approach to improve the bottlenecks associated with using vanilla LSH sketches.
Other comments:
In regards to the observation with figure 3, I am curious what properties of a dataset leads to this kind of behavior? are there any quantifiable properties? Is this true for any dataset? Are there previous works that explains why this is the case?
Final feedback: I understand the idea of the proposed framework and the theoretical claims look natural and believable, but the presentation needs to be improved. I am willing to increase my score if the concerns mentioned above are properly addressed.
=====================================================================================================
Added after author response
I believe authors have clarified many things I asked and addressed the issues I and other reviewers raised. Therefore, I increase my score. I believe the idea of using LSH for efficient training has a lot of promise and this paper brings a possible way to do this into light.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","Some neural network runs involve layers with a large number of neurons. These require large matrix-vector or matrix-multiplication which can slow their training/inference. However, if the output of mat-vec/mul is dominated by a few neurons with which the activation has large inner product (a matmul can be thought of as a weighted sum of inner products), then the computation can be sped up by approximating mat-vec/mul by a limited weighted sum with the dominant terms. This requires maintaining an ANNS data structure that is up to data with the back prop. These updates to ANNS have to be done carefully -- too frequent and the training will slow down, or too infrequent and the results of the mat-mul are way off. This paper studies how to do this in a principled way using data-dependent LSH updates and backs it up with experimental data.
The ideas and algorithms explored in the papers are as follows:
The weights changed in a limited manner over time, so it should be possible to take advantage of this.
Concentrated changes to a subset of weights can be tracked and patched upon.
An LSH update rule to make these changes effectively
An earlier algorithm that is reused to decide when the LSH scheme is updated.
The paper also talks about how to identify the layers that benefit the most from this scheme. then it goes on to show the training time benefits of the smart scheduler and the update scheme on extreme classifications tasks as well as transfomers.
Few questions:
Why no serious consideration of graph based ANNS? They are data dependent and SOTA for search efficiency and it is possible to update them. Why is LSH the better choice for ANNS here? This needs a rigorous argument.
Is this really a general and serious enough problem amongst practitioners that a solution merits publication at a popular conference? It might be, in which case a better quantification of potential impact can help.
Is wiki-325K really the largest dataset for XC? What about larger language models -- sec 4.1.2 seems to study more medium sized networks. Larger scale experiments could make this paper more compelling.
I would more strongly recommend this paper if these questions can be addressed.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","Q1. I agree that rebuilding HNSW from scratch can be slow, and thanks for adding this discussion and measurements. The measurements add weight to the choice of LSH. However, I would still be open to the possibility of updating graph based indices -- this is ongoing work in the community and would not conclude that graph based ANNS is not easy to update.
Q2. Thanks for the context. I understand a quantitative summary of impact would be difficult to produce so I am fine with your arguments.
Q3. On the XC repo (http://manikvarma.org/downloads/XC/XMLRepository.html), there are datasets much larger than the wiki-325K dataset you have experimented with. Is there a reason you picked this and not the largest? If there is no strong reason to the contrary, please consider presenting results on the largest dataset.","",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","[1] Gong et al. iDEC: Indexable Distance Estimating Codes for Approximate Nearest Neighbor Search. VLDB 20.
[2] Wang et al. Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search. SIGMOD 18.
[3] Deng et al. Pyramid: A General Framework for Distributed Similarity Search.
[4] Xiangnan he et al “Neural factorization machines for sparse predictive analytics” SIGIR 2017
[5] Medini et al “Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products” NeurIPS 2019
[6] Vaswani et al “Attention is all you need” NeurIPS 2017
[7] Deng et al “Imagenet: A large-scale hierarchical image database” CVPR 2009
[8] Brown et al “Language models are few-shot learners” Arxiv 2020
[9] Yiqiu et al “Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search ” SIGMOD 2020","",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","Summary: The authors make a good insight into the slowly changing of Locality-Sensitive Hashing (LSH) hash codes for the weights (or model parameters) during the Neural Network (NN) training. With this new insight, they introduce a framework Mongoose with a newly designed schedule mechanism to reduce the LSH update overhead. The authors also analyse their model and show some bounds for batch speedup and LSH maintenance. Experimental results validate the efficiency and effectiveness of Mongoose over original LSH methods in NN training.
Pros:
First of all, I like the problem the authors focus on. Efficient NN training is very necessary. The idea of using LSH for acceleration is a good direction.
The observation of the slow change of LSH hash codes is good and impressive. The authors also conduct many experiments to validate this observation.
The idea of using a scheduler for lazy updating is interesting, and they add theoretical analysis about it. And based on the experiments, it seems that this scheduler can capture the changing of model parameters.
The proposed framework Mongoose seems to be general for different deep learning model.
Cons:
Compared to the good insight and the promising framework, the practical improvement is fair. Although the authors provide many experiments to demonstrate the effectiveness of Mongoose, I still suggest the authors conduct more experiments about parameter studies and memory usage which may be good to improve the paper quality. More details can be found in minor comments (1 & 2) later.
Since the idea of the scheduler is inspired by a former work (Cohen et al., 2019), I suggest the authors add a discussion about their connection and difference.
The presentation is not very clear. Many notations are used without pre-defined. More details can also be found in minor comments (3 ~ 6) later.
For the learnable LSH (Section 3.3.1), when selecting positive/negative samples, the authors use the inner product, while for the loss function, they consider cosine similarity. I suggest the authors make an illustration about the use of these two different measures.
Minor Comments or Typos:
The framework Mongoose consists of many parameters. A fresh user may do not know how to set up them. A discussion of parameter settings or some experiments about parameters studies seems to be necessary.
The authors claim Mongoose is memory-efficient for NN training. However, I cannot find analysis about space overhead or experiments about memory usage. It should be done to correspond to this claim.
For Assumption 3.1, what are the definitions of C_1 and C_2? Is user-defined or auto determined? Since Theorem 3.3 and some Lemma in the appendix also use these two notations, are they the same? For a rigorous expression, I think some declarations are necessary. Also, in Theorem 3.3, the condition of r for g_r is not clear.
More problems can be found in Algorithm 1. It seems the core idea of this paper, but there are many typos and errors. For example, The parameter ‘A’ is not defined and may be unnecessary in Initialize function (line 2). In line 4, what is the definition of \epsilon_{mp}? Does it have any connection with \epsilon_{mds}? In order to have guarantee for LSH, c_1 - \epsilon_{mp} should be larger than c_2 + \epsilon_{mp}. How to ensure this? Why setting 1.5 for a smooth cut (lines 13-15)? Is it an empirical value or has any benefit? In line 17, LSH may update with w^{new}_{\pi([r])} instead of \pi([r]).
The y-axis of Figure 3 should be \Delta H (left-subfigure) and \Delta W (right-subfigure) instead of Hamming and L2 distance.
The caption of Figure 5 is not clear. And for the sentence “Figure 5 shows P@1 and P@5 for Mongoose and other baselines…” in the paragraph of results in Section 4.1.1, “and P@5” should be removed.
In summary, I like the motivation and the observation in this paper, and the method Mongoose looks good, but I still have many concerns about the idea. So at this stage, I first give a borderline for this paper. I hope the authors can address my concerns.
====================================================================================================
Update: Thank you for your new experiments and detailed feedback. Most of the concerns have been addressed and the experimental results look better. I believe this paper will provide new insight into efficient neural network training. Thus, I raise my rating and recommend this paper to be accepted.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","For your detailed comments.
For Assumption 3.1, what are the definitions of C_1 and C_2? Is user-defined or auto determined? Since Theorem 3.3 and some Lemma in the appendix also use these two notations, are they the same? For a rigorous expression, I think some declarations are necessary. Also, in Theorem 3.3, the condition of r for g_r is not clear.
Response: We have added the detailed definition of
C1
and
C2
in Section 3.1. Briefly speaking,
C1
is an upper bound on the (expected) movement of the weights of the neural network,
C2
is an upper bound on the variance. They need not be known in advance or fine-tuned (see remark 3.4). We correct the expression of
gr
.
More problems can be found in Algorithm 1. It seems the core idea of this paper, but there are many typos and errors. For example, The parameter ‘A’ is not defined and may be unnecessary in the Initialize function (line 2). In line 4, what is the definition of \epsilon_{mp}? Does it have any connection with \epsilon_{mds}? In order to have guarantee for LSH, c_1 - \epsilon_{mp} should be larger than c_2 + \epsilon_{mp}. How to ensure this? Why setting 1.5 for a smooth cut (lines 13-15)? Is it an empirical value or has any benefit? In line 17, LSH may update with w^{new}_{\pi([r])} instead of \pi([r]).
Response: We correct these typos in Algorithm 1. In particular, (1) eps_{mp} equals eps_{mds}, (2) we remove redundant notation like A, (3) the constant 1.5 is arbitrary, the theoretical property of our scheduler holds as long as it belongs to (1, 2), (4) we take
ϵmds
to be sufficiently small (and greater than
1log2n
) in theory. In practice, we need fine-tune
ϵmds
. We also update some discussion on the influence of this parameter (see Section 4.2).
The y-axis of Figure 3 should be \Delta H (left-subfigure) and \Delta W (right-subfigure) instead of Hamming and L2 distance.
Response: We correct the caption of Figure 5 and rewrite the sentence.
Q4: For the learnable LSH (Section 3.3.1), when selecting positive/negative samples, the authors use the inner product, while for the loss function, they consider cosine similarity. I suggest the authors make an illustration of the use of these two different measures.
Response: Because hashing operates on unit-length vectors, [2] normalizes key vectors: “ we additionally normalize the length of the keys K”. Therefore, selecting samples based on inner product and cosine would be the same. But to avoid confusion, we have added this Section 3.3.1.
[1] Cohen, Michael B, et al “Solving linear programs in the current matrix multiplication time” STOC 2019
[2] Kitaev et al. ""Reformer: The efficient transformer."" ICLR 2020.
[3] Maxim et al “Deep Learning Recommendation Model for Personalization and Recommendation Systems” Arxiv 2020
[4] Xiangyu et al “Memory-efficient Embedding for Recommendations” Arxiv 2020","",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","Thank you again for helping us improve the paper!","",""
"MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training","Keywords: Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms","+++Pros.
-----The observation that model parameters evolve slowly is quite inspiring for more efficient neural network training.
-----The paper proposes MONGOOSE, which is equipped with a scheduler to adaptively perform LSH updates and learnable hash functions to improve query efficiency.
-----Experiments demonstrates the effectiveness of the proposed method, and ablation studies give the readers further insights.
+++Cons.
-----The paper is overall good, but with some minors, such as “Figure 5 shows P@1 and P@5 for MONGOOSE and other baselines during the training process.” in Section 4.1.1.
-----Besides, there are several mathematical symbols should be explained clearly when they first appeared, such as “w” in definition 2.1, “C1, C2” in assumption 3.1, “t_r” in assumption 3.2.
+++Conclusion.
-----Based on the above analysis, I would prefer to make an “ACCEPT” recommendation.
-----By the way, I’m curious about why you named your method “MONGOOSE”? Could you give some reasons?
+++Suggestions.
-----Better make the mathematical symbols more clearly for readers.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","This paper presents an elegant and effective approach to knowledge transfer in RL by learning a policy prior from expert data. The paper is generally well structured and well written. Generally, all the reviewers were favourable about this paper, with its simple idea and convincing results. It was thought that the paper would benefit from the addition of more discussion around related work, and more experimental results, but it remains a strong paper.","This paper presents an elegant and effective approach to knowledge transfer in RL by learning a policy prior from expert data. The paper is generally well structured and well written. Generally, all the reviewers were favourable about this paper, with its simple idea and convincing results. It was thought that the paper would benefit from the addition of more discussion around related work, and more experimental results, but it remains a strong paper.",""
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","We thank the reviewers for their comments. Based on the feedback, we have expanded upon the experiments and analysis in the paper. All major updates have been highlighted using blue text in the paper PDF.
We have also responded to each of the reviewers individually in the comments below.","",""
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","This paper introduces PARROT, a novel approach for pretraining a reinforcement learning agent on near-optimal trajectories by learning a behavioral prior. Essentially, the authors learn a word2vec style embedding of actions for a simple virtual single-arm environment. This embedding will naturally place more common examples from its training data towards the center of the gaussian, making sampling them during training time more likely. The authors demonstrate that this approach outperforms existing pretraining methods in this domain.
This paper presents an interesting novel approach and presents strong support for its value. In particular I appreciate that the basic intuition is relatively straightforward and therefore should be relatively easy to test in new domains. In addition, the evaluation results are impressive.
The paper claims in two instances that human examples would be appropriate for the training data. However, this is not confirmed in the current evaluations to my understanding. The paper indicates that the training data should be “near optimal” multiple times but never explicitly defines what is meant by “near optimal”. Is it that actions must be “useful” (also vague)? More clarity on this would be appreciated.
I have some concerns around certain claims or arguments made in the paper (more on that below). However, this paper still ticks all the boxes for me in terms of novelty and value, and so I would argue for its acceptance.
Some questions for the authors:
What is meant by near optimal? Can this be more formally defined?
The constraint that the action dimensionalities are fixed seems like a large one. I understand that it’s necessary for the invertible requirement, but I could imagine that a similar approach to this could still be helpful in sim2real problems without this constraint. Would the authors agree?
The paper domain is still relatively simple compared to real world problem domains, to what extent do you expect this approach to generalize?
Some smaller points/additional feedback:
The introduction and abstract are a bit vague, and I didn’t grasp the approach intuition until figure 1. I’d appreciate if some of this information could be moved up into paragraph 3 of the introduction. -I’d replace “large such datasets” with “such large datasets” or just remove “such” -I recognize that theoretically that the approach should be able to represent every possible environment action, but it would seem to me still possible that good/useful actions could be placed so far from the center of the Gaussian as to make them very unlikely, depending on the training data. Some discussion about how to avoid this/whether it is a concern would be helpful.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","In the paper ""Parrot: Data-Driven Behavioral Priors for Reinforcement Learning"", the authors propose a new approach to leverage previously acquired data to learn a new conditional search space (called representation) that accelerate the convergence of RL algorithm.
The paper is clearly written and well structured. The figures provide a nice illustration of the algorithm and the results are appropriately reported in the graphs. I am just annoyed to see so many important technical details, like the exact definition of tasks, the networks and the different parameters relegated to the appendix, while they are essential to understand the work. However, I understand that is sadly now customary in this venue.
The presented results demonstrate the benefits of the proposed approach, which looks to be at the same time simple and yet very effective. That's great. However, I have two main issues with the paper:
This paper is directly related to the research domain of transfer learning or knowledge transfer, but this is completely ignored in the related work section. In particular, concepts like learning transferable features would appear quite relevant to this work and should also be considered in the experimental reason.
I was particularly disappointed by the experimental analysis. More precisely, by the 13 lines of the result section that briefly summarise the content of figure 5. In particular, it shows that PARROT outperforms all the other baselines, great, but there is no further comment/discussion/analysis than that. Ideally, a scientific paper should explain (or at least try to) the observed phenomenon, and thus I was expecting some further analysis that would demonstrate the main hypothesis of the paper, for instance via an ablation study.
Overall, I think that this is an interesting paper which shows promising results, while there are a couple of sections in the paper where the same concepts are repeated multiple times and space could be saved and reused to expand the technical description and the result analysis.","6: Marginally above acceptance threshold","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","This work proposes a method, PARROT, to learn data-driven priors for deep reinforcement learning agents. Motivated by the idea of pre-training with existing data of similar tasks, the authors propose to learn state-conditional behavioral priors from a set of similar tasks for reinforcement learning agents, such that a learning agent explores its environment in a meaningful way. Successful trials from past tasks are used as training data to learn a mapping from pixel-level state input to actions. Experiments in simulated robotic manipulator domain demonstrate the benefit of learning behavioral priors, comparing PARROT against algorithms learning from scratch as well as agents pre-trained with behavioral cloning.
Pros: The problem setting tackled in this paper is important for applying deep reinforcement learning algorithms on real world robotic tasks; Training the behavioral prior with PARROT is fully offline, which makes it favorable for practical concerns, as existing meta learning algorithms often require online data (interacting with the environment); The selection of baseline algorithms in this paper are satisfactory, showcasing the benefits of each design choices made with PARROT; The paper is well-written and easy to follow in general; the related work section clearly identifies the novelty of PARROT comparing with existing literature
Cons: PARROT only conducted experiments in a simulated robotic environment, possibly due to the fact that the RL step still takes a considerable amount of data to converge; ideas from works in the imitation learning literature such as GTI by Mandlekar et al. can be leveraged for improving the overall data-efficiency of PARROT
Mandlekar, Ajay, et al. ""Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations."" Proceedings of Robotics: Science and Systems (RSS), July 2020.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","Summary
This paper proposes PARROT, a method for learning a policy prior from a dataset of expert state-action pairs that have been derived from multiple similar tasks. The policy prior is parameterized as a deep conditional generative model that maps a noise input and a state to an action. The latter map can be inverted, which is important to guarantee that the prior assigns nonzero probability to the full action space for all states. Given a new task, the policy prior is used to parameterize a new policy; the new policy outputs noise inputs to the policy prior’s invertible mapping, which in turn outputs an action in the original action space. This parameterization of the new policy leads to much more targeted exploration versus sampling actions uniformly from the original action space. Experiments are on a suite of pick-and-place robotic tasks in simulation.
Pros
The writing is overall very clear and persuasive. The introduction is especially compelling. The sections are well organized.
Related work is quite thorough, with just a few potential omissions (see Cons)
The problem setup section is very much appreciated. It is difficult to formalize or even discuss the assumptions behind an approach like this, but I thought the authors did a very good job.
The problem setting in general is very motivating. It does seem realistic to suppose that a set of expert trajectories from related tasks are available, but that they are not necessarily annotated with rewards. The authors do a good job explaining how this differs from other problem settings like LfD, meta-learning and meta-imitation learning.
The whole method is elegant. It is a sensible integration of existing techniques to address a well-motivated problem.
The experimental results are strong (though there are many opportunities for additional experiments)
The baselines are well chosen
The pseudocode in appendix A is clean and clear. All of the notation throughout the paper is too.
The paper provides a good level of detail in appendix B for reproducibility
Cons
Missing references:
“Learning Action Representations for Reinforcement Learning” Chandak et al. ICML 2019
“Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning” Siegel et al. ICLR 2020
“Residual Policy Learning” Silver et al. (2018) and “Residual Reinforcement Learning for Robot Control” Johannink et al. (2018)
The empirical results are very encouraging, but I would like to have a better understanding of when the overall approach might fail. Some experiments in this direction would include:
Evaluate test-time performance as a function of the number of training tasks
Deliberately bias the training tasks so that they non-uniformly sample from the set, and see how much of an effect that has on the downstream performance. For example, train only on pick tasks and test only on pick-and-place tasks, or vice versa.
More domains beyond the single one considered
It would also be nice to see some ablations. In particular, I am curious about the extent to which parameterizing the new task-specific policy with the behavior prior is important, versus just using the behavior prior to gather data. A simple comparison would be to use the behavior prior as an “exploration policy” that is called epsilon% of the time during RL.
I highly encourage the authors to release code for the paper
Three random seeds is not enough
Detailed Comments
Figure 1 is visually compelling, but a little hard for me to follow. I am most thrown off by the “exploration” part, which says: “attempt all possible tasks in a new scene.” I am interpreting this to be somewhat metaphorical -- it is not as though there is actually a list of tasks, and you are attempting each one in a new scene. Also, as far as I understand, there is not a strict separation between exploration and “learn a specific task via RL” -- exploration is part of that RL process. Let me know if my understanding here is correct. If so, I would recommend perhaps cutting the exploration part of this figure.
Figure 2 is very helpful and clear
In the problem setup, there is discussion of fixed state and action space dimensionalities, but it is not stated that the spaces are vector spaces. Would it suffice to instead say that
S
and
A
are assumed fixed?
In the problem setup, in two places, there is an expectation over single step rewards that should really be an expectation over temporally discounted returns.
In the problem setup, it says: “Note, however, that the optimality assumption is not very restrictive: every trajectory can be interpreted as an optimal trajectory for some unknown reward function.” I think this is misleading. It is true that there is some reward function for every trajectory, but the whole point of the problem setup is that there is assumed to be a nontrivial distribution over MDPs, and the important question is whether the observed data are representative of this distribution.
Please indicate in the caption what the lines and shaded areas represent in Figure 5. I assume they are means and standard deviations, but other choices are plausible too.
Typos:
“Each task can considered a Markov decision processes” (two typos)
“(making it expressive)” is missing a period
“each coupling layer in the real NVP take as input” (should be “takes”)
“hyparparameters”
“This comparisons”
“behavioural” (I know this is an alternative spelling, but the paper should be internally consistent)
Questions
How does the asymptotic performance of PARROT compare to vanilla RL? Is there a point at which RL starts to outperform PARROT in the tasks considered in this work? It looks like this happens in “Pick up Baseball Cap”.
What steps would need to be taken to try PARROT in an environment with discrete actions?
How might PARROT be used in a continual/lifelong learning setting? My concern is that if the behavior prior is changing over time, all of the policies that have been parameterized using the behavior prior would also change, and would potentially need to be relearned. (This question does not necessarily need to be addressed in the submission -- I am just curious.)
What would it take to apply PARROT in a domain where it is not possible to write down good scripted policies (like the ones in Appendix C :) )?","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Parrot: Data-Driven Behavioral Priors for Reinforcement Learning","Keywords: reinforcement learning, imitation learning","Thank you to the authors for the additional experiments and insights. Almost all of my concerns were addressed. Two outstanding concerns that are minor but still worth addressing:
In the problem setup, it says: “Note, however, that the optimality assumption is not very restrictive: every trajectory can be interpreted as an optimal trajectory for some unknown reward function.” I think this is misleading. It is true that there is some reward function for every trajectory, but the whole point of the problem setup is that there is assumed to be a nontrivial distribution over MDPs, and the important question is whether the observed data are representative of this distribution.
Figure 1 is still confusing re: exploration. (Thanks for clarifying my understanding in your response.) I do think removing or changing the exploration part of this figure would make it easier for readers to calibrate their expectations as they start to look at the paper.
Otherwise I think the paper is in very good shape.","",""
"SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness","Keywords: Algorithmic fairness, invariance","All of the reviewers agree that this paper is well-written, and provides sound theoretical analyses and comprehensive empirical evaluations. Overall, this paper makes a useful contribution in the direction of individual fairness. The authors have also addressed the concerns raised by the reviewers in their response.","All of the reviewers agree that this paper is well-written, and provides sound theoretical analyses and comprehensive empirical evaluations. Overall, this paper makes a useful contribution in the direction of individual fairness. The authors have also addressed the concerns raised by the reviewers in their response.",""
"SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness","Keywords: Algorithmic fairness, invariance","We thank all reviewers for their time and thoughtful feedback. We address all questions individually and we have revised the paper accordingly. The main changes are: Subsection 2.2 comparing DIF and the original definition of IF; Appendix B.1 providing details regarding the fair metric learning in the experiments.","",""
"SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness","Keywords: Algorithmic fairness, invariance","This paper extends the individual fairness definition in order to (i) do statistical analysis (e.g., generalization bound) (ii) have the form of regularization so the practitioner can tune the parameter. Theorem 2.3 shows how to do stochastic optimization on the new definition, and Theorem 3.1 shows that that the new definition generalizes.
The paper is extremely well written, it was effortless to follow the arguments, and I enjoyed reading it. The theory part is sound and interesting, and the authors did a thorough experimental analysis on three datasets and compared their work with two baselines.
The authors motivate this work as a new formulation of individual fairness that decouples accuracy and fairness with a regularization term, enabling the practitioner to tune the parameter. I found this argument unconvincing. It is much easier for the practitioner to tune epsilon and delta, and the algorithm finds the minimum error while satisfying DIF. Tuning \ro after setting epsilon and delta is not very intuitive for practitioners.
Minor concerns: In the related work, the authors argue that their work is different from previous extensions of the IF since they use the metric instead of the oracle. What is the big difference between using oracle and having access to metric? Can one serve instead of the other? In the experiment section, the number reported for CLP is very different from their paper on comment toxicity detection. I think having a few sentences on how to learn the individual metrics would be really good. (Right now, you refer to many work on learning metrics multiple times, but it’s never clear how they learn the similarity metric). Comparing equation 2.1 and definition 2.1 is somewhat confusing. I think it would be more clear if you say 2.1 and 2.2 instead.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness","Keywords: Algorithmic fairness, invariance","Quality
This paper is largely well-written with clearly stated theoretical results, and a range of experiments on three datasets.
Clarity
What is the notation
Δ(X×X)
?
In the experiments section, SenSeI is compared with CLP, which is a method that uses hand-crafted counterfactuals. However, it appears that SenSeI’s implementation of individuals crucially requires hand-crafted counterfactuals as well? It was not clear to what the distance metric used in the experiments was. If the SenSeI implementation also uses the same counterfactuals, please comment on whether this is a fair comparison with CLP, and if other implementations of SenSeI (with different metrics) would yield the same results.
Why are group fairness metrics compared in the experiments section? None of the methods studied target group fairness in general so the connection and relevance of group fairness should be elaborated upon.
Originality/Significance
This paper’s original contribution is variant definition of Dwork et al’s “individual fairness” notion, that circumvents its computational intractability, by making it smooth. This is a significant enough contribution to the study of individual fairness, that makes it more applicable in practice.","7: Good paper, accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness","Keywords: Algorithmic fairness, invariance","############# Summary of contributions ##############
This paper tackles the problem of enforcing individual fairness. They define a notion of “distributional individual fairness” (DIF) which related to Dwork et al. 2011’s definition of individual fairness. They then provide an algorithm that enforces their DIF definition of individual fairness. They take as given the similarity metric required for evaluating individual fairness.
Specifically, contributions include:
Theoretical: This paper proposes an “average case” definition for individual fairness which they call “distributional individual fairness” (DIF) (Definition 2.1). They theoretically compare DIF to the previous definition of individual fairness from Dwork et al. (Equation 2.1). DIF has the advantage of being computationally easier to enforce.
Algorithmic: This paper proposes an algorithm for enforcing DIF using stochastic gradient methods.
Theoretical: They provide a generalization bound for their algorithm (Theorem 3.1).
Empirical: Using three real-world datasets, they evaluate their algorithm’s ability to satisfy both individual and group-based fairness constraints.
############# Strengths ##############
The experiments are well set up and reflect real-world problems. The toxic comment detection experiment includes a counterfactual individual fairness metric that is reasonable and has precedent.
Experiments evaluate group-based fairness metrics in addition to individual fairness metrics. These metrics are all relevant to their respective datasets and problem setups.
The paper also discusses the theoretical relationship between their definition of “distributional individual fairness” (DIF) and the existing individual fairness definition proposed by Dwork et al. (Equation 2.1). This comparison is presented well.
The paper provides theoretical analysis of the proposed algorithm. Theorem 3.1 provides a generalization bound for the SenSeI algorithm. Theorem 2.3 provides the dual form of the DIF regularizer which is used to solve their optimization problem for enforcing distributional individual fairness.
The related work is well organized. It covers related work in enforcing individual fairness and learning similarity metrics.
The hyperparameter selection procedure is well documented in Appendix B.
############# Weaknesses ##############
It would be nice to have a clearer picture of the cases in which satisfying DIF actually leads to satisfying the individual fairness definition of Dwork et al. (Equation (2.1)), and vice versa. Currently, the theoretical comparison of DIF and Equation (2.1) feels a bit abstract and difficult to translate into real-world applications. Are there some distributional assumptions under which satisfying DIF implies satisfying Equation (2.1) (or vice versa)?
############# Recommendation ##############
Overall, my recommendation is 7: Good paper, accept. This paper provides a theoretically-backed algorithm for enforcing their definition of “distributionally individual fairness.” Their analysis is understandable, and their experiments appear detailed enough to be reproducible. My only major ask is for a more concrete and practically actionable description of the difference between DIF and Dwork et al.’s individual fairness.
############# Questions and clarifications ##############
After Equation (2.3) on page 2, the authors state that the T(x) map corresponding to Equation (2.1) may not be an optimal point of Equation (2.3). Can the authors give a concrete example of a probability distribution P in Equation (2.3) when this T(x) is not optimal? This would give us further intuition for the difference between DIF (Definition 2.1) and Dwork et al.’s individual fairness (Equation (2.1)).
The notation in Equation (2.4) is unclear. Is x’ a random variable with distribution P’, and if so, why is x’ lowercase (it seems that up to this point the paper has kept the convention that random variables are upper case and fixed values are lowercase)? What about the random variable Y? Is P’ the joint distribution of X’, Y’? If that’s the case, why does it say that W_{dX}(P, Q) is the Wasserstein distance between distributions on \mathcal{X}? It seems there are some typos here that the authors should clarify.
In Definition 2.2, Is \Delta(\mathcal{X} \times \mathcal{X}) supposed to represent the set of all probability distributions over \mathcal{X} \times \mathcal{X}? I don’t see this defined explicitly anywhere.
Is the \hat{h} notation really necessary at the top of page 6 in the last sentence before Section 4? The \hat{h} notation has not been defined or used up to this point. It seems that it would be sufficient to just use h in this sentence, as h has already been referred to as the trained ML model.
Figure 1 is hard to read. The red/blue dots are not colorblind-friendly -- maybe you can use different symbols in addition to different colors, e.g. x’s for the red dots and o’s for the blue dots. The x and y axis labels are also extremely tiny.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness","Keywords: Algorithmic fairness, invariance","This paper proposes a variant of individual fairness, develops an algorithm to enforce this definition, and evaluates it. The fairness definition is based on transport, considering the maximum difference in outcomes for individuals drawn from a distribution ""close"" to the original data distribution in terms of a similarity metric.
The paper is well-written and thorough. The technical definition of individual fairness is a natural one, and it is amenable to stochastic optimization. Compared to other methods, SenSeI enforces the fairness constraint more strongly, at the cost of accuracy.
SenSeI comes with reasonably strong theoretical guarantees, which I appreciate.
The individual fairness constraints used in the experiments are all essentially counterfactual in nature -- certain pairs or groups of examples should get the same outcome, and there are no other constraints. Are there scenarios (even synthetic) where one could use more continuous similarity metrics?
This may be outside the scope of this work, but it would be interesting to dig deeper into which examples lead to fairness violations. For example, in the toxicity classifier, could an investigation into fairness violations tell us something about which counterfactuals are hard to treat equally, and therefore something about the way language is used?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","The paper looks into theoretical analysis of self-training beyond the existing linear case and considers deep networks under additional assumption on data. namely: expansion and minimal overlap in the neighborhood of examples in different classes. The results shed some light on self-training algorithms that use input consistency regularizers. Although the assumptions are very hard to check for all input distributions, the authors make an attempt by considering output of BigGAN generator. In summary, the paper is a great first step in understanding self-training for deep networks.
The paper is overall clearly written. please add the explanation of Assumption 4.1 as requested by Reviewer 4.
Pros: - given the extensive use of self-training the paper is of great importance to the community -extending the analysis of self-training to deep networks -the paper is clearly written and easy to follow
cons: -the assumptions are very hard to validate on all datasets","The paper looks into theoretical analysis of self-training beyond the existing linear case and considers deep networks under additional assumption on data. namely: expansion and minimal overlap in the neighborhood of examples in different classes. The results shed some light on self-training algorithms that use input consistency regularizers. Although the assumptions are very hard to check for all input distributions, the authors make an attempt by considering output of BigGAN generator. In summary, the paper is a great first step in understanding self-training for deep networks.
The paper is overall clearly written. please add the explanation of Assumption 4.1 as requested by Reviewer 4.
Pros: - given the extensive use of self-training the paper is of great importance to the community -extending the analysis of self-training to deep networks -the paper is clearly written and easy to follow
cons: -the assumptions are very hard to validate on all datasets",""
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","In general, it is not clear, at least theoretically, how, and when unsupervised data helps to generalization of nonlinear methods. In the literature there are important and elegant works exists that analyses the impact of usage of unlabelled data during training, however, (if I am not mistaken) all these analyses have been done for linear models. Authors analyse and shed some light to several aspects of using unlabelled data during training. They formalize their analyses based on expansion assumption. I think it can be restated as the similarity between members of the same classes is bounded by below. Intuitively such an assumption is quite reasonable. The authors use the term input consistency for defining a broad set of methods e.g. transformations of the image should be similar to each other, and they also couple their analysis using the expansion assumption with input consistency. In their view input consistency brings a local stability/generalization and expansion property brings global stability/generalization. This is again quite reasonable way of thinking because intuitively just forcing an input point to be close to transformed version of itself sounds a weak property for a good generalization performance. The authors supply quite a bit of theoretical novel material to support their intuition and analysis. Furthermore, they present some supportive experiments albeit not an extensive one.
Strong and weak points: a) Strong points: Please see above. b) The paper is quite dense, and the reader needs to be familiar with learning theory concepts. I wonder if authors would have focused on only one aspect of the problem which they are dealing. In the current version semi-supervised learning methods, unsupervised domain adaptation and unsupervised learning are covered. Every of them is a field by itself. I understand the desire of a unified and generic framework however I can imagine that there is a risk of diluting the message. c) Another understandably weak point is the experiments. I personally think that conducting an experimental study in the scope of this quite challenging however it will be nice see expansion property on a real dataset. Recommendation: Overall, I would like this paper to get published because (if I am not mistaken) paper develops an initial understanding extremely important field e.g. self-training/self-supervised learning.
Supporting arguments: a) I found the assumptions paper quite intuitive and necessary. Authors also supply population level guarantees for unsupervised learning. Moreover, they extend their work finite-sample guarantees by using margin concept and Lipschitz continuity. They extend their work to domain adaptation and semi-supervised learning. The novel material in the paper is extensive.
Questions: a) As I mentioned before I would like to expansion property on some real-world datasets. For example, can authors present some evidence of expansion property for a chosen deep neural network on a dataset (or multiple datasets) and quantify the expansion property based on some metric.
Improvement Suggestions: a) The paper is quite dense, and the reader needs to be familiar with learning theory concepts. I would recommend authors to decrease density of the paper and may be move some parts to the supplementary material.
Although I am quite positive about paper, I would like to see the discussion and comments. I am open to change my review to any direction if some new evidence/discussion/published work supplied.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","This work provides a unified framework to analyze the self-training, semi-supervised algorithms. The key assumptions are 1) the “expansion” assumption which characterizes the low-probability data subset must expand to a neighborhood with large probability; and 2) the neighborhoods of samples from different classes have small overlap. Then the authors established the upper bound of the prediction error on the population when minimizing the self-training and input-consistency based loss on the population. They also extend their results to a finite-sample setting and semi-supervised setting as well.
merits
In the theoretical aspect, the established results can explain the success of self-training training and is of high quality.
Moreover, this work is well written and easy to follow.
Questions
Though this work provides good theoretical results, it seems that the results do not bring any practical insights to further improve the analyzed algorithms. E.g. the population-based loss in Eq. (3.4) is not used in practice, since the loss is on expectation and is relatively complex. Actually, when the network is large, then sampling data to estimate E_p[1(G(x)=y)] and R_B(G) is of the high cost.
About the assumption that neighborhoods of samples from different classes have a small overlap, it also may be restrictive. For example, in imagenet, there are many images that contain several objects actually. This means that the overlap between samples from different classes may be not small.
Finally, the key expansion constant c is not investigated on the toy and real datasets. What is the magnitude of c? this is very important since the upper bound will be large if (c/(c-1)) is hugh.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","Summary: This paper provides a theoretical analysis of self-training for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. The authors propose a novel assumption that they dub expansion to effect this analysis. The expansion assumption requires that the neighborhood of small sets have a class conditional distribution that is large. Under this assumption, the authors show population results for an algorithm that performs self-training under the objective that enforces input consistency. They also provide finite sample guarantees based on off-the-shelf generalization bounds for unsupervised learning.
+ves
The expansion property seems neat, and seems like a natural quantity for making progress in understanding self-training theoretically
The authors also provide support for this empirically in Section D.
The paper is very well written. In particular, the summary of the theoretical results in the main paper is very well done. I also appreciated the proof intuition for Theorem 4.3's proof
Concerns/Comments:
I think it would be helpful for the reader to have a brief proof sketch of the theorems immediately after the statement. The results are already summarized (well!) informally in the introduction.
It is unclear to me how realistic Assumption 4.1 is.
Questions for the Authors:
Can you expand on the optimization objective of (4.1)? How may one implement this in practice? Or, how is this analogous to what is being done in practice?
Can you comment on the realism of Assumption 4.1? Is it possible to verify this on toy examples as you have done with the assumptions in Section 3?
Can you comment on obtaining a finite sample version of Theorem 4.3?","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","Thanks for the response. I think a brief discussion you provided for my question on Assumption 4.1 would make a nice addition to the paper if there's room!","",""
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","Summary of the paper: The paper gives a theoretical justification of self-training. It proposes a new notion of ""expansion"" - the amount of data distribution in the neighbor of an example. Here the neighbor means adding perturbations to the example, or augmentations of the example. When the label distribution satisfies nice expansion properties and that classes are properly separated according to the neighbors, the paper proves distributional guarantees of self-training. Combining with generalization bounds of DNNs, the paper also derives finite sample bounds for DNNs. The paper also verifies the expansion assumption via experiments using a GAN.
Review: Overall, the paper is written nicely and gives a novel perspective on self-training. Several questions: 1) It is a bit strange that semi-supervised learning requires a stronger assumption on expansion than unsupervised learning. Is it possible to use the unsupervised method and then somehow align it with the semi-supervised labels? 2) The assumptions depends heavily on
B
- if
B
is too large than we have inconsistency, if it is too small than we lose expansion. How will the results change if
B
is of other forms (e.g., coming from a model)?
I feel that the paper gives an interesting notion to consider for self-training and the statements are rigorous. I would recommend acceptance.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data","Keywords: deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory","Thank you for the response! That resolves my questions.","",""
"Growing Efficient Deep Networks by Structured Continuous Sparsification","Keywords: deep learning, computer vision, network pruning, neural architecture search","The paper proposes a method to grow deep network architectures over the course of training. The work has been extremely well received and has clear novelty and solid experiment validation.","The paper proposes a method to grow deep network architectures over the course of training. The work has been extremely well received and has clear novelty and solid experiment validation.",""
"Growing Efficient Deep Networks by Structured Continuous Sparsification","Keywords: deep learning, computer vision, network pruning, neural architecture search","We thank all the reviewers, and answer questions in individual responses to each review below. We have revised the paper, incorporating suggested changes, including:
For experiments in Table 1 (CIFAR), we now report our method's performance in the fashion of mean +/- standard deviation across 5 runs with different random seeds. In the final version, we will a include similar update for all competing methods.
We conducted additional experiments on ImageNet to analyze the full accuracy vs FLOPs trade-off curve of our method in comparison to NetAdapt. A newly added Figure 3 shows that our method's trade-off curve dominates that of NetAdapt.
To the ImageNet results in Table 4, we added EfficientNet-B0. In order to facilitate comparison with ProxylessNet at equal accuracy, we also trained two additional models using our method. We match ProxylessNet's accuracy, while still training faster and producing a smaller model.
We add a random sampling baseline experiment to Section 4.4, with results displayed in a new Table 5. Our method's learned strategy significantly outperforms this random baseline.
The main text now contains a brief summary of our LSTM experiments, referring to the Appendix for full details.","",""
"Growing Efficient Deep Networks by Structured Continuous Sparsification","Keywords: deep learning, computer vision, network pruning, neural architecture search","This paper proposes a new principled approach to growing deep network architectures based on continuous relaxation of discrete structure optimization combined with a sparse subnetwork sampling scheme. It starts from a simple seed architecture and dynamically grows/prunes both the layers and filters during training. Through extensive experiments, the authors show that this method produces more efficient networks while reducing the computational cost of training, still maintaining good validation accuracy, compared to other NAS or pruning/growing methods.
Strength: (+) The proposed idea of formulating the problem as a continuous relaxation of discrete structure optimization is interesting. It seems to be a more principled approach than previous NAS or separate pruning/growing approaches. (+) Extensive experimental results are provided to verify the superiority over recent other methods and also to show the performance behavior of the proposed method. The overall experimental setup is systematic and comprehensive. The experiments were done on widely used deep networks on various tasks.
I only have concerns about the clarity of the notation and the representation of the figures. Specific examples are as follows:
In Eq. (3), it is said that f is the operation in Eq. (1). However, I couldn’t find f in Eq. (1).
It should be clarified how many temperature parameters β are in the proposed model. Only one or as many as channels and layers? If it is only one, it does not seem reasonable that all the probabilities growing or pruning channels and layers are the same. Equation (7) seems to imply β to be a vector, but earlier notations (e.g. in Algo 1, Equation 6, etc.) seem to present it as a scalar.
Overall, there are confusing symbols, whether it’s a scalar or a vector. I recommend the channel and layer indicators are denoted as vectors. It seems that each channel and each layer has its unique indicator, respectively. Also, notations should include channel and layer index if they are different depending on channels and layers.
Additionally, all the experimental results shown in the main manuscript are on convolutional neural networks while the abstract mentions recurrent neural networks. The appendix has some, but very little has the main manuscript. If it’s an important part of this manuscript, the authors should include at least a brief summary of the results.
Some figures (and the text inside) are too small while containing many details, probably because of the space limit. For example, Figure 3 has many lines that are hard to analyze and texts that are not readable.
In Table 1, what’s the meaning of the underlines? I guess the second best results, but for RestNet-20, the method with the second-best FLOPs is SoftNet, not Provable. And the explanation about the boldface and underlines should be included.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Growing Efficient Deep Networks by Structured Continuous Sparsification","Keywords: deep learning, computer vision, network pruning, neural architecture search","This paper proposes a novel NAS method that searches the model architectures by grows the networks. This searching strategy determines the channel and layer configurations by assigning a binary learnable parameter m for each channel or layer. The objective is to optimize a trade-off between the model performance on the given task and the regularization on the binary indicator m.
Pros:
The general idea of searching the architectures by growing the networks sounds very interesting. The authors propose a novel framework to achieve their idea, and also apply some tricks to speed up and simplify the optimization (e.g. budget-aware growing and learning by continuation).
The paper is well-written and easy to follow.
The authors conduct a series of solid experiments to verify the effectiveness of their proposed methods. The experiments show the performance of channel pruning, the remarkable improvement on AutoGrow model, and the comparsion with other NAS methods.
Cons:
Compared to ProxylessNet, the proposed model can reduce half of the training time but does harm to the model performance.
Questions:
What's the exact meaning of ""Top-1 valiadation accuracy""? What's the different with Top-1 accuracy? Is this metric evaluated on the valiadation set?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Growing Efficient Deep Networks by Structured Continuous Sparsification","Keywords: deep learning, computer vision, network pruning, neural architecture search","Pros:
This paper nicely unifies two different classes of approaches (NAS + sparsity) for determining the topology of neural networks. They are combined into a single optimization problem, with binary indicators on network components and connections.
Experiments illustrate the behavior of the method. It is good to see that the experiments dig a big deeper than end-result accuracy. For instance, the ""budget-aware growing"" is shown well to work as described by Fig 3.
Cons:
No attention to random seeding.
The sparsification dynamics seem likely to change somewhat from one run to the next. The submission does not describe how random seeding was done for training. Multiple runs with different seeds are not shown, and the distribution of accuracies across runs is unknown. Attention to randomness for this kind of training process seems especially important given the variances in results in the Lottery Ticket hypothesis paper.
No comparison to simple random baseline.
A large portion of the method consists of a search method over the space of possible sparse networks, combining it with growing the network to get a NAS-like method. It has been observed, though, that in a sufficiently general space of this kind one can randomly sample connections and see high accuracies. So to identify the sources of empirical gains, it is good to consider experimental baselines, such as random sampling, that separate the contribution of the search space and the search method:
Xie et. al. ""Exploring randomly wired neural networks for image recognition""
Li & Talkwaker ""Random search and reproducibility for neural architecture search""
Yu et. al. ""Evaluating the Search Phase of Neural Architecture Search""
Radosavovic et. al. ""On Network Design Spaces for Visual Recognition.""
Note that the submission's method also is randomly choosing connections, through a somewhat involved process that also accounts for the observed sparsity of G during training. The simplest baseline seems to be the ""uniform pruning"" described in Section 4.4. This only ablates part of the method that doesn't seem to meet the same criterion here.
Incomplete illustration of the cost/accuracy tradeoff.
The gold standard for comparison in both sparse-neural-network papers and NAS is to consider the accuracy at a range of different model costs. See for example:
Blalock et. al. ""What is the state of neural network pruning?"" Figs 1, 3
(Yang et. al., 2018) Figs 5-9
This clearly illustrates whether a method is overall better (i.e. produces better models across the entire pareto frontier), or is only better for some ranges or on one metric. For a result like the first one in Table 5 in the submission, it is unclear which model is better: they may simply be considering different points on the same cost/accuracy curve.
As a less important aside, ""budget-aware growing"" seems to be an ad-hoc reinvention of something similar to an Augmented Lagrangian method. Explicitly describing the differences from standard optimization techniques might be good.
Reasoning for rating:
While the experiments are extensive, I think they miss the key comparisons that show how useful the method and each of its components is. Given that many different innovations are included in the submission, it may be a muddle for follow-up research to sort out how good each individual one is.
Misc comments
Check spacing around (6) in Algorithm 1
Colon instead of comma after ""trainable variables"" in §4.1
""For better analyze the growing patterns"" -> ""To better analyze the growing patterns"" on page 14
Wortsman et. al. ""Discovering Neural Wirings"" is another closely related work at the intersection of NAS and pruning. (with major differences from the submission)
After rebuttal
The authors have gone above and beyond in providing additional experimental results. All of the points raised above that deal with methodological issues are completely addressed.
The sole significant weakness that remains is the lack of the kind of ablation/component studies that would justify individual design decisions. I do not disagree with the authors that this will be difficult for this work, but I still feel they would have been helpful for researchers who will be building upon this method.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Growing Efficient Deep Networks by Structured Continuous Sparsification","Keywords: deep learning, computer vision, network pruning, neural architecture search","Summary:
This paper proposes a NAS-type work for growing a small network to a large network by adding channels and layers gradually. The authors apply the method to both CNN and LSTM networks.
Strong points:
This paper is well-written and shows good results.
The proposed algorithm is sound and effective. E.g. use less wall-time as compared to other NAS approaches.
Weak points:
It seems that the number of channels and number of layers still need to be predefined (the mask size).
Questions:
How does the FLOPs reduction translate to runtime saving?
What is the target sparsity u in the experiments?
When growing with layers, does the author observe any middle layer is dropped and then recovered? If so, does it happen frequently?
At section 4.2 and 4.3, what is the size of the channel/layer mask? I believe the author still needs to define the upper bound of the network can grow. If so, does the upper bound affect the optimization? Or the proposed method gradually expands the mask?
In table 4, I think Efficient-B0 should be taken into consideration as it is a recent representative approach.
After rebuttal:
The authors' rebuttal addressed all my questions and I upgrade my rating.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments","Keywords: causal inference, continuous treatment effect, doubly robustness","The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF. Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network.
Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on e.g., assuming away the confounders. However, I believe the authors address the criticisms of R4 satisfactorily.
Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper.
I do have a few quips myself and some comments that may help the authors to further improve the paper.
Re: the design that models each parameter as a spline. This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks. t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t.
If you use a B-spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat. As a side note, the authors should clearly write out how they are choosing the knots to specify the basis functions. Otherwise the paper will not be reproducible.
I am not sure how this method would compare to naive (non-deep) baselines. Maybe this was considered in a prior work? If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain. Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines.","The paper designs a new way (in some sense a new perspective) on how neural networks can be used to model intervention variables when the goal is to estimate ADRF. Basically, the idea is to emphasize the importance of the intervention variable by ensuring that it appears not just in every layer but also in every neural of a neural network.
Reviewers mostly agree that this is a good paper with varying degrees, although there are some criticisms on e.g., assuming away the confounders. However, I believe the authors address the criticisms of R4 satisfactorily.
Overall I find the idea new and interesting and the experimental results strong, hence I happily recommend accepting the paper.
I do have a few quips myself and some comments that may help the authors to further improve the paper.
Re: the design that models each parameter as a spline. This is equivalent to introducing additional parameters (coefficients for spline basis) and adding a fixed linear layer (spline basis themselves) to every layer of the neural networks. t is taken as an input in all layers thus it makes sure that the model prioritizes on learning the impact of t.
If you use a B-spline basis (that comes with kernels of bounded support), then the proposed method is very similar to stratifying the data according to different bins of t, and then fitting a separate model for each t. The only difference is that the different bins are now smooth kernels and they overlap somewhat. As a side note, the authors should clearly write out how they are choosing the knots to specify the basis functions. Otherwise the paper will not be reproducible.
I am not sure how this method would compare to naive (non-deep) baselines. Maybe this was considered in a prior work? If not, then I tend to side with Reviewer 4 that the evaluations are mostly ablation studies and they are not really comparing to representative work in this domain. Given that there is a large body of work on this before deep learning takes over, it is important to somehow compare with the right baselines.",""
"VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments","Keywords: causal inference, continuous treatment effect, doubly robustness","We uploaded revisions of the submission in which
We reorganize the introduction to make the logic flow more smoothly.
We add a sentence to explain the motivation of using loss (1) in section 3.3 of the revision.
We change the index of assumptions in Theorem 2.
We also checked the proofs thoroughly and corrected several typos and improved the presentation of the proofs.
We make a few editing on abstract to improve the presentation.","",""
"VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments","Keywords: causal inference, continuous treatment effect, doubly robustness","Solid theoretical results are provided to confirm the doubly robustness of the treatment estimator and outcome estimator. This paper is well-written, however, I still have some concerns about the contributions. 1. The author claimed that confounder is one challenge for treatment effect estimation. Some existing deep learning methods resort to balanced learning or reweighting. For the continuous treatment, how the proposed method addresses the confounder factors? 2. Another big concern is the varing coefficient neural network. Why the VCnet designed to be dependent of treatment information? The dependence is not theoretically discussed in this paper. 3. The experimental results are not convincing for me. Only two baselines are compared with the proposed method. Also the sufficient theoretical discussion is given which however is mainly about the doubly robust estimators. But the design of VCnet needs the numerical results to confirm its effectiveness for ADRF.","5: Marginally below acceptance threshold","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments","Keywords: causal inference, continuous treatment effect, doubly robustness","This paper is to develop a varying coefficient neural network to estimate average dose-response curve (ADRF). Although this paper has several interesting results, the paper is full of many typos and small errors. The current paper needs substantial improvement. Here are some detailed comments.
The introduction section is not well written since the logic does not flow very smooth.
The motivation for （1）can be improved. You miss period in (1).
In Theorem 2, please not use (1)-(6) to itemize the assumptions, since you use them to denote the equations.
In the proof of all theorems, there are some obvious mistakes inside.","6: Marginally above acceptance threshold","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments","Keywords: causal inference, continuous treatment effect, doubly robustness","This problem is well-motivated --- estimating dose-response is a challenging and practically important problem. The paper is extremely well written. It explained complex (poor written) ideas in the semiparametric literature clearly. The comparison against existing works is clear. The theory, as far as I can tell, is sound. It improves the existing results in targeted regularization and can be adapted to analyze one step TMLE. The experiment shows that the model outperforms existing benchmarks on the task it set out to do.
My main suggestion to improve the paper is to use some datasets that actually have continuous treatment in evaluating the method. In particular, I would be interested in seeing an application of the methods on real-world datasets. That being said, while it will improve the paper, it's probably asking too much for an 8-page conference submission. I believe the paper at its current state is sufficient for acceptance.
I want to thank the authors for writing such an elegant paper. I really enjoyed reading it.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","This paper introduces a novel game-theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner. The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large-scale dataset (ResNet-200 activations). This paper is generally clearly written, and in particular Section 2 provides an easy-to-follow reasoning leading to the proposed game-theoretic reformulation of PCA. I felt that the later sections are a bit condensed, including the figures. In the authors response major concerns raised by the reviewers have been appropriately addressed. I would thus recommend acceptance of this paper.
What I found particularly interesting in their game-theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints
u^j⊤u^i=0
have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality. Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient-ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable.
Pros:
Provides a novel game-theoretic reformulation of PCA.
Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game-theoretic reformulation.
Provides theoretical guarantee for the global convergence of the sequential algorithm.
Demonstrates that the proposed decentralized algorithm is scalable to large-scale problems.
Cons:
The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high-dimensional settings.
Significance of the proposed game-theoretic formulation in the context of game theory does not seem to be well explored.","This paper introduces a novel game-theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner. The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large-scale dataset (ResNet-200 activations). This paper is generally clearly written, and in particular Section 2 provides an easy-to-follow reasoning leading to the proposed game-theoretic reformulation of PCA. I felt that the later sections are a bit condensed, including the figures. In the authors response major concerns raised by the reviewers have been appropriately addressed. I would thus recommend acceptance of this paper.
What I found particularly interesting in their game-theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints
u^j⊤u^i=0
have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality. Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient-ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable.
Pros:
Provides a novel game-theoretic reformulation of PCA.
Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game-theoretic reformulation.
Provides theoretical guarantee for the global convergence of the sequential algorithm.
Demonstrates that the proposed decentralized algorithm is scalable to large-scale problems.
Cons:
The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high-dimensional settings.
Significance of the proposed game-theoretic formulation in the context of game theory does not seem to be well explored.",""
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","We have made a few changes to the paper in line with some of our responses below. Specifically, we
Fixed the typos noticed by the reviewers and made a few minor edits to the writing,
Added an explanation of
∇R
and Riemannian gradients to Section 3, in the paragraph adjacent to Figure 2,
Repeated Figure 3a with an angular tolerance of
π32
instead of the original
π8
,
Repeated Figure 3a with 10 repeated eigenvalues in the middle of the spectrum to show EigenGame still converges to the correct eigenvectors on either side of the spectrum. Any orthogonal basis spanning the subspace corresponding to the 10 repeated eigenvalues is a PCA solution, so we do not measure eigenvalue error among those vectors,
Added Matrix Krasulina to the synthetic experiments to confirm that the results are similar to the MNIST experiment (and uninteresting),
We expand the related work section (Section 5) to include a discussion of deflation, the Power method, the Jacobi algorithm, and others.","",""
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","Principal component analysis (PCA) is a well-known dimensionality reduction and feature learning technique in the literature that leads to uncorrelated features. While there are a plethora of algorithms for PCA, along with accompanying analysis, a majority of these works have been developed from an optimization perspective. This paper differs from existing works in that it motivates the
k
-PCA problem, which involves learning the
k
-dominant eigen vectors of the sample covariance matrix, as a competitive game between
k
players in which each player is supposed to estimate one of the eigen vectors and the PCA solution is the unique strict-Nash equilibrium. The main contributions of the paper in this regard are the following:
Setting up the PCA problem as a competitive game between
k
players and showing that the Nash equilibrium corresponds to the PCA solution (Theorem 2.1)
Development of two games (algorithms), with one a sequential algorithm and the other a decentralized algorithm, for solving the PCA problem (Algorithms 1 and 2)
Convergence analysis of the sequential algorithm under a restrictive set of assumptions (Theorem 4.1)
Establishment of the equivalence between the decentralized algorithm and the Generalized Hebbian Algorithm (GHA) of Sanger (Proposition H.1)
Overall, this is a novel paper in that it offers an alternate view of the PCA problem, which might lead to further advances in our understanding of PCA-type algorithms in the future. I therefore have a favorable view of this paper. There are however several important aspects of this paper that need to be clarified by the authors in a subsequent revision before it becomes ready for publication.
Major Comments
Theorem 2.1 is based on the assumption of the top-
k
eigenvalues being distinct. Algorithms such as Orthogonal Iteration (""subspace"" power method), to the best of my understanding, only require an eigen gap between the
k
and
k+1
eigenvalues and do not require the first
k
eigenvalues to be distinct. This needs to be discussed clearly in the paper.
While Theorem 4.1 for the sequential game does not explicitly state it, it appears that it also requires the eigenvalues to be distinct (Theorem L.4, e.g.). This, once again, is a major assumption that is neither discussed clearly in the paper, nor compared to other works that do not seem to have this limitation.
Majority of the works in the PCA literature require the initialization subspace to not be orthogonal to the
k
-PCA subspace. This work, however, requires the stringent assumption that each eigenvector is initialized to within
π/4
radians of the original eigenvector. Not only is this a strict probabilistic assumption in the case of random initialization, but it also becomes harder to satisfy as
k
increases (as the authors also discuss). In light of this strict condition, this reviewer is confused by the claim in the paper that ""these theoretical findings are strong relative to other claims."" I would also have liked the authors to discuss this assumption up front in the paper.
The sequential game appears to be very similar to other approaches that have been proposed in the literature that estimate an eigenvector, subtract its contributions from the data, and then estimate the next eigenvector (see, e.g., Allen-Zhu and Li, 2017 and Raja and Bajwa, 2020). Such approaches of course suffer from the fact that they require distinct eigenvalues, but they don't require any QR decomposition. There is however no discussion of the connections between such approaches and the proposed sequential game.
Why is the decentralized game being called ""decentralized""? Is there a distinction the authors are making between distributed and decentralized? What's the topology being considered by the authors in relation to this game and what exactly does ""broadcast(
v^i
)"" mean in terms of reaching out to other nodes? Some discussion of this would be useful.
While the decentralized game has not been analyzed in this paper, the distributed variant of GHA has been analyzed in the literature; see ""Fast and communication-efficient distributed PCA"". It would be helpful for the authors to comment on the differences between their decentralized algorithm and this distributed GHA work.
Why is ""the longest streak of consecutive vectors with angular error less than
π/8
radians"" the right metric for the experiments?
The claim in Figure 3 that ""We omit Krasulina’s as it is only designed to find the top-
k
subspace"" is not clear to this reviewer.
It would be useful for the authors to discuss the use of
∇vR
, rather than
∇vi
, for updates in both algorithms.
Minor Comments
In my opinion, it is incorrect to say that PCA leads to interpretable features.
The claim ""An exponential convergence rate in the full-batch setting is possible using Riemannian acceleration techniques"" is perhaps too ambitious, unless the authors are confident that this is doable, in which case one wonders why this was not shown in the paper.
The experimental plots in the paper is too hard to see clearly. It might be a useful idea to add them to the appendix also, where they can be shown in larger sizes.
Post-discussion period comments
The authors have satisfactorily addressed all of my comments as well as, in my opinion, comments of other reviewers. Based on the latest revised version of the paper, I am increasing my score to 8 (from 7). I believe this paper is worthy of publication in proceedings of ICLR 2021 and I recommend it as such.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","Decentralized (5, 6) -- The exact topology we consider is presented in Figure 1. Each eigenvector
vi
“lives” on its own node/machine. Not pictured with each node is its own datastream. Also, the “broadcast(
vi
)” step is explained in Figure 1; each node must broadcast its vector
vi
to the other nodes down the hierarchy so that they may compute the necessary gradients. Thank you for pointing out the work on distributing GHA/Sanger’s algorithm. That work replicates the entire PCA computation across several nodes, each of which is observing an independent data stream. In contrast, we consider distributing a single PCA computation across the eigenvectors, so our approach is orthogonal. Replicating EigenGame (with all k vectors) across several nodes and sharing information between the EigenGames would be analogous to the approach explored in the distributed GHA paper. In other words, replace each node in the distributed GHA paper with EigenGame and then expand each EigenGame node to reveal that the game is additionally distributed across k eigenvectors. We referred to this as decentralized as we don’t require a “master” node for coordinating the computation. If after our explanation above, you think this is better described as distributed, please let us know and we will consider changing our terminology.
Longest Streak (7, 8) -- The threshold of
π8
for angular error was chosen somewhat arbitrarily. We can add experiments to the appendix for
π16
and
π32
. We present the longest streak as opposed to “# of eigenvectors found” because, in practice, no ground truth is available and we believe the user should be able to place higher confidence in the larger eigenvectors being correct. If an algorithm returns 100 vectors, 50 of which are accurate PCs but does not say which, this is less helpful. This is not the only important metric. We also plot subspace distance. Krasulina's is not designed to return an ordered set of eigenvectors. It is designed to find a set of vectors that spans the top-k subspace, therefore, it expectedly does poorly on the longest streak metric (aka returns a streak of zero). We have results for it and can include them, but they are similar to the MNIST experiment.
Riemannian Opt (9) -- Thank you for pointing this out. That discussion appears to have been omitted from the submission.
∇R
projects the gradient onto the tangent space of the sphere (a Riemannian manifold) -
R
stands for Riemannian. This gradient projection step is illustrated and discussed in more detail in section G of the appendix.
Minor:
Interpretability (1) -- We agree that PCA does not, in general, lead to interpretable features (unless possibly if the data is Gaussian). Interpretability is difficult to define, but among all the possible orthogonal bases for the top-k subspace, the one that aligns with the directions of maximum variance is at least chosen according to a principle, and this makes the resulting PCs interpretable according to that metric. In contrast, the individual features given by a randomly chosen basis for the top-k subspace are, by definition, not chosen according to any principle and hence are less interpretable. We can change our statement to indicate the eigenvectors are
more
interpretable than a random basis for the top-k subspace. In future work, it would be interesting to consider extending our game-ified approach to a similar problem, non-negative matrix factorization, which has been found, in practice, to extract sparse and easily interpretable factors from data.
Exponential Convergence (2) -- We will leave this out. Accelerated Riemannian optimization techniques are sufficiently new that incorporating them would be a nontrivial extension.
Plots (3) -- We will add enlarged versions of Figures 3 and 4 to the appendix.","",""
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","We made a few more changes according to your comments
Added ""We leave proving convergence in this setting to future work."" to the paragraph discussing the ""bubble"" experiment,
Added the qualifier ""some"" --> ""some other claims"",
Removed the second ""the"" in ""the the top-k"".
Regarding longest streak, no, we have not seen this in the literature before. That being said, we also have not seen eigenvector error measured in terms of angular error either, although it has surely been measured by someone. Also, we added a few sentences (similar to our response above) to the first paragraph of the experiment section to clarify this choice of metric:
""We present the longest streak as opposed to “# of eigenvectors found” because, in practice, no ground truth is available and we think the user should be able to place higher confidence in the larger eigenvectors being correct. If an algorithm returns
k
vectors,
k2
of which are accurate components but does not indicate which, this is less helpful.""","",""
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","A. Summarize This paper proposes to not only maximizes the trace of the projected covariance matrix R but also minimizes the off-diagonal element of R, which helps to recover the real principal components(eigenvectors of the covariance matrix) from data, while the other large-scale algorithms only recover the top-k subspace. Furthermore, the authors utilize the hierarchical relation between eigenvectors and design a utility function for each eigenvector. Therefore, each eigenvector serves as a player in a game and they will achieve strict-Nash Equilibrium at the end, which enables a decentralized algorithm for large-scale PCA problems. In the experiment, the authors conduct experiments on synthetic data, moderate scale data, and large scale data by resnet activation maps. The first two experiments demonstrate that the proposed algorithm is competitive with Oja's algorithm and even better under some conditions. The large scale experiment on resnet activation maps is only feasible by the proposed algorithm and demonstrate that it is a powerful tool to achieve interpretable representation.
B. Strength
This paper is well organized and easy to follow. The explanation about why other algorithms only recover the top-k subspace but not the principal components is step-by-step. Based on this observation, the authors propose to minimize off-diagonal elements and derives the utility function, which adds a generalized Gram-Schmidt step to the gradient and can be decentralized naturally.
Besides the algorithm itself, the proposed method also opens new doors for other interesting future research other than recovering principle components.
The experiments are simple yet sufficient to demonstrate the superiority of the proposed algorithm. To the best of my knowledge, the proposed algorithm is the first that dealS with a problem as large as in the resnet-200 experiment.
C. Weakness:
My only question is that the proposed algorithm focuses on recovering the real principle components and finding interpretable features. So it would be good if we can see some comparison of the lower-dimensional features in some downstream applications. For example, can we cluster the input data into meaningful clusters better than other algorithms?
D. Justification of score: This is a great paper that gives a new perspective on PCA and derives a novel decentralized large-scale algorithm and will inspire a lot of further research along this line. So I vote to accept this paper.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"EigenGame: PCA as a Nash Equilibrium","Keywords: pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition","The authors present new insights on PCA analysis by reconceiving it in terms of a Nash equilibrium among different players, related to the different components. The importance of an objective function minimizing the off-diagonal elements of R is emphasized. The insights lead to parallel algorithms and are demonstrated on large scale problems, which is nice. Overall the new insights can be very valuable and also inspiring for future work and for new developments, from a broader perspective.
Points that can be improved:
related to eqs (1)(2) the authors claim that there is a problem with some classical interpretation of PCA analysis. However, this statement is unclear and possibly incorrect: for coming to (2) the authors start from the solution i.e. the eigenvalue problem, while this should be the result of the derivation. This part should be clarified. Probably it is better to replace this part of the paper by a standard formulation and derivation of PCA analysis as given in standard textbooks. Also one can find it component per component and add orthogonality constraints in each step.
it is nice that a parallel algorithm is obtained. However, at this point the positioning of the result within the existing literature is not clear yet. In the areas of signal processing and neural networks, parallel algorithms and parallel implementations (systolic arrays, VLSI, etc.) of PCA analysis exist for more than 30 years, see e.g. the book Principal Component Neural Networks: Theory and Applications by K.I. Diamantaras, S.Y. Kung, and related work.
section 5: missing methods are e.g. the power method, Lanczos method, Jacobi etc.
related to future work the authors mention the role of PCA analysis with respect to VAE. There is existing work on generative kernel models, eigenvalue problems and disentanglement that is related to it, see Pandey et al https://arxiv.org/abs/1906.08144","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","Dear authors,
This is an exciting work to someone like me who uses AD on a daily basis. But I have some doubts about its practicality. In a typical large NN, frameworks like tensorflow/pytorch do AD by leveraging the chain rule at every layer. The Bauer's ""path integral"" approach to AD seems only useful when aimed at computing a single gradient. Is it really a fair starting point for your memory efficiency claim?
Another idea in reducing memory footprint is RevNet, which allows not storing gradient of the next layer during multi-layer transformer back-prop. This has been used in the Reformer network. I look forward to hearing your thoughts!","",""
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","Hi authors,
I have a question regarding the fairness of comparison with the ""reduced batch"" baseline.
If I understand correctly the reduced batch baseline sees only 20 examples per iterations compared to 150 examples in the large batch baseline. From a statistical point of view, doesn't it make sense to use the number of examples processed instead of number of iterations as the horizon axis?
(I understand that the focus of the paper is memory but this also means that ""reduced batch"" baseline requires 150/20-times less compute per iteration.)
Thanks!","",""
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","The reviewers agree that this is an interesting and original paper that will be of interest to the ICLR community, and is likely to lead to follow up work.","The reviewers agree that this is an interesting and original paper that will be of interest to the ICLR community, and is likely to lead to follow up work.",""
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","Dear reviewers,
Thank you for your valuable comments. We have uploaded a new revision with the various clarifications and nits discussed below. In particular, we clarified the unbiasedness of random matrix injection in 3.2, the discretization of the PDE in section 5, and the discussion of the two extreme examples in section 3.3. We also moved the empirical variance analysis from appendix A into the main paper.
Thank you, The authors","",""
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","Summary:
The paper proposes to subsample the computational graph to obtain an unbiased gradient estimator with less memory requirement, in the same spirit as minibatching reducing the memory of the full-batch gradient descent. The authors propose to do so by modifying the forward pass of a linear layer to only save a subset of the hidden units to estimate the gradient.
Contributions:
Just like minibatching is introduced to reduce the memory requirement of full-batch gradient descent, this paper introduces another dimension to trade off between noisy gradient estimate and memory by subsampling the computational graph for computing a noisy gradient. The experiments, albeit small-scale, clearly demonstrate the effect of using SGD with randomized AD to train standard neural networks. The application on stochastic optimization of PDE is also very interesting and relevant. The paper is very well written and explains the idea very clearly. I look forward to seeing future work using this principle to train larger scale models as well as how this work will enable the development of many more research ideas the same way minibatch SGD acts as a workhorse of deep learning / machine learning.
Additional details:
In the exposition of the “alternative SGD scheme with randomized AD“, it might help to understand the sources of the variance of the gradient estimator by decomposing it as Var(x_ij) = Var(x_i) + E[Var(x_ij | x_i)], where i is the index for minibatching and j is for subgraph sampling. It will also help to understand the different curves of Fig 5.
Is there any speculation of why the drop in performance is more drastic in MLP and CNN than RNN in Fig 5?
I’d like to see some negative results where the variance is too high and harms training, to get a sense of how important it is to tailor the sampling strategies for different computational graphs.
Perhaps explain what M in section 5 is. I had to go to the appendix to figure out that it is a matrix that summarizes the discretization of the space and time.
Related work: Perhaps in the discussion of constant-memory via invertible transformation, Neural ODE can be included since it’s constant memory the same way as Gomez et al 17’.
--- post rebuttal --
Thanks for the response. I think the contribution of this work is solid and I vote for clear acceptance.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","The authors introduce the novel idea of producing unbiased gradient estimates by Monte Carlo sampling the paths in the autodiff linearized computational graph. Based on sampling paths it is possible to save memory due to not having to store the complete linearized computational graph (or the intermediate variables necessary to reconstruct it). Memory is the main bottleneck for reverse mode autodiff for functions with lots of floating point operations (such as a numerical integrator that performs many time steps, or a very deep neural network). The authors' idea can therefore potentially enable the gradient based optimization of objective functions with many floating point operations without check pointing and recomputing to reverse through the linearized computational graph. The tradeoff made is the introduction of (additional) variance in the gradient estimates.
The basic idea is simple and elegant: The linearized computational graph of a numerical algorithm is obtained by a) having the intermediate variables of the program as vertices b) drawing directed edges from the right-hand side variables of an assignment to its left-hand side variable c) labeling the edges by the (local) partial derivatives of assignments' left-hand side with respect to their right-hand side.
The derivative of an ""output"" y with respect to an ""input"" x of the function is the sum over all paths from x to y through the linearized computational graph taking the product of all the edges in the path. The sum over all paths corresponds to the expectation of a uniform distribution over the paths times the number of paths. That expectation can be Monte Carlo sampled.
The authors suggest a way of producing the path sampling based on taking a chained matrix view of the computation graph (see e.g https://arxiv.org/abs/2003.05755) and injecting low rank random matrices. Due to the fact that the expectation of the product of independent random variables is equal to the product of the expectations this injection is unbiased as well if the injected matrices have the identity matrix as expectation.
To take advantage of the simple idea it is in practice necessary to consider the concrete shape of the computational graph at hand in order to decide where to best randomize and save memory without letting variance increase too much.
The authors present a neural network case study where they show that for some architectures the suggested approach has a better memory to variance trade off than simply choosing a smaller mini-batch size. Furthermore, they present a 2D PDE solver case study where their approach can save a lot of memory and still optimize well.
I recommend to accept the paper.
Remarks:
I would love to see a more in depth analysis of the variance for example for simple but insightful toy examples.
For exampl simple sketching with random variates v with E[vv^T] = I can be used to obtain an unbiased gradient estimate via E[gvv^T] = g, i.e. by evaluating a single forward-mode AD pass (or just a finite difference perturbation). But of course the gradient estimate has such a high variance so as to not give any advantage over finite difference methods (since with each sample / evaluation we are only capturing one direction in the input space). We are not gaining the usual benefit of reverse mode autodiff of getting information about the change of the function in all directions.
In order for paths to be efficiently Monte Carlo-friendly it is probably necessary that they are correlated with other paths. In practice this will perhaps have something to do with e.g. the regularity of the PDE solution (the gradient with respect to the solution is similar to that of its neighborhood).
A simple example (ODE integrator):
p1 = x p2 = x for i in range(n): p1 = p1 + h * sin(p1) p2 = p2 + h * sin(p2)
y = 0.5 * (p1 + p2)
The two paths in the program compute exactly the same values so leaving one path out randomly does not make any difference at all (if we correctly re-weight the estimate).
Mini-batches are often like that: Independent samples from the same class give correlated computations, hence the variance is related to the variance in the data.
But if two paths involve completely independent and uncorrelated computations then the variance is such that we do not gain anything. We need at least two gradient steps to incorporate the information from both paths. Since we do not systematically cycle through them but sample randomly, we are actually going to be less efficient.
In terms of arguing about memory savings for machine learning applications it would be interesting to see a case study with a large scale architecture that does not fit into memory.
The random matrix injection section could be clarified by moving the sentence ""the expectation of a product of independent random variables is the product of their expectation"" further to the front and state clearly the idea that: E[A PP^T B QQ^T C] = A E[PP^T] B E[QQ^T] C = A I B I C = A B C
In the PDE example you could clarify the notation used to properly distinguish between the continuous and the discretized solution.
Also the PDE constrained optimization problem is not inherently stochastic (as can be argued for the empirical risk minimization setting in machine learning). Therefore, it is possible to use non-SGD methods with linear or even super-linear convergence rates (quasi-Newton methods). SGD with unbiased gradients has a sublinear rate of convergence. But the ideas of the paper are of course still useful even when trying to find the optimum up to machine precision in finite time. We can first use the RAD SGD approach in the early optimization and then go to the deterministic setting later in the optimization.
Page 3: eqn equation 1 -> Equation 1
Page 6: figure 5 -> Figure 5
Throughout: Perhaps clarify the meaning of batch vs mini-batch (in other papers batch can refer to full-batch)
Figure 5 (a) has blue curves but blue is not in the legend of Figure 5 (c)
Page 8: backpropogate -> backpropagate","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","In the context of deep learning, back-propagation is stochastic in the sample level to attain bette efficiency than full-dataset gradient descent. The authors asked that, can we further randomize the gradient compute within each single minibatch / sample with the goal to achieve strong model accuracy. In modern deep learning, training memory consumption is high due to activation caching. Thus this randomized approach can help attain strong model accuracy under memory constraints.
The authors proposed a general framework for randomized auto differentiation to achieve unbiased gradient estimators. This general framework allows randomization at different granularity such as layer level and individual neuron level. It also includes conventional minibatch gradient estimators as a special case at the sample/minibatch level for randomization. The memory saving here is achieved by trading off gradient variance for activation memory saving.
Empirically, the authors show that for 1) convolution nets on MNIST and CIFAR and 2) RNN on sequential-MNIST, under the same memory budget, neuron-level randomized gradient estimator can achieve higher model accuracy than conventional SGD with smaller minibatch size.
Strong point: This paper is well written with novel thoughts on the fundamental aspects of auto-diff when applied to deep learning (and also to PDE as demoed in section 5.). It can also provide new options for practitioners to train models with high accuracy under memory constraints. Thus I recommend to accept this paper.
I have the following comments / questions on the technical aspects. I only raise these questions up for improvement on the paper; they are not concerns on the quality of the current version. Nonetheless, I am happy to raise the score if the authors can demonstrate results on these aspects.
The authors mentioned about leveraging model specific structures to control/reduce gradient variance for fine-grained randomization (such as at the neuron level). Specifically, they considered using the randomized activation sparsification only for activation memory-consuming layers. I was wondering if other model structures can also help here. E.g. can we sample at the full-channel level for convolutional layers or column / row level for linear layers. Would this has a significant impact of the attained model accuracy?
The main focus on practical implications in this paper is about high accuracy training under memory constraints. However, I was wondering how the authors think about the implication on compute, especially related to the sparse training trend. E.g. can we also make the forward compute itself also sparse and still minimally influence model accuracy?
NITs:
First letter capitalization for figure 3 at the beginning of section 4.
Would it be possible to provide some preliminary in appendix on the solution to the section 5 PDE example.
In section 3.3, the discussion on two extremal cases needs a more precision in text. Assuming the sampling on each connection (i.e. segment on paths) are independent, I think both case will have variance exponential in terms of depth. Currently it reads like only the fully connected case have exponentially large variance.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","The paper proposes a novel approach to reduce memory in backpropagation by sampling the paths in DAG. The paper developed and proved the proposed method in a general framework (which is nice!). Still, I feel that the explanation for applying this method to neural networks is somewhat lacking. I summarize the missing parts in the following.
(1) For fully-connected layers (matrix multiplication), the explanations in Section 3.2 and Section 4.2 are not consistent --- Section 3.2 suggests sampling from the paths (connections), while Section 4.2 suggests sampling from the vertices (activations). I believe in practice sampling from activations is used (correct me if I am mistaken), which indeed implies that random matrices {P_1, ..., P_n} are not independent (I guess it forms a Markov chain?). Therefore, the analysis in Section 3.2 is not applicable. I think an analysis using dependent sampling (ancestral sampling) is needed in this case.
(2) The explanation for convolutional layers is not sufficient. The paper mentions that the compact structure could be exploited multiple times, but exactly how it is somewhat missing. I am particularly wondering whether the activations (pixels) or the feature maps (channels) are sampled. If the activations are sampled, I think it is not friendly to the CUDA kernels as the activations within the same channel are computed together; And if the feature maps are sampled, it again violates the analysis in Section 3.2 (since some paths must be chosen together as a group).
(3) It is not clear how the proposed method is compatible with normalization layers. In normalization layers (e.g., batch normalization), the statistics of the activations are computed. However, since the activations now are sampled, it is not explained how the normalization layers should be modified accordingly to preserve unbiasedness.
(4) It is not clear how the proposed method is compatible with randomized operations in neural networks (e.g., dropout, reparameterization, etc.) I am wondering if the combination is still unbiased.
Given that the technical explanation is lacking for the moment, it is hard for me to judge this paper's correctness. I will temporarily give a reject. I am very willing to increase my score if the authors address the confusion above.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Randomized Automatic Differentiation","Keywords: automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization","Thanks for the technical explanation. The clarifications convince me from a high level, and I will update my scores after the manuscript update.
However, I have a follow-up question: Since the random matrices are correlated anyway, is there a principled way to derive the sampling strategy with the lowest variance? The question is motivated by your explanation that the pixels are sampled instead of feature maps. I am wondering about the principle behind this choice.","",""
"A Distributional Approach to Controlled Text Generation","Keywords: Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families","The paper studies the problem of being able to control text generated by pre-trained language models. The problem is timely and important. The paper frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm, Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact.","The paper studies the problem of being able to control text generated by pre-trained language models. The problem is timely and important. The paper frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm, Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact.",""
"A Distributional Approach to Controlled Text Generation","Keywords: Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families","We sincerely appreciate the reviewers for their thorough reading, helpful feedback and overall appreciation of many aspects of the work. We have tried as best we can to provide clarifications and answer questions.
Additionally, we have uploaded an adapted version of the manuscript containing the following:
Expanded and clarified notation in the method section 2.2 and 2.3 as suggested by AnonReviewer1 to increase accessibility for readers.
Added section A.3 in the appendix replying to an interesting question of AnonReviewer3, containing a figure and a proof showing that according to the transitivity property of Generalized MaxEnt [Csiszar 1996], incrementally adding new constraints can be done directly from
p
and
πθ
without the need of restarting the whole process.
Updated caption of Figure 4 to clarify the process of performing the token frequency analysis (AnonReviewer4).
Fixed typos overall in the main paper and the appendix.","",""
"A Distributional Approach to Controlled Text Generation","Keywords: Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families","The paper studies the controlled sequence generation problem based on pretrained language models, i.e., controlling a generic pretrained LM to satisfy certain constraints, e.g., removing certain biases in language models. Specifically, the paper proposes a distributional view and imposes constraints based on collective statistical properties. The problem is formalized as a constraint satisfaction problem, minimizing a divergence objective. The paper proposes to use KL-Adaptive DPG algorithm for approximating the optimal energy-based model distribution. Experiments were conducted over both pointwise constraints and distributional constraints, showing the effectiveness of the model over the compared baselines.
Pros:
The problem under study is an important problem and can have extensive impact on many downstream language generation applications.
This paper makes solid contributions by proposing a formal view on generation controlling. It provides a framework to handle pointwise, distributional, and hybrid constraints.
The method proposed to sample from the sequential EBM makes sense and is empirically vilified to be effective.
The experiments and analyses support the claims and conclusions.
Overall, the paper is well organized and easy to understand.
Cons:
The paper may benefit from some human evaluation for text generation.
It is somehow not easy to tell which model is better from figure 2, GDC or Ziegler. It seems that Ziegler is superior in generating attribute-related sentences while inferior in diversity. The sentence quality might be similar as the converged values of (π, a) are close.
The current submission contains a number of typos, grammatical and other style issues, in both the main sections and appendixes, but these are rather easy to fix.
Questions:
For real-life applications, whether the proposed framework has scalability issue; e.g., if a task has a large number of constraints to consider or if the constraints are more complicated than what are tested in Section 3?
Assuming one has already got an adjusted LM with some attributes based on GPT2, which would be better if she/he wants to add a new attribute to generation: starting scratch from GPT2 or continuing with the adjusted model?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"A Distributional Approach to Controlled Text Generation","Keywords: Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families","The authors' response addressed my questions. Updating the paper by following the discussions in the response helps. Thank you!","",""
"A Distributional Approach to Controlled Text Generation","Keywords: Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families","The authors addressed my concern so I increased my score to 8.
This is a very interesting idea for controlling a pretrained model for some sort desired criteria. The authors argue that existing approaches for this have taken a pointwise view for instance using REINFORCE to optimize for a particular reward. This can lead models to over-optimize on the criteria and sacrifice diversity and other criteria.
The authors instead propose to take a distributional view. Given the pretrained LM distribution a, they would like to find a distribution c as:
p = arg min_{c∈C} D_KL(c, a)
where C is a set of distributions that pass the constraints. Some of these constraints are point-wise but some are distributional. For instance when generating biographies, the authors would like a constraint e.g. X% should talk about a certain gender or occupation.
The authors describe how their approach leads them to an EBM (energy based model) and subsequent derivations. I think some of this section could be better written for those who are not familiar with EBMs.
The experiments are quite interesting and show how the author's ""soft"" approach allows them to elegantly adjust the distribution of the LM without degeneration.
Pros: -Very interesting idea. -Thorough experiments. In addition to comparing with REINFORCE based methods, the authors also compare with CTRL and PPLM in the appendix.
Cons: -I think the method section (especially the optimization part) could be explained better for readers who are not familiar with EBM, and allow the paper to have more accessibility.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"A Distributional Approach to Controlled Text Generation","Keywords: Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families","In this paper the authors have proposed a mechanism for controlled text generation both pointwise and distributional. That is they not only can generate each sentences bearing some specified contraint or attribute but also takes care of overall property distribution of the generates set of sentences. Though pointwise or per sentence level control is well explored, the distributional control is a new and promising direction which the authors have proposed.
The authors proposed a method Generation with Distributional Control (GDC), which is nothing but a constraint satisfaction problem over the probability distribution p representing the desired target Language Model.
Overall I find the problem challenging and promising. This is a nicely written paper. However, I have some quetions regarding experimental evaluation.
In the Figure 4, the authors have reported the generated sentences controlling sentiment and also report the frequency of the sentence present in the corpus. By corpus does it mean the original training corpus? or the generated corpus by GPT-2?
The proposed method is imposing a constraint so that the generation distribution becomes closer to the original distribution (in this case GPT-2) and still satisfy the pointwise and distributional constraints. If the distributional constraints are not imposed, the generated sentences should be similar to that of the original GPT-2 generated sentences bearing which satisfy the pointwise constraint. What does the freq signifies here? The authors should provide some discussion regarding the same.
Are the sentences generated sequentially keeping the context of the previously generated sentences? or they do not have any context of the previously generated sentence? If each generated sentences are independent from previously generated sentences, how meaningful it is to impose distribution constraint on that ?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","Hi,
thanks for your great work. I wonder why not perform pertaining using the autoregressive language model (LM) or masked LM like GPT and Bert pertaining. Should we compute the similarity between the input embedding and output hidden using some loss? I am afraid Just adding a linear projection layer at. the beginning may not help learn the best representation.","",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","This paper has generated a lot of great discussion and it presents a very different way of doing image recognition at scale compared to current state of the art practices. All reviewers rated this paper as an accept. This work is interesting enough that in my view it really deservers further exposure and discussion and an oral presentation at ICLR would be a good way to achieve that.","This paper has generated a lot of great discussion and it presents a very different way of doing image recognition at scale compared to current state of the art practices. All reviewers rated this paper as an accept. This work is interesting enough that in my view it really deservers further exposure and discussion and an oral presentation at ICLR would be a good way to achieve that.",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","We thank the reviewers for their thoughtful reviews. The reviewers appreciated the combination of the method’s simplicity and good performance (R2, R3, R4), comprehensive experimental evaluation (R1, R2, R3, R4), and good presentation (R2, R3).
Since the concerns voiced by the reviewers are mainly non-overlapping, we provide detailed responses in individual responses to each review. Below we summarize the key changes made to the paper:
Added a discussion of inductive biases in section 3.1.
Added two larger ResNets and one larger hybrid architecture to Figure 5 (scaling of different architectures). The trends are as before, but now visible even clearer.
Adjusted the discussion of related work of Cordonnier et al., added several more related works.
Added an ImageNet-21K-pretrained model to Table 2. It is somewhat below SOTA, but still performs very well and takes less compute to pre-train.
Slightly improved the fine-tuning results on ImageNet and VTAB by better tuning the hyperparameters.
Added a table with detailed results of scaling experiments (Table 6).
Added additional technical details and polished the text throughout .","",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","There is a confusion in the discussion of (Cordonnier et al., 2020), whose approach matches ViT-Small/2 in your terminology. (Cordonnier et al., 2020) should not be termed local, but uses global self-attention on the full size images by extracting 16x16 patches of size 2x2 to reduce memory footprint.
We would propose to remove “We are not aware of prior application of Transformers with global self-attention to full-sized images” and to replace
Such local multi-head dot-product self attention blocks can completely replace convolutions (Ramachandran et al., 2019; Cordonnier et al., 2020; Zhao et al., 2020). Alternatively, works such as Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-attention in order to be applicable to images.
by
Such local multi-head dot-product self attention blocks can completely replace convolutions (Ramachandran et al., 2019; Zhao et al., 2020). Alternatively, (Cordonnier, et al. 2020) extract patches of size 2\times 2 on the full size images to limit the memory footprint. Sparse Transformers...","",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","Dear authors,
Thanks for this very interesting work.
According to table 3, the base model with 16x16 patches ViT-B/16 pre-trained on ImageNet has a Top-1 accuracy around 78 %. According to the ablation study shown in table 7, the gap between a model with positional embedding and without positional embeding (= bag-of-patches) is ""relatively small"" (3% out of 64 %).
Do you think that such a ""relatively small"" gap would hold for a ViT-B/16 model pre-trained on ImageNet ? Do you expect the ""bag-of-patches"" version of the model ViT-B/16 pre-trained on ImageNet to have a Top1 accuracy greater than 70% ?
If this is the case, your model compares very favorably with a BagNet using similar patch-size ( Bag-of-patches ResNet50 , see the ICLR 2019 publication https://openreview.net/pdf?id=SkfMWhAqYQ ). With patch-size 17, a BagNet with a ResNet50 backbone has 58.8 Top1 Acc and 81.2 Top5 accuracy. Then your model is more powerfull than ResNets to classify the images based on patches/textures. This would also suggest that images can be very well classified as a Bag-of-small-patches and that ability to recognize shapes is not necessary for a high performance image classification pipeline.
Thanks in advance,
Louis THIRY","",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","The paper does not discuss bag of visual words (BoW)/multi-instance learning (MIL) approaches. These approaches also splits an image (bag) into words (patches or instances) and then learn representations on words, similar to what this paper does. Though current CNN-based models do not use BoW/MIL approaches for standard computer vision tasks (classification, detection, and segmentation), they are widely used in histopathological images [R1]. Most of these also use attention (e.g., [R2]).
Also, I do not understand the need of positional encoding for image classification. In NLP, it makes sense because the same token can appear in multiple places in a sentence. But for images, patch order is fixed. Another parallel work [R3] that uses Transformers for classifying histopathological images (order of GigaPixels) shows that with transformers with CNNs (similar to hybrid model in ViT), networks can learn clinically relevant biomarkers without any positional encoding. I would appreciate if you can elaborate more on the effect of positional encoding.
[R1] Hou, Le, et al. ""Patch-based convolutional neural network for whole slide tissue image classification."" CVPR. 2016.
[R2] Maximilian Ilse, Jakub Tomczak, Max Welling ; ICML, 2018.
[R3] https://arxiv.org/pdf/2007.13007.pdf","",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","This paper introduces a Transformer-based image recognition model that is fully built on the Transformer layers (multi-head self-attention + point-wise MLP) without any standard convolution layers. Basically, it splits an image into patches and takes as input the set of linear embeddings of the patches and their positions. For classification, a learnable class token is added to the input and a classification head (MLP) is attached to the class token output of the final Transformer layer. Extensive experiments of transfer learning show that when pretrained on a sufficiently large dataset (100~300M), the proposed Vision Transformer can outperform state-of-the-art convolutional networks with less training cost as well as less number of parameters.
Pros
Clearly motivated and well written
The background of the research, the motivation of Vision Transformer, and the related work are all clearly stated and summarized.
The first fully-Transformer-based image recognition model
The simple yet effective Vision Transformer is introduced by adapting the original Transformer with minimal modification; patch embedding, class token, and class head.
Extensive evaluation and good analysis
This paper demonstrates the power of the Vision Transformer model by extensive large-scale experiments, outperforming SOTA CNN models. Comparative evaluation with important baselines, ResNet and hybrid models, are well designed and conducted with different scales of datasets and models. The results are impressive and interesting. Additional discussions in Appendix are also useful in understanding the model.
Cons
No significant technical novelty
The proposed model is incremental modifications of the original Transformer and its existing variants.
Lack of further analysis of the inductive bias
The authors contrast the Transformer with CNN, stating “Transformers lack some inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.” and “the convolutional inductive bias is useful for smaller datasets, but for larger ones, learning the relevant patterns is sufficient, even beneficial.” However, it is not clear which inductive bias of CNN prevents better generalization; is it translation equivariance, locality, static weights of kernels, or static size of kernels? I don’t find any clear answers from the paper. Since there exist different inductive biases in standard convolution, it may be the case that some of them help generalization and some others do not; worse generalization of CNN with a larger dataset may be affected by all those factors in a tangle. In this regard, further analysis of this issue would improve this work. E.g., in order to check the benefit of locality bias, ViT with local self-attention can also be compared, etc.
Some misleading statements
The authors repeatedly call the input of patch embeddings an “input sequence”, which I guess is intended to remind of the NLP origin. But, this may confuse readers. The Transformer used in this work does not process the input as a sequence, and actually the output is equivariant to any permutation of the patch (+position) embeddings.
“Transformers lack some inductive biases inherent to CNNs, such as translation equivariance and locality”. This needs to be clarified. As I noted above, since the Transformer in this work is permutation equivariant, it can also be seen as translation equivariant. As for “locality”, the Transformer also performs point-wise (patch-wise) processing, thus leveraging locality.
“Unlike prior works using self-attention in computer vision, we do not introduce any image-specific inductive biases into the architecture.” This statement in conclusion is an overclaim, conflicting with the statement at the end of Sec. 2.1 (resolution adjustment and patch extraction).
The title of this paper “An image is worth 16x16 words”, what does it mean? I don’t find an answer from the paper.","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","This paper explores the Transformer architecture for image recognition. The authors mainly follow the settings of BERT in NLP where the input is a sequence of words. To adopt Transformer for image recognition, the authors split the image into grid patches as inputs for the vision Transformer (ViT). Other settings like adding a special token at the beginning for classification and adding position embeddings are the same as those used in BERT. While Transformer is not new, the way it is applied on raw input images directly for image classification has not been done before. I also like the analogy with the hybrid model where the early layers in ResNet serve the role similar to the patch embedding projection. More importantly, the authors have conducted extensive experiments to validate the effectiveness of the proposed approach. Some of the visualizations are also insightful. The main conclusion is that ViT can outperform BiT(ResNet) when pre-trained on large-scale training data with supervised training.
As the Transformer model has become more popular in the vision community, it seems natural for the authors to come up with ViT for image recognition. I think the main contribution of this paper is not to propose this idea, but rather to make it work. The fact that ViT can outperform BiT with ResNet only when it scales-up to a huge model trained on a huge dataset makes it impossible for most researchers with limited resources to conduct such experiments. Although the results may not be that encouraging in my opinion, I think the experiments conducted in this paper are valuable for the research community.
My major concern is that the pre-training conducted in this paper is fully supervised. This is different to BERT that can use unlabeled text data or Vision-Language Pre-training where weakly-labeled image-text pairs can be leveraged. The authors briefly mentioned that the masked patch prediction can still improve ImageNet classification accuracy compared to training from scratch, but it has a gap with supervised pre-training. It would be great to explore self-supervised pre-training in the future as also pointed out by the authors.
Another concern is about the comparison with the hybrid model. The main drawback of applying ViT on the full image is that it cannot scale to large input resolution due to memory constraint. A hybrid approach seems to be a good work-around to handle this case. While the authors have mentioned the hybrid model, very limited comparisons are conducted in the experiments. Only Figure 5 shows some results of hybrid models. In some cases, the hybrid model outperforms ViT. Why not add more data points for hybrid models at higher pre-training compute cost? Why not compare the numbers in Table 2 and Figure 2-4?
In the appendix, the authors mention that the absolute numbers of BiT are lower than those reported by Kolesnikov et al. (2020), since they pre-train only for 7 epochs, not 30. Is it possible that there is more room to improve for BiT (ResNet) in Figure 5 if we increase the pre-training cost? At least the trend does not show saturating performance of BiT.
Overall, I think the authors have done a great job in conducting all these experiments. Although the results are not that encouraging as these experiments require too many resources, I think the experiments can provide some value for future research. I would like the authors to comment on the comparison with BiT and the hybrid model in Figure 5.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","Summary
This paper studies to adapt transformer model for image classification task. The new model performs comparably well on various popular benchmarks, and will out-perform when pre-trained on large dataset.
Pros
The technical solution is surprisingly simple, yet achieves strong performance. All it needs is to cut input images into patches, and reshape it as an input sequence. The transformer architecture is kept almost the same as in NLP tasks. The simplicity of the model makes it easy to generalize many vision tasks potentially.
The experimental section has clearly demonstrated the pros and cons of the proposed model. The authors not only show the strong performance but also stay upfront about the limits.
The additional analysis, visualizations, and self-supervised pre-training sections are very informative.
The writing is mostly clear and easy to follow.
Cons
The arguments about ""inductive biases"" are confusing and self-contradictory. On one hand, the introduction section says that CNN generalize better due to the inductive biases such as translation equivariance and locality. On the other hand, the rest of the paper claims that avoid inserting inductive biases into the transformer is an advantage. In my understanding, these inductive biases have been shown to be beneficial for most vision tasks in CNN. Why not introducing some of them into transformer? Please clarify.
It's not clear to me how the few shot accuracy is computed. Is the the same evaluation as in self-supervised representation learning? What kind of regularization is applied to the linear regression, and how to learn the mapping?
Minor
It might be better to give reference when mentioning ````100B parameters in the first paragraph of introduction.
z0L
on page 2 bottom seems to be a typo. Should it be
zL0
?
In Sec.1 ```Hybrid Architecture, the statement ""In this hybrid model, the patch embedding projection E (Eq. 1) is replaced by the early stages of a ResNet"" is misleading. As in Eq.(1)
E
is applied on different input patches
xpi
, whereas early stages of ResNet are applied on the same input (image
x
).
Questions
If I understand correctly, a model like ViT-L/16 means that it takes 16x16 fixed-sized patches as input. Does this mean that the images get resized to fixed resolution before cutting into patches? How big are those patches? Does the size matter?
As shown in prior work (Unsupervised Visual Representation Learning by Context Prediction, Doersch etal., ICCV 2015), enforcing the patches to be discontinued and with some randomness is beneficial to self-supervised representation learning. Have the authors tried similar processing method? An ablation on different ways of generating patches (discontinued, overlapped, mixed-scale...) might be useful as some simple techniques might greatly improve the performance.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","Keywords: computer vision, image recognition, self-attention, transformer, large-scale training","Following the transformer successes in NLP, this paper explores the performance of a vanilla Transformer (with few simple modifications) for the task of image classification. It has been experimentally validated that this Transformer (ViT) can attain superior performance for this task compared to SOTA CNN architectures if it is pre-trained on large amounts of data and then applied to mid-sized or small image classification benchmarks. Moreover, it has been shown that ViT requires considerably fewer computational resources to train compared to computationally demanding CNN backbones such as ResNet. Clearly, the paper has the potential to re-ignite another wave of excitement for exploring this great learning model on different computer vision tasks. To this end, I believe this paper has enough merit to be accepted.
The paper delivers a strong message "" transformers can be a more powerful, yet efficient, compared to the SOTA CNN backbones for image recognition tasks if there is a large-enough dataset available "" and the authors prove this claim by performing comprehensive experiments on several large-scale image recognition benchmarks. This level of experimental verifications is only possible if huge computation re-sources are only available, which is not accessible for most research teams, esp in academia. Otherwise, a similar message (replacing CNN with transformers/attention) has been attempted to be verified in a few earlier works (as acknowledged in this paper as well).
Also, it is hard to imagine that ViT can yet compete with the CNN backbones for vision tasks beyond the considered tasks because:
1- it is not clear how the simple ViT can be extended for the vision tasks which require pixel-level predictions, e.g. image segmentation, depth prediction etc, or 3D vision tasks while being still computationally tractable.
2- As argued in the paper, transformers are data-hungry to perform as well as ResNet for a task and the availability of such large-scale annotations (e.g. 300M samples) beyond image labelling may not be feasible yet.
3- Considering transferring a pre-trained model, there is no evidence that a pre-trained ViT (e.g. on a large amount of data) can still learn a reliable representation for the other basic vision tasks beyond image recognition such as image segmentation or object detection, and still carry a superior performance compared to ResNet when it is fine-tuned... The superiority of DETR (Carion et al 2020) on the detection task is not solely due to the use of a transformer but rather formulating object detection as a set prediction problem to avoid heuristics such as NMS.
Other comments:
I couldn't find any ablation study on different choices of 2D patches sizes versus training time and accuracy except partial experiments on 14,16 and 32. I am not sure what would be best patch size given any image, eg a panoramic image with high resolution when considering both accuracy and training computation compared to ResNet.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Thank you for your work. Interesting read. The authors should consider citing [1] as it proposes a generative method for deriving counterfactual explanation, very relevant to the current work.
[1] Singla, Sumedha, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich. ""Explanation by Progressive Exaggeration."" In International Conference on Learning Representations. 2019.","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","This paper presents an uncertainty quantification method that is conceptually interesting and practical. All reviewers are in consensus regarding the quality and significance of this manuscript.","This paper presents an uncertainty quantification method that is conceptually interesting and practical. All reviewers are in consensus regarding the quality and significance of this manuscript.",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","We would like to thank all reviewers for their time in reviewing our paper. We are glad the reviewers appreciated the novelty of designing methods aimed at increasing the transparency of uncertainty estimates in ML. We are also happy that the reviewers enjoyed seeing our user study and found its results persuading.
We thank the reviewers for their helpful suggestions. We have incorporated these to improve our paper; the newly uploaded manuscript contains the following changes:
We have modified Section 1 (introduction) to make it clear that the described workflow and Figure 1 are simply an example of a possible application of CLUE, not the main guiding motivation behind CLUE. We also better explain why feature attribution is typically not suitable for uncertain inputs.
In Section 1, we clarify that our proposed method, CLUE, can be applied to any probabilistic model, not just BNNs. However, this work focuses on BNNs.
In Section 2.3, we added a comment relating counterfactuals to the broader category of contrastive explanations and cited relevant work suggested by AnonReviewer4.
In Section 2.3, we added a citation to Bayes-TrEx (https://arxiv.org/abs/2002.10248)
In Algorithm 1, we updated the notation used to be more general.
In section 3, we have added a short discussion on the usage of contiguity/smoothness constraints for saliency maps and added relevant citations.
In Section 4, we go into additional detail on how “ground-truth generative models” are used to evaluate counterfactual examples in our proposed evaluation framework.
In Section 4, we have added a reference to Appendix I, where we give extensive details on the “ground-truth generative models” used in our evaluation framework.
In Section 5.2, we clarify that our user study subjects are graduate students in ML, a proxy for ML practitioners.
We perform an additional experiment where the typicality of counterfactual explanations is measured through Nearest Neighbour distance to the training set, as suggested by AnonReviewer2
We are open to your feedback and additional suggestions.","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Summary
The authors consider the problem of post-hoc explainability for decisions rendered by machine learning models. They focus on addressing uncertain model predictions, producing counterfactual data that is both likely under a generative model of the data, as well as more certain in the classification task. They present both experimental evidence, as well as a user study geared towards practitioners, that show the benefits of counterfactual explanations targeting uncertainty.
Strong points
What really sets this paper aside for me is its explicit focus on uncertain model decisions, as well as its inclusion of a user study.
While other works do use the point estimates of model softmax outputs as part of their methods (e.g Progressive Exaggeration), they do not focus on providing counterfactual explanations specifically for those points where the model is uncertain.
Most works in explainable AI, even those targeted towards practitioners, do not evaluate the utility of their tools in front of an audience of machine learning model developers. Bravo for undertaking this work which is (currently) under-valued in the ML community.
Weak points
In step 4 of Algorithm 1, why do you need to focus on a BNN for obtaining
H(y|x)
? Going back to my previous point about limiting usefulness of CLUE, there are many methods that yield
P(y|x)
, even by approximate Bayesian inference.
Is it the case that as the method converges (if it converges?),
H(y|x)
is likely to decrease, while
d(x,x0)
is likely to increase? Do you attempt to anneal the relative contribution to the loss to account for this? Furthermore, do you ensure that the relative contribution to the loss is balanced between
d(.,.)
and
H(y|x)
?? If these become imbalanced, one will surely dominate the direction of
∇ZL
In the description of the baselines used for experiments in section 5, the localized uncertainty sensitivity analysis seems artificially weak; why not include a more robust ensemble of models that produce softmax output over class assignments? Or a proper probabilistic model like a GP?
Recommendation
The effectiveness of CLUE, and indeed of every counterfactual explanation method cited that makes use of an auxiliary generative model of the data is bounded by the faithfulness of this DGM to model the density. This is a more fundamental limit on the applicability of these methods, not specific to CLUE, but worth stating IMO.
I disagree with the statement detailed in section 4 after the evaluation procedure involving
Hgt
capturing ground truth aleatoric uncertainty:
pgt(y|x)
is just another generative model trained by estimation of the data, it’s not special. A better measure of aleatoric uncertainty would involve the variance of
p(y|x)
taken over multiple independent models (see Snoek et al.). I also find the argument about adversarial weakness that immediately follows is a bit confusing ""Approaches that exploit adversarial weaknesses in the BNN will not transfer to the g.t. VAEAC, failing to reduce uncertainty on error"".
Fundamentally, though I see areas where the work could be improved, I believe the work is sufficiently different to existing counterfactual explanation methods to be accepted.
Questions for the authors
In section 3 where you penalize the distance
d(x,x0)
, given that the motivation of having a penalty on
d(x,x0)
is to try and ensure minimal changes, what about instead ensuring this by doing projected gradient onto the space of plausible data?
Again in section 3, is
dy(f(x),f(x0))
intended to be high, or low? Traditional understanding of counterfactual explanations in the literature would suggest
f(x)!=f(x0)
, but I can see the value in not caring about enforcing this to focus on driving down
H(y|x)
. Could you spend some space touching upon this design decision here?
In Section 5.1, I'm curious why FIDO was chosen as a baseline? If memory serves, the FIDO objective has additional constraints to try and ensure the B form a contiguous set, which your U-FIDO formulation (equations 6,7) does not admit.
Suggestions for improving the paper
Algorithm 1 presents a minor nomenclature issue: the output
xclue
produced might not be an actual counterfactual in the sense of Wachter et al., in that no effort is made to orient the latent space edits (z) towards crossing a decision boundary for Y.
At the beginning of section 4, I find step 4 of the procedure is not clearly presented. Step 4 suggests it’s used as a way to measure whether the discovered x_c are likely given the density of the model. Is that so? If not, could you be more clear why evaluation of
xc~
by the VAEAC is helpful?
At the end of section 3, it would be helpful here to consider recent efforts to encourage counterfactual explanations to be confined to small contiguous regions (cf. Dabkowski and Gal 2017, Chang et al 2019). This issue of potentially large, disparate, sparse signals was a flaw of original gradient based saliency maps, and would be well addressed here.
In section 5.1 where you discuss the criteria for evaluation of counterfactuals (We would like counterfactuals to explain away as much uncertainty as possible while staying as close to the original inputs as possible). This is achieved, albeit indirectly, by other counterfactual generation methods (e.g Progressive Exaggeration https://openreview.net/forum?id=H1xFWgrFPS), which vary latent representations along a continuum of class output probabilities. You could use their method as a comparator by selecting uncertain points and generating counterfactuals that move away from the decision boundary instead of towards it.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Section4: On the uncertainty captured by a ""ground truth"" generative model and adversarial perturbations.
We are afraid that there might have been some confusion here. We train a generative model on real data and then generate artificial data with said generative model. In the realm of artificial data, the generative model is the ground truth. Thus, its uncertainty represents the true aleatoric uncertainty of the artificial data with respect to its generating process. We make no claims about the true aleatoric uncertainty of the real dataset / real data generating process. Our goal is simply to set up an artificial environment where we control the data generating process. Section 4 has been updated to further clarify this.
We are not sure we understand your comment regarding the ensemble. Could you please elaborate? -- An ensemble might capture uncertainty in our model’s specification (uncertainty in the weights / architecture). This is known as epistemic uncertainty. We are not sure how it would be relevant to our scenario.
With regards to our comment “Approaches that exploit adversarial weaknesses in the BNN will not transfer to the g.t. VAEAC, failing to reduce uncertainty or error”; you could imagine an adversarial attack, which takes in an uncertain input and modifies it such that the uncertainty of our BNN is minimized while not differing from the original input in any meaningful way (this is illustrated in figure 3). Because this attack targets the BNN and not the data generating model, an input constructed in such a way would still seem uncertain to our ground truth generative model and would fare poorly on our metrics.
Additional Questions:
Projected Gradient descent: This is a very interesting idea which could lead to smoother optimization. However, it is not trivial what the projection step would be in our case.
Penalty on prediction similarity
dy
: This penalty can be leveraged to generate counterfactuals which our model believes to belong to a certain class. Which class that is will be scenario dependent. You could imagine a case where you know what the target class is and you would like to find a low uncertainty input which is similar to your original input while being classified correctly. You could also imagine a setting where you want to gain information about a model’s uncertainty while preserving the originally predicted class. Our experiments in the main text do not cover those cases. Thus, we set the prediction similarity loss weight
λy
to be 0. We experiment with the prediction similarity loss in appendix H.1
Choice of FIDO baseline - use of contiguous set constraints: We choose FIDO as the method to adapt to uncertainty because it seemed similar to CLUE and produced state of the art results for counterfactual generation for classification. You are correct in that the original FIDO formulation includes a smoothness penalty which encourages adjacent pixels to have the same mask values. This makes sense in the context of high resolution images. We do not consider this penalty, as our experiments focus on tabular data, where there is no local coherence among input dimensions and where counterfactual explanations are most commonly used in practise. MNIST images are only sized (28x28). They are not large enough to warrant spatial coherence constraints.
Other Suggestions:
Thanks, we have incorporated your comments in the new submission
Algorithm 1 nomenclature: Thanks, we have updated the algorithm to refer to counterfactuals in the sense of uncertainty.
Clarification on step 4 of section 4: You are right, the g.t. VAEAC is used to measure the density of the counterfactuals under the true generative process of the artificial data
pgt(xc¯)
. This is useful because it tells us if the counterfactuals represent plausible input variable settings. The g.t. VAEAC is also used to measure the uncertainty of the counterfactuals through their conditional density
pgt(y|xc¯)
. We can use this distribution to calculate the uncertainty associated with
xc¯
under the true generative process of the artificial data
Hgt(y|xc¯)
. The expressions necessary to do this are provided in Appendix B.2.
Hgt(y|xc¯)
provides us with a reliable way to see if counterfactual explanations represent low uncertainty configurations. We have further clarified the above in section 4.
Constraining explanations to to small contiguous regions: We see this as being relevant to methods that are geared towards natural images, as is the case in the papers that you cite. As suggested, we now mention this type of constraint at the end of section 3.","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","This paper tackles the problem of making Ai/ML-systems more trustworthy making the uncertainty associated with a model, in this case BNN, visible, more interpretable. Interpretability and knowing the limitations and uncertainties associated with a model are definitely very interesting research challenges. These topics are very relevant for ML, AI, Explainable AI etc., but I still think that they are also for ICLR (even if many conferences in the ML/AI/XAI will also fit this paper)
I find the main ideas innovative, the paper is well-written, explained and even includes some kind of “small” user study. The authors also provide a framework for evaluating the counterfactual explanations of uncertainty provided, using informativeness, and they carry out well-designed experiments for validating CLUE.
P. 7, under section 5.2. Can the first sentence be referred to Hoffman? I don’t think so. Are the participants used in the user study be good representatives of the possible users/practitioners of CLUE (as also stated in the conclusions)?
There has come a recent survey on counterfactuals, that I think it is relevant for this work:
Verma, S., Dickerson, J., & Hines, K. (2020). Counterfactual Explanations for Machine Learning: A Review. arXiv preprint arXiv:2010.10596.
I understand that it is out of the scope of this paper, but the notion of counterfactuals used in the ML community, like the one used in this paper (section 2.3), is quite narrow compared to how we use counterfactuals and contrastive explanations in real life. I think the richness and complexity of counterfactual explanations is well illustrated in
Byrne, R. M. (2019, August). Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning. In IJCAI (pp. 6276-6282).
Perhaps this is something to discuss in the future.
(just to make clear: I am not involved in any of the references given, just thought that they can be of interest for this paper).","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Hi authors,
Thanks for your response. I think you have a very solid paper, and I only had minor comments, including the Hoffman citation comment, but I agree that I was not so clear. The reference is absolutely relevant in this context, but I had a problem regarding the sentence itself. The sentence was ""CLUE’s promising results in computational evaluation do not substitute for human-based evaluation (Hoffman et al., 2018)""... I don't know what Hoffman reference adds there, do you mean that in general Hoffman et al. says that computational evaluation does not substitute for human-based evaluation (the second part of the sentence)? Is it not better to change the sentence so you make the point that human-based evaluations are needed (cite Hoffman) and then here comes an evaluation for CLUE?
As I said, this was really just a minor comment...","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","This paper addresses the problem of explaining the uncertainty of a prediction made by a differentiable probabilistic model (as opposed to the prediction itself) through counterfactual explanations. They propose a technique, CLUE, which optimizes their counterfactual metric in the latent space of a deep generative model. To validate their approach, they introduce a quantitative evaluation for uncertainty metrics and conduct a user study, while also analysing CLUE's reliance on the auxiliary DGM.
Strengths: The empirical validation of the approach is strong.
The authors deserve particular credit for ""creating their own baseline"" by adapting a previous counterfactual approach (FIDO) to the problem at hand.
The user study is well-executed, and produces a pretty stark improvement over baselines, including human-selected counterfactuals.
While the introduced approach does require the non-trivial complexity of training a DGM, conceptually the method is an elegant way of dealing with one of the big challenges for counterfactual explanations - staying on the data manifold.
Weaknesses:
The framing (abstract/introduction) of the paper took a while to wrap my head around, and could probably be improved. In particular, for classification problems, the ""simple, stupid"" approach of looking for the most negative feature attributions for a prediction seems like it would produce an explanation of uncertainty (though not a counterfactual one, so not competitive to CLUE). This makes Figure 1 pretty puzzling, as it's not clear to me why standard attributions aren't useful for uncertain predictions.
Qualitatively, how would CLUE compare to a standard counterfactual explanation?
Why isn't U-FIDO included in the user study? Given that it performed the best in section 5.1 on the datasets used, the results would be interesting.
In the user study, is the test example linked to the two context points at all? I wouldn't expect unrelated context points to be that useful in classifying a random test example.
Nitpicks:
In the appendix, CLUE is compared against Shapley/LIME on MNIST. LIME is a pretty strange choice, and has never been shown/claimed to be remotely SOTA on neural networks. Something like integrated gradients would be more relevant/interesting.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Why isn't U-FIDO in the user study?
Our access to users (4th year or Masters level Machine Learning students) is decently limited. This complicates running as many user study experiments as we would have liked. We decided to pick Uncertainty Sensitivity Analysis instead of U-FIDO for our user study, since Uncertainty Sensitivity Analysis is the only existing ML interpretability method from the literature that targets uncertainty (recall we had to retrofit FIDO to become U-FIDO to compare to CLUE). We also include the very strong human baseline, where counterfactual explanations are manually selected by other users.
""In the user study, is the test example linked to the two context points at all? I wouldn't expect unrelated context points to be that useful in classifying a random test example.""
This is an important point. Test data points are related to uncertain context data points. The selection of these is as follows: A participant who will not take the main study is shown a pool of certain and uncertain points and a separate pool of uncertain points. They are told to select pairs of related points, one from each pool. The points selected from the first pool will act as test points while the points selected from the second pool act as uncertain context points. We then apply our explanation methods to the uncertain context points to generate counterfactual explanations. Because the test and uncertain context points are selected together, we expect there to be some transferability between the counterfactual explanations for uncertain points and the test points.
The above is described in the first two paragraphs of section 5.2 and in Figure 8.
In appendix F: LIME is bad for explaining NNs, integrated gradients could be used instead.
As discussed in Appendix F, we tried to apply Deep SHAP, a feature importance approach similar to Integrated Gradients in that it takes into account the internal structure of our classifier NNs by evaluating their gradients. Unfortunately, this method produced very noisy explanations when applied to our BNNs. We conjecture that this high variance might be induced by disagreement among the multiple weight configurations from our BNNs.
We found approaches that treat the model as a complete black box to perform better. For this reason, we chose Lime and Kernel SHAP as baselines in the appendix and a FIDO based method as a baseline in the main text. You are of course correct that linear explanations, like LIME, of a non-linear model, like a NN, will necessarily be unfaithful. We discuss this in Section 2.2. We never claim LIME to be state of the art for NNs. However, we did find it to qualitatively outperform gradient-aware methods for explaining our BNNs’ uncertainty.
We believe that the application of feature importance techniques to models that capture weight uncertainty is a very interesting direction for future research but it is outside of the scope of this work.","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","This paper introduces CLUE -- a method to explain uncertainty estimates. The method utilizes a VAE trained on the original data set to search effectively for low confidence instances. The method utilizes a gradient based search through the latent space of the VAE. The authors assess their approach on a variety of tabular data sets and MNIST. They evaluate along change in uncertainty of the counterfactual as well as human evaluation. They generally find improvements using their method over baselines. Additionally, their method works much better in human evaluations.
Comments + Questions
Section 1:
The authors argue that CLUE can be used to complement feature importance techniques like LIME, Saliency Maps, etc. They point out that when the model is confident, you can use a feature importance technique. When it is not confident, you can use CLUE. However, the motivation behind why this dual approach is useful is not quite clear. With feature attribution methods, the goal is to understand what features the model is locally relying on. If the goal is to understand how the model behaves and the model is uncertain for a particular point, it could still be quite insightful to use feature attribution methods. The authors could better argue why their complementary technique is useful and make explicitly clear why you wouldn't want to use feature importance methods for uncertainty data instances like they suggest. Right now, it is not so clear.
One minor point is that in the second paragraph, the authors motivate their method by saying CLUE can be useful to understand features contributing to uncertainty for instances underrepresented in the training data. This motivation doesn't connect quite so clearly to the example in figure 1 --- we can see that the data instance is somewhat ambiguous. Would the solution here to be to collect more ambiguous 6's? This motivating example could flow more clearly if it were a tabular instance because it would immediately connect to the motivating scenario given immediately before.
Section 3:
In section 3, we see CLUE ""aims to find points in latent space which generate inputs similar to an original observation x0 but are assigned low uncertainty."" However, in the introduction it was stated that “CLUEs answer the question: What is the smallest change that could be made to an input, while keeping it in distribution, so that our model becomes more certain in its decision for said input?” These two claims seem slightly at odds. Should the claim in the introduction be revised?
Section 4:
The evaluation procedure claims using data generated through a VAE as the training data will reduce the possibility of clue exploiting adversarial vulnerabilities. Recent work has pointed out adversarial vulnerabilities might be part of the training data for image data sets as nonrobust features [1] and in this way could be captured by the VAE making this procedure less effective. The bulk of this work is focused on tabular datasets and MNIST where this is less likely an issue, so I am not too concerned. However, scaling this procedure up to larger image data sets could produce issues.
A larger concern is that by only using data produced through a VAE, CLUE has a bit of an unfair advantage over methods like localized sensitivity which don't explicitly require the use of a VAE. Meaning, the representational capacity of the VAE trained for CLUE is limited and in this way may not find certain diverse data points with low confidence. A method that doesn't use a VAE might be able to find these points -- though search could be more challenging. By forcing the set of images we're considering to be those produced by a VAE, this technique is shifting the playing field in favor of CLUE in what feels like a bit unfair way. The evaluation should take into consideration that the requirement to train a VAE could be a disadvantage of CLUE but establish it is worthwhile nonetheless.
Section 5:
From table 1, CLUE's performance seems relatively well balanced with U-FIDO in many of the tabular tasks. The authors point out at the bottom of page 6 that CLUE performs better on higher dimensional data sets. However, this is only apparent in the MNIST data set. If CLUE's merits lie with higher dimensional data sets like images, it could be better to provide more evaluation in these settings.
In the reference for appendix h.2 and the
logpgt(⋅)
test, it again feels like the assessment is a bit unfairly advantaged to CLUE; it feels very likely for CLUE to produce the best counterfactuals according to this metric because we assess data likelihood using a VAE. At the same time, CLUE only generates counterfactuals on the VAE manifold.
Maybe the authors could consider including some metric like nearest neighbor distance to the original training set for both clue and the baselines (where the baselines are run without being restricted to VAE generated data)? This could help us better understand the limitations imposed by using a VAE with CLUE. I think that an additional metric that disentangles the effect of the VAE within CLUE is needed here.
The human study results add a lot of merit to the CLUE approach. It's clear from these that the counterfactuals produced by clue are much more human interpretable.
Though the right hand side of figure 10 helps use understand the limitations of using a VAE and is much appreciated, I still think an additional evaluation metric is needed for the 5.1 experiments. An additional metric would help understand if the section 4 technique is giving CLUE an unfair advantage over the baselines.
Overall: I am convinced after reading the paper that the method exhibits useful performance in finding human meaningful uncertainty focused counterfactuals. Further, there are a number of strong experiments in the paper -- I particularly liked the human evaluation and found this convincing. That said, there are a number of weaknesses that I'll mainly divide into two categories: (1) Introduction: per my comments in the introduction, I think this section could be significantly strengthened. Most importantly here, the authors describe their method being used in an explanation workflow where uncertain instances are explained with clue and certain instances with something like LIME. This thread, which seems like a key focus initially, is dropped for the rest of the paper. It's currently unclear why this workflow makes sense and warrants much more justification. Further, it could be worthwhile just to motivate CLUE as a method to explain uncertainty estimates in its own right because the connection with methods like LIME isn't explored in the rest of the paper. (2) Evaluation technique from section 4: Using data generated from a VAEAC as the set of legitimate images could give CLUE an unfair advantage over baselines because it reduces the potential effects caused by representation capacity of the CLUE VAE. It would make this section much stronger to include another metric to try and isolate these effects. I would appreciate some author clarification here as well, in case I am misunderstanding something about the evaluation technique.
One final minor point is the authors claim their method works better than baselines in higher dimensional data. However, they only evaluate using MNIST where VAE's tend to be very strong. To fully substantiate this claim, it could be worthwhile to consider a few slightly more challenging data sets for VAEs (street view house numbers, celeba, etc).
My sentiments are currently leaning towards reject mainly due to the motivation and experimental issues described in (1) and (2). If the authors could remedy these concerns, I'd be inclined to raise my score because I think their are a number of potential valuable contributions in the work.
One related method that isn't discussed is https://arxiv.org/abs/2002.10248. The authors generate instances at certain levels of prediction confidence --- though different it could be good to bring up.
[1] https://arxiv.org/abs/1905.02175
---- update ---- In response to the author's comments and extensions, I've raised my score.","6: Marginally above acceptance threshold","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Credit (23) MNIST (784)
dNN−2(xc,D)
ΔH
ΔHdNN−2
dNN−2(xc,D)
ΔH
ΔHdNN−2
0.770 0.224 0.121 6.903 0.601 0.087
1.025 0.147 0.293 4.374 0.628 0.153
1.863 0.017 0.052 4.887 0.409 0.088
CLUE outperforms U-FIDO in terms of
dNN−2
on all datasets except wine, where both approaches are very similar. The same is true for the ratio
ΔHdNN−2
. Like in the artificial data experiments, the difference between both methods is most stark in high dimensional datasets (MNIST and Credit). Here, CLUE is able to explain away more uncertainty while providing counterfactuals that are similarly close to the training data.
Sensitivity is able to greatly reduce uncertainty in high dimensions. However this comes at the cost going off the training manifold. In low dimensions, there are less possible directions in which steps can be taken, rendering the direct gradient based approach less powerful.
We would like to note that these results are similar to the ones obtained in the analogous experiment with artificial data (table 5).
We are open to additional feedback and suggestions.","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Evaluation Framework (Section 4): Potentially unfair evaluation when using VAE data generators
We appreciate your concerns regarding the use of a VAE-based model as a generative process for artificial data. Thanks for suggesting the Nearest Neighbour distance to train set experiment. We agree it would help dispel concerns regarding our evaluation under a ""ground truth"" generative model. Please see the comment above for this experiment's results and some discussion.
We carefully designed our functional experiments such that the result would not be impacted by our choice of “ground-truth” data generating model. This is discussed in Appendix I and summarised below.
First of all, we would like to clarify that the generative model which acts as a ground truth generative process for artificial data is different from the generative models used by CLUE and U-FIDO. As described in Appendix I, we employ a 2-level VAE structure (https://arxiv.org/abs/1903.05789) for our ground truth DGMs, which increases these models’ expressiveness significantly. We train our ground truth DGMs on real data and then generate artificial data with said generative model. In the realm of artificial data, this generative model is the ground truth. Thus, its uncertainty represents the true aleatoric uncertainty of the generated data with respect to the generating process of the artificial data. This allows us to measure the effectiveness of different explanation methods exactly. In our experiments with artificial data, both CLUE’s and FIDO’s generative models are trained on artificial data from the ground truth generative model.
In appendix I.1, we dispel two potential concerns with the above setup:
The first potential issue is that the synthetic data might not be diverse enough to highlight differences among the methods being compared. The two-level VAEAC setup described above partially addresses this point. More importantly, our results from Table 1 and Table 5 show noticeable differences in performance across methods. This includes CLUE and U-FIDO, both of which use VAE-based generative models to build counterfactuals.
We now address the opposite concern; methods that leverage auxiliary VAEs might be unfairly advantaged under our functionally grounded framework, because the generative process of our synthetic data is also VAE-based. Because VAEs are very flexible neural network based generative models, using them as a ground truth generative process provides relatively little inductive biases for auxiliary DGMs to take advantage of. Again, we do not observe hints of this sort of bias in our results for either experiment.
In our first experiment (Table 1), for each method, we compare the amount of uncertainty explained away by counterfactuals
ΔHgt
with the distance between counterfactuals and original inputs
||x0−xc||
. Only
Hgt
is measured with the ground-truth generative model. Critically, the metric of interest,
Hgt
, only depends on the ground truth VAEAC’s conditional distribution over targets
pgt(y|x)
. U-FIDO and CLUE’s auxiliary DGMs only model inputs
p(x)
.
In our second experiment (Table 5),
||x0−xc||
is substituted by counterfactual explanations’ probability density under the ground truth VAEAC
pgt(xc)
. Both our proposed method and baseline, (CLUE and U-FIDO) leverage VAE-based auxiliary DGMs. Thus, both would be equally advantaged under this setup. However, we observe larger differences in performance between CLUE and U-FIDO when using this metric than in the first experiment.
To further clarify this, we have added a reference to Appendix I from section 4.
Indeed, the results from the suggested experiment using real data and Nearest Neighbour distance resemble those from table 5 in our original manuscript.","",""
"Getting a CLUE: A Method for Explaining Uncertainty Estimates","Keywords: interpretability, uncertainty, explainability","Thank you or the detailed responses and extensions. These experiments are convincing and have quelled my earlier concerns.
In regards to the discrepancy point, I took a look at this again and realized I misunderstood what was being said. I appreciate the authors clarification!
As such, I've updated my score.","",""
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","The paper presents some exciting results on the convergence of averaged SGD for overparameterized two-layer neural networks. The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews. The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript.","The paper presents some exciting results on the convergence of averaged SGD for overparameterized two-layer neural networks. The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews. The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript.",""
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","We thank all reviewers for the positive feedback and helpful comments. We have revised our paper by taking into account some of their suggestions. (During the rebuttal phase the page limit is 9 pages). The major changes are summarized below.
We have elaborated on some description of the mathematical definition, assumptions, and algorithm.
We have made a small change: In Assumption (A2),
γ∈(0,1]
→
γ∈[0,1]
, and added a remark. See also comment to Reviewer 5.
We have added some citations.
We have added a simplified version of Proposition A to the introduction in order to highlight the contribution. See also comment to Reviewer 1.","",""
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","Summary: This paper considers the convergence property of averaged stochastic gradient descent on a overparameterized two-layer neural networks for a regression problem. This paper is the first to achieve the optimal convergence rate under the NTK regime. They show that smooth target functions efficiently specified by the NTK are learned rapidly at faster convergence rate.
##########################################################################Pros:
The paper is technically sound. It adapt the neural networks and the RKHSs theory into the NTK regime and the proof techniques are different from existing literatures.
It achieves the minimax optimal convergence rate of
O(T−2rβ/2rβ+1)
which is always faster than
O(T−1/2)
.
This work shows the connection between RKHS and NN in term of the
L∞(ρX)
norm while the convergence result does not need the positivity of the NTK
########################################################################## Cons:
The writing of this paper is not clear enough, for example
L2(ρ(x))
appeared without any explaination
The practical choice of M in their experiment is 2e4, which is impractical in NN tasks
########################################################################## Overall, I vote for accepting. This paper combine the convergence analysis of averaged stochastic gradient descent on kernel methods with the connection of kernel method with neural network to derive an optimal convergence rate of NN in NTK regime while the proof technique is novel. My major concern is about the practical influence of this paper to the theory of deep learning training, as the width of the output layer is required to be large.
#########################################################################
Questions: In Assumption A3, why the parameter
r
has to be in the range [1/2, 1] Is the averaging mechanism at the output of the algorithm beneficial to the rate in Theorem 1?","7: Good paper, accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","This paper analyzed the averaged SGD for overparameterized two-layer NNs for regression problems. Particularly, they show that the averaged SGD can achieve the minimax optimal convergence rate, with the global convergence guarantee. To achieve, they propose a new parameter which captures the ``complexities’’ of the target function and the RKHS associated with the NTK.
The paper is well-written, and the result looks very interesting. I am tending to accept the paper. This paper is a theory, so experiments are a plus. If the authors can address some of my comments, I am tending to increase the score of the paper.
Here are some comments about writing.
I believe Assumption 1 and 2 are reasonable. But each statement is just math, it is good to write a 2~4 words to summarize each A1, A2 .. Also several math statements highly replied on the definitions, and it is hard to find them in the paper.
After Assumption 1, in the next page, page 5, there is a Remark that has 4 bullets, maybe write Ai at the beginning of each bullet.
Algorithm 1 requires more words and explanation, it is hard to understand this algorithm in the following sense : the size of each matrix/vector is not mentioned and hard to find them in the paper.
In page 6, is it possible to simplify the statement of Theorem 1 a bit? e.g. write a simplified version here, and put the full version in appendix.
In Theorem 1, what is M_0? Is that over-parameterization size? Is that polynomial in parameters or exponentially in parameters? (This is not major point of the paper, I am just curious about the bound)
In page 4, Eq. (2), is it possible to consider a simple model where gamma = 0? This is quite common in previous work.
In appendix, e.g. page 30, the last step of many equations use ->0. I don’t follow the meaning of this notation. Is that possible to avoid it?
This paper is focusing on average SGD, is there any intuition why non-average SGD won’t give the similar result?
I felt the following paper is highly close to this work, and should be cited and discussed more deeply. Usually optimization has two parts, one is the number of iterations, and the other is cost per iteration. This paper focused on improving the number of iterations. The following paper improved the cost per iteration, in the NTK overparameterized regime. Training (Overparametrized) Neural Networks in Near-Linear Time Jan van den Brand, Binghui Peng, Zhao Song, Omri Weinstein
Minor comments
In page 1, second paragraph, the place cited Du et al. 2019b, Allen-Zhu et al. 2019 and Du et al. 2019a.
The following two papers should also be cited
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. [This paper shows the result for recurrent neural networks. Note that RNN is a harder case, In deep neural networks the weight matrices in different layers are different. However in RNN, the weights matrix are the same over all the layers]
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. [This paper improved the over-parameterization bound from m >= n^6 (Du et al. 2019b) to m >= n^4, where m is the width of a neural network, and n is the number of input data points.]
In page 2, the first paragraph, the place cited Du et al. 2019b, Arora et al. 2019a, Weinan et al. 2019, Arora et al. 2019b, Lee et al,. 2019.
The following papers should also be cited
Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score sampling for neural networks. [Arora et al. paper shows a connection between neural networks with neural tangent kernel regression. This paper generalizes the Arora et al result, and shows the connection between regularized neural networks with neural tangent kernel ridge regression.]
Similarly, in page 5, some citations should be added.
Small typos: The last paragraph, Page 2 “the key to show” -> “the key to showing” The third paragraph, Page 3 “which enable” -> “which enables” The fifth paragraph, Page 4 “ A stochastic gradient descent” -> “Stochastic gradient descent” The third paragraph Page 5 “a neural networks” -> “a neural network” The third paragraph Page 6 “arbitrary small” -> “arbitrarily small” The first paragraph Page 8 “the single-layer learning” -> “single-layer learning”","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","Here is the review of the article: ``Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime''.
SUMMARY
The authors show that under the Neural Tangent Kernel Regime investigated lately, averaged SGD achieves optimal rates in the attainable case. Note that this result is not a plug and play based on the current kernelized-SGD litterature: the difficulty the authors achieve to overcome is the fact that they could bound the difference between the dynamics of SGD on the neural networks and on the neural tangent kernel. This is stated in Proposition A and it represents the novelty of the article
Moreover, they give an explicit representation of the capacity condition (decrease rate of the eigenvalues of the covariance matrix) in the cases where they have a smooth approximation of the ReLu.
Clarity
The paper is outstandly clear. Indeed, despite the fact that optimality in RKHS can be technical to introduce, I found the paper very clearly presented and well motivated. It was very pleasant and smooth to read. The references are also both precise and sufficient to understand well the problem.
The only default of the paper is that, in my opinion, the authors do not stress enough their contribution and their novelty. Indeed, despite Proposition A and the sketch of the proof, the article consists in a plug and play result on averaged SGD. The authors should then stress the outline of the proof (going to a M-approximation of the Neural Tangent RKHS) and comparing the dynamics on these.
Quality and Originality
The paper is not super original, and as accustomed to this literature, there is no surprise seeing this result. However, the quality of the paper is undeniable and fills the gap between optimality of kernel methods and the NTK literature. I thank the authors to have done it very clearly.
Comments
-My main comment is about the fact that except from Proposition A, this article is a plug and play one. This proposition, and the full sketch of the proof should be emphasized as they consist on the true novelty of the work.
-Three remarks concerning the plots :
1-Minor comment. They should be bigger. 2-Minor comment. Figure 1 should illustrate the fact that
β=1+1d−1
. I suggest a log-log plot. 3-Intermediate comment. I do not really see how exactly the discussion of the experimental part illustrates really the result. The discussion is fairly interesting, but I really would like to see a theoretical proposition showing that taking the two layers is better for learning.
*** Conclusion***
Yet the fact remains that, I would really like this article to be published when the commentaries of the reviewers will be taken into accounts.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","This paper considers the optimization of a wide two layers neural network (for a regression task) using averaged SGD. The authors consider the Neural Tangent Kernel (NTK) regime. The NTK is a kernel defined using the activation function and the initial distribution of the parameters of the input layer. The RKHS H associated to this NTK is assumed to contain the Bayes predictor. Based on this, the authors derive a convergence rate for the predictor constructed from the T-th iterate of averaged SGD and the Bayes predictor in terms of the L2 distance wrt the distribution of the features. By specifying this bound in terms of the decay of the eigenvalues of the integral operator in H, they obtain an explicit generalization error bound, which is optimal for the class of problems considered in the paper.
It seems that the paper solves an important problem related to the training of neural nets. The paper is rather well written even if some paragraph are hard to understand for a non expert. For example, several paragraphs of the paper are dedicated to compare the obtained results with those of concurrent work. The level of technicality of these discussions makes the reading experience difficult (e.g. last paragraph of Page 6), often because the discussion happens at a step where the reader is not familiar with the results (e.g. Section 1.2). These paragraphs seem like a discussion with the authors of these concurrent works. I would suggest to gather these discussions at the end of the paper, once the reader understands the results. Moreover, a better approach in my opinion would be to explicitly state the results (mathematically) of these concurrent work. This way, the comparison will be easier.
As a non expert, I believe that the results are new but I cannot be sure because I cannot compare with existing works (see my comment above). I recommend acceptation.
Overall, the paper shows an important result: optimal rate for generalization bounds for 2 layers NN in the NTK regime. The result is well explained but some precision could make the paper even more insightful. For instance, why considering the NTK regime? What is the intuition behind that? How would you define mathematically ""the NTK regime"" ? I would also like to understand better the relaxation of the positivity of the NTK. Does it have to do with the assumption that the norm of the integral operator is greater than lambda?
Moreover, Proposition A, which is fundamental in the approach, should be stated in the main paper for a better understanding. I think that the authors can move the numerical experiments to the appendix to win some space.
MINOR:
Check the def of excess risk, last equation of Page 3 ""negative dependence on"" should be ""inverse dependence on"" "", That is, "" Why is Figure 1 in Section 1? Is it a mistake? It should not be placed here. Moreover it is not commented in the text.","8: Top 50% of accepted papers, clear accept","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","Thanks for your answers and your explanations, I keep my score as it is.","",""
"Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime","Keywords: stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel","Summary:
The paper focuses on the understanding of neural tangent kernel (NTK), which has been a central topic in deep learning theory recently and plays an important role in characterizing the generalization ability of artificial neural networks. In Particular, the authors derive minimax-optimal learning rates of the averaged stochastic gradient descent method for over-parametrized two-layer neural networks with smooth activation functions. The results are novel and offer insights into the connections between deep learning methods and kernel methods. One difference of this paper from other studies is that the positivity of NTK is not required in the error analysis. Numerical experiments are also illustrated to confirm the theoretical results. The paper is well written and interesting to read. Overall, I vote for accepting.
Concerns:
Although the paper considers smooth approximations of the ReLU and also shows the explicit optimal rate, its analysis does not apply to ReLU networks which may be of greater interest to researchers.
It looks a little bit strange to me that the regularization is conducted around the initial values. Will this lead to a big difference in the numerical experiments?
Is there an explicit form for the lower bound of network width in Theorem 1, i.e.,
M0
?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Learning Invariant Representations for Reinforcement Learning without Reconstruction","Keywords: rich observations, bisimulation metrics, representation learning, state abstractions","This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning. The method is generic, effective, and is supported by both theoretical and experimental results. All reviewers and I think this is a strong contribution to the area.","This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning. The method is generic, effective, and is supported by both theoretical and experimental results. All reviewers and I think this is a strong contribution to the area.",""
"Learning Invariant Representations for Reinforcement Learning without Reconstruction","Keywords: rich observations, bisimulation metrics, representation learning, state abstractions","The authors propose an approach to robust representation learning of observations for reinforcement learning by training a model to align the euclidean distance between two observations with bisimulation metrics that quantify how similar the states that generated the observations are in terms of the control problem. This reduces the effect of irrelevant features in the observations on the representations.
The paper is well written, the problem is clearly motivated and the approach and technical contribution is easy to follow.
The approach to use the state bisimulation metric to supervise observation representation is intuitive and clearly motivated. Theoretical analysis is provided with generalization guarantees.
""As an example, in the context of autonomous driving, an intervention can be a change in weather, or a change from day to night which affects the observation space but not the dynamics or reward."" I do not agree with the example as weather can directly alter the dynamics and desired behavior of an AV system. The point in this paragraph is still clear but I would suggest a different example.
The evaluations are strong and run on a number of different experiment settings against multiple strong SOTA models.
In Figure 4 the proposed approach is outperformed by contrastive learning in the default setting. I understand that the goal is to learn robust representations for the natural setting, but can the authors comment on why it fails to beat the contrastive approach here and provide insight on how this may be addressed. Some problems have only a few distractors and may fall between the natural and the default setting.
recommendation and reasoning
The paper is well written and presents a clear approach to a well motivated problem with strong evaluation results. I recommend acceptance.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Learning Invariant Representations for Reinforcement Learning without Reconstruction","Keywords: rich observations, bisimulation metrics, representation learning, state abstractions","Summary The paper focuses on how learning state-representations that encode information relevant to the task can improve reinforcement learning from pixels. Often, observations in an MDP can contain information that are irrelevant (“distractors”) to the task at hand and can likely “distract” the downstream RL algorithm used. Unlike existing reconstruction based approaches (which don’t explicitly incentivize ignoring task-irrelevant information), the authors propose Deep Bisimulation Control (DBC) that relies on bi-simulation metrics (as the task-aware criterion) that encode behavioral similarity b/w states with respect to the reward structure. Instead of explicitly learning a bi-similarity distance function, authors enforce the representations which under L1 distances correspond to bi-simulation metrics. DBC demonstrates learning these representations in conjunction with the control policy, reward and a dynamics model. Furthermore, the authors highlight connections to causal inference which can hopefully further provide insights into which “new” reward structures can the learned representations generalize to (since bi-similarity metrics themselves are heavily dependent on the reward structure). Results obtained by the authors demonstrate that DBC can learn task specific representations and a control policy in a robust manner in the presence of distractors on the Deepmind Control Suite and the CARLA simulator. Additionally, the authors also demonstrate how DBC can generalize to new reward functions on Mujoco.
Strengths
The paper is generally well-written and easy to follow for the most part. The authors do a good job of walking the reader through the preliminaries, highlighting distinctions with prior usage of bisimilarity metrics and how DBC ties in with slight modifications to the Soft Actor-Critic algorithm.
The results obtained on DM control suite demonstrate that while DBC is competitive or slightly worse compared to other approaches in the absence of distractors, it performs better compared the set of baselines when distractors are introduced — significantly outperforming the reconstruction and the contrastive approaches. Furthermore, compared to a VAE, DBC places observations that are similar in terms of task information closer in the learnt embedding space.
The authors also show that compared to an adaptation of prior usage of bisimulation metrics, DBC is more sample efficient. Additionally, within the considered family of reward functions to generalize to “walker_run” and “walker_stand”, DBC shows stronger generalization compared to DeepMDP. Furthermore, learned embeddings on CARLA demonstrate qualitatively that DBC learns to group behaviorally similar distractor states (manifesting as obstacles).
Weaknesses I don’t have any major weaknesses to point out. I will highlight minor comments / weaknesses which, I think if addressed, would definitely make the paper stronger.
While the authors state that DBC in practice can be combined with any model-free of model-based algorithm, the paper would definitely benefit if this claim was backed up with results demonstrating DBC combined with another model-free or model-based algorithm since it might be unclear off-the-shelf if an algorithm requires major or minor changes to work in conjunction with DBC.
Task generalization (or generalizing over different reward functions) relies on the assumption that the new reward function depends on contributing factors that likely influence the earlier “seen” reward function as well. While empirically validating this claim from walker_walk -> walker_run / walker_stand is a reasonable starting point, demonstrating another instance where inferring the source and target reward functions is not immediately obvious would definitely benefit the paper.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Learning Invariant Representations for Reinforcement Learning without Reconstruction","Keywords: rich observations, bisimulation metrics, representation learning, state abstractions","Paper Summary
The paper presents a new method for embedding visual images into a state space suitable for effective control by an actor-critic style RL algorithm. They show how a previously explored idea of using a bisimulation between state abstractions and reward sequences to group states that are similar from a decision theoretic perspective can be extended to a continuous deep embedded representation using twin network style learning through standard gradient descent optimization. They call the approach Deep Bisimulation for Control (DBC). The paper also argues for the correctness of their approach using a contraction proof and a theoretical argument for generalization to new problem domains. The approach contrasts directly with algorithms that use an autoencoder to find a compact representation by reconstructing input frames or predicting future input frames. These approaches necessarily represent enough information to reconstruct both task relevant and incidental details. Experimentally, the paper shows that performance of reconstruction-based approaches degrades by a large and significant amount when extraneous background detail is present in image frames, while the proposed method is immune. The evaluation is done on widely respected benchmark of Deep Mind MuJoCo based articulated figure simulations and a CARLA realistic image simulation of car driving which shows that focusing on task specific detail is important on realistic tasks.
Pros and Cons
The work addresses a key problem in reinforcement learning, which is learning effective policies from unlabeled visual images. The ability to find compact, task relevant representations of high-dimensional inputs is central to the ICLR community.
The paper covers relevant background in state abstraction in RL and it is clear how it relates to the paper's contribution.
The paper clearly explains the background of bisimulation and how the idea of grouping states according to their probable reward sequences can be practically realized by grouping states by their immediate reward and future state distributions. It is also clear in principle that a twin network can be used to induce a latent space with these properties.
The paper shows that the policy will converge and that the error will be bounded and that the representation will generalize to any problem domain in which causal dependencies are a subset of the problem domain the representation was trained on. Which is nice.
There is a nice evaluation on both MuJoCo articulated figure problems from the Deep Mind Control suite as well as the CARLA simulated driving application that uses large realistically rendered images. Comparison against state-of-the-art RL algorthims makes results convincing. I think the CARLA example is nice as it shows that practical problems without artificial augmentations have the property that the input details can be distracting if fully reconstructed. The superiority of DBC in this case really makes this point well.
The paper does not report computational load associated with their approach. Presumably due to the closed form Wasserstein approximations, we are probably looking at only a fractional increase in time for the encoder network above the policy, dynamics and reward models. So roughly 30% more??
Key figures for central results are too small to get meaningful interpretations unless enlarged by a factor of 4 or 5. Somehow this seems to go against the spirit of page limits to me.
Figure 6 and the description don't seem to be aligned, but I can imagine that it can be readily fixed.
Recommendation
I recommend acceptance. The paper shows how to extend bisimulation principle for grouping states to continuous deep actor critic methods and provides convincing evidence on standard benchmarks that it is effective in extracting a task-relevant abstraction that is robust to noise in observations and focuses on task specific detail.
Questions
Figure 6: it seems that the “simple_distractors” environment is different than “setting 2” natural video setting. This caption could use some reworking to get parallelism clear. The ‘ideal gas’ is the same as simple_distractors? Also the graphs seems to be about different experimental types. Does the first experiment also use frozen encoder? The graphs do not seem to relate to the text which talks about walker_stand, walker_run reward functions. I am confused.
Can the paper have any insight into the relative performance of their algorithm versus benchmarks algorithms across different tasks (finger spin, cheetah, walker) given that they are all stick figures?
Feedback
I was able to work through Definition 1 and assure myself it made sense. In the end I made a small picture that helped.
There seems to be some ambiguity in notation between observations, underlying state and latent variable spaces. For instance, in section 3, script S is defined as a state space. In section 4, the function d is defined on script S x script S which is described as an observation space. Admittedly, the work seems to be situated in an approximately fully observable world in which states and observations are somewhat equivalent. I suspect that is why the 5-camera 300-degree suround view was necessary in the CARLA experiments.
Interesting that the paper employ stop gradients on the latent representation terms when they appear in the reward and Wasserstein terms in the loss function. This is to enforce the separateness of the optmizations in algorithm 1?
Definition 2, the bisimulation metric contains a max … so this is worst case discrepancy between the futures between state space actions and empirical actions. Was average discrepancy considered? This might be relevant later in discussion about intractability in previous approaches (section 4 paragraph 3).
Equation 4 specifies an L1 metric for the distance in abstraction space ||z_i – Z_j ||. Is there a motivation for this choice? Again trying to bound the worst error?
Theorem 4 references Theorem 5 which is not in the main text.
In section 5, ideally epsilon would be briefly described before it appears in a bound.
Figure 3 is very small... the labels – particularly the subscripts are unreadable. It may not be making a point important enough to include it. The causal variables section could be shorted in general. It is a good point but not completely surprising.
Figure 4 is a key figure to support the paper's hypothesis that deep bisimulation is effective for state abstraction in noisy images. This figure really needs to be bigger. In particular, I had to strain to read the legends to understand if the axes were different between the uncluttered video of articulated figures and the cluttered video of articulated figures with a background movie in them. It was also hard to make out which line was which. In particular, the “cheetah” column does not show DBC improving on cluttered video scenario --- it tops out at around 250 in both top and bottom graphs, but the scales are very different. It confuses the message a bit.
There is a statement “a single loss function would be less stable and require balancing the components”. Is this speculation or based on experience? Separate optimizations implicitly define a balance between these terms wouldn’t they? Namely equal balance?
To some degree, figure 9 is making the same argument as figure 5. Could leave this out if you were tight for space.
Figure 5: What is the first column of figures to the left? It doesn’t seem to be relevant? Otherwise, I think the figure is very effective in conveying the structured embedding space does a better job of grouping similar states. It is also very small. Drawing a white border between the figure pairs would underscore visually that there are two states in each “image”
Figure 9: I could not make sense of the images on the sides of the figure. Particularly, the images on the left side. They seemed to be abstract geometric shapes and I could not get an intuition about what the driving scenario was.
Section 6.4 “reward highway progression an penalizes collisions” an => and
Future work – another likely avenue for future work would be to introduce some sort of memory to handle partially observable worlds. For instance, can the agent drive a car with only a forward view if given memory? Does this break down if it does not have memory? This could either be an explicit neural memory or implicit memory such as an LSTM … Estimating uncertainty could also be important to produce agents that can work in the real world and assess when they know what they are doing and when they do not. Another future work area is in modeling of transition distributions as something more complex than Gaussians … what if there are distinct possible futures that are equally valid: it is ok for the robot to turn left or right as long as it avoids the object straight ahead?","9: Top 15% of accepted papers, strong accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","This paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction. The claim in this paper is that ""the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions"". The main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves. Nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.","This paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction. The claim in this paper is that ""the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions"". The main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves. Nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.",""
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","A note on the changes made to the draft.
Included discussion of Finite-difference Score Matching (ref. comment by Tianyu Pang). Broad difference between the two is the use of centered vs uncentered finite differences, and the application to discriminative neural networks in our case, as opposed to noise contrastive networks or denoising score matching.
Included hyper-parameter sweep experiments in the supplementary material, as suggested by Reviewer2.
Included references and suggestions by Reviewer3. We renamed Section 4 which was previously called ""Interpretability through the lens of density modelling"" to ""Implications of the density modelling viewpoint"", but did not change its location as this section introduces discriminative pixel perturbation test, which is a pre-requisite for Section 5.
Included discussion suggested by Reviewer4 in the conclusion section.","",""
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","We understand the main purpose of the paper is to rethink how the logits gradient reflects the interpretability in a discriminative model. This is very interesting.
We find that Eq. (3) in this paper applies a finite-difference (FD) method to approximate the Hessian trace in score matching (SM). Actually, there is a previous work on finite-difference score matching [1], which provides a better FD approximation (i.e., less biased when sampling) and faster implementation (i.e., parallelizing the FD calculations) for the SM methods. We think [1] may be helpful for the proposed method in this paper.
Reference
[1] Efficient Learning of Generative Models via Finite-Difference Score Matching, NeurIPS 2020.
Paper: https://proceedings.neurips.cc/paper/2020/file/de6b1cf3fb0a3aa1244d30f7b8c29c41-Paper.pdf
Code: https://github.com/taufikxu/FD-ScoreMatching","",""
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","This paper examines gradient-based attribution methods that have been proposed in the explainability literature from a theoretical perspective motivated by a recent observation in energy-based generative models. First, the authors point out a general weakness of gradient-based attribution that derives from the fact that input-gradients do not provide well-defined explanations, since the shift-invariance of the softmax output makes them arbitrary. The authors then propose that the reason for the success of gradient-based attribution models can be explained by the fact that discriminative models ""contain an implicit"" class-conditional density model (the mentioned recent observation about energy-based generative models). They then go on to elaborate on this idea showing how aligning the implicit class-conditional generative model to the ""true"" generative model of the data would help provide relates to gradient-based attribution, how the alignment can be efficiently promoted with a novel implementation of score-matching, and how this mechanism can be practically realized as regularization costs. The authors then carry out empirical studies that convincingly confirm the prediction of their theoretical ideas. First, they show that samples generated with score-matching and the proposed gradient-norm regularization are better in the sense of being less noisy and in terms of their discriminative accuracy via a trained discriminative model as proposed by the ""GAN-test approach"". Finally, they show that the quality of gradient-based explanations are better according to a discriminative version of the pixel perturbation test, a method to evaluate gradient explanations by perturbing pixels ranked in increasing order of relevance. In conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","Summary
The key message of this paper is that input-gradients (gradient of the logit wrt to input) or loss-gradients are/might be unrelated to the discriminative capabilities of a DNN. The input-gradient is a key primitive in several interpretability and visualization methods. Until now, it has been taken as a given that these gradients reveal 'why' or what parts of the inputs the model is sensitive to. However, this paper questions this reasoning and says that if the input-gradients can be easily manipulated without changing the generalization ability of the model, then does the input-gradient really contain discriminative signals?
To test their hypothesis, the paper re-interprets the input-gradient as class-conditional generative model using the score matching view. The paper then develops a 'regularizer' that is called a taylor trace estimator that requires less backward passes than the hutchinson estimator, which when added to the model objective can modulate how 'generative' the model is. With this regularizer, the papers tests the hypothesis that improving the implicit density model also improves input-gradient interpretability. The paper tests this through experiments on an image dataset and finds that this is the case.
Significance
This work has far reaching significance for the field of visual interpretability of DNNs. It suggests that reading into these input-gradients might be akin to reading tea leaves. The key insight in this work is simple and the demonstration is quite powerful in my opinion. The argument in this paper seems obvious in hindsight, but that is exactly why the paper is a significant one. I have several additional questions later in my review, but this work is important and suggests that insights based on input-gradients might be spurious.
Clarity
Overall, the paper is relatively clear and easy to read. Several of the key experiments are well-justified.
Originality
The insight in this paper via the score-matching perspective is new in the interpretability domain. The claim that input-gradients can be easily manipulated is not new, but the general insight in this work is new and important. Overall, this paper opens up several questions about what input-gradients really convey.
Taylor Trace Estimator
I am confused about the derivation of this estimator and I am probably missing something, so can you walk me through this? Let's say the Taylor series expansion around a point x is:
f(y)=f(x)+∇f(x)⊤(y−x)+12(y−x)⊤∇2f(x)(y−x)+O(‖y−x‖3)
now if solve for
(y−x)⊤∇2f(x)(y−x)
, we get:
12(y−x)⊤∇2f(x)(y−x)=f(y)−f(x)−∇f(x)⊤(y−x)−O(‖y−x‖3)
v=y−x
in your notation, and it is a zero mean gaussian, so we get:
E[∇f(x)⊤v]=0
, which leads to the approximation that you get. However, where did the
1σ2
outside the expectation come from in the final form?
The point on Adversarial Training
In section 4, this paper notes that recent work has shown that when a model is trained with explanation penalization, it results in more 'interpretable' gradients. This connection was made more formal in recent work (https://arxiv.org/abs/1810.06583.pdf) that notes that training models while penalizing their integrated gradients explanations is equivalent (Thm 5.1 in that paper, for the right loss function and some other assumptions) to
ℓ−∞
adversarial training.
Other Feedback and Questions
The current section 4 is really a discussion/implications section. I suggest the authors call it that. It should also likely come after section 5 since that is where the experimental results are. From reading section 4, am I right to conclude that the paper is also suggesting that activation maximization, pixel perturbation, and the results of adversarial training say more about the implicit generative model than discriminative information for a DNN? That is, I should also not take the results of activation maximization as explaining to me what a neuron that learned?
Do these results extend to methods that post-process or use the input-gradients as a primitive? For example, smoothgrad adds noise to the input and takes an average of the corresponding input gradients. Integrated gradients can be seen as as sum of interpolated input-gradients along an all-zeroes input to an input of interest. Should I also take it from these results that Smoothgrad and integrated gradients don't indicate discriminative behavior as well?
For example, consider grad-cam, which looks at the output of a convolutional layer in computing a sensitivity map as opposed to the logits, does your analysis apply to that case too? I think it probably doesn't, unless one can also view the output of convolutional filters are implicit density models as well.
Does the analysis in section 2 apply to the probability output from softmax? I.e., can the 'probability-gradients' also be arbitrarily manipulated?
Overall, this work raises several important questions like why trained models have implicit density models that are aligned with the inputs in the first place. This question seems key to tying up the remaining loose ends in this work. This said, this paper is still a thought-provoking one and a useful one for the literature on DNN interpretability.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","Thanks to the authors for clarifying some of my confusion and providing answer. After reading these responses and digesting the work, I now some additional questions.
When is a generative model encoded within a discriminative one?
The crux of the argument in this paper seems to be that logits are class conditional generative models. The authors allude to the 'score-matching' literature to justify this view. However, under what assumptions can we view the logits this way? For example, for a binary logistic model, I don't think this is the case. Is it only for a softmax-based classifier? Specifically, can a softmax logistic multi-class classifier be interpreted in a similar fashion? Section 3 of https://arxiv.org/pdf/1912.03263.pdf is useful, but not quite clear on what assumptions need to be in place for the EBM view to be valid. What if we replace the final layer of say a resnet/deep NN with a single unit and minimize the l2 regression loss. Will this also fit under the view here? In general, I am trying to understand when we can say an output unit encodes a generative model.
A basic follow-up question on the TTE: so can this estimate only be used for functions? Let's say I form a random matrix A (k by k), and I want to use the TTE for this setting, how will I do that? In the case of hutchinson, I just do: v'Av where v is rademacher or gaussian. What is the equivalent for TTE? I know this defeats the purpose of the estimator in some sense since if one could form the matrix, then there is no need to approximate the trace, one could just compute it. I just wonder if I am missing something.","",""
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","In this work the authors explore the link between the explanatory power of input-gradients and the alignment between the ""implicit density model"" of the softmax-based deep model and the ""ground truth"" class-conditional data distribution. The authors propose using score-matching method to create models with varying degrees of alignment. The paper is full of interesting insights and ideas, such as soft-max shift invariance property and trivial input-gradient perturbation, connections between score-matching and adversarial training, and others. However, in the end, the paper's impact on how ML engineers will use interpretability tools is in my opinion limited. The authors successfully introduce some interesting heuristics to make the training of score-matching models more scalable and stable. However even with those heuristics ,gradient-norm regularized models are comparable if not superior in the 3 evaluations presented by the authors: GAN-test score, pixel perturbation results, and perceptual alignment of input-gradients. The authors provide enough evidence to validate the main hypothesis of the paper. There are no inconsistencies or errors that I can see in the paper to the best of my knowledge. The paper is clearly written and well structured. To improve the paper, the authors could add some comments expanding on the practical impact that this results will have on the work of ML engineers who use input-gradients as a tool to improve model accuracy.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","This paper investigates the utility of interpreting deep classification models using the gradients of the output logits w.r.t the inputs, a common practice that is also potentially misleading.
The hypothesis stated for the paper is ""input-gradients are highly structured because this implicit density model is aligned with the ‘ground truth’ class-conditional data distribution?""
For the observation in section 2 (also maybe number blocks like that similar to lemmas and hypotheses to make it easier to refer to them) if g is 0 then this is trivially true and not saying anything. I think I know what you want to say but this formalism is not adding any clarity. I think adding some constraints to g or simply calling it a variable which takes on specific values. It is not clear why it needs to be conditioned on x either.
3.3 Stabilized score matching: It seems multiple published methods did not help you prevent collapse of the Hessian-trace, but your heuristic did. Is this a common trick or a novel contribution? It would be nice to indicate it one way or another, if it solves a real problem that previous score-matching approaches fail to solve. Furthermore, it would be important to know the sensitivity of your approach to the choice of this hyperparameter.
The importance of section 4.1 is not clear to me… it appears that the authors believe activity maximization is a biased explainability measure and therefore should not be used if one accepts their framework. Their intention for this paragraph should be more explicit.
Section 4.2 draws a tight parallel between the pixel perturbation test and their density ratio test, demonstrating that the pixel perturbation test captures the sensitivity of the implicit density model and not the discriminative model. They therefore suggest this test always be done while tracking changes in classification output, which is a nice takeaway.
In section 4.3, the authors draw a parallel between their score-matching objective and adversarial training, although they state that “score matching imposes a stronger constraint”. I am not an expert on these topics, but I think these kinds of more speculative observations should be moved to the discussion in general.
In section 5, the authors introduce their experimental setup. They used a baseline model ResNet-18 model with 78.01% accuracy, and compared it with their regularized model that only achieved 72.20% accuracy (5.8% drop). The authors weight the entire regularization term with a single lambda =1e-3. This raises a few important questions. First, the strength of the stability regularizer and the score matching terms should likely be decoupled to achieve maximum classification performance which is crucial in practice. The difficulty of tuning these hyperparamaters are also extremely important for us to understand the utility of the Author’s proposed approach. It would be good for us to see the results of a proper hyperparamater search over the weighting of the score-matching and stability regularization term independently. Ideally, the performance drop observed can be reduced or eliminated.
Second, presumably, if the score matching loss itself hurts classification performance, then intuitively the intuitions built early in the paper cannot be correct: if the aligned density functions p_theta(X) and p_data(X) do can not arise from logits that produce the optimal classifier, then the saliency maps produced by this method cannot be used to diagnose model correctness (as the practitioners who utilize saliency maps would hope). Since the score matching loss is only accurate up to some constant, perhaps this is the source of the issue, but we cannot conclude one way or the other from the data provided.
Furthermore, few details are given for the training setup: how long was each model trained for, was early stopping employed, were multiple seeds evaluated, are we convinced that all models converged? These are important for doing the relevant comparisons, and many of the later results are hard to interpret in the absence of hyperparamater tuning or these experimental details.
Anti-score matching is a great baseline experiment, but the use of the threshold seems arbitrary. This is doublyThis doubly true because the lambda of the regularization is smaller (1e-4). Why this discrepancy? It makes it harder for us to compare the results. How does this model perform as that threshold is varied? If one does a hyperparamater search with a fixed threshold, one would find the best performing anti-score matched model, which would potentially be easier to interpret. Crucially, this approach seems to actually outperform the score-matching model and underperform the baseline, implying that either the lower lambda or the anti-score constraint improved the performance of the classifier, but we cannot know which.
A similar comment can be made about the gradient-norm model RE: hyperparameter searches.
The Density Ratio experiments are excellent, but the y axis is hard to read.
For the Sample Quality experiments: why would the gradient norm model outperform score matching? I am not convinced by the speculation given in the paper. It might be the case that the score-matched models were not converged, highlighting the importance of improving or simply explaining the experimental setup as I mentioned earlier.
Finally, the Gradient Visualization experiments are very unclear. Intuitively, it might be the case that a small portion of the image is enough to explain the class of the data, which is what saliency maps are typically used for. The gradient norm approach and your score matching approach appear to perform almost identically, and it isn’t clear how much better they perform in a practical sense. It would be nice to have a more convincing demonstration of examples where the model obviously classifieds an image using the appropriate information present in using your method but not the baseline. As it stands, your results appear to be largely due to the fact that the gradients are smoother when using score matching or gradient-norm regularization.
In summary, I really like the approach and theory presented in the paper, and tackles an important issue with broad relevance to the field, but the experimental results as they stand are not sufficient to convince me that this approach works in practice.
Typo Section 4: ""show how these can interpreted from""","5: Marginally below acceptance threshold","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","""The difficulty of tuning these hyperparamaters are also extremely important for us to understand the utility of the Author’s proposed approach. It would be good for us to see the results of a proper hyperparamater search over the weighting of the score-matching and stability regularization term independently. Ideally, the performance drop observed can be reduced or eliminated.""
We present results on a hyper-parameter sweep on the
λ
and
μ
parameters of score-matching, where we provide both test-set accuracy on CIFAR100 and the corresponding GAN-test scores. We find upon performing a hyper-parameter sweep that
λ=1e−5
and
μ=1e−3
seems to perform the best, whereas in the paper we present results for
λ=1e−3
and
μ=1e−4
(mistakenly indicated as
μ=1e−3
in the draft). It is possible that changing the training schedule by increasing the number of epochs or learning rate may further improve these results, but we did not explore that here. However please note that this does not change the main findings of the paper, and only strengthens it by showing that more accurate score-matched models can be obtained. We will add these results in the supplementary section of the paper and include a short discussion in the main paper.
λ↓/μ→
1e−2
1e−3
1e−4
1e−5
1e−2
48.68 % / 51.60% 64.57% / 58.90% 64.75% / 76.46% 9.08 % / 0.97%
1e−3
64.64% / 56.78% 71.37% / 40.72% 72.34% / 73.39% 34.46% / 3.3%
1e−4
69.85% / 41.30% 73.97% / 72.07% 75.65% / 79.39% 72.97% / 61.52%
1e−5
73.29% / 68.94% 75.37% / 85.64% 76.40% / 63.96% 74.80% / 78.41%
1e−6
75.43% / 82.81% 75.90% / 66.11% 76.77% / 65.91% 75.91% / 65.52%
Table: Results of hyper-parameter sweep on score-matching. The numbers presented are in the format (accuracy % / GAN-test %).","",""
"Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability","Keywords: Interpretability, saliency maps, score-matching","We continue the response to AnonReviewer2 in this comment.
""The Density Ratio experiments are excellent, but the y axis is hard to read. ""
Thank you for this suggestion, we will change this in an update of our draft.
""For the Sample Quality experiments: why would the gradient norm model outperform score matching? ...""
We agree that this phenomenon is unusual, and we leave the investigation of this to future work, as specified in the conclusion. It is certainlypossible that score-matching models underperform due to suboptimal regularization, but we believe that the models have converged because of the performance plateau we observe, and any suboptimality in generative modelling may be due to instability of the score-matching framework itself. Please note that this still verifies our hypothesis: both gradient norm regularized models and score-matched models have better implicit density models than the baseline model (as shown in Section 5.1), which leads to improved saliency maps (as shown in Section 5.2).
"" It would be nice to have a more convincing demonstration of examples where the model obviously classifieds an image using the appropriate information present in using your method but not the baseline. As it stands, your results appear to be largely due to the fact that the gradients are smoother when using score matching or gradient-norm regularization. ""
We evaluate these saliency maps quantitatively in Figure 3 using the pixel perturbation test, which exactly measures this effect. Figure 3 shows that score-matched and gradient-regularized models indeed outperform baseline models in identifying ""important"" pixels according to this test.
"" In summary, I really like the approach and theory presented in the paper, and tackles an important issue with broad relevance to the field, but the experimental results as they stand are not sufficient to convince me that this approach works in practice. ""
We thank you for your positive comments regarding the theory presented in this paper, and we hope to answer any more questions regarding the relevance of the experimental results, or clear any confusion that may remain.","",""
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper:
The experiments are exhaustive, identifying many domains where the approach can be applied
The presented results are compelling
The paper is well written
The paper introduces a new problem setup that has not been studied before
I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.","The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper:
The experiments are exhaustive, identifying many domains where the approach can be applied
The presented results are compelling
The paper is well written
The paper introduces a new problem setup that has not been studied before
I agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.",""
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","This work proposes an RL approach SMiRL that is able to learn effective policies in unstable environments without the need for external reward. The idea at a high-level is almost the opposite of intrinsic motivation RL approaches, which encourage novelty-seeking behaviors. The proposed method instead aims to minimize surprise or state entropy. To train the agent, rewards come from state marginal estimates, but because this distribution is changing, the authors create an augmented MDP. Through experiments on game domains and robot control tasks, the authors show that SMiRL outperforms intrinsic motivation methods. The authors also show that SMiRL can be used to do imitation and can be combined with regular reward signals.
Pros:
The problem formulation is interesting and novel. Intrinsic motivation is well studied, but this problem considers the setting where the environment is unstable rather than static, which requires new methods.
The paper is written well and is clear. The motivation is described well.
The authors evaluate on many domains, highlighting the diversity of settings in which the approach can be applied.
Cons:
It seems like this approach is only applicable to unstable environments. Does the approach fail for regular, static environments? I’m assuming the agent might just end up staying still because it’s trying to seek stable states. It can be combined with the external reward signal but will the minimizing entropy objective hurt you?
Without common sense knowledge, this approach would take many iterations to learn. So while this formulation might be more similar to the real world, the real world would only allow for a few interactions with the world.
Comments:
How is SMiRL doing better than the oracle in Figure 3 center?
In Figure 3 left, it might be a problem that with minimal episodes, SMiRL does worse. If SMiRL is useful for more real-world unstable environments, this would require a simulator good enough to model the real world.
Recommendation: Overall, the paper is interesting and novel. The approach is reasonable and experiments show the value of the method in unstable environments. I recommend “accept”.
Post-rebuttal response: The authors addressed most of my concerns so I continue to recommend acceptance of the paper. Specifically, they answered my question about whether the approach will work in static environments and how prior data can be used to improve sample efficiency. They also conducted additional experiments to verify some of my questions.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","Thanks to the authors for answering these questions!
Q1 - Yes this makes sense. This was mainly to understand if the approach could be extended to handle static environments as well, but it's fine if the approach is best suited for unstable environments.
Q2 - Thanks for expanding on this. Using demonstration data to speed up learning is a great step towards improving real-world sample efficiency.
Q3 - If keeping the board clear is a better objective, why wouldn't the oracle be trained with this objective rather than one that minimizes deaths?","",""
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","Thank you for your response - that would be great to see!","",""
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments. A state distribution is fitted during the interaction with an environment and the probability of the current state is used as a virtual reward. The parameters or sufficient statistics are also applied to the policy. The motivation is clear and verified. It is generally a good paper.
It is surprising that the exploration is achieved in the long term even minimizing state entropy. Is that possible the exploration events are from the 'unstable' environment? What if there are some patterns underlying the exploration events but only part of the 'unstable' environment? Is that OK to totally rely on unexpected events from the environment to explore the environment? Is that possible to add some exploration strategy in the developed model?","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","Summary:
This paper proposes a new intrinsic objective for RL agents: surprise minimization. This may come as a surprise, as other related works usually propose to maximize surprise, or to maximize novelty. The authors present motivations and conduct an empirical study on several environments to support their idea.
Strong points:
Overall, I think it is a good paper. Let me list some strong points:
The idea is simple, novel and well motivated. The paper positions this new intrinsic objective with respect to variants of novelty maximization objectives and brings a new perspective.
It’s well written and organized
The algorithm is tested on a relevant selection of environments and against state-of-the-art algorithms using intrinsic objectives.
The empirical evidence seems to support the claims.
The related work is quite complete.
I liked the discussion about stable vs unstable environments. It is the first time I see it discussed.
The website brings visualizations of trained policies.
Weak points:
I will now list a few weak points of the paper.
Some descriptions of the results are missing. In figures, what does the shaded area represent? (std, sem, confidence intervals, etc). In Table 1, what are the numbers? (what is the central tendency, what is the error, how many seeds?). Same for Appendix D.
The paper does not provide all necessary information to reproduce the results. There is no detail about the RL algorithms (TRPO and DQN), no description of the architectures, and no hyperparameters. This is important and should be contained somewhere in the Appendix.
Will the code be released? If not, why so? Same questions for the environments, are they accessible somewhere?
It seems to me that this approach could potentially tackle harder problems, but the paper is limited to a simplification of the tetris game, planar humanoid variants and Doom. The x-axes of the figures also tell us that only a few episodes were needed to solve them. I am not saying that I need the hardest games solved to find an algorithm interesting, but I am wondering whether it would scale. Could you tell us whether you attempted to tackle harder environments, and if so, why do you think SMiRL failed? I think we can gain a lot of understanding by looking at negative results. For example, I feel like testing this algorithm on a 3D humanoid would better demonstrate the power of this approach.
It seems to me that there might be a confounding factor that could partially explain the success of the surprise reward. Indeed, it seems that all environments presented here can terminate when the agent dies. This is a guess, as I could not find this information in the paper (please add it). If so, then the expected cumulative rewards is an increasing function of the lifetime of the agent in the game. Because of this, maximizing the cumulative surprise might be a good idea because it goes in the same direction (by construction) as maximizing survival. A counter-argument from the paper can be the performance of SMiRL + reward in Walk, that is superior to the performance of reward alone. However, this is unclear as I could not find the description of the reward function in the paper (please add it). Another way to disprove this hypothesis would be to compare the performance of SMiRL to the performance of an agent maximizing a reward function that gives +1 whenever the agent is alive (survival bonus). If it performs better, then surprise maximization brings something to the table, if it does not, then it might work because it is correlated to the survival time. Another way could be to have episodes that do not include death-related resets (fixed length episodes).
Recommendation and justification: This is overall a strong paper. However I’m concerned about the potential confounding factor of the survival time. I’ll give a score of 6, but I would happily increase that score if
The authors convince me that the success of the surprise maximization is not due to the survival confounding factor.
The authors include all necessary details for reproducibility and/or release the code.
Discuss the scalability to harder problems (e.g. 3D Humanoid).
Feedback to improve the paper (not part of assessment)
I would move the discussion about how surprise minimization and novelty maximization can be complementary to the intro. These two approaches seem to go in opposite directions and, as a reader, I would be happy to read this discussion early.
It would be interesting to discuss how it plays out in natural agents. The intuition is that minimizing surprise leads to finding a stable configuration and staying there. In practice, it is probably balanced with other driving needs like the need for food. I guess it is discussed in related papers like Friston 2009. In natural agents, surprise minimization must also be model-based. Indeed, animals do not need to jump out of cliffs several times to know that it’s bad.
Not sure I understand the inequality in Eq1. Maybe I missed something.
Discuss the surprise maximization approach of Achiam et al and whether it differs from yours.
Table 1: the legend seems to disagree with the results. It seems to me that the entropy difference is as low in your environment as in the others, but the caption says “note the clear negative entropy gap on our tasks, whereas this clear trend is absent on the Atari games”.
Fig 4. Is it really training in 80 episodes? There are very few images to train a VAE, especially if the episode resets when the agent falls (does it?) How many steps per episode?
How do you get demos for humanoid tasks? I guess there are not human demos but previously trained agents?
Results in Fig. 6 are not super satisfying, they are quite far from the target (although I guess it is a difficult task). I am not even sure the second example is achievable. In the traditional Tetris, wouldn’t cubes fall due to gravity?
Typos:
“our results are available online” → missing full stop.
“In such environments, which we believe are more reflective of the real world” → previous sentences do not discuss environments but intrinsic objectives.
“unexpected events don’t happen” → “do not”
“deep DQN” → “Deep Q-Networks”, or “DQN”
Update post-rebuttal The authors addressed most of my concerns, especially the one about the confounding factor. I am updating the score from 6 to 7.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","Thank you for this detailed answer.
Q1: Is SMiRL effective only by providing a survival bonus to avoid death-related resets? Thank you for answering this concern. Using fixed-length episodes does correct for the survival bias I agree. However I'm not sure I understand the point in Humanoid tasks. It does use death-resets, but how do you define death here, if it is not falling of ?
Q2: reproducibility concerns Great.
Q3: more difficult environments Thank you for running these experiments. I'm looking forward to the results.
Others Thank you for the other answers and modifications.","",""
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","This paper presents and studies an unsupervised learning approach to the emergence of ""meaningful/useful"" behaviour in a Deep RL setting, based on an algorithm which objective consists in minimizing surprise. It is possible to see this paper as studying a Deep RL implementation of cognitive homeostasis, an old idea recently popularized through the work on the free energy principle by Friston and colleagues. The paper presents two versions of this algorithm, one without representation learning, and one with representation learning (VAE), and compares it to Deep RL algorithms that maximize prediction error/novelty from two perspectives: emergence of behaviours and utility as an auxiliary reward to solve sparse reward problems. The paper discusses the relevance of the approach in environments that are said to be ""unstable"" (with an attempt to formalize this concept). The paper also shortly presents an application of the algorithm for imitation learning. Experiments are presented in a variety of environments.
Major strengths:
The paper is globally clear and well-written
The qualitative results showing and analyzing emergent behaviour are stimulating
Implementation of the free-energy principle in high-dimensional spaces is known to be challenging and this paper contributes towards understanding how to do it (yet the precise formal articulation between SMIRL and the FEP remains to be done, and this does not seem to be the primary objective of the paper)
The discussion of the complementarity of within-episode SMIRL and across-episode novelty seeking intrinsic rewards is interesting
The shortly described application for imitation learning is promising
The are many and varied environments in experiments
Major weaknesses:
The environments chosen for experimentation are such that minimizing surprise aligns very well with either producing ""interesting"" behaviour or maximizing an external reward. One could easily consider slight modifications of most environments where a trivial solution to minimizing surprise could be found instead (e.g. giving the ability to push on the ""pause"" button in the Tetris game or having safe rooms in opposite directions than enemies in Vizdoom/HauntedHouse). In a real robot, this would lead a robot to hide in the corner of a room and just look at a uniform wall that does not move. This problem of trivial solutions to surprise minimization approaches has been called the Dark Room problem in the FEP literature. This is known to be a limit of this general approach when applied to the real world, and how to address it operationnally (in a tractable manner) under this paradigm remains an open question.
Related to the point above, the fact that different environments are used in different experiments of the paper does not allow to get a good grasp of the general behaviour of the algorithm: I would recommend to run all experiments in all environments (maybe to report in Annex), and at least to justify why environmnets change across experiments.
The paper is positioned in comparison with novelty seeking/prediction error maximizing agents, and motivates the surprise minimizing approach by arguing that real-world situations are problematic for novelty seeking agents, as the real world spontenously generates novelty through distractors that should be avoided rather than explored. This is accurate, but an incomplete discussion of the literature in the area of unsupervised learning of behaviours: approaches maximizing learning progress were precisely introduced to adress these limits of novelty seeking approaches and to scale to real world environments with two families of distractors: novel unlearnable parts of the environments (which is a problem for novelty seeking approaches) and trivial low-entropy parts of the environments (which are a problem in principle for surprise minimization approaches). Thus, the paper should position itself in comparison with LP-based approaches and show experiments with at least some of the existing LP-based algorithms (e.g. Schmidhuber, 1991; Lopes et al., 2012; Kim et al., 2020).
As the authors acknowledge, the version of SMIRL using represention learning is not fully ""within episode surprise minmization"", as the learned representation depends on past episodes: as a consequence, it is unclear what one should conclude about the relation between within vs across episode mechanisms and the properties of emergent behaviours
The paper lacks discussion of related work aiming to implement surprise minimization approaches that scale to high-dimensional spaces, especially in a RL framework, e.g. Tschantz et al., 2020; Annabi et al., 2020
The paper does not provide code to enable reproducibility of results (and does not say it will)
Overall, the topic of this paper is interesting and the work could make a valuabe contribution, especially from the perspective of studying incentives to emergent structured behaviour in unsupervised learning,. However, the weaknesses mentionned above need to be addressed to better establish the contributions of this paper, both in terms of understanding the generality of the results and the contributions in relation to the existing litterature.
References:
Annabi, L., Pitti, A., & Quoy, M. (2020). Autonomous learning and chaining of motor primitives using the Free Energy Principle. arXiv preprint arXiv:2005.05151. Kim, K. H., Sano, M., De Freitas, J., Haber, N., & Yamins, D. (2020, January). Active world model learning in agent-rich environments with progress curiosity. In International Conference on Machine Learning (ICML). Lopes, M., Lang, T., Toussaint, M., & Oudeyer, P. Y. (2012). Exploration in model-based reinforcement learning by empirically estimating learning progress. In Advances in neural information processing systems (pp. 206-214). J. Schmidhuber, “Curious model-building control systems,” in Proc. Int. Joint Conf. Neural Netw., Singapore, 1991, vol. 2, pp. 1458–1463. Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020, July). Scaling active inference. In 2020 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE. Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement Learning through Active Inference. arXiv preprint arXiv:2002.12636.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","Hi,
thanks a lot for all the answers.
Q1: Dark Room problem in the FEP literature (trivial entropy minimizing solutions)
I think it is indeed much better that the paper now discusses this issue. I disagree with authors about the fact that the real world should contain few dark rooms, as even if a robot has access to is energy level it may find some strategies to remain pluggued (or go back to plug easily), close the eyes, and do nothing else. However, as the issue is acknowledged in the paper, I do not think this is a problem here: it is a more general debate outside the scope of this work.
we have released the code
Great! It is not very well documented at this point, I encourage you to do so at a later point.
Q2: Different environments are used in different experiments or justify why environments change across experiments We will run the missing experiments and add them to the paper.
Thanks, I am looking forward to see the full set of experiments.
Q3: Discussion of the literature in the area of unsupervised learning of behaviours A3: We thank the reviewer for the suggestion of positioning our work in relation to LP-based methods
I appreciate the answer, contrasting the objective of controlling the marginal state distribution rather than identifying the system parameters. From this perspective, it seems that both novelty-based and LP-based intrinsic motivation approaches can be seen as complementary to the within-episode surprise minimization intrinsic motivation mechanism you propose. Like R3, I think explaining this would be very useful in the introduction. I will be interested by experiments using LP-based approaches, but given the code of the paper I mentioned does not seem to be available, and given the positioning in terms of complementarity (and control of marginal state distrib. vs identifying systems parameters), it seems ok if you're not including it.
Furthermore, I do not think authors responded to my question about the within- versus across- episode issue with the version of SMIRL using representation learning.
Also, it would be useful to discuss in more details links with empowerment intrinsic motivation: re-reading the paper, it seems to me that another approach in the litterature that has strong links to SMIRL is empowerment-based intrinsic motivation, which has links with suprise minimization and FEP (see for e.g. the short discussion in Biehl et al., 2015), in particular for the within-epidose perspective.
References:
Biehl, M., Guckelsberger, C., Salge, C., Smith, C., & Polani, D. (2015). Free energy, empowerment, and predictive information compared. Technical report, University of Hertfordshire. URL: https://www. mis. mpg. de/fileadmin/pdf/abstract gso18 3300. pdf.","",""
"SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments","Keywords: Reinforcement learning","Thanks for all the updates! I think overall they have significantly improved the paper, which I think now explains well its contributions and will be of interest to many readers. I raised my score accordingly.","",""
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","The paper leverages concepts coming from hindsight relabelling methods to define a novel ""iterated"" supervised learning procedure to learn policies to reach different goals. The algorithmic solution is well supported in terms of intuition, preliminary theoretical guarantees, as well as strong empirical validation.
There is a general consensus among the reviewers that this is a strong submission and the rebuttal helped in clarifying some aspects of the paper (e.g., the comparison with Go-Explore) and reinforced the empirical analysis. This is a clear accept.","The paper leverages concepts coming from hindsight relabelling methods to define a novel ""iterated"" supervised learning procedure to learn policies to reach different goals. The algorithmic solution is well supported in terms of intuition, preliminary theoretical guarantees, as well as strong empirical validation.
There is a general consensus among the reviewers that this is a strong submission and the rebuttal helped in clarifying some aspects of the paper (e.g., the comparison with Go-Explore) and reinforced the empirical analysis. This is a clear accept.",""
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","We thank the reviewers for their positive assessment of our work and helpful suggestions for improvement. Please find responses to specific questions directly commented after each review.
We have updated the manuscript and included related work, clarified experimental details about the policy architecture and RL comparisons, and added a discussion about the relevance of the final timestep objective. Based on suggestions from reviewers, we have also added a number of additional experimental results to the paper.
Additional experiments and plots:
Alternative metrics: Upon suggestions from Reviewers 2 and 4, we have provided alternate metrics for evaluating the goal conditioned policies (“success at final timestep”, “time to reach goal”, “minimum distance to goal”, and “success at any timestep”) in Figure 12.
Frequency of Policy Updates: As suggested by Reviewer 3, we have added a new plot showing that GCSL remains performant when the frequency of policy updates is increased (Figure 8).
Exploration strategies: As suggested by Reviewer 1, we provide a demonstration that GCSL can avoid issues of exploration by using exploration strategies. We evaluate GCSL combined with a simple exploration strategy inspired by Go-Explore, enabling GCSL to solve a larger environment requiring more directed exploration (Appendix A.7, Fig 11).
Quantitative metric of “directness”: As suggested by Reviewer 2, we provide a quantitative metric for how quickly the goal is reached by GCSL policies (Appendix C.1 Table 1).","",""
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","the authors of the paper propose a new way to learn goal-reaching policies by utilizing the previously collected trajectories in an iterative manner. Their novel approach, called goal-conditioned supervised learning (GCSL), learns to reach goals from a target distribution by running the policy and collecting suboptimal trajectories, and then relabeling these collected trajectories during training to perform supervised learning on them to update the policy. This behavioral cloning is done iteratively until convergence.
Relatively ample evaluations and experiments were conducted in multiple settings (5 control environments). GCSL outperforms some RL algorithms in these tasks and is shown to be more robust to hyperparameters.
This work is very close to hindsight relabelling methods [Schaul et al., 2015; Andrychowicz et al., 2017; Rauber et al. 2017], but authors state that their method is more stable and does not estimate a value function.
Pros: The paper is well written and structured. The idea of using previous rollouts and relabelling them to use as training data iteratively is very interesting. The analysis and ablation of learned behaviors with different data collection and relabelling settings is well done.
Cons: While many ablation studies have been done, the impact of the frequency of performing supervised learning via behavioral cloning on the performance of GCSL is not clear. How many trajectories are collected and relabelled before every policy update via behavioral cloning? and did you do any ablation studies for this?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","In the paper ""Learning to Reach Goals via Iterated Supervised Learning"", the authors propose a new approach to build conditional policies for reaching tasks that can reuse the previous failed attempts as new examples on how to reach the state that was actually reached during the failed execution. This approach is similar to the approach introduced in HER, but is based on Behavioural Cloning algorithm (thus the name self-imitation) instead of a value function.
The paper is overall well written, clearly illustrated and appropriately structured.
In my opinion, the proposed method shares significant similarities with the ""First return then explore"" paper (Ecoffet et al. 2020), which is a new version of the GO-Explore algorithm that builds a similar conditioned policy to ""return"" to a state and then explore from this state using a random action, which is then used to extend and improve the learned policy. Given the strong links between these two papers, I am surprised to not see this discussed related work section and compared in the experimental section.
My main concern about the algorithm is the risk of a lack of exploration. An extreme example is a degenerated policy that outputs 0 (i.e., no movement) regardless of its inputs. The relabelling will just reinforce this behaviour and the algorithms will quickly converge in a local optimum (which is actually far from being optimal). Obviously, this is an extreme example which is quite unlikely, but the same can happen if some states/goals are very unlikely to be observed given the policy. This can happen for instance, if the policy has a strong bias. In GO-Explore this is avoided by performing a random action at the end of a rollout so that the exploration is not conditioned by the current policy. However, the combinatorial generation of goals/labels might be more challenging in this case and thus leading to fewer data per rollout. Ideally, this should be evaluated and compared in the experimental results. However, I have to say that the experimental evaluation is already quite elaborated and provide interesting insights into the performance of the algorithm.
Overall, this is a nice paper with an interesting algorithm, which could be made better by more discussion on some related work and potential limitations.
Update after rebuttal: I am very pleased by the answers of the authors, in particular, with the additional experiment showing that the algorithm could be extended with more advanced exploration strategies. I reviewed my rating accordingly.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","Dear authors,
Thank you for your response. I have updated my review accordingly.
Best regards,","",""
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","The paper proposes a new RL algorithms dedicated to learning goal-oriented policies. In the described setting, a policy has to reach a particular goal in T steps such that s_T = goal i.e the objective is not to reach the goal as soon as possible but in a particular number of steps. The proposed algorithm is very simple: it is an iterative algorithm where, at each iteration, i) a policy is used to collect trajectories. ii) Then, trajectories are relabelled based on the reached states -- reached states are becoming goals in the relabelling, iii) a new policy is learned by using behavioral cloning. In addition to the algorithm, the authors provide some theoretical insight about how and in which conditions one can prove that the algorithm is working well. At last, experiments are made on different environments and compared to some baselines showing that it allows to solve complex problems in less learning iterations. Moreover, an ablation study is provided to allow one to understand the effect of the different aspects of the learning technique.
First, the proposed technique is very simple, sharing some similarities with existing techniques (hindsight relabelling), but simplifying these techniques by just using behavioral cloning to discover a new policy instead of other and more complex value-based techniques. This idea may have two advantages: i) it is simple and ii) it may benefit of the robustness of supervised learning techniques, and particularly the fact they supervised learning may need less hyperparameters than other RL approaches. This is an interesting point and a good aspect of the article.
But the consequence seems to be that the algorithm is optimizing a criterion which is not the classical criterion used when learning goal-oriented policies. Indeed, here, the objective is to find a policy that reach a particular goal in T steps, and not to reach a goal as soon as possible. I think that this aspect is opening different questions:
A) the first one is the interest of learning such policies. The article does not really justify why and in which applications discovering such policies may be interesting. Indeed, reaching a goal in T steps may appear more difficult than reaching a particular goal in less steps, or as fast as possible. Moreover, in a stochastic environment, reaching a goal in T steps may be impossible due to the stochasticity of the environment while reaching the goal in less than T steps may be easier. I think that a better discussion on that point would improve the quality of the paper, while right now, it is not clear to me when such an objective function is interesting to study.
B) The second problem is related and in the experimental section, when comparing to classical RL techniques. Indeed, the reward function defined in this paper is very specific (i.e the agent get one if it reaches the goal at the last state of the trajectory), and classical RL techniques are using a discount factor lower than one which thus encourages the models to discover policies reaching goals as soon as possible. Again, a discussion explaining how exactly the comparison is made, and what is exactly compared would be interesting. It gives me the impression that RL techniques are used in a particularly difficult setting, why they may perform well on different but interesting reward functions encouraging for instance to reach the goal as soon as possible, or to reach the goal once during the T timesteps (and not only at the last state).
C) Also, some aspects are confusing: in section 5.1, it is written ""we parameterize the policy as a neural network that takes in state, goal, and horizon as input"" while in appendix A.1, it is written ""We parameterize a time-invariant policy using a neural network which takes as input state and goal (not the horizon),"" which is completely different. This is a crucial aspect since I do not understand how a policy that is not using the horizon as input is able to reach a goal at a particular timestep.
Other comments: Theorem 3.2 focused on deterministic environments is not really interesting since it considers a very particular case which is not a classical RL setting. The way T is chosen is not clear to me. For instance, in many environments, in order to be able to reach interesting goals, T may have to be very large, and is unknown until a good policy is known. The extension about using demonstrations is an interesting aspect that would be interesting to develop more in the paper
Summary: The paper proposes an interesting and simple model that optimizes a particular and not classical objective. It is not clear if this objective is really interesting, and in which applications such a method can be used. The comparison with other techniques is fair, but considering this very particular objective while other objectives may be easier to learn and more interesting. Some aspects are not clear in the paper and the experimental section has to be improved.
Regarding the answers provided by the authors, I increase my score","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Learning to Reach Goals via Iterated Supervised Learning","Keywords: goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL","Summary
The authors propose “GCSL” (goal-conditioned supervised learning), an algorithm that bridges the gap between reinforcement learning and imitation learning. Specifically, the authors are motivated by the limitations that RL is brittle when used with sparse rewards, and that IL requires expert demonstrations. Their technique performs learning without expert demonstrations, and does not require a learned value function or reward function. The authors formulate GCSL as iterative trajectory collection, goal relabeling, and policy refinement through behavioral cloning, and formally derive performance bounds on this technique. In addition, the authors demonstrate strong goal-reaching performance and robustness improvements over current RL techniques.
Strengths
The authors’ proposal is strongly motivated, in that RL techniques are clearly sensitive to hyperparameters and face stability challenges, and that using demonstrations is more robust. The paper is clearly written and the authors very cleanly explain their key ideas and insights.
The authors’ proposed algorithm is based on a simple idea, in a very good way. Their technique is stable, requires no value functions or reward function engineering (as is common with traditional RL techniques), and their technique of relabeling generates a large number of samples, resulting in better data efficiency.
The authors’ main theoretical insight that iterating on data from sub-optimal agents leads to optimal behavior is both non-trivial as the authors claim, and can likely be used to inspire other similar ideas or areas of study.
The authors clearly showcase the experimental use cases of their technique by demonstrating its benefits in terms of stability to hyperparameters and in leveraging expert demonstrations. Apart from this strong set of experimental results, the authors also present detailed ablations in the supplementary pages that are very convincing.
Weaknesses
The theory section does seem a bit contrived. Specifically, optimality emerges when using the objective proposed by the authors, even if this is not commonly used. Further, the proof assumes that trajectories are collected from a single policy and relabeling is only performed on the last timestep, whereas in practice these conditions are not met. The performance guarantee also only holds with deterministic transitions. Finally, the guarantee on the convergence behavior is a good property, but is maybe less meaningful if in practice GCSL is difficult to converge to (if the optimization is challenging).
Notably, the authors do not evaluate the generalization ability of their technique, and instead evaluate with the same train and test environment. Although this is common in RL, it would be interesting to see how GCSL performs in environments that require more sample efficiency and test generalization, like Procgen.
The authors quantify performance as the distance of the agent to the goal at the last timestep. I’m not convinced this evaluation metric makes the most sense, especially when success ratio is the main metric we care about.
On door opening, do the authors have an idea for why TD3+HER performs so much worse than PPO or GCSL? Similarly, on Sawyer pushing, do the authors have a hypothesis for why PPO and TD3+HER do not learn at all? Is there potentially a qualitative analysis of this unexpected behavior that can be performed? What about GCSL makes it so much more successful on Sawyer pushing?
I’m surprised that GCSL is not any more sample efficient, since PPO and broadly speaking other policy gradient techniques are generally well-understood to be greedy sample-wise.
The authors earlier in the paper hypothesized that an optimal policy would be non-Markovian, but that GCSL with a Markovian policy would outperform a time-varying one. This seems rather counter-intuitive to me, why do the authors suspect this is the case? Is the model overly exploiting instead of exploring when conditioned on the remaining horizon? Does this behavior change depending on the number of timesteps in an episode?
In Figure 4, do the on-policy methods that converge slower do so with an empirically visible better convergence guarantee? For instance, even if it takes longer, the technique is guaranteed in the long-run to arrive at an optima?
In Figure 4, the authors compare against limited-horizon relabeling as in prior work and show this method has drawbacks. Are the drawbacks due to the lack of multi-horizon relabeling, or due to having much fewer trajectories after relabeling?
The authors compare robustness to hyperparameters against TD3, but do not compare against PPO. Because PPO is generally well-understood to not require much hyperparameter tuning empirically, I would recommend including this comparison as well.
The authors claim qualitatively (in Appendix C) that despite the different objective, the learned trajectories generally take a direct path to the goal. Is there a way to quantify this quantitatively against an oracle that does take the shortest path; for example by comparing the time taken to get to the goal, or the time spent at the goal waiting for the episode to complete?
Recommendation
Overall, I vote for accepting. I think this current submission is already relatively convincing as an accept, as it is clearly written, has well explained motivations, strong experimental results, and extensive ablations in the supplementary pages. The authors’ idea is conceptually simple, in a good way. My main gripe is that the theoretical results are only weakly held, or less relevant in practical settings, but that is rather typical and the strong experimental results do speak for themselves. I do have a few clarifying questions on the experimental results, but am regardless confident that this paper meets the ICLR acceptance criteria.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Self-training For Few-shot Transfer Across Extreme Task Differences","Keywords: few-shot learning, self-training, cross-domain few-shot learning","The paper introduces an approach to self-train a source domain classifier on unlabeled data from the target domain, considering the few-shot learning setting when there is significant discrepancy between the source and target domains. While the reviewers pointed out a few weaknesses, such as somewhat limited methodological novelty and lack of comparisons with other methods, they all recommend acceptance as final decision. The paper is beautifully written. The proposed method is very simple, but yields excellent results in a very practical problem, which should be of wide interest to the ICLR community. The experimental evaluation is rigorous and the ablation studies are convincing. The AC agrees with the decision made by the reviewers and recommends acceptance.","The paper introduces an approach to self-train a source domain classifier on unlabeled data from the target domain, considering the few-shot learning setting when there is significant discrepancy between the source and target domains. While the reviewers pointed out a few weaknesses, such as somewhat limited methodological novelty and lack of comparisons with other methods, they all recommend acceptance as final decision. The paper is beautifully written. The proposed method is very simple, but yields excellent results in a very practical problem, which should be of wide interest to the ICLR community. The experimental evaluation is rigorous and the ablation studies are convincing. The AC agrees with the decision made by the reviewers and recommends acceptance.",""
"Self-training For Few-shot Transfer Across Extreme Task Differences","Keywords: few-shot learning, self-training, cross-domain few-shot learning","Problem: The paper introduces the problem of few-shot transfer when there is an extreme difference between the base task and the target task. The usual few-shot learning setup considers a representation that is trained on a large amount of labeled data. This base representation is then fine-tuned for the target task (that has a few examples, say 1 or 5 labeled examples per class). This strategy works well when the data distribution of the base and target task is similar. However, few-shot learners fail when the data distribution for the two domains are different (e.g., imagenet and crop-diseases) as shown by Guo et al., 2020.
Solution: In this work, authors intuit that there may be a large amount of unlabeled data for the target domain that can be used to learn a good representation of the target task. This is done by using a teacher model trained on a large dataset to generate soft-labels on the unlabeled dataset. A new model is trained using: (1) base dataset; (2) soft-labels on unlabeled data; and (3) contrastive learning on unlabeled data. The new model is initialized using the base representation of the teacher model and the classifier (last layer) is trained from scratch. At test time, this new representation is used for learning a classifier on the few labeled examples. This approach has been demonstrated to improve the performance by an average 2.9 points on four tasks: (1) X-Ray; (2) EuroSat; (3) Crop Diseases; and (4) ISIC.
Pros:
simple approach in yielding better performance!
use of unlabeled data from the target domain to learn a better representation.
while authors position themselves as ""self-training"" -- I think it is not self-training and is basically about ""conditioning the existing representation on the target task"". To the best of my knowledge, the authors have a very unique and interesting perspective that I have not seen previously.
Cons:
Following are my concerns with the submission:
Chest X-Ray and ISIC dataset vs. Crop-Disease and Satellite Images - The proposed approach works for Crop Disease and Satellite Images which are partly in the domain of base task. It does not show improvement for Chest X-Ray and ISIC dataset that is extremely different from the base task. Is it not against the premise of this paper (specifically the first paragraph of Sec.1)?
Section - 4 (Training a new student model): The optimization consists of three parts: (a) supervised loss using base task; (b) pseudo-labels on target task; and (c) contrastive learning on unlabeled images. The evaluation does not show a meaningful difference in the use of these losses except for Crop Diseases dataset.
Note: The authors mention the similarity of (a) and (b) in optimization used in Xie at al., 2020. The statement is nuanced. Strictly speaking, Xie et al., 2020 did not use (a) and (b). They rather trained using the hard pseudo-labels generated on the unlabeled images (using the teacher model).
The student model is initialized using the base representation and classifier from scratch. Different initialization strategies are explored. However, conclusions in Section 4.2 are not in sync with Table 7 in Appendix.
Section-5.2.1: Hypothesis-1 -- The conclusion of Xie et al., 2020 are valid for their setup (which is strictly for ImageNet classification).
Section-5.3: This experiment is inconclusive because of the limited unlabeled data used here.
Now zooming-out and some high-level concerns:
First line of the Abstract: ""All few-shot learning techniques must be pre-trained on large, labeled dataset."" --> Why?
I can buy that currently all few-shot learning techniques are pre-trained on large, labeled dataset, but I don't see why it is a ""must""?
First paragraph of Section.1: What has few-shot learning to do with computational time? If it is true, then the proposed formulation does not come any close to solving it because it requires training a new model on a large labeled and unlabeled dataset for a new target task.
POST-REBUTTAL: I have updated the score from 5 to 7 following the clarifications from the authors.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Self-training For Few-shot Transfer Across Extreme Task Differences","Keywords: few-shot learning, self-training, cross-domain few-shot learning","Summary
The submission tackles the problem of cross-domain few-shot learning in a setting where unlabeled data is available for the test domains.
It introduces an approach called ""Self Training to Adapt Representations to Unseen Problems"" (STARTUP) which first pre-trains a teacher model on the (labeled) base dataset, and then distills the teacher model into a student model (initialized with the teacher parameters and a random output layer) on the base set of classes as well as the (unlabeled) target domain dataset. For the target domain examples, the loss is a combination of the KL-divergence between logits output by the student and teacher models and an additional unsupervised/self-supervised loss function (SimCLR in this instance). For test episodes, STARTUP is free to choose any applicable inference approach, and the paper opts to fit a linear classifier on top of the frozen feature extractor.
The proposed approach is evaluated on BSCD-FSL using a random 20% subset of each target domain to form unlabeled datasets and the remaining 80% to form evaluation episodes. STARTUP is compared with recent cross-domain few-shot classification approaches (which do not use unlabeled target domain data) and a purely self-supervised baseline which ignores the base dataset and learns a representation from the target domain using SimCLR.
The submission also investigates two hypotheses to explain why STARTUP improves performance, and concludes that its purpose is not to simply introduce noise during training and instead appears to emphasize the natural groupings induced by the base classifier on the target domain data.
Strengths and weaknesses
+ Clarity: the paper is well-written and easy to follow.
+ Cross-domain few-shot classification is very relevant to the few-shot learning research community, and the availability of unlabeled data for the test domains is a plausible assumption.
+ The proposed approach is sound and straightforward.
+ The paper makes an effort at explaining why STARTUP provides a performance improvement.
- The paper should be more rigorous when reporting which approach performs best in a given setting.
Recommendation
I recommend acceptance. Overall the paper is clear, the problem being tackled is relevant to the research community, the proposed idea is sound, and evaluation is rigorous. My main concern has to do with the way in which best-performing approaches are reported, but that’s easily fixable in a subsequent version of the paper.
Detailed justification
I appreciate the quality of the writing. The proposed idea is straightforward and makes intuitive sense. The baselines chosen for comparison are reasonable.
The main concern I have is with the way in which results are reported in Table 1. I believe the authors bolded the best-performing entry in each setting without taking the 95% confidence intervals into account. As an example, can we say that STARTUP performs significantly better than Transfer in the ChestX 5-way 1-shot/5-shot settings when their 95% confidence intervals overlap as much as they do? In my opinion the more rigorous way to determine that would be to run a 95% confidence statistical test on the difference between the means and bold all entries for which the test is inconclusive in rejecting the hypothesis that the difference in mean to the best-performing entry is zero, like is done in the Meta-Dataset paper. This doesn’t change the main conclusions drawn by the paper, but should nevertheless be addressed.
Questions
When training the student model, would there be a benefit to compute the loss as the KL-divergence between the label distributions output by the teacher and student models instead of the cross-entropy with the true label? Have the authors investigated this?
The proposed approach has the downside that it requires training and storing a separate student model for each target domain. Have the authors thought of more compact approaches, like domain-conditional self-training?
Additional feedback
The abstract mentions evaluating on a ""challenging benchmark with multiple domains"" but does not name it (BSCD-FSL). This is information that would be helpful to the reader.
The insistence on ""several hours of compute"" in the introduction as being a drawback of current recognition systems could be toned down. To me, training a model for several hours doesn’t sound unreasonable.
The Visual Task Adaptation Benchmark (VTAB) and Big Transfer (BiT) papers would be relevant to mention in the related work section. In particular, the VTAB paper investigates various representation learning strategies (including self-supervision) when transferring between very different base and novel domains.
The strategy to use logits on the label set of the base dataset as targets (and therefore leveraging the natural groupings induced by the base dataset classifier) reminds me in a way of Ngiam et al. (2018)’s work on ""Domain Adaptive Transfer Learning with Specialist Models"", which uses this to inform the weighting of classes used to pre-train a target domain-aware classifier on the base dataset. This paper’s proposed approach differs in many ways, but the similarities would be interesting to expand upon.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Self-training For Few-shot Transfer Across Extreme Task Differences","Keywords: few-shot learning, self-training, cross-domain few-shot learning","Summary of the paper
This paper studies transfer learning in an episode learning setting, where at evaluation time a few-shot few-example task is generated. In contrast to the standard setting, two modifications are made:
large domain differences (base dataset is (mini)ImageNet, target datasets are plant crops, xrays, satellite images) from the BSCD-FSL benchmark (Guo et al., 2020);
the learner is given unlabeled target images for domain adaptation. The technical novelty of the paper is to use the unlabeled dataset by (a) obtaining soft-labels from miniImageNet classes, and (b) using SimCLR (Chen et al, 2020) as self-supervised loss.
Strengths
This setting is interesting especially due to the large domain differences. Adding self-supervision under large domain shift seems a logical choice.
Weaknesses
There is no comparison to other self-supervised methods / unsupervised domain adaptation methods for few-shot learning. In this paper only a single self-supervised method (SimCLR) is evaluated, while there are other methods also focussing on self-supervision for few-shot learning (Gidaris et al., 2019, Su et al., 2020). While these approaches are not evaluated on the large domain shift, does not tell anything about the relevance for this setting.
There is no comparison on a standard benchmark, while a benchmark dataset is used, the benchmark evaluation is omitted, only per dataset / task scores are provided.
There is no comparison on other domain transfer tasks, for example on more standard MAML types of domain transfers. To allow to compare to other self-supervised methods for few-shot learning.
The title might be misleading: Self-Training might no do so much, according to table 1. The performance difference between Transfer and SimCLR baselines are either about equal (ChestX, CropDisease) or Transfer is much better (ISIC +7, EuroSAT +21). Startup shows a small (in the range +.2 - +4) performance increase over Transfer, which might suggest that SimCLR acts only as a small regularizer. This is confirmed by the ablation of different startup strategies. Unfortunately, the plain combination of Transfer + SimCLR is not explored.
Summary
This paper presents an interesting line of research on few-shot learning on large domain shifts. The technical novelty is minor: it propose to use SimCLR as auxiliary loss in a student-teacher setup. Without comparing to other work or related methods. Experimentally this setup is evaluated on the BSCD-FSL benchmark dataset (yet, without using the benchmark evaluation). Therefore I give an 'OK but not good enough' rating. It seems a good start, but more comparisons, and evaluations are needed for publication.
Post Rebuttal My main concerns for the paper are not withdrawn, yet not shared with the other reviewers. I think that is fine, it shows that perspectives differ, and that it is in part why multiple reviewers should read a paper. I also do see that the paper has been improved based on the provided feedback. Given that the paper does not contain major flaws, I upgrade my vote to 6.","6: Marginally above acceptance threshold","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Self-training For Few-shot Transfer Across Extreme Task Differences","Keywords: few-shot learning, self-training, cross-domain few-shot learning","This nicely written paper explores the situation in which one tries to do domain transfer from a large dataset with many labels to a partially labeled dataset with relatively few unlabeled and even less labeled examples, if the domain gap is very large.
The paper presents a pre-training methodology in which the dataset is first co-trained on the large, labeled dataset and the unlabeled examples. They use the SimCLR objective for pretraining on the unlabeled examples.
This situation explored here is extremely common in practice, especially in medical imaging: even unlabeled training data is relatively rare, however, it is even rarer to have high quality training data.
This approach show premise in tackling the domain transfer problem from image-net images to chest X-rays. The paper reports significant progress on chest-ray images and therefore providing important practical contributions in this domain.
While the novelty of the approach is somewhat limited: it just combines an existing pretraining approach with co-training on another dataset, its simplicity makes up for it. Also, I am not aware of any work that tackles this important issue so elegantly and directly.
Also the paper presents two hypothesis on why this approach works. One just posits that the additionally unsupervised data helps independently of the target domain. The second posits that the pretraining helps grouping in the target domain. The paper presents convincing ablation analysis to back the latter claim: training on the target domain is helpful for transferring to that specific domain.
Strength:
High quality writing. Easy to read paper
Good practical results
Tackles a practically highly relevant problem
Convincing ablation analyses
Simplicity of the approach
Weaknesses:
Somewhat limited novelty
No theoretical analysis
Evaluated on a single scenario
Despite the fact that the approach is somewhat straightforward and of limited novelty and while it was evaluated on one particular use-case, the approach seems to be highly relevant in practice, it is relatively simple and practical and exhibits good performance in an extremely important scenario.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Dear FedDyn authors,
Thank you for the very nice work. Reducing the per-round communication requirement of SCAFFOLD by moving the global
c
correction to the server was very innovative!
A few comments:
There seems to be a mistake in Algorithm 1 as is currently written. Consider the following 1D problem with 2 devices both of which participate every round:
L1(θ)=θandL2(θ)=−2θ, for θ∈[0,1].
Over the range
θ∈[0,1]
, the minima is at 1. However, if we run Algorithm 1 starting from
θ0=0
, it will always remain at 0 with
θkt=0
for all
t
and
k
. I believe this is because the initialization is incorrect and
∇Lk(θk0)
cannot be set to 0 as is currently done (perhaps this was a typo?).
Since the clients compute full gradients, the variance is 0. In this setting, SCAFFOLD actually obtains a rate of i)
O(mP1T)
for general convex functions, and ii)
O(mP2/31T)
for non-convex functions. FedDyn matches the rate for general convex functions, but for non-convex functions FedDyn is slower than SCAFFOLD. In an earlier version of our paper, our analysis for the general-convex setting was slightly loose since we did not focus on the case when
σ=0
. Please see Remark 11 in our latest version.
Thanks!
SCAFFOLD authors","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Thanks for the fast response. We could run the algorithm by dropping the constraints, (or using projection steps are mentioned in the answer). Either way, the problem remains.
Consider the above example where
L1(θ)=θ
and
L2(θ)=−2θ
. The client gradients can be computed as
∇L1(θ)=1
and
∇L2(θ)=−2
and are both constant. We thus have for all
k
that
Lk(θ)−⟨∇Lk(θ),θ⟩=0.
This means that the subproblem solved by FedDyn is
θkt=argminθLk(θ)−⟨∇Lk(θ),θ⟩+α2||θ−θt−1||2=argminθα2||θ−θt−1||2=θt−1,.
Thus, throughout we will have
θk−1t=θ0=0
. The algorithm makes no updates and remains stuck at initialization.
However, clearly the global function is
−12θ
. To minimize this function, we need to move towards larger positive numbers.","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Thank you for the very nice work, and the clear intuition provided for the dynamic regularization.
I have a question regarding the relationship between the proposed algorithm and the FedPD algorithm. In the paper, it is commented that the difference between these two algorithms are mainly in the way that they save communications. But I wonder if, both of the algorithms do not attempt to save communication (i.e., FedPD does not skip communication, while the proposed algorithm use full user participation), then what's the relationship between them?
We have some preliminary derivations, see the link below. It will be great that we can discuss about this issue. Thank you!
http://people.ece.umn.edu/~mhong/FedDyn_FedPD.pdf","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","It is true that the partial participation is very interesting in FL. I just want to also mention the connection to distributed ADMM for the full participation case. Introduce a variable
θ0
at the server, and let
θ0=θk
be the constraint. In this case
ht
at the server will be the average of the gradient
∇Lk(θkt)
. The update of
θ0
is the update of
θt
. The gradient in the algorithm is the dual variable (or multiplier).
For the partial participation case, it is not equivalent to ADMM, and there are many challenges in this case. Thanks for the nice work.","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","The paper introduces a new federated learning algorithm that ensures that the objective function optimized on each device is asymptotically consistent with the global loss function. Both theoretical analysis and empirical results, evaluating communication efficiency, demonstrate the advantages of the proposed FedDyn method over the baselines.
All the reviewers recommend accepting the paper. To summarize the discussion:
R1 mentioned a very recent (NeurIPS 20) related paper and asks several questions. I believe that the authors nicely answered the questions and discussed the relation to the previous paper in detail.
R2 mentioned that the paper focuses solely on minimizing communication costs, ignoring costs of local computations. The authors argued that the local computation costs are comparable to those of the baselines, and, in general, communication costs are the main source of computation energy costs (pointing to previous work), and, thus, are a natural objective to optimize. I believe that this adequately addressed this (and other) reviewer's concerns and the reviewer kept their score unchanged.
R3 had several concerns, which according to the reviewer were addressed in the rebuttal (they increased the score).
R4 points out several limitations of the method and theoretical analysis and believes that the rebuttal did not quite address the concerns. Nevertheless, remains positive about the paper, and believes that the shortcomings can be addressed in follow-up work.
We share the reviewers' sentiment: it is a very nice and interesting paper, and should be accepted.","The paper introduces a new federated learning algorithm that ensures that the objective function optimized on each device is asymptotically consistent with the global loss function. Both theoretical analysis and empirical results, evaluating communication efficiency, demonstrate the advantages of the proposed FedDyn method over the baselines.
All the reviewers recommend accepting the paper. To summarize the discussion:
R1 mentioned a very recent (NeurIPS 20) related paper and asks several questions. I believe that the authors nicely answered the questions and discussed the relation to the previous paper in detail.
R2 mentioned that the paper focuses solely on minimizing communication costs, ignoring costs of local computations. The authors argued that the local computation costs are comparable to those of the baselines, and, in general, communication costs are the main source of computation energy costs (pointing to previous work), and, thus, are a natural objective to optimize. I believe that this adequately addressed this (and other) reviewer's concerns and the reviewer kept their score unchanged.
R3 had several concerns, which according to the reviewer were addressed in the rebuttal (they increased the score).
R4 points out several limitations of the method and theoretical analysis and believes that the rebuttal did not quite address the concerns. Nevertheless, remains positive about the paper, and believes that the shortcomings can be addressed in follow-up work.
We share the reviewers' sentiment: it is a very nice and interesting paper, and should be accepted.",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","This paper proposed a new federated learning algorithm called FedDyn, which was motivated by the observation that the local objectives for each device might lead to inconsistent models between devices for heterogeneous data. Such observation is already observed in SCAFFOLD, and this paper further improves over SCAFFOLD with better communication efficiency. Both theoretical convergence rates and empirical results about communication efficiency are presented to support the advantages of FedDyn methods.
I have the following comments on the paper:
The choice of alpha: it seems alpha is an important hyperparameter which can largely affects the theoretical convergence and empirical efficiency of the proposed FedDyn method. Unfortunately, I could not find much discussion in the paper about alpha: I think how alpha affect the convergence rates, and how alpha is chosen in the empirical results should be discussed, it would be better if the authors could provide an empirical study about the parameter sensitivity in alpha.
How the local objective is optimized: since at each round FedDyn requires each device to solve a local optimization problem which would requires iterative algorithms (such as SGD) to find an approximate solution, how to solve the local optimization problem, and how the accuracy of the local optimization affects theoretical communication efficiency as well as empirical results should be discussed.
Related work: there is a related paper (to appear in NeurIPS 2020) which proposes to solve a similar local objective, it would be good to discuss the differences: FedSplit: an algorithmic framework for fast federated optimization https://arxiv.org/abs/2005.05238.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Q3: Differences between FedDyn and FedSplit,
We were unaware of this work, and thank the reviewer for pointing us to it. Let us outline a few major differences.
Scope. As described in our introduction, our goal with FedDyn is to be able to handle partial device participation (communication), heterogeneity, data imbalance, and massive device levels all together. As such proposed FedSplit does not account for partial communication, which is a significant issue in the context of federated learning.
FedSplit evaluation with partial participation. During this rebuttal period, we implemented FedSplit with partial device participation (see Appendix A.4). To do so we experimented with different modifications of FedSplit that appear reasonable (nevertheless we do not suggest our modifications are optimal). On CIFAR10, 100 devices,
10
% participation setting, these adapted versions of FedSplit perform significantly worse than FedDyn and SCAFFOLD.
Conceptual Difference. We describe differences for full participation. FedSplit update per device per round is
zjt+12=Proxsfj(2xt−zjt)
and
zjt+1=zjt+2(zjt+12−xt)
where devices transmit
zjt
s and keep
zjt+12
s as an internal state. The server averages device models and sets the server model as
xt=1m∑i∈[m]zit
. The first order condition of device level optimization gives
s∇fj(zjt+12)+zjt+12−2xt+zjt=0
. So, the relation of device-wise gradients (
∇fj(zjt)
) between consecutive rounds is implicit. On the other hand, in FedDyn, the first order condition gives
∇Lk(θkt)=∇Lk(θkt−1)−α(θkt−θt−1)
as described in Eq. 2 (Pg. 4). This explicitly, in fact, linearly relates device-wise gradients between consecutive rounds and significantly simplifies the convergence reasoning (see the paragraph starting with 'Key Property of Algorithm' on pg. 4). Hence, the update mechanisms of FedDyn and FedSplit are conceptually different.
Empirical FedSplit comparison for full participation. Our FedSplit discussion in Appendix A.4 includes an experiment on Cifar10 with full participation. We observe that FedDyn outperforms FedSplit even in full participation level.
Non-Convex Case. As it stands, FedSplit theory is applicable to the convex setting, and no analysis for non-convex case is presented. On the other hand, our results hold for nonconvex functions as stated in Theorem 4. Indeed, an important reason our analysis is tractable is because, at the stationary point we have a linear update equation for the gradients between consecutive rounds. This is no longer the case for FedSplit since the gradients between consecutive rounds are implicitly related as we described earlier.
To summarize, FedDyn and FedSplit exhibit differences in terms of scope and operation. Empirical results reveals Scaffold and FedDyn outperform FedSplit for both full and partial participation levels (see Appendix A.4).","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Summary:
This paper proposes FedDyn, a dynamic regularization method for federated learning. In FedDyn, the objective function of each active device in each round is dynamically updated, so that the device optimum is asymptotically consistent with the global optimum. The authors consider both the convex and non-convex case, and give convergence rates with theoretical guarantees. In the convex case the proposed algorithm converges faster than the SOTA algorithm SCAFFOLD. In the experiments the authors show that their algorithm takes less communication cost to achieve the same performance than existing algorithms.
The main contribution of the paper includes:
Proposing the dynamic regularization method to tackle the inconsistency issue in federated learning.
Proving the convergence rate of the proposed algorithm.
Saving communication cost compared to existing algorithms, both theoretically and experimentally.
Pros:
The problem of finding locally consistent distributed algorithms is well motivated. I appreciate the authors' discussion in the introduction.
The experimental results seem comprehensive. This includes the comparison between FedDyn, SCAFFOLD, FedAvg and FedProx using both synthetic and real datasets in four regimes of interest. The extensive experiments make the claim (saving communication costs) more convincing.
Overall the paper is well written. The proposed algorithm is well justified with the discussion on the so-called fundamental dilemma. Also the comparison between the proposed algorithm and SCAAFFOLD makes the claim (saving communication costs) much clearer.
Cons:
What can be said about the computational time of each device during each iteration, compared to that of existing algorithms (say, SCAFFOLD)? This is also mentioned on Page 2 (""This approach, while increasing computation for devices...""). This would be interesting, although I understand the authors focus on a communication point of view.
The proof techniques of the main theorem somehow seem standard. I did not check other proofs though.
Post-rebuttal:
I appreciate the authors' feedbacks and I keep my evaluation unchanged.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Context: the authors propose a new distributed optimization algorithm, called FedDyn, to minimize a sum of smooth functions. Their motivation is the context of federated learning, in which each function is the loss of one user corresponding to its data stored locally. The goal is to reduce the communication burden to achieve the true global solution (not an approximation thereof) to a given accuracy.
Strengths:
The main strength of the algorithm is its robustness to partial collaboration, in which only a subset of the devices participate at every iteration. Indeed, in case of full participation, it is not clear that the algorithm has any advantage in comparison with classical (S)GD-type methods.
The experiments are extensive, well described, and convincing: the method achieves the goal of convergence to a given accuracy with a speed and total communication load competitive w.r.t other methods.
Weaknesses: The comparison to methods based on local steps, like FedAvg and Scaffold, is somewhat unnatural: these methods go along the idea of doing more computations between communication rounds. This is not the case of the proposed method, which is much closer in spirit to the class of SGD methods. Thus, I find the discussion, which turns around 'correcting' the drawbacks of methods based on local steps, in particular the fact that FedAvg converges to an approximate solution, convoluted. Moreover,
No regularizer at the master in the objective function.
The local loss functions must be smooth and in addition, their proximity operators must be computable (see point 5. below).
No comparison to accelerated SGD-type method with accuracy in O(1/T^2) instead of O(1/T).
No discussion of possible linear convergence in case of strong convexity.
alpha, the inverse of the stepsize, must be large (>25L) for Theorem 1 to apply. This shows that the analysis is not tight at all.
assessment: I think the paper deserves publication, since the proposed algorithm is new, comes with some convergence guarantees, and shows good performance in practice. However, I view the theoretical analysis as a preliminary one, and several aspects would deserve to be investigated more in depth.
More detailed discussion:
There should be a discussion about the literature of SGD type methods. Indeed, partial participation takes the weak form, in Theorem 1, of a subset selected uniformly at random. Some papers by Richtarik et al coming to my mind: ""A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent"", ""Unified analysis of stochastic gradient methods for composite convex and smooth optimization"", ""A unified analysis of stochastic gradient methods for nonconvex federated optimization"".
You should talk, even very shortly, about the other strategy to decrease the communication burden: compression, see for instance the recent papers and refs therein: ""On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning"", ""Distributed learning with compressed gradient differences"".
Further on, one could view the proposed method as GD with unbiased compression: all devices compute their new gradient/model but then with probability P/m it is sent to the master, otherwise it is not sent. It would be good to investigate this relationship more closely.
When mentioning prior work on methods using local steps of SGD, which is ""inconsistent with minimizing the global loss"" (or later when mentioning that ""performance degrades in non-IID scenarios""), you can cite the paper ""From Local SGD to Local Fixed Point Methods for Federated Learning"", ICML 2020, which gives a precise characterization of this ""inconsistency"" in the strongly convex case (Theorem 2.14). For the non-strongly convex case, you can refer to the paper ""Tighter theory for local SGD on identical and heterogeneous data"".
Each local computation step is actually a call to the proximity operator of L_k: we have theta_k^t = prox_{L_k/alpha}(theta^{t-1}+grad.L_k(theta_k^{t-1})/alpha). This should be mentioned clearly, as well as the assumption that L_k is proximable (in addition of being smooth). The operation is not a proximal gradient descent step, since there is a + and not a - in front of the gradient. Still, the variable h has the flavor of a dual variable. So, the relationship to proximal splitting algorithms should be investigated, since there seems to be a connection there. Thinking a bit about this connection, I found out the ""distributed Davis-Yin algorithm"" in the paper ""Distributed Proximal Splitting Algorithms with Rates and Acceleration"", see p. 22 of https://arxiv.org/pdf/2010.00952.pdf. Rewritten in your context, its iteration is: |for each device in parallel do | theta_k^t = prox_{L_k/alpha}(2.theta^{t-1}-s_k^{t-1} | -grad.L_k(theta^{t-1})/alpha) | s_k^t = s_k^{t-1} + theta_k^t - theta^{t-1} | transmit theta_k^t to server |end for |s^t = s^{t-1} + 1/m.sum_k (theta_k^t - theta^{t-1}) |theta^t = s^t There seem to be several differences between this algorithm and yours, but still, they look similar in spirit. It would be very interesting to compare them.
In the last two steps of your algorithm (set h^t=..., set theta^t=...) you can remove the multiplication and division by alpha.
For the nonconvex case, the rate in Theorem 1 is with respect to the gradient norm. You should tell a bit about the literature, see e.g the discussion in ""Primal-dual accelerated gradient descent with line search for convex and nonconvex optimization problems"" by Nesterov et al.
Typos:
convergences -> converges
non convex -> nonconvex
Table 1: I guess the ""Acc."" column is to provide the accuracy. This should be said, since the reader might be confused and think that this is an accelerated method included in the comparison
theta_k^infty triangle theta^infty: what does the triangle mean?","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Q2: Missing regularizer in the objective formulation,
A global regularization can be added through device level functions. We do not have a regularizer in the formulation for simplicity. Since regularizers are data independent, we can add regularizers to the local functions
Lk
. For example, in the case of an L2 regularizer, the global objective becomes
arg⁡minθ∈Rd[λ2|θ|2+1m∑k∈[m]Lk(θ)]
. We can recover the same global objective as average of local functions by pushing the regularizers to the device level functions as
Lk(θ)←Lk(θ)+λ2|θ|2
. Therefore, Theorem 1 as well as the convergence rates extend to the regularized objectives. Indeed, in the experiments, we use this idea to prevent overfitting.
Q3: Could we also prove convergence for strongly convex functions?
Yes. FedDyn, in fact, achieves linear rate for strongly convex functions which is a significant improvement over convex rate. We refer to the revised draft (see Theorem 1 and Appendix B.2. for analysis.).
Q4:
α>25L
, tightness of analysis,
The convergence analysis is tight up to constants. In our analysis, our focus is on the problem dependent variables such as
L,m,P
so we keep the scalars in a simple integer form. For example, we need both
κ
and
κ0
to be positive in Lemma 1. This implies that
α≥(10+140)L
. We used
α>25L
to simplify the scalar expression. Indeed,
α
can be
25L
, we changed it accordingly. Hence, the analysis is tight up to constants.
Q5: References; Compression strategy, non IID setting degradation
Compression techniques are orthogonal to our proposed method. As a future direction, FedDyn can be investigated with a compression technique to further decrease communication costs. We updated our related work.
We included papers related to inconsistency of local objectives in our introduction where we talk about this issue.","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","I read carefully the response of the authors to the points raised by the different reviewers. I find that the authors tend to evade the issues by verbose comments and repeating the same things several times. But I take it on the good side and I will keep my score unchanged : the proposed work is a nice step forward in the brainstorming of the community on how to deal with the communication bottleneck. There are several open and interesting questions, about the links between the proposed method with SGD-type methods on one hand, proximal methods on the other hand, compression, about partial participation... Investigating these questions is left as future work for the authors... and the readers!","",""
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","The paper proposes a new optimization method named FedDyn to handle data heterogeneity inherent in FL via a dynamic regularization. Such a dynamic regularization modifies each local objective by adding a linear and quadratic term that makes the local stationary point is asymptotically consistent with that of global objectives. The authors prove convergence results for FedDyn in both general convex and non-convex cases and test FedDyn on synthetic and real-world datasets.
Pros:
The paper is well written and easy to follow. All proof seems correct.
The experiment setup follows many previous works, which facilitate comparisons, and I appreciate a lot. Besides, a lot of details are given, which helps reproduction.
The proposed method has great superiority over other baselines in communication efficiency, shown by a lot of experiment results.
Cons:
I think the author could also analyze FedDyn in a strongly-convex case since in that well-conditioned case the convergence performance of FedDyn could give more insights for researchers. I expect FedDyn would converge to the optima with a rate exponential in communication rounds.
The author declare that FedDyn has great advantages over competing methods in cases of a large number of devices. However, from the main theorem, I couldn't figure out the reason.
The author declare that FedDyn is more robust to unbalanced data than baseline methods. From experiments, it seems correct as FedDyn achieves a larger factor of gains over others in unbalanced data. However, again, no theoretical analysis is given. The main theorem is about the convergence of balance data in terms of communication rounds not about the effect of unbalance data.
Since the proof mainly follows from that of SCAFFOLD and many empirical findings have no theoretical support, I would regard the main contribution as the algorithm itself. The experiment indeed did well, however, there is still some shortcomings. There are other algorithms proposed to (or able to) handle data heterogeneity like FedPD [1], FedSplit [2] and VRL-SGD [3]. However, the authors didn’t consider these competing methods and even didn’t mention them.
[1] Zhang, Xinwei, et al. ""FedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data."" arXiv preprint arXiv:2005.11418 (2020). [2] Pathak R, Wainwright M J. FedSplit: An algorithmic framework for fast federated optimization[J]. arXiv preprint arXiv:2005.05238, 2020. [3] Liang, Xianfeng, et al. ""Variance reduced local SGD with lower communication complexity."" arXiv preprint arXiv:1912.12844 (2019).
Other points:
The description of SCAFFOLD’s linear term seems imprecise. The subtracted term is not
1m∑k∈[m]∇Lk(θkt)
, since SCAFFOLD only activates a small fraction of devices and this term is impossible to compute. But I can understand what the author wants to convey: their affine function saves communication costs.
The authors mention that at each communication round SCAFFOLD communicates the current model and its associated gradient while others communicate only the former. So, if I want to say FedDyn is more effective in saving communication rounds, I would expect it to achieve at least 2x gains over SCAFFOLD. However, this is a rare case when the participation rate is 10% (see Table 2). It seems that FedDyn is not so effective in saving communication rounds and the main reason for its effectiveness seems to be that the modified affine function requires once communication per round.
I have read the authors' responses and almost all my concerns have been well addressed. So I increase my point to 8.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Federated Learning Based on Dynamic Regularization","Keywords: Federated Learning, Deep Neural Networks, Distributed Optimization","Q4: What is the relationship of FedDyn to recent works (FedSplit, FedPD, VRL-SGD)?
We were unaware of these works, and thank the reviewer for pointing us to them. We cite them in our revised draft. Let us list the key differences:
FedDyn handles partial participation, while these recent works, in essence, require full participation. In passing we emphasize that partial participation, where devices are activated in a random fashion is of fundamental practical importance in federated learning.
We refer to our detailed response to reviewer 1 for FedSplit comparison. VRL-SGD also requires full participation. On the other hand, FedPD appears to allow for partial participation. But the nature of partial participation is such that all devices are simultaneously active or not with some probability. As such, this is a strong assumption, which is not met in practice (think of massive number of devices). Note that the existing full-participation results in ours and prior works directly apply to this case by trivially freezing updates when inactive. Nevertheless, the interesting situation is to update models even when no device is active. We agree that this could be interesting but somewhat outside the scope of our framework.
In summary, like we point out for FedSplit, partial participation, as in our framework, poses fundamental challenges in theory, and in practice reasonable modifications do not appear to yield good performance.
There are also important conceptual and operational differences even for the full-participation setting. We refer to our Reviewer 1 response for FedSplit.
VRL-SGD is a SGD type method, where the goal of the server, in essence, is to leverage devices to produce gradient information. In its extreme, an SGD method would choose a device at random, and the device performs a few SGD steps, and return the resulting gradient as output to the server. In contrast, our devices serve as full optimizers, and as such, we are agnostic to as to how to solve the optimization problem. In point of fact, in practice, we use SGD to optimize device objectives as a matter of computational convenience, and as evident the number of such SGD steps could be as large as
105
, a number so large that it ceases to be an SGD type method! On the other hand, Theorem 5.1 in VRL-SGD paper states its convergence rate where
k
is the number SGD updates,
γ
is the learning rate and the product
kγ
is upper bounded with a problem dependent constant. Due to the bound on
kγ
, if we allow more SGD steps, we need to decrease
γ
. But, this we believe in turn negatively impacts convergence. Evidently, VRL-SGD accuracy degrades with more SGD steps, while FedDyn is agnostic to it so long as we reach a stationary point.
We further refer to our response to Reviewer 4 to highlight other differences between SGD methods and ours. As pointed out in our response to Reviewer 1, an important aspect is that our gradient updates bear a linear relationship (a consequence of optimality condition), which is critical for our ease of analysis in the partial participation setting. VRL-SGD updates are somewhat more implicit, and it is not clear whether it is easy to extend their analysis to partial participation. Although FedPD is not an SGD-type algorithm, their updates bear an implicit relationship between the model and gradient (like FedSplit), which we believe is somewhat difficult to extend in the randomly active (our partial participation) scenario.
To summarize, FedDyn and the cited works exhibit differences in terms of scope and operation. We test FedSplit among these works and empirical results reveals Scaffold and FedDyn outperform FedSplit for both full and partial participation levels (see Appendix A.4).","",""
"Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator","Keywords: gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell","The paper presents a variance reduction technique to the Straight-Through version of the Gumbel-Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field.","The paper presents a variance reduction technique to the Straight-Through version of the Gumbel-Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field.",""
"Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator","Keywords: gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell","We have incorporated the feedback of the reviewers and summarize here the changes in our revision (new upload!):
Additional baselines (R1, R4): We ran experiments for FouST (Pervez et al., 2020, as kindly suggested by R1) where applicable (binary VAE): FouST improved clearly over ST, but is outperformed by our method.
Low temperatures and low bias (R1): We have clarified throughout the paper that our method extends the space of trainable temperatures to those that have lower bias, but higher variance. Empirically, these tend to be low temperatures.
ListOps experimental set-up (R1): We added a reference to Havrylov et al. (2019) in the main body and highlight the differences between their and our experimental set-up in the appendix to put results into perspective.
We clarified the discussion in 3.3 as suggested by R2 and added a footnote.
Please see individual responses below for more details.","",""
"Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator","Keywords: gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell","Summary:
This paper proposes a Rao-Blackwellized version of the straight-through gumbel-softmax gradient (STGS) estimator.
The Gumbel-Rao estimator remains single-evaluation (but multiple sample), does not have higher variance than the original straight-through estimator.
The estimator exhibits lower variance at lower temperatures in the experiments.
Contributions:
Proposes a single-evaluation estimator that cannot have higher variance than the STGS gradient estimator.
Demonstrates effectiveness of proposed estimator in terms of the variance of the gradient estimator and the ELBO on a toy task, a simple parsing task (ListOps), and a mixture model for MNIST.
Strengths:
The method is simple and the computational overhead is very small compared to the original STGS estimator.
The empirical results support lower variance claims and effectiveness at lower temperature.
Weaknesses:
I am not convinced that the relative gains from training at lower temperatures are significant.
The overall gains over ST-GS seem to be modest on MNIST as well as the L <= 50 setting in ListOps.
In the ListOps experiments, lower temperatures barely achieved better accuracy.
Decision: Marginally below acceptance threshold
Improving gradient estimators for discrete latent variable models is an important problem.
The method is straightforward and the claims of performing better at lower temperatures are supported by empirical evidence.
However, the overall performance on the ListOps dataset is lower than related work [1], and there does not appear to be a large gain from low temperatures.
Questions:
The main argument of this paper hinges on the claim that lower temperatures result in lower bias of the gradient estimator. This claim seems reasonable, and is supported by figure 2b. Is there a proof or citation for it, and do we know more? It would be nice to know how variance and bias are traded off, as that would tell us how much (or how little) we could gain from training at lower temperatures.
Is there an explanation for the difference in performance between the 99% accuracy obtained in Havrylov et. al. 2019 [1] and the performance obtained at low temperatures in this paper?
How does this method perform versus the estimator proposed in Pervez et. al. [2], which is also single-evaluation?
Suggestions:
The GR estimator is not guaranteed to have lower variance than ST-GS, just not higher.
Is there an application where lower temperatures are necessary for training? That would strengthen the argument.
[1] Serhii Havrylov, German Kruszewski, and Armand Joulin. Cooperative Learning of Disjoint Syntax and Semantics. In Proceedings of NAACL 2019.
[2] Pervez, A., Cohen, T., & Gavves, E. 2020. Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks. ICML 2020.
Edited score after author comments.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator","Keywords: gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell","Thank you for the thorough response, and correcting my interpretation of the argument. All of my concerns have been addressed, although I do have some lingering concerns about the performance on ListOps in comparison to Havrylov et. al. I am still worried that there is a large gap between the upper bound performance by training via the proposed gradient estimator vs an unbiased one, but that does not detract from the contribution of the paper which demonstrates improvements over the ST estimator baseline. There are enough confounding factors, in particular model differences, that it is unclear whether the results in Havrylov et. al. are comparable. I will update my score accordingly.","",""
"Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator","Keywords: gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell","This paper introduces the Rao-blackwellization technique to reduce the variance of the straight-through gumbel-softmax gradient (STGS) estimator wrt the parameters of discrete distributions. The proposed method introduces almost trivial computational costs (relative to function evaluations) and is empirically and theoretically shown to systematically improve STGS.
I don’t have a lot of nitpicking to make for this paper, as it is quite well executed. The proposed method is very clean and the improvement over the STGS baseline is very consistent, and makes it even competitive with concrete-relaxation in the discrete latent variable model experiment.
Details:
Why not show the curve of ELBO during training, but the arrival-time-at-certain-thresholds in Fig 2-c? Last paragraph of sec 5.4: The larger batch size here also reduces the variance of minibatch SGD, not just the variance of
∇GR
in (13). In fact each instance is a different approximate posterior, which has different base variance. This makes the discussion in 3.3 a bit misleading.
Suggestions: For figure 1, perhaps visualize the variance of both separately. An improvement by 2 is not that meaningful if the variances of both are >> 2.
-- After rebuttal
Thank you for revising the paper. I've read the revised section, and stand by my original evaluation.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator","Keywords: gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell","Summary: The paper presents a new way algorithm to compute the straight-through variant of the Gumbel Softmax gradient estimator. The method does not change the estimator's bias, but provably reduces its variance (with a small overhead, using Rao-blackwellization). The new estimator shows good performance on different tasks, and appears to lead to more efficient optimization for lower temperatures (lower bias).
Clarity: The paper is well written.
Originality: The use of Rao-blackwellization in the proposed way is, up to the best of my knowledge, novel.
Pros of the paper and significance:
Relaxation-based gradient estimators are widely used, and the proposed method may their variance quite significantly.
The proposed algorithm has a clear justification from a theoretical perspective, and admits a simple implementation.
The proposed algorithm does not require additional model evaluations, and thus may lead to large reductions in variance without incurring a high computational cost.
The proposed method leads to more efficient optimization at lower temperatures (lower temperature translates to lower bias, but often higher variances).
Cons: I'd say one thing that could be included are additional baselines in the experimental section. There are other estimators that may be use. For instance, you could compare against VIMCO. While this is a different type of estimator (non single evaluation, not based on relaxations), it could be interesting to see how the results compare using this estimator too.
Recommendation: Accept (reasons in the ""pros"" list above).","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper. The results are solid and impactful. Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways. All four reviewers overwhelmingly recommended accept. I recommend that this paper be selected as an oral presentation.","A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper. The results are solid and impactful. Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways. All four reviewers overwhelmingly recommended accept. I recommend that this paper be selected as an oral presentation.",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","At the outset, we would like to thank all four reviewers for their thorough and patient reading of our article. Their fair criticism and constructive suggestions have enabled us to improve the quality of our article. A revised version of the article is uploaded. We proceed to answer the points raised by each of the reviewers individually, below. We would also like to point out that all the references to page numbers, sections, figures, tables, equation numbers and references, refer to those in the revised version. Yours sincerely’ Authors of ""Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies""","",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","The paper proposes a novel RNN architecture (CorNN) to tackle the infamous problem of vanishing and exploding gradients in RNNs. The novel CorNN architecture is based on time-discretized forced coupled damped nonlinear oscillators. For the gradient norm of CorNN analytical lower and upper bounds are calculated implying that CorNN avoids vanishing and exploding gradients. This is accompanied by numerical results (including code) that demonstrate the improved trainability of CorNN in permuted sequential MNIST, and adding task and noise-padded CIFAR10 compared to some other RNN architectures (GRU, LSTM, antisymmetricRNN, IMDB sentiment analysis, and a human activity recognition task.
In summary, the paper proposes a useful and mathematically transparent way of tackling the challenge of training RNN on tasks with long-time dependencies.
Weak points of the paper:
In the mathematical analysis the paper claims to ""rigorously prove precise bounds on [...] gradients, enabling the solution of the exploding and vanishing gradient problem"". If I am not misunderstanding, the mathematical bounds are actually only shown for the initial gradient norm (plus some number of steps afterward in section C4 of the Supplementary Material).
A more in-depth comparison of numerical gradient norms and their respective upper/lower analytical bounds would be desirable, as the lower bound on the gradient in (16) is only given in as O(∆t^(3/2)) without any prefactors.
While the numerical results demonstrate improved trainability and performance on a number of tasks, the underlying reasons remain mostly unclear. Supplementary Material B gives some heuristics on how a superposition of forced coupled damped nonlinear oscillator can generate complex output, but it doesn't explain the avoidance of exploding and vanishing gradients and the superiority to other solutions (e.g. gated units like LSTM or GRU).
Missing: What are the limitations of CorNN, when would you expect them to fail?
The link to biological networks is not yet very convincing. While there exist without doubt many oscillations on different scales in the brain, it is not clear how the insights gained here could be applied to the brain.
Some smaller comments:
How could expressivity be quantified?
figure 1: Plotting MSE with a logarithmic or semilogarithmic axis would help to distinguish small errors.
figure 3: It would help to also plot this for other tasks.
figure 3 lines for other tasks would be helpful","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","Regarding the reviewer's question: ""What are the limitations of CorNN, when would you expect them to fail?"", it is indeed very interesting. As we point out in the main text in the first paragraph of page 4 (and supplement this statement with a precise results in proposition D.2 and proposition E.1 of the SM), the structure of our proposed RNN (and the underlying ODE) rule out chaotic behavior. Thus, a clear limitation of coRNN is in the prediction of chaotic time series. This was also identified by one of the reviewers and following their suggestion, we have expanded this point in the discussion (please check the third paragraph of page 9 in the main text) and also in section A of the SM, where we present results with coRNN and LSTM (with the same number of parameters) for learning the chaotic trajectories of a Lorenz system. Clearly, coRNN is inferior to LSTM in the chaotic regime.
The reviewer's point about the connection to oscillators in neurobiology is completely valid. We wish to point out that we do not claim that coRNN will shed any light on the functioning of the brain. The link is in the other direction i.e, we were motivated by the presence of networks of oscillators in brain circuits to propose coRNNs. However, the link is more in terms of an analogy. As we state in the main text, see the last sentence of the last paragraph on Motivation and Background (page 3), we abstract the idea of oscillators in brain circuits to propose coRNN based on much simpler mechanistic systems. In general, we have toned down the neurobiological connection in the revised version but prefer to keep the sentence on motivation in page 3.
Smaller Comments
We have followed fairly common terminology in the machine learning literature about expressivity. Regarding the reviewer's very valid question about how to measure expressivity, we did not find a clear cut answer in the literature. The most common answer that we found was in some of the more mathematically oriented papers where the authors measure expressivity in terms of the number of tunable parameters that are necessary for a neural network to approximate/predict some underlying ground truth to a certain error tolerance. However, we would not like to assert anything definitive in this regard. We have used expressivity loosely as the ability of a neural network to process complex inputs/outputs and have more clearly stated this in the revised version.
We have followed very standard practice in the literature (for instance reference Arjovsky et al) to plot figure 1 in a standard format in order to enable the reader to compare our results with other published results.
Following the reviewer's suggestion, we have updated figure 3 to present these results for all LTD tasks that we consider. Please see answer to point 1 of the major comments on this issue. We have also added a short discussion on the results of this figure on page 8.","",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","The paper introduces a novel recurrent neural network architecture which approximately preserves the norm of the gradient irrespective of the number of unroll steps. This complements a rapidly growing line of research that aims to better understand dynamical properties of RNNs and their gradients, thus potentially enabling training of models that capture long term dependencies while avoiding exploding and vanishing gradients.
The submission has the following strengths to it:
It offers a clear and succinct proof of the gradient stability as well as the stability of the forward dynamics.
It provides convincing experiments on relevant datasets, all while showing competitive results.
It's exceedingly well written, and readily understandable even without prior knowledge.
I firmly believe that based on those, it should be accepted. It has all the hallmarks of a good, paper with potentially wide application.
That being said, I do have some minor objections:
When presenting Eq. (1), the 'intuitive' interpretation of
γ,ϵ
should given right away, rather than deferred to later sections of the paper, especially the appendix.
The motivation for the use of non-linear oscillators is well-written but perhaps should be de-emphasized. I would like to put-forward the following argument for it. It appears that the choice of the dynamics only constitutes half of the 'puzzle'. The choice of IMEX is mentioned en passant, but seems to be rather crucial to obtaining the theoretical guarantees viz. Proposition 3.2 and 3.3. I did not have time to check the derivation against other schemes, but I presume that the choice of IMEX was highly non-trivial in designing the new architecture. If that is indeed the case, then the role of the solver ought to be emphasized.
I appreciate the authors comments regarding the expressivity of the proposed architecture, as well as the demonstrations in Appendix B. However, I would also appreciate a simple example of the kind of dynamics where the coRNN cell ought to break down -- given the corollary of Proposition 3.1, it would be interesting to show the potential break-down of the network on task that involves approximating a chaotic dynamical systems. In particular, it would be very interesting how coRNN fares against similarly sized gated RNN (LSTM or GRU).
For my own understanding, is the exponent
r
in Eq. 8 there only to conveniently related
η
to
Δt
? If not, does it admit some more intuitive interpretation?
Since, the proposed architecture requires a sufficiently small step-size, are the resultant equivalent (in some suitable sense be it topologically, having the same invariant set) to the continuous time dynamics?
Lastly, the authors report the hidden unit dimensionality, but from the main text it is entire unclear whether that's the dimensionality of
y
or
[y⊤,z⊤]⊤
. Having looked at the code, it appears to be the former.
Edit:
Out of curiosity I ran the submitted code for the permuted sequential MNIST task, and noticed that the following:
The numbers that the authors report in the paper seem to be result of a single network realization. While granted, this is somewhat consistent with practices common in the community, it makes one question how representative they are of different initial seeds. In the current setting it's hard to disambiguate whether the random seed was chosen coincidentally or rather specifically because of the purported state-of-the-art outcome.
For this reason I suggest the authors compute additional iterates of the model and report some distributional information about the best loss/accuracy for all the tasks covered in the submission.
Attaining ""state-of-the-art"" results is notable, but is by no means pre-requisite for this to be considered a good submission.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","Suggestions for Edit.
(1,2). We completely agree with the reviewer's point about the need to provide distributional information on results in order to check the robustness of the proposed network with respect to random initializations and perturbations. As the reviewer correctly points out, we have followed widely prevalent practice in the community to report best performing results with coRNN for each learning task. This allows us to compare with other methods as baselines, as most of the published papers only provide best performing results and seldom provide distributional information. Hence, we retain the best results with coRNN in Tables 1,2,3,4 in order to enable the reader to compare the performance of coRNN with baselines. On the other hand, we follow your suggestion and compute distributional information on coRNN for each learning task. This is now provided in Table 5, where the mean and standard deviation, over
10
retrainings, of test accuracy with respect to all the learning tasks is reported. As observed from the table, coRNN is quite robust in its performance, with the mean being close to the best results and low standard deviations. The standard deviation is somewhat higher for psMNIST. This is not surprising as the random perturbation also varies in each of these runs.
(3). The reviewer's point about ""Attaining ""state-of-the-art"" results is notable, but is by no means pre-requisite for this to be considered a good submission"" is completely justified. We agree with this assertion and prefer to tone down the phrases comparable to or better than state of the art in many places in the text, by replacing it with competitive. We submit that our rationale for this article is contained in the last sentence of the first paragraph of page 9 i.e. ""Thus, we provide a novel and promising strategy for designing RNN architectures that are motivated by the functioning of natural systems, have rigorous bounds on hidden state gradients and are robust, accurate, straightforward to train and cheap to evaluate.""","",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","This paper proposes a new continuous-time formulation for modeling recurrent units. The particular form of the recurrent unit is motivated by a system of coupled oscillators. These systems are well studied and widely used in the physical, engineering and biological sciences. Establishing this connection has the potential to motivate interesting future works. The performance of the proposed recurrent unit is state of the art.
Reasons for my score: Overall, I vote for marginally above acceptance threshold. I like very much the proposed approach for modeling recurrent units. Further, the presented results are intriguing, and the paper is well written. However, I have some concerns (see below). I am happy to increase my score if the authors can address my concerns in the rebuttal period.
Pros:
Second-order systems of ODEs seem to be a promising approach for modeling recurrent units, and this approach has not received much attention for this task before. Indeed, this paper impressively demonstrates that a unit motivated by a system of coupled oscillators is able to achieve state of the art performance on a range of benchmark tasks.
The analysis shows that the particular form of the proposed continuous-time unit mitigates the vanishing and exploding gradients problem by design, which is very appealing. The analysis is mathematically sound.
Code is provided!
Cons:
In Eq. (3) the authors advocate the IMEX scheme to obtain a discretization of (2). I was very curious to see how the authors implement this scheme in practice, however, the provided implementation revealed that the authors use an explicit scheme in practice. Please, comment why you chose the IMEX scheme here. Does the analysis also hold if you use an explicit discretization in (3), and if, why do you mask the fact that you are using an explicit scheme in practice. I feel, it would be relevant to discuss how you train the unit in practice.
Section 3 is no pleasure to read. It is not clear to me what the value of the sketch of the proofs are, since the proofs are pretty standard. Instead, the space could be better used for an extended qualitative discussion of the analysis and the nice properties of the proposed recurrent unit. For instance, you can extend the discussion around proposition 3.1 and provide some context on why you want to rule out chaotic behavior (this might not be obvious for everyone); further it would be nice to see a better discussion on the effect of dampening and forcing on the performance of the recurrent unit. Also, I would like to suggest to move parts of Appendix B into the main text, since this discussion actually helps to build some intuition for the proposed unit.
The extremely good performance on the sMNIST task is slightly surprising, since my intuition would not suggest that the particular form of the unit has the ability to substantially improve the expressivity as compared to some other recently proposed units. I used the provided code to evaluate the coRNN (N=256 and 128) on the sMNIST task and the highest accuracy that I was able to obtain (out of 8 runs on 4 different GPUs) was 99.2% on the test set. These results are still very good, but they do not match the reported results. (Note, that the code is printing out the accuracy for a smaller validation set which indicates a higher accuracy than is actually obtained on the test set.) This said, I would like to ask the authors to double check the experiments on sMNIST. (Also, I assume that the model can be trained in less time if the learning rate is decayed much earlier, e.g., around epoch 30 and a second time around epoch 60.)
It is not clear to me how sensitive the RNN is to the particular choices of \gamma and \epsilon. It would be good to provide some form of ablation study that studies how the performance varies for different values of \gamma (and \epsilon) while keeping all other tuning parameters fixed (I assume that you have all these results handy since you have performed an extensive hyperparamter search). This would help to gain some better intuition for how difficult it is to tune the proposed unit. In other words, I would like to see how sharp the performance drop is if you perturb the tuning parameters slightly (i.e., plot the test accuracy as a function of \gamma and \epsilon).
Minor comments:
Given additional space, it would be nice to see an extended related work section.
It would be nice to so results for a language modeling task.
In Table 2, the citation for the Fast RNN is incorrect.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","We start by thanking the reviewer for your latest comment. Motivated by your question, we decided to baseline the performance of coRNN on the adding problem with yet another example of RNNs designed to learn LTDs. Given the very limited amount of time available to us for performing additional tests before the deadline of this discussion phase, we had to choose between the two architectures that you suggest, namely expRNN and non-normal RNN (nnRNN). We chose expRNN as even in the reference [1] where non-normal RNN is proposed, both the results and the discussion imply that expRNN is superior to nnRNN in learning LTDs. Thus, we tested expRNN on the adding problem (surprisingly we did not encounter any published results on either expRNN or nnRNN on the adding problem). The results are now presented in the Figure 1 of the latest version of the article. As can be clearly observed from this figure, expRNN is better performing than both FastRNN and anti-sym. RNN. However, even on the sequence length of
T=500
, coRNN readily outperforms expRNN. This difference in performance is further accentuated for a sequence length of
T=2000
, in which case the expRNN beats the baseline but does not reach a desired test MSE within training time. In comparison, coRNN converged very fast to this level of MSE. Finally on the most challenging sequence length of
T=5000
, only coRNN is able to beat the baseline whereas expRNN fails to do so. Thus, these new results clearly demonstrate the superior performance of coRNN on this particular problem when compared to state-of-the-art baselines. We emphasize that exactly the same hyperparameter selection protocol was used for all the tested architectures in order to ensure a fair comparison. We expect that nnRNN will perform similarly to expRNN for the adding problem and we would be happy to add the results in the camera-ready version of the article in case it is accepted for publication. Regarding the copying task, we had to make a choice for the experiments that we presented in the article and we felt that the adding problem was a more reasonable choice to test the performance of coRNN as it involves both memory and computation in contrast to the copying task where only the ability of an RNN to memorize is tested. We sincerely hope that we have adequately addressed the reviewer's concern.","",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","I would like to thank the authors for the detailed response. All my concerns have been addressed and clarified and I am happy with the revised manuscript. In particular, Table 5 is convincing. Hence, I will change my rating!
This said, I am curious if you have some intuition for why the ablation study (Figure 4) shows that the model is largely insensitive to
ϵ
, but relatively sensitive to
γ
.","",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","Minor Comments.
We have added some further details in the extended work section and compared our approach to other ODE inspired RNNs.
The use of coRNN on a language modeling example is indeed a very good suggestion and we intend to do it in a possible future article as the focus in this article is on learning tasks with long-term-dependencies where the mitigation of the exploding/vanishing gradient problem is of crucial importance.
We thank the reviewer for bringing this to our attention. While we followed standard practice and cited the publication where we collected the experimental results from in table 1,2,3,4, we double-checked again that we also consistently cited the original publication of each method in the main text.","",""
"Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies","Keywords: RNNs, Oscillators, Gradient stability, Long-term dependencies","Firstly, this paper conducts the rigorous analysis of the coRNN via the formula deduction to verify the bound. Then the coRNN is proved to mitigate the exploding and vanishing gradient problem and this is also validated in a series of experiments. Also, the performance of the coRNN is comparable or better compared to state-of-the-art models. This paper provides a new idea to address the exploding and vanishing gradient problem, which hinders the development of deeper neural networks tremendously. In my opinion, this coRNN model is meaningful for practical application, especially for the extension of more complicated neural networks.
Besides, for the biomedical signals with high temporal resolution (e.g., electroencephalogram, electromyogram), the coRNN model can be a good alternative in future work. Furthermore, the efficiency of the proposed model also be proved by the mathematic formulation and experiments ranging from pure synthetic tasks designed to learn long-term dependencies to more realistic tasks rigorously. Considering the whole structure of this paper, I argue that the clarity is clear and logical. Different from the recently published literature, this paper has explicit use of networks of oscillators with the underlying biological motivation, so this paper expresses the originality in some extent. To sum up, the quality of this paper is suitable for the publication in ICLR2021.
There are two main pros in this paper: 1. The theoretical verification is clear and rigorous, readers can easily catch good understanding of the bounds this paper proves following the formula deduction. Specifically, this paper demonstrates how to avoid the exploding and vanishing gradient problem for the RNN in theory.
2. The experiments are quite abundant, experimental results show that the coRNN can not only avoid the exploding and vanishing gradient problem, but also achieve better performance with fewer parameters compared to recent studies.
But some cons should also be noticed. Firstly, the illustration of proposed coRNN should be presented in the paper, which is more comprehensible. Secondly, the related work part, when mentioning the similar works, it will be better to describe the main differences and correction with this paper more specifically. Lastly, in the part of discussion, the practical significance of proposed coRNN should be emphasized with more words.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"DiffWave: A Versatile Diffusion Model for Audio Synthesis","Keywords: diffusion probabilistic models, audio synthesis, speech synthesis, generative models","I join all five reviewers in recommending acceptance.
There was some discussion about a comparison with WaveGrad (Chen et al., 2020), a contemporaneous work that explores a similar modelling approach for speech generation. While I agree that such a comparison is a useful addition to the manuscript, I do not think it is reasonable to request anything beyond an acknowledgement and citation of the work from the authors as a condition for acceptance. Further discussion and comparison experiments could be valuable, but I believe that should not factor into the final decision. My position is most similar to Reviewer 4's in this sense. The current version of the manuscript briefly discusses the differences between WaveGrad and DiffWave, which I think is more than sufficient. (As an aside, another difference potentially worth discussing is that the ""noise schedule"" for WaveGrad can be adapted at inference time, enabling a trade-off between inference speed and sample quality, which I believe is not possible for DiffWave in its current form.)
There was some debate about the weakly conditioned generation results; I believe they are a nice addition to the paper, although it would have been suitable for publication without them. They certainly do not detract from it, and might inspire further work in weakly conditioned audio generation (e.g. music). There were also concerns about the clarity of writing, which I believe the authors have addressed in the current version of the manuscript.
This work stands out because it applies a relatively fresh idea in generative modelling to a domain of great practical importance, which has long been dominated by traditional likelihood-based models, with compelling results. While this implies a limited degree of technical novelty, I do not think that is grounds for rejection, and in fact I would argue that making new ideas work well for practical problems is just as important.","I join all five reviewers in recommending acceptance.
There was some discussion about a comparison with WaveGrad (Chen et al., 2020), a contemporaneous work that explores a similar modelling approach for speech generation. While I agree that such a comparison is a useful addition to the manuscript, I do not think it is reasonable to request anything beyond an acknowledgement and citation of the work from the authors as a condition for acceptance. Further discussion and comparison experiments could be valuable, but I believe that should not factor into the final decision. My position is most similar to Reviewer 4's in this sense. The current version of the manuscript briefly discusses the differences between WaveGrad and DiffWave, which I think is more than sufficient. (As an aside, another difference potentially worth discussing is that the ""noise schedule"" for WaveGrad can be adapted at inference time, enabling a trade-off between inference speed and sample quality, which I believe is not possible for DiffWave in its current form.)
There was some debate about the weakly conditioned generation results; I believe they are a nice addition to the paper, although it would have been suitable for publication without them. They certainly do not detract from it, and might inspire further work in weakly conditioned audio generation (e.g. music). There were also concerns about the clarity of writing, which I believe the authors have addressed in the current version of the manuscript.
This work stands out because it applies a relatively fresh idea in generative modelling to a domain of great practical importance, which has long been dominated by traditional likelihood-based models, with compelling results. While this implies a limited degree of technical novelty, I do not think that is grounds for rejection, and in fact I would argue that making new ideas work well for practical problems is just as important.",""
"DiffWave: A Versatile Diffusion Model for Audio Synthesis","Keywords: diffusion probabilistic models, audio synthesis, speech synthesis, generative models","Summary:
The authors adapt the recent trend of work on denoising diffusion probablistic models to the task of conditional or unconditional waveform generation. Using the same principles as in (Ho et al 2020), as well as a Wavenet-like non causal model, the authors provide state of the art results for both tasks, as evaluated on spoken digits dataset (for unconditional and conditional generation) and on the LJ speech dataset (for deep vocoding). The model proposed is faster to evaluate than WaveNet, and has less parameters than WaveGlow. It achieves a slightly higher MOS than WaveFlow for a comparable model size. The generation speed is comparable to previous methods.
Review
The paper is clear, well structured and the authors provide many experiments to validate their approach. While serious, the paper does lack novelty, as the method is completely taken from (Ho et al. 2020). The architecture is similar to Wavenet, but non causal. Note that this exact same non causal wavenet architecture has already been used for source separation [Rethage et al. 2018, Lluis et al. 2019].
One limitation is that unconditional, or weakly conditioned generation (i.e. not conditioned on a mel-spectrogram) is only evaluated on single digit generation, which is relatively limite. While the samples shows an improvement over WaveNet, it seems the proposed architecture would still struggle to generate longer sequences, like an entire sentence for instance. It would be interesting to add WaveGlow or WaveFlow to the SC09 comparison.
Overall I recommend acceptance as the paper show that denoising diffusion process can be used for waveform generation, even though the paper does not bring further novelty.
References: Rethage et al. 2018: A Wavenet for Speech Denoising Lluis et al. 2019: End-to-end music source separation: is it possible in the waveform domain?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"DiffWave: A Versatile Diffusion Model for Audio Synthesis","Keywords: diffusion probabilistic models, audio synthesis, speech synthesis, generative models","The paper develops a speech synthesis model using denoising diffusion processes, a generative model framework recently demonstrated in image generation (Ho et al. 2020). The application is straightforward and there is little if any theoretical difference from the Ho et al. paper. I didn't check the proofs included in the appendix, but they along with the learning and sampling procedure seem to be already developed in (Ho et al. 2020). The authors should take care to be very clear about the mathematical developments that are directly taken from the prior literature, and what developments are introduced in this paper.
Nevertheless the experiments on this application is valuable, and makes significant progress on a problem that has proven surprisingly difficult to solve in an efficient way.
The experiments and demos are convincing, and the results could be considered highly competitive in conditional generation and state of the art for class-conditional and non-conditional generation.
The writing at times could use improvement. In the abstract, line 1, ""we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation"". I don't like this style of capitalizing things in a sentence that are not proper nouns. If you want to introduce an abbreviation derived from a term or phrase, a widely accepted conventional method is to italicize the phrase and define the acronym the first time it is used, as in ""\emph{diffusion waveform} (DiffWave) model"". Later you have ""DiffWave produces high-fidelity audios in Different Waveform generation tasks"": Why is ""Different Waveform"" capitalized? DiffWave has already been defined relative to ""diffusion waveform"". If this is supposed to be cute, it's not.
Defining the acronym in the abstract is OK, but not necessary. In any case, you still have to define it again in the body of the paper, since the abstract is considered a standalone summary of your document. Also ""audios"" is not a word. Please use ""audio signals"". This is repeated throughout the paper. Other examples:
""This avoids the ... issues *stemmed from the joint training"" stemmed --> stemming "" for generating very long waveform"" : waveform --> waveforms","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"DiffWave: A Versatile Diffusion Model for Audio Synthesis","Keywords: diffusion probabilistic models, audio synthesis, speech synthesis, generative models","This paper describes a neural vocoder based on a diffusion probabilistic model. The model utilizes a fixed-length markov chain to convert between a latent uncorrelated Gaussian vector and a full-length observation. The conversion from observation to latent is fixed and amounts to adding noise at each step. The conversion from latent to observation reveals slightly more of the observation from the latent at each step via a sort of cancellation. This process is derived theoretically based on maximizing the variational lower bound (ELBO) of the model and follows Ho et al. (2020) who derived it for image generation. Thorough experiments show that the model produces high quality speech syntheses on the LJ dataset (MOS comparable to WaveNet and real speech) when conditionally synthesizing from the true mel spectrogram, while generating much more quickly than WaveNet. Perhaps more interesting and surprising, however, is that it generates very high quality and intelligible short utterances with no conditioning, and also admits to global conditioning, e.g., with a digit label.
The paper is very clearly written, with the description of the model going into sufficient detail in the main body of the paper for the reader to understand it without getting bogged down in the less immediately relevant details. The experiments are thorough and well executed, comparing with listening tests to many state of the art neural vocoders for the conditional task. It describes a thorough evaluation of the unconditional generation task, which is in general difficult, but in this case was constrained in such a way as to make it feasible and informative, using reasonable metrics that clearly show the advantages of the proposed approach. The literature review is thorough and comes at a point in the paper where the reader understands the proposed approach and can appreciate the nuances of the differences between the approaches. Audio samples are provided on a companion website and demonstrate the effectiveness of the approach along with some additional interesting properties of the model not even mentioned in the paper (denoising most impressively, interpolation between speakers is less convincing).
The paper has two minor weaknesses. First is that it does not make a clearer distinction between the concurrent work from Chen et al (2020) along similar lines, although presumably this paper was not released prior to submission of the current paper. An extended comparison would be welcome in a camera ready version of this paper. Second, that it doesn't explicitly state the real-time factor of WaveNet generation in the results discussion on page 6, which is presumably much smaller than 1. This section compares to WaveNet in terms of quality and WaveFlow in terms of speed, slightly being slightly worse in both comparisons, but better in the opposite, partially missing, comparisons.
Overall, this paper makes a strong contribution to the field of neural vocoding and to the field of representation learning more generally for long-duration intricately structured signals (i.e., speech).","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"DiffWave: A Versatile Diffusion Model for Audio Synthesis","Keywords: diffusion probabilistic models, audio synthesis, speech synthesis, generative models","Well-written paper with strong, well-presented results. The MOS results attain or surpass the best WaveNet results. The presentation is on the whole clear.
I was a bit confused by the core model description at first. In particular, the index t on the samples x_t is not a time-index, correct? Rather, it's just a step in the diffusion process? At first I thought it was a time-index, and so the model seemed very much like an AR model, leaving me very confused.
Some additional things I liked about the work:
Compelling contrast with existing methods (WaveNet, VAE, GANs). Use of multiple metrics in the evaluation, 5 objective metrics in addition to MOS, e.g. Tables 2 & 3, with details on the metrics provided in an Appendix. Use of multiple models as reference models, WaveNet, WaveGlow, WaveFlow, Clarinet, WaveGAN, in addition to the proposed model. Focus on unconditional generation, which AIU has not received that much attention in the community Where I am unsure is the originality of the work. I personally am not aware of the diffusion approach having been applied to TTS, but this is not my primary area of expertise. Obviously, if there is related work in TTS with diffusion models, this should be cited.
Also, what I don't see in the Conclusion is any discussion of the weaknesses and challenges for the model going forward. The paper would be strengthened by having a more balanced conclusion.
Some caveats regarding my review:
I am not familiar with the specific datasets used, so cannot fully appreciate the significance of the results reported. I did not check the math in detail; the notation overall seemed clear and consistent to me (though see my first set of comments).
I have a few more specific comments.
Throughout the paper, ""... audios ..."" : ""audio"" is not usually used as a plural noun.
E.g. ""We randomly generate 1,000 audios"" --> ""We randomly generate 1,000 audio waveforms""?
""Notably, the quality of audios ... "" --> ""Notably, the quality of audio ... ""
""Note that, the quality of ground-truth ..."": nit, no comma after ""that"".","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"DiffWave: A Versatile Diffusion Model for Audio Synthesis","Keywords: diffusion probabilistic models, audio synthesis, speech synthesis, generative models","The Diffusion Probabilistic model is gaining popularity as a generative model. Diffwave explores the same for speech synthesis tasks. They show very good results i.e. matching autoregressive Wavenet on the conditional and outperforming baselines on the unconditional audio waveform synthesis tasks.
The paper is clearly written, and easy to follow. The work does not provide any novel machine learning or generative modeling insights. However, the work is significant for speech synthesis applications since it shows great results, with a small foot-print network with a very new method. This work can be expected to spark a plethora of follow-up works for speech synthesis and other real-valued time-series modeling tasks.
Pros:
Very good results on conditional neural vocoding task.
State-of-the-art results on unconditional speech synthesis task
Cons:
Lack of novelty in terms of insights/approach
Motivation and ablation-study for various design choices are missing
Further Comments: An ablation study that establishes the impact of various hyper-parameters/components (e.g. choice of diffusion step embedding function, more detailed analysis of width/depth of the network, etc.) would help the readers get a lot more value out of the paper. A qualitative study of samples, specifically, pointing out any bias that underlies generative modeling via the diffusion process would be great.","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency","Keywords: self-supervised learning, robotics","The paper proposes a new solution for cross-domain correspondence in control, which combines GANs and cycle-consistency, and separates shifts in observation space and in action space. The paper targets unpaired data / simulations, and discovers alignment of state by enforcing that domains are mappable.
The paper was received well by reviewers, who pointed out several strengths: a strong contribution on a fundamental problem, and an interesting formulation; a well written and well positioned paper; This compensates minor weaknesses, in particular the fact that transfer has been tested between two different simulated environments.
The reviewers unanimously suggested acceptance, the AC concurs.","The paper proposes a new solution for cross-domain correspondence in control, which combines GANs and cycle-consistency, and separates shifts in observation space and in action space. The paper targets unpaired data / simulations, and discovers alignment of state by enforcing that domains are mappable.
The paper was received well by reviewers, who pointed out several strengths: a strong contribution on a fundamental problem, and an interesting formulation; a well written and well positioned paper; This compensates minor weaknesses, in particular the fact that transfer has been tested between two different simulated environments.
The reviewers unanimously suggested acceptance, the AC concurs.",""
"Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency","Keywords: self-supervised learning, robotics","Dear Authors,
Congratulations on the excellent work.
I would like to point you to a previous work on cross-system motion correspondence that uses a very similar framework. It uses learned latent forwrad dynamics model as well as autoencoders trained with cyclic consistency loss.
http://proceedings.mlr.press/v120/kim20a.html Kim, N.H., Xie, Z. & van de Panne, M.. (2020). Learning to Correspond Dynamical Systems. Proceedings of the 2nd Conference on Learning for Dynamics and Control, in PMLR 120:105-117
While your specific subproblem within motion correspondence is different, your work seems to offer improvements upon the limitations of our work, namely supporting action correspondence as well as using random trajectories instead of expert trajectories.","",""
"Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency","Keywords: self-supervised learning, robotics","** Paper Summary **
This paper proposed a novel framework to establish the correspondences across observations and actions, by using dynamic cycle-consistency. The paper focused on robotics applications, but the technical solutions are based on a pioneering method computer vision, generative adversarial networks (GANs) and cycle-consistency constraints, whose robustness has shown in many previous literatures. The proposed method was applied and attained the robustness on various tasks, such as cross-physics alignment, cross-modality alignment, cross-modality-andphysics alignment.
** Paper Strength **
The paper is well organized and written.
Borrowing the concepts of GANs and cycle-consistency to solve the unpaired problems in cross-modal correspondence makes sense and good performance gains are expected.
Solving several simulation-based settings and real robot settings was nice.
** Paper Weakness **
I have no major comments on this paper, but have some minor ones.
In Table 5, Ours (E) and Ours (J) have shown the performance gaps. It would be great if the reasons are properly mentioned.
In Table 4, the authors compared the quantitative results with Cycle-GAN and INIT. It would be great if the thorough discussion for these experimental settings are provided.
Computational complexity analysis is also required.","6: Marginally above acceptance threshold","2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
"Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency","Keywords: self-supervised learning, robotics","Summary: The paper provides an interesting (and to my knowledge, novel) approach for learning a mapping of actions and observations from one domain/character to another reasonably similar domain/character. This mainly allows the transfer of skills (i.e. policies) from one domain/character to another. Immediate use cases of this approach are in imitation learning and sim2real transfer. In order to learn these mappings, the authors use the idea of cycle-consistency in generative adversarial networks and adapt it to the imitation learning task. Importantly, this allows them to obviate the need for paired state samples across domains. The authors show that their method not only works across modalities (vision from real robot to states in simulated robot), but it also works to some extent on different character morphologies.
Reasons for score: Overall, I vote for accepting this paper. To my knowledge, the method is novel and provides a viable and interesting approach for imitation learning and sim2real transfer. The experiments and the final quality are also high, however, the broader applicability to more challenging tasks and its limitations remain to be seen.
Cons:
The chosen tasks are (understandably) simple, therefore the applicability of the methods to more challenging environments remain to be seen.
The limitations of the method are not discussed well. I believe some commentary on the challenges of this method (e.g. the ease of training with GANs) would be useful. Also, more discussion on the use cases of the method and where it excels existing methods is missing.
The code is not provided which hampers reproducibility. I strongly suggest providing the code if possible.
Question:
How does the method handle partial observability? In the robot arm example, a single image of the robot does not contain information about the velocity and angular velocity (of the joints). I'm confused as to how the model can actually infer these or work without knowing them.
Can you spend more time explaining Figure 3 (c)? It seems strange that the ""L1 error"" increased when more data was available.
Related to the last question: in this case, can you pre-train G the same as F and use cycle-consistency only for H and P? A brief explanation would suffice.
Fixes or suggestions:
I will have to double check the conference style guides, but having tables and algorithms intertwined with the text in a single-column publication is not pleasing to the eyes. I suggest rearranging these elements.
The plots in Figure 3 can be improved. At the very least, the distortions caused by resizing the images should be addressed.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency","Keywords: self-supervised learning, robotics","This paper presents a technique that leverages cycle-consistency to align data across domains given only samples of one-step trajectories (x_t, a_t, x_{t+1}) from each. The cycle-consistency is used over the space of actions across two domains: e.g., given data from X [in the form (x_t, a_t, x_{t+1})] and data from Y [in the form (y_t, u_t, y_{t+1})], the system should be able to discover an ""alignment"" between states across the two domains by enforcing that the action spaces be mappable to one another (either identical, as in the ""cross-modality only"" applications, or via cycle-consistency, as in the ""joint model"" applications). The authors go on to show that their approach, among other things, allows for strong alignment between real-world images and their corresponding underlying state.
Overall, the paper presents some solid results across a handful of interesting domains/applications. The paper is occasionally difficult to follow, particularly on the first read-through, in part because it is unclear precisely what form the data takes until Section 3, a problem not alleviated by a somewhat-unclear Figure 1 (see more detailed comments on this below). The introductory paragraph beginning with ""In this paper"" is quite hard to follow. Providing some more concrete examples in the introduction would be incredibly helpful for clarity. For example, mentioning that the input data to the learning algorithm takes the form of 3-element tuples (and their components), would be helpful.
The authors might also consider combining and aligning Figures 1 & 2 to help the reader follow along with the early pages of the paper. Perhaps also specifying the domain (i.e. functional domain: the space of the variables) on the figures themselves would help here. This lack of clarity in the beginning of the paper is likely its biggest weakness, as it is otherwise a good theoretical idea supported by strong results.
Smaller comments:
Though it is clear why training all of the networks jointly would lead to mode collapse (motivating the need to train F separately), it is not made particularly clear why the remainder of the system needs to be trained using the 'alternating' procedure employed throughout. A sentence or two explaining this decision would be helpful.
Particularly in the figures, the authors should make clear that the y images are in fact renders corresponding to the predicted state. Without this ""disclaimer"" readers coming from the purely-image-driven cycle-consistency community may mistakingly believe that those images are the direct output of the CycleGAN baselines or the proposed approach. Modifying the green-backgound images in Fig. 1(a) to include ""Render of y_t"" (or similar) would help avoid such a confusion.
Figure 1(b) is potentially misleading, since really the images are assumed to be identical between the two domains. Additionally the arrows on (b) and (c) in fig. 1 are also somewhat misleading, since it is not clear what the arrows represent. Consider adding more annotations on this figure describing in more depth what this figure is trying to communicate: namely the types of applications enabled by the approach, rather than the structure of the proposed approach itself.
The Related Work section is quite thorough and helpful for providing context. Two other papers that the authors might have missed and consider citing that made progress in solving similar tasks are (1) ""Adapting Deep Visuomotor Representations with Weak Pairwise Constraints"" (Tzeng, Devin, et al, 2016) [which uses a task-specific loss for aligning sim/real robot arm images] and (2) ""GeneSIS-RT: Generating Synthetic Images for training Secondary Real-world Tasks"" (Stein & Roy, 2018) [which uses cycle-consistency for task-agnostic sim-to-real and then uses the translated data for quadcopter flight]. Both papers circumvent the need to collect paired sim and real images for real-world robotics applications.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency","Keywords: self-supervised learning, robotics","Summary
This work is concerned with learning mappings between pairs of domains that may differ in representation (imagery vs. configuration state), physical parameters, morphology, or combinations thereof, relying only on unpaired training data from both domains. The key idea is to leverage the (existing) concept of cycle consistency, incorporating dynamics to formulate a cycle loss that spans vastly different domains. The cycle is closed by learning a domain translator
G
that maps states (the paper calls them observations - this is unimportant as the system makes no such distinction, as either domain might represent a ""state"" or an ""observation"") from domain
x
to domain
y
, and a forward model
F
that operates in the
y
domain. In addition, functions
H
and
P
are learned that map actions
a
and
u
between their respective domains
x
and
y
, in either direction. The difference between
G(xt+1)
and
F(G(xt),ut)
is then the part of the overall loss function that closes the loop.
G
,
H
and
P
are trained adversarially using cycle consistency losses;
F
can be trained by regression, exploiting training sequences in the
y
domain.
The system is evaluated on four different tasks from OpenAI Gym, and is shown to be effective in diverse settings.
Strengths
This work addresses an important, fundamental problem that arises in many applications, including state estimation and sim2real transfer.
The paper makes a significant, nontrivial contribution to this problem, allowing hard cross-domain transfer problems to be solved dramatically better than before.
The work is well related to prior literature.
The claims are supported by the empirical results.
The paper is well organized, readable and clear.
The appendix provides a lot of additional detail important for reimplementing this method and replicating the results.
Weaknesses
The implementation details, in particular the neural-network architectures, appear to be very specifically tailored to the scenarios of evaluation. Instead it would be helpful to provide insight into how a user of this method should go about designing the networks, given their application domains.
For cross-modality alignment, paired training data are typically available. How can the method take advantage of this? How would this impact the results, e.g. those of Table 2? Some related results are given in Fig. 3(c) but it is unclear what exactly was done and what exactly the graph shows.
An obvious and explicitly-stated application of this work is sim2real transfer, but no such experimental results were provided (I realize that it is not easy to set up such an experiment from which strong conclusions can be drawn).
Recommendation
Unless I am missing important similar work, the paper makes a strong contribution in an important area. It should be accepted. While I wrote more text under Weaknesses than under Strengths, the weaknesses are minor compared to the strengths.
Questions for the Rebuttal
The results on cross-physics alignment (Table 1) seem to leave a lot of room for further improvement. What is the limiting factor?
Please comment on the listed weaknesses.
Details
""RL score"" = return?
There are many little typos.","10: Top 5% of accepted papers, seminal paper","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","Accept. The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results. The authors should think about comparing with other linear attention mechanisms to show the applicability of the method.","Accept. The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results. The authors should think about comparing with other linear attention mechanisms to show the applicability of the method.",""
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","Hello,
I have a question about the ""Deformable Attention Module"" presented in this paper. According to the text following equation (2), the attention coefficients
Am,q,k
""are obtained via linear projection over the query feature"". In my opinion, that does not constitute an attention mechanism per-se, since by definition an attention should incorporate both features from the query and the key (the most popular instantiation of an attention mechanism being dot-product attention). In other words, the presented mechanism aggregates features in a neighborhood irrespective of what the actual features are. From a purely terminological point of view, the role played by these
Amqk
is more akin to a gating, and in my opinion a better name for the proposed module would be something like ""multi-resolution gated deformable convolution"". What is your opinion on this? As a side note, in the proposed formulation, the
Am,q,k
could very well be computed as a dot-product as well (between the query and each of the sampled point), making it a ""true"" attention mechanism. Have you tried such thing?
Aside from the numerical results, which are impressive, it would provide invaluable insights to showcase visualizations of the ""attention"" maps in the encoder and the decoder, similar to the original DETR paper. In particular, is the proposed model learning an approximation of what the original DETR attention attends to, or is it attending to completely different things? Do you still observe instance separation inside the encoder?
The two additions on top of the vanilla model (namely ""two stage detr"" and ""iterative bounding box refinement"") are interesting and seem to be directly applicable to the original DETR as well. Do you have a ball-park estimate of the performance reached when applying them on DETR-DC5? As a side note, the gains obtained thanks to the iterative bounding box refinement might hint at limitations in the original auxiliary-loss formulation, or the L1+GIOU combinations. It would be interesting to analyze this more deeply. It's surprising that it helps mainly for large objects in the deformable setting, I would have guessed it would help for small instead.
Finally, an ablation that would be valuable in my opinion would be to more carefully disentangle the effect of the multi-resolution and the effect of the proposed deformable module. In particular, in table2, the only experiment with no multi-scale input is done with k=1. What would be the performance with k=4 or even k=8 (which would be the most similar to DETR)? I'm curious to see if the proposed deformable module can match original DETR's performance on a low-res feature map, and if it requires long training schedule to do so. That would help understanding where the convergence speed boost comes from, and the exact performance trade-offs that are being made by using this different deformable mechanism as opposed to traditional attention.","",""
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","Q#5: ""disentangle the effect of the multi-resolution and the effect of the proposed deformable module""
A#5: Thanks for your good suggestion. We tried training Deformable DETR (K = 4) with single-scale input feature maps (of stride 32) for 50 epochs and 150 epochs. The results are shown as follows. The single-scale model of 50 epochs is still slightly worse than DETR (500 epochs) in total AP, while the single-scale model of 150 epochs achieves on par accuracy with DETR (500 epochs). However, the phenomenon is still slightly different. The single-scale Deformable DETR performs better on small objects and performs worse on large objects, compared with vanilla DETR. Further study is needed.
Method K Epochs AP AP50 AP75 AP@S AP@M AP@L
DETR - 500 42.0 62.4 44.2 20.5 45.8 61.1
DETR - 50 33.3 54.1 34.2 13.3 35.9 52.0
Deformable DETR (single scale) 4 50 40.5 60.6 43.0 21.5 44.7 56.7
Deformable DETR (multi scale) 4 50 43.8 62.6 47.7 26.4 47.1 58.0
DETR - 150 39.5 60.3 41.4 17.5 43.0 59.1
Deformable DETR (single scale) 4 150 41.6 61.9 44.6 22.8 45.3 58.0
Deformable DETR (multi scale) 4 150 45.3 64.3 49.1 27.1 48.4 60.0","",""
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","As a new framework for object detection, DETR is very important. However, it suffers from slow convergence and limited feature spatial resolution. This paper proposes deformable attention, which attends to a small set of sampling locations rather than all the locations in the original DETR. Besides, the paper applies multi-scale deformable attention for better results.
The paper is well-written and obtains very impressive results. Traning for only 50 epochs, deformable Detr obtain results similar to DETR which is trained for 500 epochs. By implementing a two-stage detector based on deformable Detr, the paper obtain state-of-the-art object detection results with a very high AP (52.3) on AP.
A few suggestions for improving the paper are given as follows. (1) The training and testing times could be reported in the paper, which is useful for other researchers to implement and use this method. (2) Some related methods on sparse connected self-attention/transformer [a,b,c] should be cited and discussed.
[a] Representative Graph Neural Network, CVPR 2020 [b] Dynamic Graph Message Passing Networks, CVPR 2020 [c] CCNet: Criss-Cross Attention for Semantic Segmentation in ICCV 19 & TPAMI 2020","9: Top 15% of accepted papers, strong accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","Summary:
This paper proposes Deformable DETR with multi-scale deformable attention modules to solve the problems of DETR: slow convergence and limited feature spatial resolution. In particular, it has faster convergence and achieves better performance(especially on small objects) than DETR.
Reasons for score:
Overall, I vote for accepting. I like this paper because it solves the main problems suffered by DETR. My major concern is about the clarity of the paper and some additional ablation studies (see cons below). Hopefully the authors can address my concern in the rebuttal period.
Pros:
1.This paper solves the main problems suffered by DETR: slow convergence and limited feature spatial resolution. In my opinion, it makes DETR more practical.
2.The proposed Deformable DETR can obtain multi-scale features without a huge cost. In this way, it can be optimized easily and detect objects precisely, especially small objects.
3.This paper also introduces some improvements and variants to boost the performance of Deformable DETR.
Cons:
1.In the experiments, focal loss is used for bounding box classification. What is the reason for this choice? In addition, the number of object queries is increased from 100 to 300. Why? In the test, how to choose 100 objects from 300 objects?
2.From Table 1, we can find DETR (500 epochs) has better performance than Deformable DETR on large objects (61.1 vs. 58.0), though the overall performance of Deformable DETR is better. Why?
3.In the Table 1, there is only DETR-DC5+ (50 epochs). Could you provide DETR-DC5+ (500 epochs) ?
Questions during rebuttal period:
Please address and clarify the cons above","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","The main contribution is a new attention module called deformable attention module. Like deformable convolution, it adds a translation term into the expression of the transformer, allowing a sparse spatial sampling. The resulting model is very interesting in terms of convergence and complexity compared to the original DETR. A Multi-scale deformable attention module is also proposed. it needs to add a scale function in the attention module equation. Experiments shows that it increases the AP detection rate on MSCOCO compared to FasterR-CNN and DETR. Contributions are clearly state and validated. The complexity study is very interesting and shows the interest of deformable attention module. Figure 1 presents a general view of the model. Since the deformable attention module is the core of the contribution, it should be interesting to add a figure dedicated to this component. Combined with eq.2, it will give a better understanding of the method. It seems that the Axial-DeepLab paper presented in ECCV-2020 misses in the references. This paper proposes a simple strategy for attention modules that also reduce complexity. Results clearly show that deformable DETR provides better AP than DETR for less training-epochs. Moreover, the convergence is better than for Faster R-CNN (FPN). One of the concerns with deformable convolution is that the computation speed is slower than classical convolution. The same drawback appears with deformable attention modules. Fps decrease from 26 to 19 compared to DETR. It should be interesting to also report fps in the state of the art comparison table 3.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","Summary
This paper aims to improve a very recent detection model -- DETR, which suffers from two issues: long training time and limited feature spatial resolution. Targeting these issues, this paper proposes (1) deformable attention (2) multi-scale processing (inputs/attention) for DETR, which have greatly reduced the training time and improved the performance. Moreover, it develops two additional modules ""iterative bounding box refinement"" and ""two-stage framework"", which help to achieve the SOTA detection results on COCO.
Pros
This paper is insightful as it studies the two biggest issues of DETR. The vanilla attention is slow but hard to replace as the performance may be harmed. Similarly, it's not super straightforward to incorporate multi-scale processing into DETR due to the novel framework architecture. This paper has addressed these two issues, demonstrated outstanding performance on COCO, and properly studied the effectiveness of each single module.
The deformable attention module is a new kind of attention implementation. It samples a fix number of feature points on spatial feature map and thus greatly reduces the complexity.
The additional techniques make lots of sense to me, and improve the results greatly. I'm impressed that transformer based model can achieve very competitive as shown in Tab.3.
Cons
The deformable attention is considered the most important contribution of this paper and thus should be studied more thoroughly. Specifically, comparisons between this module and other ""linear""/""efficient"" implementations of attention should be performed. As discussed in related work, baselines like ""pre-defined sparse attention"", ""data-dependent sparse attention"", and ""low-rank property attention"" should be considered. This kind of comparisons will help us to understand better about the proposed module, and also to researchers from other field who are interested in using this.
Another missed baseline is DETR with multi-scale input and attention (vanilla version) for decently long epochs. I would want to know whether the proposed multi-scale thing could help the vanilla DETR.
Does the level embedding help? I don't find experiments to support this design choice.
If small objects are the issue, why not have a feature map of H/4 x W/4? Why are multi-scale feature maps constructed like in Appendix Fig.3?
Minor
There are lots of notations. It's pretty hard for me to remember ""mlqk, A, W, \phi, ..."". I would recommend the authors providing a lookup table in Appendix.
Personally, I think the discussion about FPN in main paper is distractive. The authors may want to move all of them into Appendix.
Questions
Do the last 5 rows in Tab.3 use ""iterative bounding box refinement"" and ""two-stage framework""? The difference between these 5 lines are just the backbone?
Fig.2 shows that Deformable DETR keeps improving over time. It doesn't converge at Ep.50. Why stick to this number in most experiments? How many epochs do the models in Tab.3 get trained?
In Tab.1, why do ""iterative bounding box refinement"" and ""two-stage Deformable DETR"" have no influence on FPS? Shouldn't the speed become slower as they are iterative and stage-wise. Does this FPS mean training FPS of Deformable DETR only? If that's the case, please also provide the total training time of everything.
Please guide me to the definition/explanation of ""DETR-DC5"". Does it mean the backbone in DETE is changed to ResNet-DC5?","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Deformable DETR: Deformable Transformers for End-to-End Object Detection","Keywords: Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection","Questions:
Q#6: Do the last 5 rows in Tab.3 use ""iterative bounding box refinement"" and ""two-stage framework""? The difference between these 5 lines is just the backbone?
A#6: Sorry for the confusion. The differences among the last 5 rows in Tab.3 are just the backbone and TTA. They all use ""iterative bounding box refinement"" and ""two-stage framework"". We shall clarify this in revision.
Q#7: Why stick to 50 epochs? How many epochs do the models in Tab.3 get trained?
A#7: Models in Tab.3 are all trained for 50 epochs. The reason we use 50 epochs for most experiments is that it is within an affordable training time. Otherwise, the experiments would take too much time for us.
Q#8: In Tab.1, why do ""iterative bounding box refinement"" and ""two-stage Deformable DETR"" have no influence on FPS? Does this FPS mean training FPS of Deformable DETR only? If that's the case, please also provide the total training time of everything.
A#8: ""FPS"" in Tab.1 refers to the inference speed as usual. To be precise, the inference speed of Deformable DETR is 19.4 FPS. Adding ""iterative bounding box refinement"" causes <0.1 FPS drop. Further adding ""two-stage Deformable DETR"" makes the inference speed drop to 18.8 FPS. The total training time of models in Tab.1 are shown as follows (measured on NVIDIA Tesla V100 GPU), which will be added in the revision:
Model Epochs Total Training Time (GPU hours)
Faster R-CNN + FPN 109 380
DETR 500 2000
DETR-DC5 500 7000
DETR-DC5 50 700
DETR-DC5+ 50 700
Deformable DETR 50 325
+ iterative bounding box refinement 50 325
++ two-stage Deformable DETR 50 340
Q#9: Definition/explanation of ""DETR-DC5"".
A#9: We quote the word of ""DETR-DC5"" from the original DETR paper (see Technical details in Page 8 of [b]). The backbone of DETR-DC5 is ResNet50 with dilated C5 stage.
[b] End-to-end object detection with transformers. In ECCV, 2020.","",""
"Learning Generalizable Visual Representations via Interactive Gameplay","Keywords: representation learning, deep reinforcement learning, computer vision","Motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment. Inspired by childhood psychology, they propose a variant of hide-and-seek game called ""Cache"" built on top of AI2-THOR, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment. They examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images.
The paper is well written and motivated, and easy to follow. All reviewers agree that the paper will be a great contribution to the ICLR community. I believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively (via static image recognition models), it synthesizes ideas from various disciplines (psychology, embodiment, ML) and provides an excellent framework for future research. For these reasons I'm recommending we accept this work as an Oral presentation.","Motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment. Inspired by childhood psychology, they propose a variant of hide-and-seek game called ""Cache"" built on top of AI2-THOR, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment. They examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images.
The paper is well written and motivated, and easy to follow. All reviewers agree that the paper will be a great contribution to the ICLR community. I believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively (via static image recognition models), it synthesizes ideas from various disciplines (psychology, embodiment, ML) and provides an excellent framework for future research. For these reasons I'm recommending we accept this work as an Oral presentation.",""
"Learning Generalizable Visual Representations via Interactive Gameplay","Keywords: representation learning, deep reinforcement learning, computer vision","The paper intends to contribute a novel task (Cache, as realized in AI2-THOR), the architecture of a strong Cache agent which learns reusable representations which allow significant transfer performance, and novel methods for evaluating the quality of dynamic image representations. The first and third contributions are directly related to the conference topics, and the second provides additional evidence in favor of the paper’s core idea: training on interactive gameplay allows learning flexible representations (in the sense of supporting many tasks via transfer) even when images are highly correlated and synthetic.
The key strength of the paper is the very general core idea it advances and how this idea is explored via a novel task. The paper is easy to read and convincing. The partition into details needed for the paper’s core argument and details specific to the experiments in the appendix is well done. (If anything even more could have been pushed to the appendix.)
One weakness of the paper is that a specific notion of “flexible” (which is mentioned in the title and twice in the abstract but nowhere else) is not advanced or integrated with the core idea. How does gameplay relate to flexibility? Why might flexibility be harder to achieve via passive learning or reinforcement learning with fixed reward functions? Because the authors place stress on the idea of how play and interaction contribute to representation learning (rather than a new method), slightly more space should be given to developing the general idea. The idea is not specific to vision, but only vision-related representations are considered. Sketching how the idea ought to work for text or audio would be useful if the focus really is on this very general idea.
Recommendation: strong accept. The philosophical aims of the paper make it stand out amongst the mass of related work that is otherwise very engineering focused. The experiments are soundly executed in a way that ends up clearly demonstrating the core idea.
Questions for the authors:
A step where the hider needs to retrieve the object they hid would seem appropriate. Are there certain limitations of the AI2-THOR environment that make adding this step (which would seem to expose more of the richness of the simulated world through fixed rules of the game) infeasible to add?
Inversely, do the authors feel that it was important that the hider manipulate the object into the desired location? How much of the richness of the simulated world comes through in the task feels relevant to the core ideas of the paper, but the paper currently does not address this kind of detail in the design of the Cache game within AI2-THOR.
Section-by-section reactions: (to see how opinions change over time)
Title+Abstract:
The notion of “flexible” seems to be at the heart of this paper’s intended contribution. Hopefully it will be defined in the body text. Uh oh, it looks like “flex” only ever appears on the first of the submission’s 36 pages. Hopefully a synonym will get defined later.
Introduction:
Excellent motivation.
Good that representations of interest (SIRs/DIRs) are named and distinguished. Many other papers, in the interest of highlighting end-to-end training, would forget to do this.
Good explicit list of contributions, excellent that two are specifically centered on representation learning.
Missed opportunity to highlight a distinct role for “flexible” representations. (I don’t quite know what it should mean beyond supporting transfer well. A representation that could easily be scaled up or down in dimensionality by stripping channels in a well defined order might be considered flexible in another sense. Likewise, one that was defined in terms of pluggable input modules to work with novel combinations of familiar input types might be considered differently flexible. What kind of flexibility do you want?)
Related Work:
Another take on learning visual representations via interactive gameplay is seen in https://arxiv.org/abs/1812.03125 where the authors learn a SIR (trained on a proxy task of predicting videogame memory state) that supports the use of low-continuous-space exploration strategies like rapidly-exploring random trees. The representations are learned offline/passively, but they are learned as to improve the efficiency of the very exploration process that builds that dataset for offline learning.
Playing Cache in a Simulation:
It seems notable that the hiding agent is never asked to retrieve the object they have hidden. Without this step, the hiding agent may find ways of manipulating objects in a way that makes them simply unretrievable (e.g. the object is pushed into a corner in a way that causes it to glitch out of the room, etc.). A step like this would require the hider to learn a finer grained representation of the hiding location that gives itself a clue as to how it should be retrieved (e.g. “under the couch in a place you’ll never be able to see but will be there if you actually reach for it).
Learning to Play Cache:
Great!
Experiments:
All well done.
Discussion:
“We believe that it is time for a paradigm shift via a move towards experiential, interactive, learning.” -- something similar has been said by many other researchers in many different decades, so it would be good to say what’s different about the situation in 2021. The difference now seems to be the availability of simulators with visual fidelity comparable enough to reality to demonstrate meaningful sim2real transfer. Are there other bullet points that could be added to a why-now argument?","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"Learning Generalizable Visual Representations via Interactive Gameplay","Keywords: representation learning, deep reinforcement learning, computer vision","In this paper, the author's propose an embodied adversarial reinforcement learning agent that can play a variation of hide-and-seek called Cache. This environment is a high fidelity interactive world. The authors argue that the agents are able to learn flexible representations of their observations which encode information such as object permanence, free space and containment.
The authors provide a well-written description of their game and provide well-designed visualizations to understand the interactions of the agent and the observations required for the learning problem. The authors present a concise and well-researched literature review which serves to distinguish their work as novel and well built on underlying prior work.
The authors make several contributions in this paper. First, an adversarial game Cache which permits the study of representation learning in the context of interactive visual gameplay. Furthermore, they present an agent which can perform strongly on the benchmark that they create even in comparison with human players. Finally, they present a study of the static and dynamic representations learned by the agent.
The authors provide an overview of the architecture for the Cache agent. It is well researched, well-reasoned and presented in a way that is easy to understand and reproduce.
The authors present multiple experiments in their paper and the corresponding deep Appendices. Specifically, they attempt to understand how agents can learn to proficiently hide and seek objects with static image representations and dynamic image representations
Of particular interest are the dynamic tasks the authors present in Section 5: Experiments. The authors make allusions to study of human children and object permanence. The results are quite compelling and well presented in a way that makes them easy to understand. I would urge the authors to consider if there are any non-ablative baselines against which they may be able to compare their model. One baseline that the author's use is human volunteer comparisons, these are compelling and presented in the appendix.
Figure 3 is perhaps the weakest figure in the paper. While it provides a great amount of information I would argue that the results are presented in a way that makes them less legible than if the figure was broken out into methods and results. The same could be said about Figure 4. The visualizations of the agents representations of synthetic and natural images are combined into a single image with performance on corresponding tasks. While I understand that the authors made these choices for space constraints, I would urge them to reconsider the visual hierarchy of the figures to emphasize the performance of their agents.
I feel like this is a paper that would be of great interest and benefit to the ICLR community.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Learning Generalizable Visual Representations via Interactive Gameplay","Keywords: representation learning, deep reinforcement learning, computer vision","Thank you for the rebuttal and delineating these modifications.","",""
"Learning Generalizable Visual Representations via Interactive Gameplay","Keywords: representation learning, deep reinforcement learning, computer vision","Summary
This paper examines the representations learned during adversarial gameplay, specifically a hide-and-seek game called Cache. The hiding agent must place an object in a room such that the seeker agent cannot find it. The authors argue that the adversarial nature of the game shapes the representations. Inspired by psychology experiments performed on children, the authors examine both static and dynamic representations to probe whether they contain information about properties of the environment such as object permanence.
Positives
The paper addresses an interesting problem by studying representations learned by agents interacting in an environment. These representations perform favorably compared to passive representations.
The paper is thorough, detailed, and well written.
The inclusion of dynamic representations is interesting, and the psychology-inspired experiments are adapted well to the Cache agent's environment.
Negatives
My primary concern with the paper hinges on a trick the authors used to get their agent to learn meaningful representations. The authors introduce Visual Dynamics Replay (VDR) to improve the agent's performance. Essentially, this is a set of auxiliary tasks and self-supervision derived from the agent's interactions with the environment. Although not a problem in itself, VDR was not used in the Navigation agent or Random agent used as baselines in the paper. This leaves the conclusions of the paper much weaker. It is unclear if these auxiliary tasks of VDR or the adversarial gameplay of the problem are the driving factor behind the learned representations.
Because the scope of the paper is so broad, it is difficult to evaluate some of the contributions. For example, the perspective simulation module seems useful, but there are no ablation experiments evaluating changes to the model.
Reasons for Score
Although the paper is intersting, detailed, and thorough, I have trouble with the strong conclusions drawn from the experiments because of the possible confounds of VDR.
Minor Issues that did not affect score
Graphs in figure 5 are difficult to see and interpret, even at full zoom on a monitor.
Update after rebuttal
I am very interested to see the results of the experiment mentioned in the rebuttal. Although you mention that gameplay dynamics and ability to interact with objects is crucial, I still feel that this isn't as cleanly demonstrated as it could be without addressing these confounds. However, in retrospect I also realize that my initial score suffered from tunnel vision on that single issue, which is important to several claims in the paper, but is by no means the only contribution. The paper is ambitious in scope, novel, and well written, so I have increased my score.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Learning Generalizable Visual Representations via Interactive Gameplay","Keywords: representation learning, deep reinforcement learning, computer vision","This paper proposes embodied game-playing with artificial agents as a method to learn better representations of their environment. They describe a game, cache, which is a variant of hide-and-seek played in a virtual environment and a method for training an agent to play the game. They present results which demonstrate that the static representations learned through game-playing perform better than other pre-training tasks within the same virtual environment, on both virtual vision and real world vision applications. They also show that the dynamic representations are useful for completing object permanence tests inspired by developmental psychology research.
This research direction (both biologically-inspired CV as a whole and specifically game playing as a pre-training task) is exciting and seems extremely promising. Assuming I understand the methods and results correctly, this seems like a clear accept. My only reservations are the complexity of the task and how the results are presented in the paper, which are relatively minor issues.
The task, Cache, seems extremely complicated to implement and train, since it involves five different stages of embodied exploration/action and adversarial reinforcement learning. While all of these steps are necessary to play the game like humans (or ravens) do, it is unclear to me whether the important learning is occurring as a result of the whole process, or of a specific stage. It seems possible that a simpler task may have comparable results, and it would be an interesting direction for future research to investigate how each of these stages are contributing to learned object permanence. I'm not an expert in developmental psychology, but I know there is some research showing that smooth-motion visual signals are the main prerequisite for object permanence in chickens (https://onlinelibrary.wiley.com/doi/abs/10.1111/desc.12796). An ablation study over game mechanics or world properties could lead to some really interesting results.
Of course, the paper you've submitted already describes a huge volume of research work and doesn't need more experiments. Unfortunately, it is a bit difficult to follow as a result. You may want move some content from Fig. 3 and 4 (those figures are difficult to parse, even with my pdf zoomed in 250%) to the appendix, if only to focus the reader's attention on specific results. While arguing that your agent successfully plays the game is important, it should take a backseat to the experiments which probe the properties of your static and dynamic image representations.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","Hi,
I really like the idea of this paper, it has a solid contribution to the continual learning community. Congrats on the oral acceptance!
I have a comment on the section where you compare to expansion-based methods. I wonder how does table 3 looks if we change the task to simple SPLIT-CIFAR-100, the same setup in the GEM paper, Lopez-Paz & Ranzato (NeurIPS 2017). Does GPM still outperform PGN a lot in this continual learning setup?
I asked this because I found PGN can reach really competitive accuracy, figure 3b in the BatchEnsemble paper, Wen et al. (ICLR 2020). Because of this, we think the expansion-based methods is more promising than the memory-based methods, leading to the idea of using rank-1 perturbation weight to reduce the memory cost of PGN.
Thanks","",""
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","The paper proposes a new approach to continual learning with known task boundaries that is scalable and highly performant, while preserving data privacy. To mitigate forgetting the proposed approach restricts gradient updates to fall in the orthogonal direction to the gradient space that are important for the past tasks. The main novelty of the approach is to estimate these subspaces by analysing the activations for the inputs linked for each given task.
All reviewers give accepting scores. R2, R3 and R4 strongly recommend accepting the paper, while R1 considers it borderline.
The authors provided an extensive response carefully considering all reviewers' comments. New experiments were introduced (training time analysis and comparisons with expansion-based methods), and several clarifications were added.
All reviewers agree that the paper is well written and its literature review adequate.
The main concern of R1 was the similarities with OGD (Farajtabar et al. 2020). R1 considered the authors’ response acceptable. R2, R3 and R4 consider the contribution well motivated and significant and highlight its simplicity. The AC agrees with this assessment.
The empirical evaluation covers most of the typical benchmarks in CL. Very strong results are reported on a variety of tasks both in terms of performance and memory efficiency, as agreed by R2, R3 and R4.
Overall the paper makes a strong contribution to the field of CL.","The paper proposes a new approach to continual learning with known task boundaries that is scalable and highly performant, while preserving data privacy. To mitigate forgetting the proposed approach restricts gradient updates to fall in the orthogonal direction to the gradient space that are important for the past tasks. The main novelty of the approach is to estimate these subspaces by analysing the activations for the inputs linked for each given task.
All reviewers give accepting scores. R2, R3 and R4 strongly recommend accepting the paper, while R1 considers it borderline.
The authors provided an extensive response carefully considering all reviewers' comments. New experiments were introduced (training time analysis and comparisons with expansion-based methods), and several clarifications were added.
All reviewers agree that the paper is well written and its literature review adequate.
The main concern of R1 was the similarities with OGD (Farajtabar et al. 2020). R1 considered the authors’ response acceptable. R2, R3 and R4 consider the contribution well motivated and significant and highlight its simplicity. The AC agrees with this assessment.
The empirical evaluation covers most of the typical benchmarks in CL. Very strong results are reported on a variety of tasks both in terms of performance and memory efficiency, as agreed by R2, R3 and R4.
Overall the paper makes a strong contribution to the field of CL.",""
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","We thank all the reviewers for their valuable suggestions and comments. We have made the following changes to our revised manuscript as per their suggestions:
Added training time comparison in section 7
Added a new experiment for comparison with expansion-based methods in section 7
Updated the results (in Table 1, 8, 9) for 5 runs.
Clarified in which experiment we use task-hint and where we don’t
Added relevant references in related works
Added miscellaneous explanations/discussions
Added class-incremental learning experiment in appendix section D.3
Correction - We used ResNet18 for 5-dataset experiment. While this was correctly specified in the appendix (C2), in the main text (section 6) we wrote that we use AlexNet for that experiment. We have corrected this mistake in the revised manuscript.
All changes are marked in blue in the revised manuscript.
Please let us know your comments on our response and revised manuscript. We are open to further discussion if needed.","",""
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","I found the idea quite novel. Lately in continual learning the focus has been more on NAS type ideas and algorithms, but this work is a nice divergence from this direction. The idea of optimizing in a space orthogonal to the previous task is novel. The execution of the idea is nothing special since it's using standard linear algebra, but I gave the authors full merit to the idea itself.
My higher score is mostly due to the fact that the experiments are limited. The benchmark algorithms definitely miss some recent works from 2019 and 2020. They should be included as otherwise the superior performance of the algorithms is questionable.
See for example: https://arxiv.org/abs/2006.04027 Ju Xu and Zhanxing Zhu. Reinforced continual learning. In Advances in Neural Information Processing Systems, pages 899–908, 2018","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","Summary:
This work targets learning multi-class classifiers in the continual learning setting. The key idea is to learn new tasks by taking gradient steps in directions orthogonal to the gradient subspaces marked as crucial for previous past tasks. The method employs SVD after learning each task to find the crucial subspaces (which it calls as Core Gradient Subspaces) and stores them in a memory. Reasonable quantitative and qualitative evaluation has been performed to compare the method against existing SOTA baselines.
Review:
The paper is overall clearly written and the method is adequately described.
The work is relevant to audience at ICLR and provides reasonable details for reproducibility.
Relationship to previous works has been explained and relevant literature has been cited appropriately.
Strengths:
GPM shows very little degradation in performance on past tasks and is very resilient to forgetting.
It provides explicit control over forgetting via the \eps_{th} parameter.
Weaknesses:
GPM performs fairly close to or marginally better than HAT and EWC on most datasets.
GPM trades-off between ACC and BWT metrics by strictly controlling forgetting. Hence, it may not be able to identify tasks with very similar structure (or repeated tasks) and may not re-use the Core Gradient Space from previous tasks to improve performance on them (BWT). So, while it minimizes negative BWT, it also restricts positive BWT.
Potential improvements and clarifications:
The authors have repeatedly emphasized that their method performs task-free inference. However, doesn't this just mean that the tasks used are such that the inputs encode the task identity and do not require a separate task identifier. Further, on page 6, under Network Architectures, the authors state that apart from permuted MNIST tasks, they use the multi-head setting, i.e., each task has a separate classifier. How does this allow task-identifier free inference?
All models have been trained using plain SGD. Would it be possible to extend the method to other optimizers, e.g., Adam?
3 runs are too few to average over. These are supervised learning experiments, so it should definitely be possible to use more runs (at least 5, but preferably 10).
At first glance, it seems that the method may have scalability issues since it relies on performing SVD. However figure 2 counter-intuitively shows GPM to be both fast and memory efficient. Is this because these graphs in figure 2 are per-epoch? What about in between tasks when GPM needs to run SVD on all layers? Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task.
Lastly, no experiments with a single class per task have been performed. This setting is known to be quite challenging in general and induces significantly more catastrophic forgetting (see Kamra et al, 2017). In this setting it is also generally harder to just set batch-norm parameters using just the first task.
[Kamra et al, 2017] Deep Generative Dual Memory Network for Continual Learning. Nitin Kamra, Umang Gupta and Yan Liu. ArXiv 2017.
Please respond to the 5 potential improvements and clarifications mentioned above. I will be happy to raise the score further if the above concerns are addressed.
-------- Post-rebuttal edit -------
The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines. I believe that the paper is strong and makes a significant technical contribution to the field of CL. Hence, I recommend acceptance and I am updating my score to reflect the same.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","Thank you for providing the clarifications and additional results. You have answered all my concerns satisfactorily and I will be happy to update my score (will update after a discussion with other reviewers during the upcoming discussion period).
One final comment though: I noticed that the comparison of GPM with other baselines in the class-incremental learning setup has not been added to the paper. I believe it is important to highlight both strengths and shortcomings of our proposed approaches as researchers. It is completely fine that GPM does not outperform DGDMN in this setting. The experiments are still interesting and insightful in their own right; further they open directions for future research. I would strongly recommend adding these experiments to the final draft (perhaps in the appendix if there is not enough space in the main paper) and discussing the conclusion: ""a hybrid approach such as combining GPM with small data replay would be an interesting direction for future exploration"" in more detail as a potential direction for future exploration.","",""
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","This manuscript proposes a new approach for continual learning problems. The key idea is to maintain bases of subspaces using SVD in the Gradient Projection Memory (GPM), in which the update direction is orthogonal to the gradient subspaces deemed important for the past tasks. Image classification experiment was conducted to justify its better empirical performance in practice. Overall, the paper is well-written. I have the following comments.
The contribution of this manuscript is kind of incremental compared with OGD (Farajtabar et al. 2020). From my understanding, the main improvement is using SVD to store the bases, which basically trades computational efficiency for memory. In addition, it is claimed that OGD only works for small learning rate. Why using SVD helps using a larger learning rate? It is not explained in the paper.
Is it possible to report running time of the proposed approach and compare with other approaches? SVD is very expensive in the high-dimensional setting such as deep neural networks. It might be impractical due to running time concerns.
What is the value of
k
in the experiments (in inequality (9)) for different
ϵthl
? It is better to report these values. In addition, when
k
becomes large, the update rule (6) and (7) may become computationally expensive.
It is mentioned that “our proposed approach can perform inference without the task identity of test samples”, but with no explanations. It is desired to illustrate the meaning in the main paper.
The literature review is not sufficient. There are several recent papers which also change the update direction using gradient projection. It is desired to see the difference between the proposed method and other relevant approaches. For example,
[1]. Yu et al. Gradient surgery for multi-task learning. NeurIPS 2020.
[2]. Guo et al. Improved schemes for episodic memory-based lifelong learning. NeurIPS 2020.
========POST REBUTTAL=========
I would like to thank the authors to answer my questions. It addressed most of my concerns. Hence I increase my score to 6.","6: Marginally above acceptance threshold","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Gradient Projection Memory for Continual Learning","Keywords: Continual Learning, Representation Learning, Computer Vision, Deep learning","Summary: The paper proposes one of the most scalable approaches to sequential continual learning with known task boundaries and related tasks, while taking steps towards enforcing data privacy and removing some of the task label constraints. At all levels in expressive deep models, SVD is used on learned representations to identify important bases of task gradient spaces and a memory is populated with such directions. Learning progresses only in directions orthogonal to gradient memory. Several recent evaluation methodologies are used to empirically validate the approach with significant success.
Strong points:
Principled and relatively simple approach, yet scalable to interesting deep models.
Manageable computational overhead.
Memory of gradients introduces a layer of data privacy, and advantage compared to many other memory-based approaches.
Strong empirical results, although not apples-to-apples in all cases, see the comment posted earlier.
Clearly written paper.
Analysis of scalability in terms of memory and computation is a welcome bonus.
Weak points:
The sequential task learning setup is of limited practical use and not too realistic.
It’s hard to say how the algorithm would be used in practical continual learning settings, e.g. reinforcement learning of single tasks without forgetting, or where clear task boundaries do not exist.
Little effort is made to see the approach relevance for non-similar sequential task learning, where there could be interference between learned representations in terms of negative forward transfer.
Recommendation and Rationale:
I strongly recommend acceptance because the method is simple, practical and the paper is well written both in terms of clarity and also analysis.
Further questions:
Do you see any positive forward transfer, e.g. later tasks are learned quicker due to previous tasks?
What are the limitations and roadblocks to extending this method to sequences of hundreds of tasks which are not necessarily related? Please discuss in the manuscript.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting","Keywords: spatio-temporal forecasting, deep learning, physics, differential equations, hybrid systems","The authors propose a method for modeling dynamical systems that balances theoretically derived models, which may be grounded in domain knowledge but subject to overly strict assumptions, with neural networks that can pick up the slack. All reviewers were enthusiastic about this work, appreciating its balance of mathematical rigor and experimental assessment. One concern was that this paper follows on decades of related work, which was difficult to adequately summarize. However, changes made throughout discussion phase did address these concerns.","The authors propose a method for modeling dynamical systems that balances theoretically derived models, which may be grounded in domain knowledge but subject to overly strict assumptions, with neural networks that can pick up the slack. All reviewers were enthusiastic about this work, appreciating its balance of mathematical rigor and experimental assessment. One concern was that this paper follows on decades of related work, which was difficult to adequately summarize. However, changes made throughout discussion phase did address these concerns.",""
"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting","Keywords: spatio-temporal forecasting, deep learning, physics, differential equations, hybrid systems","Dear reviewers, we would like to thank you for your careful reading of our work and your detailed comments. We have really appreciated your constructive feedback that has hopefully helped to improve the quality of our submission. We have thus updated it (modifications in blue in the text), including:
a more comprehensive related work section (R2 & R1) with added historical references;
additional visualizations in Figure 4 in order to illustrate the effect of APHYNITY on parameter identification (R3);
clarifications on experimental results and model training (R2).","",""
"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting","Keywords: spatio-temporal forecasting, deep learning, physics, differential equations, hybrid systems","I really enjoyed this paper. It's one of my favorite papers from 2020. Authors: thank you for writing it!
The paper proposes a framework for jointly fitting the parameters of a physical model (such as the parameters of an ODE or PDE) and learning a neural network to model the error or residual of this physical model. The idea is to find a physical dynamics model
Fp∈Fp
(e.g. where
Fp
is a set of PDEs with different parameter values) and a neural residual dynamics model
Fa∈Fa
(e.g. where
Fa
is a hypothesis class of neural networks) which minimize the norm of
Fa
while constraining the composed dynamics
F=Fa+Fp
to agree with observed data. Interestingly and importantly, the paper proves the minimum-norm decomposition of the observed dynamics into physical model dynamics and neural residual dynamics is unique, given a condition on the geometry of
Fp
. This condition is that
Fp
should be a Cheybshev set: a sufficient condition is that it is a closed convex set in a strict normed space. To me, this condition seems very mild.
The paper goes on to show that this method, termed APHYNITY, produces significant gains in predictive accuracy over purely learned methods, purely physics-driven methods, and other forms of combining physical models and learned models. The gain is most significant when the physical model is incomplete. When it is complete there may be some small advantage due to the neural residual accounting for some discretization error; regardless there is no harm, and the learned neural residual is very small.
The paper also shows that APHYNITY produces better parameter estimates for the physical model in the presence of incomplete physics. This seems important. Our physics models are always approximations and in many interesting applications (climate/atmosphere, bioengineering, mechanics of materials, etc) commonly used physics models may have an interestingly-sized gap with reality. When the identified parameters, not the predictions, are needed for some downstream task such as decision making, APHYNITY could help with better parameter ID.
I thought this paper was clear and well written. The main paper presents an easy-to-follow story, the appendices contain plenty of detail, and when while reading the main paper I wanted more detail on specific points, it was usually easy to follow links to the correct appendices. (Should be true of all papers, but often isn't).
Thoughts and feedback:
It would be useful to see some simple visual demonstration of the effect of APHYNITY on parameter estimation in incomplete models. (I should note the paper is already quite long and thorough, though).
The integral trajectory-based approach (section 3.2, 3.3, motivated in Appendix D) used to fit the parameters of both NN and physics model seems like the ""right"" way to do this to me. Nonetheless it would be interesting to see numerical comparison with the alternative (supervision over derivatives).
I wonder if the integral trajectory-based approach, vs the supervision-over-derivatives approach, can be related to traditional methods for parameter ID in ODE?
It seems to me that the authors do a good job explaining and relating to prior work on learning physical systems. However, with both the density of recent literature in this space and the decades of work combining ODE solvers and function approximators, if there are missing references (papers on similar work that the current submission does not cite), it's quite likely I would not have noticed.
It would be interesting to know if this could help figure out in what way a physical model is misspecified, and guide design of an approximate physical model that better captures reality. I suspect yes, although it might simply boil down to this method having a better estimate of the residual than if one just does a least-squares fit with the physical model.
Typos:
""bayesian"" -> ""Bayesian"" (end page 2)
""si"" -> ""is"" (page 3)
I think this is a well written paper likely to be of interest to a large number of ICLR attendees, with some important novel contributions, and that the method proposed has a good chance of being widely adopted in the subfield of ML+physical simulation.","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting","Keywords: spatio-temporal forecasting, deep learning, physics, differential equations, hybrid systems","Summary: This paper outlines a method for forecasting and parameter estimation when you have a partial physics model (possibly with unknown parameters) and time series data. This is a hybrid approach where the data-driven (deep learning) approach only learns the parts not accounted for by the physical model. A key feature is being able to decompose the problem in such a way that the data-driven model only models what cannot be captured by the physical model. The parameters of these two models must be fit jointly so that the physical model's parameters are more correct. They prove existence and uniqueness for this decomposition.
Strong points: I think this point of making the decomposition unique (and thus better fitting the physical model's parameters) is intuitive and a good one. To my knowledge, other work on hybrid models do not have theory showing this. (But I haven't extensively read the literature.)
The authors included extensive experiments with multiple versions of each of three test cases, plus several baselines. They also conducted ablation studies for three features of their method:
using a decomposition to ensure that the data-driven model only learns what the physical model cannot contribute
the trajectory (integral) approach instead of training on estimated derivatives
using adaptive weights in the objective function
Weak points: I think that the claim ""We show that APHYNITY can perform similarly to complete physical models by augmenting incomplete ones, both in forecasting accuracy and physical parameter identification."" is overstated. I compared APHYNITY for incomplete physics to the Param PDE or True PDE with complete physics in Table 1, and this claim seems true for the wave equation only.
I think that the related work section should acknowledge that people have been using neural networks for hybrids with physical models since the 1990s. Example: [1]. This is sometimes called ""gray box"" modeling or ""hybrid"" modeling. Since this section only mentions very recent work, I'm not very confident in the paper's claim that they are the first to have a principled decomposition with existence & uniqueness.
I think that ""Propositions 1 and 2 provide, under mild conditions, the theoretical guarantees for the APHYNITY formulation to infer the correct MB/ML decomposition, thus enabling both recovering the proper physical parameters and accurate forecasting."" is overstated. The theory is about the decomposition existing & being unique, not about this method being able to find that decomposition.
The theory makes no assumptions about the form of F_a (the data-driven component). Showing that the decomposition is possible is not the same as showing that there exists a neural network that could represent F_a. The different versions of the universal approximation theorem for neural networks that I've seen all require that the neural network is approximating something reasonably well-behaved (such as continuous). I could imagine that the parts of a dataset that cannot be modeled by an ODE/PDE might also be non-continuous, etc.
Further, even if the universal approximation theorem applied here, that would show that such a neural network exists, not how well any existing training methods can find that neural network.
I think ""as expected, ||F_a||^2 diminishes as the complexity of the corresponding physical model increases, and becomes almost null for complete physical models."" is too strong. It does decrease, but I wouldn't call 0.14 and 2.3 ""almost null.""
I personally can't vouch for the theory (in Section 3.1 & Appendix A & B), as I'm having a hard time following it. I hope that another reviewer can. Some things that would help me:
The intuition would be easier if it's mentioned earlier what \mathcal{F} is in this paper's examples. L^2? It would be nice to put it before Section 3.1, right when the notation is introduced. Similarly, it would help if an example of \mathcal{F}_p was given early on.
Some examples are given in Section 3.1 & the appendix of conditions that make a set Chebyshev. However, since I was having a hard time knowing which sets & spaces were important in this paper, I was having a hard time understanding which conditions were important.
Since reviewers aren't required to read the appendix, and it's quite long, I read it less closely. However, it contains the proofs and backs up many of the nice claims in the paper. This means that quite a bit of content in the paper is less vetted.
Other clarification questions:
I may have missed it, but does the paper describe how parameters are fit for the PDEs (physical models)? Example: ParamPDE(a,b) for reaction-diffusion.
Does the requirement of \mathcal{F}_p being Chebyshev mean that the parameters of the ODE/PDE need to be closed on one end? In other words, we couldn't search for a parameter in (-infinity, infinity)?
Are the results in Table 1 test errors? (as opposed to training or validation?)
It's not clear to me what you mean by the ""partially observable"" setting.
For the ablation studies, we're presumably comparing these variants to the full method's results, as described in Table 1, right? However, there are some discrepancies between Table 1 and Tables 5 & 6. Some of the numbers don't match and one of the wave equation cases is not included in Table 5. The ""Augmented True PDE derivative supervision"" case is also missing for the wave equation in Table 6.
Minor points:
It seems that the definition of the set of observed trajectories (in the beginning of Section 3) is not right.
Your observations are for a finite set of values t in [0,T].
Your observations are not across all solutions X for the dynamics: just at a finite number of IC/BCs.
For the reaction-diffusion & wave equations, X is also a function of spatial x, and the data is collected at finitely many spatial points.
I think instead of defining a set \mathcal{A} on page 3, it would be clearer to use the space \mathbb{R}^d? I don't think F maps into just the set A, where A is defined as the set of values that X takes. Couldn't the derivatives have values that F doesn't reach?
On the bottom of Page 3, the question of the minimum being well-defined is explained as the existence question. However, I'm used to ""well-defined"" in various mathematical contexts meaning roughly ""a unique answer."" Is there a different meaning of ""well-defined"" in this context?
Summary: I tentatively recommend accepting this paper. Their method has quite a few empirical results showing improvements. People are interested in combining physical models with machine learning in a variety of scientific application areas, so I could see this being a well-used method. However, I have listed some points above that I would like fixed or clarified. I also hope that someone else can vouch for the proof.
[1] Rico-Martinez, R., J. S. Anderson, and I. G. Kevrekidis. ""Continuous-time nonlinear signal processing: a neural network based approach for gray box identification."" In Proceedings of IEEE Workshop on Neural Networks for Signal Processing, pp. 596-605. IEEE, 1994.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting","Keywords: spatio-temporal forecasting, deep learning, physics, differential equations, hybrid systems","Summary of my understanding
The authors propose a method of function fitting for differential equations. They premise that a model
F
is the additive combination of
Fp
and
Fa
, which denote physics and augmenting parts, respectively. The functional form of the physics part,
Fp
, is given in accordance with prior knowledge, whereas the augmenting part,
Fa
, is modeled by neural nets. The proposed method follows the principle of least action of
Fa
, and the authors suggest solving a constrained optimization problem via a method of Lagrange multipliers. They show numerical results on three PDE/ODE-governed systems.
Evaluation
I really enjoyed reading the paper, which is well written. The motivation is clearly presented. The related work can be more detailed given the recent active studies on physics + ML but seems sufficient from the viewpoint of ODE/PDE fitting. The proposed method is simple yet reasonable, and the experiments are enough supportive to see the superiority of the method. The ablation study in Appendix F is also very interesting. Possible improvement, which the authors would be aware of, can be found in the lack of experiments on real-world datasets. (I understand the difficulty of finding illustrative real-world examples in this kind of problem, but I couldn't stop pointing out it.) Overall, I think this is certainly sound work.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","All reviewers agree that this paper is very solid work, that presents great progress in no-press diplomacy. The method and presented experiments are of very good quality and the work merits to be presented at ICLR.","All reviewers agree that this paper is very solid work, that presents great progress in no-press diplomacy. The method and presented experiments are of very good quality and the work merits to be presented at ICLR.",""
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","Summary
The paper addresses the problem of No Press Diplomacy. It applies existing search techniques to improve on an imitation policy, which is learned with improvements on existing supervised learning methods for the domain. The resulting policy is tested against previous learned agents and humans, showing impressive results.
I recommend accepting this paper. It tackles an important multiagent environment, producing the first agent to demonstrate human-level play. While the techniques used are not especially novel, their success in a complex mixed-motive 7-player game is interesting. I have no major concerns about the paper, except for the specific claim that the agent 'convincingly surpasses human performance' - I think this is overstated and should be removed before acceptance.
Strengths
Diplomacy is an important multiagent domain, involving a complex action space and a mixture of cooperation and competition between players, which necessitates new approaches. Research on this domain is timely, building on recent work as well as a long history of AI research. This paper advances the state of the art in this domain.
The search method in the paper (external regret matching on a subsampled game, evaluated with on-policy rollouts) is clearly explained and situated in the existing literature. While it is not particularly novel, the successful application to this type of domain is.
Overall, the experiments performed in the paper demonstrate convincingly that SearchBot is a large improvement on policies learned from imitation, and that it is human level.
The results in ad-hoc play with humans are impressive. They clearly show that SearchBot is the standard of a fairly strong human player. This is a particularly important contribution in the domain of Diplomacy; I agree with the authors that performance against humans is the ultimate test, and in a 7-player game it is not clear a priori that an agent that fares well against other agents will also do well against humans.
Play with expert humans shows that SearchBot is not overly exploitable; it is impressive that experts get well below the average result in this setting.
The agent exploitability analysis is good, showing that SearchBot is (almost certainly) less exploitable by RL than the blueprint it is based on.
The paper is well-written; related work is addressed well, and the methods and findings are for the most part very clear.
Weaknesses
Experiments
I think the claim made in the introduction that it 'convincingly surpasses human-level performance' is too strong, and should be removed or clarified. The natural reading of this is that SearchBot is better than the best human players; but the results in the paper do not bear this out. The reported ranking is 23/1128, and many players in this ranking have played very few games and are unlikely to have an accurate rating. The descriptions elsewhere in the paper (such as the title and conclusions) of 'human-level performance' seem to me to be the appropriate strength of claim.
It is unclear how strong the human players SearchBot faced in the webdiplomacy evaluation were. I think it would be very useful to include some information on this, and on how SearchBot fares against players of different skill levels. This may well affect how meaningful the reported rank is; if SearchBot has overwhelmingly played rather weak players, its rank is probably not comparable with top human players, who presumably face much stronger opposition.
The paper compares to two other agents; the SL and RL version of DipNet. Previous work has compared to the rules-based agent Albert; this has the advantage that this bot is independent of human imitation data. Adding this comparison would be valuable, and would help substantiate the strong claim that SearchBot ‘greatly exceeds the performance of past no-press Diplomacy bots’.
The effects of two key search parameters - rollout length and subgame size - are investigated. However, the effect of the number of iterations of ERM on play strength is not addressed in the paper; I would like to see a similar experiment to those in Figure 1 which looks at this.
Some winrates are presented with confidence intervals, but some in Tables 3, 5 and 6 are not. These should be added.
Reproducibility
The methods and results are generally well explained, but I think there are a few places where there could be more detail:
The experiments which do not involve humans do not have a clear explanation of the hyperparameters used (particularly for search). A section like Appendix F should be added for this.
The experiments between bots used the SoS rules. However, I could not find a description of the circumstances under which games were declared draws.
The imitation learning architecture is described only as changes to previous work; I think setting out the entire architecture in an appendix would aid reproducibility, as currently the reader needs to read two other papers to reproduce this.","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","Thank you, this addresses my concerns well. The addition to Figure 1 is valuable, and the comparison against Albert is already useful in that it clearly exceeds the performance of DipNet (though more games will obviously be better). The clarifying updates are good, and I agree with the decision on confidence intervals in tables 3 and 6.
I still wonder is whether the Ghost Ranking of 23rd is a true reflection of the bot's strength. While this is a good metric to report, the bot's number of games and opponent distribution is probably very atypical for strong players, and I do not have confidence that Ghost Rating is accurate in that case. Still, this is a minor point - the key claim of 'human-level performance' is well supported.","",""
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","The authors consider ""no-press Diplomacy"", a complex game played by humans which involves (limited) cooperation and competition.
The method uses a policy and value function learned from human games, together with a test-time search. The imitation-learned policy is used both to restrict the actions considered in the search and also to roll out the leaf nodes of the search (to a fixed depth, after which the value function is used). The search process is a sampled form of external regret matching.
Several wrinkles required for good performance are clearly motivated and explained, e.g. handling low-entropy policies and large action spaces.
The authors evaluate against existing bots, against multiple human players on webdiplomacy.net, and against two human experts. In all cases, the authors algorithm outperforms its opponents.
It is not feasible to compute exact exploitability in a game of this size, or even to train an approximate best response against the searching agent. The authors therefore train an approximate best-response RL agent against both the imitation-learned policy and a distillation of the searchbot, strongly suggesting that the distilled search is less exploitable than the imitation-learned policy.
The method described is novel, but also a fairly straightforward extension of prior work to this domain, incorporating a single-ply search on top of an imitation-learned policy. Although there is nothing radically new, the combination of very strong empirical results, and clear & detailed explanation of the methods involved combine to make this a clear accept. I was particularly happy to see the thorough evaluations including bots, a field of humans, and world-class players, and an attempt at investigating exploitability.
Comments on the paper:
In figure 3, it might be clearer to plot the two graphs on the same scale.
In the Qualitative Assessment of SearchBot section, I would have been interested in any comments the human experts might have made.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","In this paper, the authors apply an interesting twist on 1-ply search to the problem of playing no-press Diplomacy. Diplomacy is an especially interesting application, because it is neither zero-sum nor two-player, in contrast to many recent AI success stories. Before each action, they compute an equilibrium of the next step of the game, assuming that each player will thereafter play according to a ""blueprint"" strategy learned via imitation learning. The equilibrium is computed using regret matching. The resulting agent has very strong performance against both the previous state of the art bots, and against expert human players.
The paper is clearly organized and well written; I thoroughly enjoyed reading it. The empirical evaluations are careful. I especially appreciated that the improved blueprint strategy was evaluated on its own (i.e., without search) to clearly separate which benefits came from the improved representation vs. the addition of 1-ply equilibrium search.
The idea of finding the equilibrium for each ""stage game"" of an extensive form game was new to me, and it appears to be very effective. My main concern with this paper is that I would have liked to see some sort of justification for why we would expect this to work and where we should expect it to go wrong. The fact that Diplomacy is non-zero-sum means that we no longer have strong theoretical guarantees about equilibrium's being the ""right thing"" to play. Should we expect this technique to be broadly applicable, or is it exploiting something specific to Diplomacy?
The equilibrium-search approach is in contrast with simply attempting to best respond to the blueprint strategy (apparently this is the approach of [Anthony 2020]?) How does your technique compare to this best-response technique? I see empirical comparisons to [Paquette 2019] but not [Anthony 2020].
I was surprised by just how computationally expensive this technique turns out to be; that is surely a drawback that will need to be addressed.
Overall I think this paper makes a valuable and well executed contribution, and I recommend acceptance. I have some further minor comments below.
=== Minor comments ===
p.4: ""We add an additional value head..."": Can you give some details about the motivation behind this architecture change?
p.6: ""If the bot's performance for each of the 7 powers is weighted equally, this score increases..."": this sentence didn't make sense to me. Possibly it assumes more familiarity with Diplomacy than I have?
p.6: ""This can partly be interpreted through an evolutionary lens..."": What does the ESS view add that is not already present in the equilibrium view? I would either flesh this out a lot more or drop references to ESS entirely.
p.6: The definition of exploitability on p.6 is wrong because it doesn't subtract off
ui(π)
(also inconsistent with definition on p.8, which appears to be correct)","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","This paper proposes a combination of imitation learning and search applied to the multiplayer, simultaneous-move game of no-press Diplomacy. While both techniques have been used before, even in concert, there are some domain-specific challenges: more than two players, simultaneous moves, and a very large branching factor per player. The authors use imitation of human play for value estimates and selection of candidate actions, and External Regret Matching to generate a one-step policy. Experimental validation showed the agent to have strong human-level performance, and was hard to exploit using other machine-learning techniques.
To the best of my knowledge, I agree with the author's claim that the imitation/search agent from this paper is the first to demonstrate human-level performance in the game of Diplomacy. This is a non-trivial result: Diplomacy is a large problem, and is thought to require fairly complicated interaction between players throughout a game, as well as the ability to handle the uncertainty of other player's moves. There is no claim of guaranteed performance across a general class of problems, but the paper's method seems general enough to apply elsewhere.
The writing was generally clear, although asking ""could I duplicate these results?"" did raise a few questions about some details. The experimental results against humans seem well constructed, and do a reasonable job to support the claim that the agent demonstrates human-level play. The assorted exploitability-inspired experiments help support the notion that the agent's play is strong, rather than just doing well based on some peculiarity of human play.
---------- specific issues
section 2.2 ""(which can also be represented as v_i(a_i, a_{-i})."" Closing parenthesis missing?
""ERM guarantees that R^t_i(a_i) in \mathcal{O}(\sqrt{t}). If R^t_i(a_i) grows sublinearly for all players' actions, as in ERM, then the average policy over all iterations converges to a NE in two-player zero-sum games and in general the empirical distribution of players' joint policies converges to a CCE as t approaches infinity."" How does the sampling interact with this? The regret bound is w.r.t. the probabilistic policy pi^t_i, but sampled a_{-i}. Connecting this bound to describe the joint behaviour (i.e., Nash eq'm) prThis paper proposes a combination of imitation learning and search applied to the multiplayer, simultaneous-move game of no-press Diplomacy. While both techniques have been used before, even in concert, there are some domain-specific challenges: more than two players, simultaneous moves, and a very large branching factor per player. The authors use imitation of human play for value estimates and selection of candidate actions, and External Regret Matching to generate a one-step policy. Experimental validation showed the agent to have strong human-level performance, and was hard to exploit using other machine-learning techniques.
To the best of my knowledge, I agree with the author's claim that the imitation/search agent from this paper is the first to demonstrate human-level performance in the game of Diplomacy. This is a non-trivial result: Diplomacy is a large problem, and is thought to require fairly complicated interaction between players throughout a game, as well as the ability to handle the uncertainty of other player's moves. There is no claim of guaranteed performance across a general class of problems, but the paper's method seems general enough to apply elsewhere.
The writing was generally clear, although asking ""could I duplicate these results?"" did raise a few questions about some details. The experimental results against humans seem well constructed, and do a reasonable job to support the claim that the agent demonstrates human-level play. The assorted exploitability-inspired experiments help support the notion that the agent's play is strong, rather than just doing well based on some peculiarity of human play.
---------- specific issues
section 2.2 ""(which can also be represented as v_i(a_i, a_{-i})."" Closing parenthesis missing?
""ERM guarantees that R^t_i(a_i) in \mathcal{O}(\sqrt{t}). If R^t_i(a_i) grows sublinearly for all players' actions, as in ERM, then the average policy over all iterations converges to a NE in two-player zero-sum games and in general the empirical distribution of players' joint policies converges to a CCE as t approaches infinity."" How does the sampling interact with this? The regret bound is w.r.t. the probabilistic policy pi^t_i, but sampled a_{-i}. Connecting this bound to describe the joint behaviour (i.e., Nash eq'm) proceeds easily when considering the joint pi^t. However, in this case the joint is only connected by the specific sampled actions. Does the statement about convergence to NE apply to sampled ERM (With high probability? In expectation?) or only to ERM?
Table 1 What are T=0.5 and T=0.1? I assume they're the temperature parameter, for action selection in DipNet, but this isn't described until later.
Figure 1, ""Sampling a single action leads to poor performance"" How is the graph intended to be interpreted? In the graph, 1 search action looks to be significantly better than the blueprint.
""M_i is a hyperparameter that is proportional to the number of units controlled by player i."" Proportional how? If power i controls n units, the algorithm will sample M_i actions? M_in actions? kn actions for some hyperparameter k? Please clarify this sentence. Are the samples select independently across units somehow, or just the top orders for the round (for that possibly very large number of actions)?
""We compute a policy for each agent by running the sampled algorithm described in Equation 1 and Equation 2"". Section 2.2 describes a few modifications, one of which would seem to modify equation 2. Maybe saying instead that it is running the sampled algorithm described in Section 2.2? How many iterations are used?
""If our agent is an ESS (or Nash equilibrium), then a population of players all playing the agent's policy could not be 'invaded' by a different policy."" Wouldn't this only apply to ESS, not Nash eq'm in general?
Section 4.2.1 What does a sum-of-squares score of 0.54 or 0.42 represent? Are those scores good? Huge? Tiny? The drop is statistically significant. Is that drop of ~0.12 large? Small? Are we supposed to care about the drop? Or just how low they both are? Or just how low the SearchBot-clone result is? It seems like this section would be helped by having a short summary statement of the conclusions the authors would like to draw.
Section 4.2.2 It was nice to see that the humans beat the human-imitation blueprint, while they struggled quite a bit against SearchBot. It might be worth specifically noting that not only was the blueprint imitation an improvement on DipNet in this setting, it was specifically the search method making the largest improvement.
Section 4.2.3 CFR->ERM? The text talks about plotting versus the number of ERM iterations, while the figure talks about CFR.
Figure 3 What exactly is the average being talked about in the first paragraph? The averaging mention in Section 2.2 (which is not used during search in normal play)? The self-play strategy profile which is a combination of one strategy from each of the 7 runs (not really an average)? Something else? Is ""average"" in the first paragraph different than ""average"" in the right figure? The average line on the left doesn't seem to match the average line on the right: on the right, there seems to be a slight bump upwards at the end, which doesn't seem to be present on the left.
On the right plot, why not include all (or at least multiple) independent plots, to give a more complete picture of the variance?
Include some mention or mark indicating how many iterations are used during play? As is, I don't know how this graph relates to the search as used in SearchBot.oceeds easily when considering the joint pi^t. However, in this case the joint is only connected by the specific sampled actions. Does the statement about convergence to NE apply to sampled ERM (With high probability? In expectation?) or only to ERM?
Table 1 What are T=0.5 and T=0.1? I assume they're the temperature parameter, for action selection in DipNet, but this isn't described until later.
Figure 1, ""Sampling a single action leads to poor performance"" How is the graph intended to be interpreted? In the graph, 1 search action looks to be significantly better than the blueprint.
""M_i is a hyperparameter that is proportional to the number of units controlled by player i."" Proportional how? If power i controls n units, the algorithm will sample M_i actions? M_in actions? kn actions for some hyperparameter k? Please clarify this sentence. Are the samples select independently across units somehow, or just the top orders for the round (for that possibly very large number of actions)?
""We compute a policy for each agent by running the sampled algorithm described in Equation 1 and Equation 2"". Section 2.2 describes a few modifications, one of which would seem to modify equation 2. Maybe saying instead that it is running the sampled algorithm described in Section 2.2? How many iterations are used?
""If our agent is an ESS (or Nash equilibrium), then a population of players all playing the agent's policy could not be 'invaded' by a different policy."" Wouldn't this only apply to ESS, not Nash eq'm in general?
Section 4.2.1 What does a sum-of-squares score of 0.54 or 0.42 represent? Are those scores good? Huge? Tiny? The drop is statistically significant. Is that drop of ~0.12 large? Small? Are we supposed to care about the drop? Or just how low they both are? Or just how low the SearchBot-clone result is? It seems like this section would be helped by having a short summary statement of the conclusions the authors would like to draw.
Section 4.2.2 It was nice to see that the humans beat the human-imitation blueprint, while they struggled quite a bit against SearchBot. It might be worth specifically noting that not only was the blueprint imitation an improvement on DipNet in this setting, it was specifically the search method making the largest improvement.
Section 4.2.3 CFR->ERM? The text talks about plotting versus the number of ERM iterations, while the figure talks about CFR.
Figure 3 What exactly is the average being talked about in the first paragraph? The averaging mention in Section 2.2 (which is not used during search in normal play)? The self-play strategy profile which is a combination of one strategy from each of the 7 runs (not really an average)? Something else? Is ""average"" in the first paragraph different than ""average"" in the right figure? The average line on the left doesn't seem to match the average line on the right: on the right, there seems to be a slight bump upwards at the end, which doesn't seem to be present on the left.
On the right plot, why not include all (or at least multiple) independent plots, to give a more complete picture of the variance?
Include some mention or mark indicating how many iterations are used during play? As is, I don't know how this graph relates to the search as used in SearchBot.
=-= Comments after author discussion (Minor) concerns have been addressed. Thank you for the revisions.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Human-Level Performance in No-Press Diplomacy via Equilibrium Search","Keywords: multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning","Thank you for the updates and clarifications, and apologies for the cut-and-paste error in my comments. It addresses my concerns. I think there's now enough information that someone could produce similar results -- without necessarily reading through code (which does help too).
It's a minor thing at this point, but I am still slightly unsure about exactly what is going on in figure 1, on the left side. There seems like a disconnect between sampling a specific number of actions, and the text description which controls samples using Mk_i. Is the plot looking at directly sampling n different subgame actions, rather than using the Mk_i formula for sampling and varying M? Are there different runs with different values of M, and the scores are somehow aggregated over situations where the number of sampled actions corresponds to certain specific numbers of sampled actions? (Seems unlikely, given the data.) Or is the x axis of the graph varying M, not the number of search actions directly?
The author response is a great clarification of the different averages used in Figure 3, and why they were used. I appreciate that the level of detail here doesn't fit in the text, but I found it useful and helpful -- maybe some version of it could fit in the appendix?","",""
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","This paper studies how (two layer) neural nets extrapolates. The paper is beautifully written and the authors very successfully answered all the questions. They managed to update the paper, clarify the assumptions and add additional experiments.","This paper studies how (two layer) neural nets extrapolates. The paper is beautifully written and the authors very successfully answered all the questions. They managed to update the paper, clarify the assumptions and add additional experiments.",""
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","Dear Reviewers and AC,
We have updated our draft to incorporate the insightful suggestions of the reviewers:
Following Reviewer 3 and Reviewer 4’s suggestion, we have added additional extrapolation experiments on MLPs with different activation functions (tanh, quadratic, and cosine) in Section 3.3 (preliminary results previously presented in Appendix).
Following Reviewer 4’s suggestion, we have added a section (Section 5) on relations to other out-of-distribution settings, including domain adaptation, self-supervised learning, invariant models, and distributional robustness.
Following Reviewer 2’s suggestion, we have made the assumptions of our theorems clearer throughout the paper. We have also emphasized that our theoretical results empirically hold across different training settings (e.g., width, depth, learning rate, batch size), so the assumptions may be relaxed in practice.
Following reviewer 3’s suggestion, we have discussed the related work Neural Arithmetic Logic Units in Section 4.1. Our results may suggest an explanation why their proposed architecture improves extrapolation in arithmetic tasks.
We will improve other minor points of Reviewer 1, Reviewer 2, Reviewer 3, Reviewer 4 in the final version. Thank you all for the valuable suggestions.
Please let us know if you have additional questions.
Thank you,
Authors","",""
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","Dear Reviewers and AC,
We sincerely appreciate all the reviews. They give positive and high-quality comments on our paper with a lot of constructive feedback. We are working on incorporating the insightful and valuable suggestions from the reviewers. We will update the draft and post the response soon.","",""
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","Summary
The paper studies how neural networks extrapolate. The authors theoretically examine two-layer ReLU MLPs with mean squared loss in the NTK regime and, building on these results, GNNs. They find that the MLPs quickly converge to linear functions along any direction from the origin, but can provably learn a linear target function where the training distribution is sufficiently diverse. For GNNs, they propose a hypothesis that the success of extrapolating algorithmic tasks to new data relies on encoding task-specific non-linearities in the architecture or features. The theoretical results are supported by empirical results which sometimes go beyond the specific conditions of the theorems (like increasing the number of layers in the MLP to 4 in Appendix C.1.).
Pros
The paper provides both theoretical and practical insight into the extrapolation capabilities of neural networks, especially GNNs.
I especially liked the part about GNNs and the hypothesis that if we can encode the non-linearities outside the MLPs so the MLPs only have to learn linear functions, GNNs will extrapolate well.
Overall I found the paper very interesting and fun to read.
Concerns
The theoretical MLP results are very specific. Sometimes this is not apparent from either the abstract or the discussion of the results. Some of the constraints:
The MLPs have two layers, which I find the most limiting constraint as most practical MLPs have more layers.
The mean squared loss is used throughout the paper. I think this is not emphasized enough (it is mentioned only a single time in the paper). As far as I understand the proofs also rely on the loss, so the loss should be included in the conditions of the theorems.
We are under the NTK regime, which is of course evident from the techniques used. Nevertheless, this is not mentioned in the abstract.
The MLPs are ReLU MLPs which is emphasized sufficiently in the paper. The authors include preliminary empirical results for other activations functions in the Appendix (sin, quadratic, and tanh).
Questions
Could the proofs of Theorem 3 and Theorem 5 be generalized to MLPs with more layers?
Can we gain some insight into extrapolation with other loss functions like softmax based on these results?
Reasons for ranking
I found the paper really interesting and gained much insight from it. Some of the constraints of the MLPs are not emphasized enough and the writing is more general at parts than the results warrant. Even with the constraints I believe that this is an important step and sheds light on the extrapolation capabilities of neural networks. If the constraints can be made clearer I'm willing to improve my score further.
Minor comments
Second to last paragraph on page 5: ""for Theoreom 5"" should be ""for Theorem 5"".
Caption of Figure 1: outisde => outside
In 4.2., ""Experiments: architectures that help extrapolation"": ""GNNs with max-readout are better than GNNs with sum-readout (Fig 6a)"" should be Fig 5a.","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","Thank you for your detailed response! I really like the paper and my concerns were addressed so I updated the score to 9.","",""
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","This paper analyzes the extrapolate ability of MLPs and GNNs. In contrast to the existing theoretical works that focus on generalizability and capacity of these models, this paper emphasizes the behavior of training algorithm using gradient descent. It takes analogy of kernel regression via the neural tangent kernel as an example to study the bias induced by the gradient descent algorithm. The presentation of this paper is clear and well-organized with the most significant result shown in the first section, raising interest of the readers, as opposed to leaving them behind a massive amount of proofs. The contribution of this paper is significant as well since it draws attention of the researcher to theoretical analysis on the bias induced from the implementations of the algorithms as compared to the theoretical analysis on the model structure itself. Model extrapolation is also closely connected to topics such as meta-learning, multi-task learning, domain adaptation and semi-supervised learning since the ability of model extrapolation will limit its performance when applied to other tasks.
Pros:
This paper has shown some interesting results: for instance, MLP with ReLU trained by GD will converge to linear functions along any direction from origin outside the support of the training data. This coincide with the idea that MLP are piecewise linear in different regions. The proof is complicated though and requires the analogy to the kernel regression as basis. This result seems to suggest that the learning of MLP on data manifold supported by training data is also local linear and without support of training data, the induction follows the inertia of linearity. It is curious to see if this is due to the piecewise linearity of ReLU function. Maybe we will have better nonlinear extrapolation for MLP using tanh and other sigmoid functions.
Comparison between GNN and Dynamic programming algorithm is very intuitive and inspiring. It suggests that max/min aggregate as opposed to more commonly used sum-aggregate in GNN is more suitable for extrapolation and the similarity between max/min aggregate GNN and DP is also very convincing. In general, this paper built up a good intuition before diving into the proof, which is well-appreciated.
The suggestion to improve extrapolation is to put the nonlinearity into the architecture of the GNN or into the input representation is useful. For instance, replacing sum-aggregate to min/max aggregate helps to achieve good extrapolation. It also explains why the pre-trained embeddings such as BERT can be used in other tasks and still extrapolate well.
Suggestions:
Limitations of the study scope. This paper only discuss results of neural network using ReLU and GD. Although GD is widely used, the ReLU as the activation function plays a critical role in the study of extrapolation. It is necessary to provide analysis on the use of other common activation function to understand if the extrapolation ability is expanded.
It is interesting to see more connection with domain adaptation and semi-supervised learning as well.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","This paper investigates the extrapolation power of MLPs and GNNs (trained by gradient descent with mean squared loss) from a theoretical perspective. The authors show results of extensive experiments that back up their theoretical findings.
In particular, the authors study the question of what these neural networks learn outside the training distribution, and identify conditions when they extrapolate well. Their findings suggest that ReLU MLPs extrapolate well in linear tasks, with a fast convergence rate (O(1/\epsilon). GNNs (having MLP modules) extrapolate well when the non-linear operations are encoded in either network architecture or data representation, so as the inner MLP modules are aligned with only linear functions.
The paper is well written, ideas and definitions clearly explained and experiments laid out in detail. The theoretical contributions of the work are important as they enhance our understanding of how these networks learn and how well they generalize. These findings help us design GNNs based on the data and problem at hand. As such, this work addresses a fundamental question in GNN understanding and must be published.
Some comments/questions to the authors:
In Section 3.2, “diversity” of a distribution is informally defined in terms of training support and direction. A more thorough definition would be helpful.
The title of the paper is somewhat misleading: “from feedforward to GNN” insinuates that there are other network types that are discussed in the paper.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks","Keywords: extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory","This paper tackles the challenging question of how deep networks might learn to extrapolate knowledge outside the support of their training distribution. The paper contributes both with novel theoretical arguments as well as with empirical evidence collected on targeted cases. Differently from other recent approaches to the problem, the theoretical analyses presented here are non-asymptotic and provide precise information about the kind of functions that MLPs can learn in the proximity of the training region. Moreover, the authors provide compelling arguments about the need to explicitly encoding (task-specific) non-linearities in the input representation and/or in the model architecture in order to promote successful extrapolation. Overall, the paper addresses important issues and can be considered at the frontier of deep learning research. The paper is well-written and the recent literature is properly reviewed. In light of this, I think the paper would be of interest to the ICLR community. However, I would like to explicitly mention that I was not able to carefully review all the details and proofs reported in the Appendix, which is of an unusual length (almost 40 pages) for an ICLR paper.
Comments for possible improvements:
The analyses reported in Appendix D.3 / C.4 regarding the extrapolation capability of MLPs with different activation functions (sin, tanh, quadratic) are relevant and should be emphasized. They could also be expanded, for example by considering some data generation tasks analyzed in the main text.
It would be very interesting to extend this analysis to other simple problems, where MLPs cannot extrapolate appropriately. I am specifically referring to the simple counting and arithmetic tasks discussed in [1], where generalization outside the training distribution was achieved by adding ad-hoc gate units to the network. I think this domain is particularly relevant here, given that arithmetics is mentioned by the authors in the opening sentence of the paper.
[1] A. Trask, F. Hill, S. Reed, J. Rae, C. Dyer, and P. Blunsom, “Neural Arithmetic Logic Units,” in arXiv:1808.00508, 2018.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","Nice and interesting paper. However, I have a few questions.
In regard of Lemma 2, you compute the MSE of the positive SM. Then you deduce that if SM tends to 0, then also the MSE of the positive SM. This is correct, but what happens if
SM(x,y)≠0
and
x,y
becomes big. In that case the MSE explodes as well ? Why isn't this a problem?
Section 4.2 in Figure 4 you wrote d=16. Is this a typo? What feature dimension did you really take? Also, which estimators are shown in the left plot (I assume it is cos/sin) ?
A more general question. I found it unintuitive, that the you take only
m<d
feature samples to approximate the expectation in equation 3. The expectation (or integral) for example of the positive SM is with respect to
ω
that it is taken from
Rd
. So now taking only a few sampling points
ωi
as in equation (5) is not a sufficient approximation of an integral over
Rd
. What guarantees that this approximation is sufficient?","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","Thanks for writing this very interesting paper, I am learning alot from it. I am currently going through the proof of Theorem 1, and there's a step I don't understand. Could you please clarify it?
In particular, I don't know how to derive equation (21). In the infinite sum, instead of getting the 2-norm
||z||22k
for all
k
, I am getting the
2k
-norm
||z||2k2k
. Could you please give us a sketch of the proof of equation (21)?
Here's my derivation for equation (21): https://imgur.com/a/6BNSsbh
Thanks!","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","I really enjoyed reading this paper with a nice idea on the approximation of softmax-kernel. Unfortunately, after proofreading the Appendix I encounter some issues I believe. Could you clarify these?
How do you come with Eqn. 56? I assume that you used the same set of canonical basis vectors
e1,...,em
, but use their independence in the i.i.d. samples. However, I believe, to derive Equation 56, we should use independent random unit vectors instead of
e1,...,em
in Eqn.53 since
g
is fixed. In that case,
gidi
should be the linear combination of {
gi
} to
di
.
I assume that Eqn. 61 is a re-written form of Eqn. 56. However, you regard here
gidi
and
g12+⋯+gd2di
are independent, but it's not. Therefore we cannot have Eqn. 61 from Eqn. 56.
Typos in F.4.2. Proof of Theorem 5:
After Eqn. 49, ""int"" the formula -> ""in"" the formula
In Eqn. 50, there is a missing coefficient term for
Δ^
.
After Eqn. 63, ""firsr"" -> ""first"",
Δ((d1,…,dm)^)
->
Δ^(d1,…,dm)
, ""at least of of"" -> ""at least one of""
a missing expectation in the LHS of Eqn. 65 and the missing coefficient for L like in Eqn. 50.","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","The ""any fixed deterministic unit-length"" makes sense to me.
I checked with Lemma 5. If the statement of
r
is correct, we can get:
E[r1k1]⋅⋯⋅E[r1ks]
=
Πi=1s
E[g1ki❘❘g❘❘2ki]
Πi=1sE[❘❘g~❘❘2ki]
(a simpler arrangement than in Eqn. 58)
This states Eqn. 61 comes from Eqn. 56.
However, the related question is raised, is it fact that:
r∼g
where
r=g❘❘g❘❘2❘❘g~❘❘2
.
in the proof of Lemma 5? It seems to state the distribution of
g
is not affected by the change of the magnitude ratio of
❘❘g❘❘2
and an independent copy of it, but I have no idea how to proof this simple statement considering the dependency of
g
and
❘❘g❘❘2
. Could you elaborate this statement kindly?","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","I believe your explanation makes sense. Thank you very much!","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","This is a solid paper that proposes a new method for approximating softmax attention in transformer architectures that scales linearly with the size of the sequence. Even though linear architectures have been proposed before using a similar idea (Katharopoulos et al 2020), this paper provides a better solution along with theoretical analysis and makes a rigorous empirical comparison against other methods. All reviewers agree that this is a strong paper that should be accepted. I suggest citing the recent paper https://arxiv.org/abs/2011.04006 (Long Range Arena, mentioned in the discussion) which provides further comparisons on long-range benchmarks, including the method presented in this paper and Katharopoulos et al 2020, along with a detailed discussion of the differences between the two methods.","This is a solid paper that proposes a new method for approximating softmax attention in transformer architectures that scales linearly with the size of the sequence. Even though linear architectures have been proposed before using a similar idea (Katharopoulos et al 2020), this paper provides a better solution along with theoretical analysis and makes a rigorous empirical comparison against other methods. All reviewers agree that this is a strong paper that should be accepted. I suggest citing the recent paper https://arxiv.org/abs/2011.04006 (Long Range Arena, mentioned in the discussion) which provides further comparisons on long-range benchmarks, including the method presented in this paper and Katharopoulos et al 2020, along with a detailed discussion of the differences between the two methods.",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","This paper proposes a set of techniques called Fast Attention Via positive Orthogonal Random features (FAVOR+) to approximate softmax self attention in Transformers and achieve better space and time complexity when the sequence length is much higher than feature dimensions (
L≫d
). The resulting architecture, Performers, is provably and practically accurate in estimating regular full-rank attention without relying on any priors such as sparsity or low-rankness. It can also be applied to efficiently model other kernalizable attention mechanisms beyond softmax, achieving better empirical results than regular Transformers on some datasets with such strong representation power. Performers are tested on a rich set of tasks including pixel-prediction, language modeling and protein sequence modeling, and demonstrated competitive results with other examined efficient sparse and dense attention models. I think this paper is a solid step towards more efficient Transformers for practical long-sequence data, and should be accepted. Below I raise two potential questions and look forward to the solutions in the future.
For Performers, speedup can only be achieved when
L≫d
, which means Performers might have less advantages on wider Transformers. The time complexity of Performers for approximating the attention matrix is
O(Ld2log⁡(d))
, while a regular Transformer has a time complexity of
O(L2d)
to compute the attention matrix. From Figure 3, it looks like for a model similar to BERT-Base, Performers is only faster when the sequence length is larger than 512. I wonder how wide Performers can be while preserving faster speed than Transformers with the same size on the current datasets, and how their test scores compare.
It looks like Performers do not converge as fast as Transformers on larger language modeling datasets like PG-19. I wonder whether similar phenomenon will happen in other domains when the dataset is larger, and feel quite curious about the cause.","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","This is a solid paper that presents a computationally less expensive, unbiased, low-variance estimator of the Transformer architecture.
Strengths:
The authors provide mathematical guarantees for the suggested estimator.
The estimator (called FAVOR+) seems a more scalable replacement for regular attention.
Theory is empirically verified in a variety of experimental settings.
Question: In Introduction, you mention that your method does not rely on low-rankness, but doesn't your equation (4) as well as Fig. 1 assume that the attention matrix
A
allows low-rank decomposition? (I suppose
r<d
)
Suggestion: Will it be possible to compare a pre-trained Performer-based encoder (like BERT) against its competitors (Linformer, Reformer) on a GLUE benchmark or on probing tasks? I see that you compare them (Per-/Lin-/Re-former) against the original Transformer on the pre-training task itself (Fig. 5), but I am wondering how well the pre-training performance transfers to downstream tasks in each case.
Minor issues:
In the proof of Lemma 1 you first prove that
exp⁡(‖x+y‖2/2)=Eω∼N(0,I)[exp⁡(ω⊤x)⋅exp⁡(ω⊤y)]
, however this is simply an m.g.f. of
ω
at
x+y
, and thus IMHO does not need to be redirived. (I assume that the formula for the m.g.f. of a multivariate Gaussian random vector is well-known).
Similarly, eq. (16) is the m.g.f. of
ω∼N(0,I)
, it does not have to ""immediately follow from"" some other fact.
Typos:
In eq. (18), a norm is missing in the last term:
z2
->
‖z‖2
.
In the text after eq. (18): inequality -> equality
Update after the author's response: The authors have answered my questions during the rebuttal period and I am satisfied with the response. Hence updating my score: 7
→
8.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","Downstream Tasks - suggestions
Thank you very much for the comment! We will add additional results comparing different architectures on the downstream tasks in the final version of the paper. In fact several downstream-tasks experiments were already conducted in other papers. For instance, in the following paper: Long Range Arena: A Benchmark for Efficient Transformers (https://arxiv.org/abs/2011.04006), Performers are compared against several additional (scalable and not scalable) methods not included in our submission: Local Attention, Sparse Attention, Longformer, Sinkhorn Transformer, Synthesizer, Big Bird and the aforementioned Linear Transformer on challenging long range context tasks. Performers obtain the largest LRA (Long Range Arena) score among all tested scalable Transformers methods (which we define by having speed of more than 100 examples/sec).
Note: For reviewers’ convenience, we summarized the main results of that paper (by copy-pasting main tables and figures with the results) on the 9th page of the updated version of our manuscript. Therefore reviewers do not need to explicitly access that paper and can see a comparison of different methods with anonymity preserved (i.e. with no information about authors of different algorithms revealed).
Tested tasks from that paper involved in particular [we included the authors’ description]:
A longer variation of the standard ListOps task proposed in [Nangia & Bowman, 2018], which was designed to investigate the parsing ability of neural models.
Byte-level text classification using real-world data. “This task also benchmarks the ability of the models to deal with compositionality as it is required to compose characters into words into higher-level phrases. Compared to ListOps, boundaries are less well defined and need to be learned from the data, which is a challenging problem in its own right (Kawakami et al., 2019).”
Byte-level document retrieval. “This task is mainly about modeling a similarity score between two documents in a ‘two tower setup’ in which compressed representations are concatenated and passed into a linear classifier. Note that we deliberately prevent models from using cross attention. This task thus serves as a test of how well models are able to compress long sequences into representations suitable for similarity-based matching. We use the ACL Anthology Network (AAN; Radev et al., 2013) dataset, which identifies if two papers have a citation link, a common setup used in long-form document matching (Jiang et al., 2019; Yang et al., 2020).”
Image classification on sequences of pixels. CIFAR-10 dataset (Krizhevsky, 2009) was used for the image classification task.
Pathfinder (long-range spatial dependency). “The Pathfinder challenge (Linsley et al., 2018; Kim* et al., 2020) was first introduced for learning long-range spatial dependencies. It is a synthetic visual task motivated by cognitive psychology (Houtkamp & Roelfsema, 2010). The task requires a model to make a binary decision whether two points represented as circles are connected by a path consisting of dashes.”
Theory: Thank you very much for all the suggestions. They will be incorporated in the final version of the paper.
Typos: We fixed all these typos in the updated version of the paper.","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","We would like to sincerely thank the Reviewer for the comment. It would be definitely insightful to run the proposed experiments. Given the time range of the rebuttal period, we leave to future work running comprehensive additional studies regarding pre-training and fine-tuning / probing.
We want to mention that in addition to LRA tasks, Performers were also recently independently applied for end-to-end speech recognition in Conformers (with Performer's attention module replacing the regular one), and the results are reported in the paper: ""Efficient End-to-End Speech Recognition Using Performers in Conformers"" (https://arxiv.org/abs/2011.04196v1). Again, as before, to preserve anonymity, we summarize the results below so that Reviewer does not need to read that paper:
Results: Performers yield competitive performance on the LibriSpeech corpus (the training set contains 960 hours of read English speech) with 10 million parameters and only linear computational complexity. For comparison, the regular Conformer model consists of 116.4 million parameters. The proposed Performer-based architecture also outperforms the dynamic and lightweight convolution approach (Y. Fujita, A. S. Subramanian, M. Omachi, and S. Watanabe, “Attention-based ASR with Lightweight and Dynamic Convolutions,” in Proc. of ICASSP. IEEE, 2020, pp. 7034–7038.) by about 20% relatively in word error rate, with a substantially smaller model size.","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","##########################################################################
Summary:
The paper proposed a theoretically grounded O(N) approximation of the softmax attention. The key idea is to interpret attention as a kernel function and construct the random feature projection that can reproduce this kernel. It is highly non-trivial to derive a feature mapping that can accurately approximate the softmax kernel. To better approximate the softmax kernel, the author proposed some important design choices, all of which are supported by theoretical and empirical evidences. The author showed that 1) adopting non-negative random features is very essential to the approximation and the proposed Positive Random Features (PRF) can effectively reduce the variance when the attention values are small, 2) drawing orthogonal random matrices can further reduce the variance of the approximation, 3) the final proposed Performer model runs faster, takes less memory, and has better performance than other O(N) and O(N logN) attention methods.
##########################################################################
Reasons for score:
The paper is very well-written and should be accepted. This is an important landmark in the research about O(N) attention. The design of the random feature mapping is reasonable and theoretical analysis is convincing. Experiments show that Performer is better than the other O(N) attention methods and also other efficient attention methods.
##########################################################################
Pros:
The paper gives a provable O(N) approximation of the softmax attention. The method works without assumptions on the structure of the attention map (like sparsity). The theoretical proofs provide good insights on how to design a good O(N)-complexity approximation to the attention mechanism.
Apart from approximating the softmax attention, the proposed FAVOR+ method can be utilized to approximate other attention kernels. In fact, the author has experimented with Performer-ReLU, which outperforms Performer in some experiments. This provides the insight that softmax attention may not be the best choice.
The author conducted very comprehensive ablation studies on different components of the proposed method. This includes: 1) effectiveness of using the positive features, 2) drawing orthogonal random samples, 3) redrawing the random samples
##########################################################################
Cons:
From Figure 5 and also Figure 15, periodic redrawing is quite essential. However, the author has not mentioned about the implementation details on how they redraw the random samples and how to choose the period. For me, I feel that this hyper-parameter should be important because the model may need to ensure that each group of samples has been trained for a sufficient amount of time.
Performer-ReLU has replaced the attention kernel and can sometimes be better than the softmax attention. Thus, I feel the author may also want to compare with the linear attention method in ((Katharopoulos et al., 2020) ""Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"".
##########################################################################
Questions during rebuttal period:
Please address and clarify the cons above
#########################################################################
Typos:
(1) Page 5, after ""than those from SM_{2m}(x, y)"", there is an additional right bracket.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","Comparison with Linear Attention Method
Thank you for the question!
The attention mechanism of Linear Transformers (that uses shifted elu nonlinearity) is a special instantiation of the generalized attention from Performers. In fact we already run extensive ablation studies over kernels defined by different nonlinearities in the Appendix, Sec. D3. Experiments conducted by us and independently by other researchers (see: below) comparing Performers with Linear Transformers show that in general the latter are less robust.
For instance, using the attention implementation of the Linear Transformer from https://arxiv.org/abs/2006.16236 resulted in exploding NaN gradients which prevented complete training in the 36-layer ProGen runs for both unidirectional and bidirectional variants (see our comparison curves in the 9th additional rebuttal page we’ve added to the paper). We did not observe these issues in Performer training.
Furthermore, the comparison with Linear Transformers was actually already independently conducted in the following paper: Long Range Arena: A Benchmark for Efficient Transformers (https://arxiv.org/abs/2011.04006). In fact in that paper, Performers are compared against several additional (scalable and not scalable) methods not included in our submission: Local Attention, Sparse Attention, Longformer, Sinkhorn Transformer, Synthesizer, Big Bird and the aforementioned Linear Transformer on challenging long range context tasks. The results are summarized in Table 1, 2 and Fig. 3. Performers obtain the largest LRA (Long Range Arena) score among all tested scalable Transformers methods (which we define by having speed of more than 100 examples/sec), with Linear Transformers being a close competitor (see: Table 1 & Fig. 3). Performers provide the best models for the Pathfinder-problem and second best for Text-problem and Image-problem and get overall LRA score 51.41 across all 5 tasks. In comparison, Linear Transformers provide the best model for the Text-problem and get overall LRA score 50.55 across all 5 tasks (see: Table 1). Furthermore, Performers are characterized by the best space/time complexity profiles, as illustrated in Table 2.
Note: For reviewers’ convenience, we summarized the main results of that paper (by copy-pasting main tables and figures with the results) on the 9th page of the updated version of our manuscript. Therefore reviewers do not need to explicitly access that paper and can see comparison of different methods with anonymity preserved (i.e. with no information about authors of different algorithms revealed).
Typos: We fixed all these typos in the updated version of the paper.","",""
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","Summary
The authors propose to use the kernel feature map self-attention formulation introduced in [1] to efficiently approximate the softmax attention. The main contribution of the paper lies in the proposed positive random features that can approximate softmax with a strictly positive feature map without which the training is unstable. The authors also show that an approximation of softmax is not necessary for good performance and actually use ReLU random features to achieve their best results when training from scratch.
Strengths
The paper deals with a very pressing and important issue, that of the scalability of self-attention.
The positive random features are also useful outside of the context of self-attention for efficiently approximating softmax.
The experimental results provide strong evidence about the performance of training transformers with kernel feature-maps.
Weaknesses
The biggest weakness of the paper in my opinion is the lack of comparison with a simple feature-map as proposed in [1]. Since the authors also use the ReLU random features, we establish that approximating softmax is not necessary for good performance.
What would the performance be if a simple deterministic feature map was used?
What would it be if the computational cost was equalized either by adding more layers or by increasing the dimensionality of the queries and keys?
The second weakness of the paper concerns the evaluation of the practical softmax approximation capabilities. I find the theoretical results interesting and important but I would like more experimental evidence. Without fine-tuning, the authors provide evidence that the approximation does not work (Fig 5).
What would happen, for instance, in a toy task where the Lipschitz constant of the transformer layers was kept low? How big would the feature map need to be in order for the approximation to work in such a simple case?
What is being approximated in Fig 4? Is it a randomly initialized attention? What is the rank of the attention matrix?
How good would the approximation be for an attention matrix that is almost full rank and how many features would we need then?
Reasons for my recommendation
I am recommending acceptance because I believe that the positive random features is an important contribution both for transformers and for kernel approximation. In addition, the experimental results are impressive and show that fast kernelized attention indeed works in practice. My only reservation for a higher score is, as mentioned in the weaknesses section, the lack of comparison with simpler feature maps under equalized computation time.
[1]: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Rethinking Attention with Performers","Keywords: performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot","2. Equalizing computational cost
Thank you very much for the question! We considered many methods of comparing different architectures in a fair way, and ultimately decided that the most sensible one was to fix the computational budget allowed for every run (e.g. every model is allowed to use a 8x8 TPU-v2), as this is one of the most realistic scenarios in practical use cases. This means that the total memory consumption over all accelerators (which is fixed) would be a combination of the model size and batch sizes for a given X-former.
We prioritized using the same model sizes for all comparisons (and thus maximized the batch sizes, i.e. sizes of query/key tensors), in particular to show that the Performer’s softmax approximation is very accurate and leads to the same performance as a regular Transformer, as well as to demonstrate that the Performer can outperform other X-formers given the same model sizes.
In the example of modeling complexes of protein-sequences, we showed (right subfigure of Fig. 7) that by adding more layers to equalize computational budget, we can train Performer-models substantially outperforming more shallow Transformer variants. We also note that experiments with varying model sizes might in general be difficult to rigorously compare, since increasing model size would imply lowering batch size (within a fixed computational budget) and might have a detrimental effect on the overall performance if the batch size is not large enough. Finally, several solutions for more space efficient training can be actually easily combined together rather than compared with each other. For instance, reversible layers from Reformer: The Efficient Transformer (2019) can be easily combined with Performers’ FAVOR+ mechanism.
Weakness 2
Lipschitz-continuity: Thank you for the good question. Formally speaking, Transformers are not Lipschitz-continuous because they incorporate layer normalization, where the hidden state is divided by the standard error of its entries. Hence, when the standard error is small enough (i.e. entries of the hidden state are almost equal), the gradient through layernorm can be of an arbitrary magnitude. Layer normalization is crucial for empirically stable training and is used in all Transformer architectures we are aware of. Therefore, even in simple setups, we cannot guarantee a small Lipschitz constant.
Fig. 4: To clarify our setup, we pass in the same randomly generated (but properly scaled)
Q,K,V
matrices into both the exact softmax attention, as well as our tested approximation mechanisms, and compute their empirical mean squared errors. We reran the experiment and checked that all generated ground-truth attention matrices
(A)
are full-rank. See also our response below regarding the rank of the attention matrix
A
.
Approximation of the full-rank attention matrices
A
: Thank you very much for a good question. We also addressed it in the response to Reviewer 2 (see: “Low/Full-Rankness” section), where in particular we explained the difference between the common SVD approximation and FAVOR’s approach.
We would like to clarify that our presented upper bounds for the number of random features m needed for accurate estimation of the attention matrix via FAVOR is completely independent of the rank of
A
. Therefore our bounds are useful in practical applications, where we cannot assume that
A
is low-rank. We do not exclude the possibility that extra structural assumptions regarding
A
(such as low-rankness) might lead to even stronger upper bounds on
m
, yet we do not rely on any such assumptions. We would like to explore this topic in more detail in future work.","",""
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","There is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners. I have no doubt that many will use it as a reference when implementing and using RL algorithms (especially since the authors said they would release their code).
This is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings.","There is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners. I have no doubt that many will use it as a reference when implementing and using RL algorithms (especially since the authors said they would release their code).
This is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings.",""
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","We really appreciate and enjoyed reading this work, as it sheds light on the impact of hyperparameters for on-policy RL algorithms.
We wish to share our concurrent related work, as published earlier this year. It examines the impact of environment design choices for RL. While not formally hyperparameters, these choices impact the learning in a similar manner, and thus understanding these choices is important for RL practitioners.
“Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning” Daniele Reda, Tianxin Tao, Michiel Van de Panne MIG 2020 (ACM SIGGRAPH Conference on Motion, Interaction and Games) https://www.cs.ubc.ca/~van/papers/2020-MIG-envdesign
It is complementary to this submission in many ways; we examine the impact of: state representations, initial state distributions, reward structure, control frequency, episode termination procedures, curriculum usage, the action space, and torque limits. Our experiments are performed with TD3 as the RL algorithm of choice, Pybullet as the physics-based simulator, and using common locomotion benchmarks.
As one area of overlap, we also experiment with different ways of handling the termination signal due to timestep limits: in contrast with your findings, our results show that bootstrapping the termination state can improve overall performance. This could imply that the result is either sensitive to the choice of algorithm (on-policy vs off-policy) or the simulator (Mujoco vs PyBullet).","",""
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","Thank you for this excellent paper! I just had one point of curiosity, namely, it seems that the scores for some of the environments are significantly lower than I think they would be for a good converged policy. For example, a good policy in hopper will score above 3000 but it seems that your policies score significantly lower than this, likely due to the limited number of training steps. Given this is true (perhaps it is not and there's some difference between your hopper envs and the ones I am used to), do you think that your conclusions still hold for converged policies? Is it possible that suggested hyperparameter choices that appear good for policies that are not fully converged would actually harm the final converged reward? Or is it the case that reward curves do not tend to cross over and so conclusions can be drawn with partially converged rewards. It seems that it would be interesting to take a subset of these policies and give them more training time to see how the conclusions are affected. I'm very curious to know your thoughts on this issue.
Note, if I've misunderstood the reward scale then this question is totally moot.
Thank you!","",""
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","This paper carries out a large-scale study for understanding of on-policy deep actor-critic. The study looks into a large choices of many implementation settings and design decisions, and investigate their impact on the task performance. The evaluations are done with 250000 RL agents on 5 different continuous control tasks. For each evaluation category, there is a finding summary that provides practical recommendations.
In overall, this study is exhaustive and helpful to both RL researchers and practitioners. The experiment organization which separates all design choices into 7 main categories is very excellent in a systematic way. They cover most design choices in recent works of on-policy RL methods. The reports and the interpretation of results are very interesting and easy to read. The main and important findings are summarized concisely and expected to play important hints.
The only performance metric studied in the paper is a score that is proportional to the area under the learning curve. I was wondering if there should be an additional metric, i.e the final policy or an average reward of the final 100 policies? Would the final or best policy be of more interest to the choice of a practitioner?
As many recent work investigates the design choice of only on-policy RL methods, it would be interesting if in introduction there is discussion on why off-policy methods are not considered or should it be addressed in a different way in another research?
Beside the focus on only the performance in terms of rewards, it would be interesting if the discussion can be expanded to look at other matters, e.g. numerical stability of design/hyper-parameter choices, convergence behaviors (it might requires plot to see if a method show premature convergence, fast learning but sub-optimal, fluctuating, etc.).
Although the paper only uses Mujoco simulator, would the hyperparameters' domains be subjective to it, e.g. inertia, fiction, joint limits, contacts, etc.? It would be helpful if the discussion can show if such those factors play any role in the results? It would lead to more helpful finding summary.
As a final comment, this is a solid work and will be very helpful to the community. Given that it is implemented on the new SEED RL framework, so it would be better if the implementation code can be published.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","##########################################################################
Summary: This paper conducted a large-scale empirical study that provides insights and practical recommendations for the training of on-policy deep actor-critic RL agents
##########################################################################
Reasons for score: Overall, I'd vote for acceptance to the paper. The paper is informative and practical; however, I'm not sure that the paper meets ICLR's requirement.
Pros: 1. Reproducibility is one of the main issues for various RL algorithms. This paper conducts a large-scale empirical study for popular on-policy algorithms. 2. The paper is well-written, and the suggestion is useful to me.
Sorry, but I didn't go through all the details in the appendix.","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","The authors survey a wide variety of implementation-level and hyperparameter decisions in reinforcement learning for continuous control tasks. They train over 250.000 agents with different settings and suggest empirical guidelines.
As the authors indicate, there's not too much related work so one could call this work pioneering: The sort of work conducted by the authors is crucially important for a field afloat with tricks and tweaks, many of which are typically not discussed in the scientific literature due to a misplaced conceit around this being ""not research, just engineering"" entirely absent from established fields of science such as experimental Physics. It is also typically not done as it is just plain hard to do. Combined with the often lackluster response from the community, the cost-benefits trade-off has just not been worth it, especially for junior researchers. It's all the more commendable that the authors have engaged with this formidable task of bringing some of the ""secret sauce"" out of the heads of senior engineers in the various labs and into published and peer-reviewed science.
The authors compare various choices of configurations obtained from the Cartesian product of 8 factors which they call thematic groups: Policy Losses (Sec. 3.1), Networks architecture (Sec. 3.2), Normalization and clipping (Sec. 3.3), Advantage Estimation (Sec. 3.4), Training setup (Sec. 3.5), Timesteps handling (Sec. 3.6), Optimizers (Sec. 3.7), and Regularization (Sec. 3.8). The high-variance nature of training RL agents makes it such that the individual factors in these configurations often have surprising non-linear cross-relations such that the problem space cannot be evaluated incrementally (i.e., it's often not possible to establish ""the best"" architecture first and select the right learning rate afterwards). The authors propose a novel approach of considering for each choice the distribution of values among the top 5% configurations trained in that experiment. Their experimental design is such that the values for each choice are distributed uniformly at random and thus if certain values are over-represented in the top models this indicates that the specific choice is important in guaranteeing good performance.
As for improvements on the paper, I have one major and only a few minor comments. My major comment is that the paper does not indicate anywhere that the research code is released, only that it's based on SEED RL. I believe an authoritative public implementation of the configurations considered would be extremely worthwhile, both for the community and the authors. If they haven't already done so (there's no supplements to this submission and I refrained from doing any web searches to preserve anonymity), I'd urge the authors to invest the time to release a (possibly cleaned-up) version of their code.
As for minor comments, I'm not clear about the philosophical distinction of something being ""due to the algorithms or due to their implementations"" (in the Introduction). I very much see the point the authors are making, which is an important one -- what makes RL results work is often ""nitty-gritty"" details not mentioned in the main part of the relevant publications (and often just barely mentioned in appendices). However, in the strictest sense, the algorithm very much is the implementation -- that's what produces a given result. It's worthwhile to keep the distinction between an idea (say, PPO) and a given implementation of that idea (e.g., presumably the authors had to re-implement PPO in TF2 when using SEED RL and couldn't use OpenAI's original implementation). It's also fine to call the idea ""the algorithm"", but I'd have preferred to see this distinction more clearly defined.
Somewhat related: The authors are very much correct about what they call ""standard modus operandi of algorithms [...] such as PPO"", namely iterating between generating experience using the current policy and using the experience to improve the policy. I'd add that strictly speaking no iteration is necessary, as for instance IMPALA, coming from the A3C line of development, does both asynchronously in parallel, and I suspect so do the authors given their use of SEED RL. My suggestion would be to slightly rephrase this sentence and mention IMPALA along with PPO. Perhaps there could also be a comment somewhere about what constitutes ""PPO"" (or ""IMPALA"") -- e.g., IMPALA consists of (1) an asynchronous actor/learner split [with further choices of when/how the weights are copied from learner to actor, see e.g. this comment], (2) a specific type of policy-gradient loss, v-trace, (3) a specific neural network architecture, optionally including recurrence via an LSTM and potentially even (4) a specific type of preprocessing for environments such as Atari or DMLab [according to some papers, swapping out the implementation of the frame-downscaling algorithm in Atari has measurable impact on final performance -- this will matter a lot when evaluating re-implementations of an ""algorithm""]. I'd like for the authors to take on this opportunity and propose a common language to discuss these distinctions, which in practise are often confusing to junior researchers (and some senior researchers, too).
Further related to IMPALA and v-trace, I was surprised about the word ""unsurprisingly"" and the explanation in ""Perhaps unsurprisingly, PG and V-trace perform worse on all tasks. This is likely caused by their inability to handle data that become off-policy in one iteration, either due to multiple passes over experience [...] or a large experience buffer in relation to the batch size."" While the results speak for themselves, my understanding of v-trace was that it was specifically designed for the very goal of dealing with the ""slight"" off-policiness produced by asynchronous actor/learner splits in a PG setting. Perhaps the authors have an intuition I'm lacking at this point, but if so I'd appreciate further elaboration.
As a final and perhaps trivial comment, I was slightly irritated by the notation/typography for the inverse cumulative density function of a binomial distribution. In
LATEX
, the symbols
icdf
read as the in-context nonsensical
i⋅c⋅d⋅f
while the authors would presumably want to use
icdf
(compare
exp(x)
vs
exp⁡(x)
or
sin(x)
vs
sin⁡(x)
). I'd propose \mathrm{icdf} as the correct syntax for this in
LATEX
.
In follow-up work, I'd like to see a similar paper for various ""discrete RL"" tasks (a subset of Atari, VizDoom, DMLab, MiniGrid, BabyAI, ProcGen, and perhaps even Obstacle Tower, Minecraft, StarCraft (I or II) or the recent NetHack environment) with similar factors of configurations. I assume this is a task yet more daunting, but no less useful to the overall community of researchers.
Overall, this is a strong paper and I recommend it for publication.","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study","Keywords: Reinforcement learning, continuous control","The paper presents an empirical evaluation of many algorithmic choices made in the implementations of on-policy actor-critic algorithms in deep reinforcement learning (RL). The authors group those choices in clusters in which they expect some interactions. For each cluster, they test sets of randomly made choices while assuming that choices outside a cluster are set to competitive default values. Based on those experimental results, the authors formulate recommendations about how to make those choices for each cluster.
PROS The paper is well-written and clear. This paper is part of the string of recent papers that discuss the difficulty of evaluating deep RL algorithms. I appreciate the breadth of the choices that the authors consider. The justification for their overall experimental design (i.e., evaluating per choice clusters) is reasonable. While some findings are as expected, others are indeed unexpected and not discussed in the deep RL literature.
CONS I have a doubt about the robustness of the results. The authors decided to use the median over 3 seeds for the evaluation. Although the median is used, is it reliable given the observations made by Henderson et al., which implies that performance can vary a lot with respect to seeds? Could the authors comment on that point?
I think one important missing experiment is the evaluation of the combinations of all the recommendations made in the paper. Do the recommendations depend on the default setting for other choices, do they have a synergetic effect or could there be some negative interactions?","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Neural Synthesis of Binaural Speech From Mono Audio","Keywords: binaural audio, sound spatialization, neural sound synthesis, binaural speech, speech processing, speech generation","Interesting method for binaural synthesis from moving mono-audio
Nice insight into why l2 isn't the best loss for binaural reconstructions.
Interesting architectural choice with nice results.
Nicely motivated and clearly presented idea -- especially after addressing the reviewers comments.
I agree with the idea of a title change. While I think its implied that the source is probably single source, making it explicit would make it clearer for those not working in a closely related topic. Hence, ""Neural Synthesis of Binaural Speech from Mono Audio"" as suggested in the review process sounds quite reasonable.","Interesting method for binaural synthesis from moving mono-audio
Nice insight into why l2 isn't the best loss for binaural reconstructions.
Interesting architectural choice with nice results.
Nicely motivated and clearly presented idea -- especially after addressing the reviewers comments.
I agree with the idea of a title change. While I think its implied that the source is probably single source, making it explicit would make it clearer for those not working in a closely related topic. Hence, ""Neural Synthesis of Binaural Speech from Mono Audio"" as suggested in the review process sounds quite reasonable.",""
"Neural Synthesis of Binaural Speech From Mono Audio","Keywords: binaural audio, sound spatialization, neural sound synthesis, binaural speech, speech processing, speech generation","The paper is about a method for synthesizing binaural audio from a mono recording of a single speaker's speech.
First, I think the title is too general. The paper does not attempt to convert all possible sounds, but it tries to convert a single speaker's monaural speech signal to binaural audio where the speaker is moving. I think this inherent assumption is important since the method will probably not work for multiple overlapping audio sources. I suggest changing the title to ""Neural synthesis of binaural speech of a single moving speaker.""
The first part of the network ""neural time warping"" is an interesting component that is capable of adjusting the delays conditioned on the location and orientations of the source and microphone such that a location dependent binaural audio is formed by estimating time-varying delays of the original mono recording separately for two channels. It is believable that such a module would be helpful for a single moving speaker. However, such a model would not help or work when there are more than two active audio sources. A separation module would be required for that scenario. Neural time warping is an autoregressive model which can work online.
The second stage convolutional network which uses conditioned hyper convolutions is also an interesting architecture that takes the warped signals and applies time-convolutions with kernels obtained from the conditioning input which has the time-varying locations and orientations of the source and the microphone.
The section about the loss function is also interesting in that, the time domain l2 loss is shown to not work well for accurate phase estimation, so the authors propose to add a separate phase loss term to compensate for that. I think it would be better if Figure 2 is replaced with a plot of epsilon/|yhat| versus amplitude error divided by |yhat| in (a) and versus the phase error in (b). It could be clearer than the current 2D figure which is hard to interpret.
The use of ""sine activation"" is not well justified. ""sine"" activation is useful in the first layer of a ""signal representation network"" which is different from a signal prediction network. I do not see how and why that could be helpful here.
In terms of comparisons, 2.5D method uses visual information as conditional information to generate complex masks to produce binaural audio. In this paper, visual information is replaced with the spatial conditioning information. It would help to get more information about the window size and hop size used in 2.5D since they may be an important factor that relates to the amount of delays they can introduce. For wavenet comparison, it was not clear how the wavenet model was trained to generate binaural data. Did it use any conditioning information? If so , how? Was it applied in an auto-regressive way with randomized sampling? The wavenet audio example sounded noisy which is not typical of wavenet generated audio. It looks like the DSP method can utilize a listener specific HRTF which may be difficult to incorporate for the proposed neural model. Is it an important issue?
How does the model generalize to unseen speakers and rooms? The training and testing strategy uses the same room and the same speaker(s). Would we have any problem when the monaural audio is recorded in some other room with some other speaker?
In Figure 8, maybe it is OK not to draw the original binaural signal for every method.
In general, I liked the neural warping and conditional convolutional components which are interesting and I liked the analysis about the loss function. The approach is an interesting way to obtain binaural version of a monaural mono speaker recording in a room. The dataset produced for the paper would also be useful for research.
Update after revision
The revision improved the paper. Thanks for taking care of my comments.
Justification of sine activations, generalization to unseen speakers experiment are nice additions.
The new title is a bit better and I think it may be OK since the goal is to perform a moving source simulation for single speech sources. Multiple speech sources can be simulated separately and added together as mentioned. The authors may consider possibly a better name: ""Neural binaural synthesis from mono speech"" which emphasizes that the synthesized target is ""binaural speech"" from a single speech recording.
Just a few more points.
I think it is essential in wavenet to apply the model in an auto-regressive fashion over samples. Just using the network architecture and the loss function from wavenet is not equivalent to ""using a wavenet model"" since an essential part of the model is the autoregressive sampling which makes sure the samples are dependent and coherent. Without auto-regressive sampling, the resulting sound is poor as observed by the authors. So, I suggest to emphasize that ""autoregressive sampling"" is not performed in the paper to avoid misleading the readers.
More explanation of 2.5D is appropriate. One wonders if using a larger STFT window size would improve its results.","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Neural Synthesis of Binaural Speech From Mono Audio","Keywords: binaural audio, sound spatialization, neural sound synthesis, binaural speech, speech processing, speech generation","Strengths:
The paper is well written. It includes clear math notations and figures. Readers can easily follow the thought process of the authors. For example, Figure 2 shows the relation of l2 loss and phase loss with respect to target energy, indicating the importance of penalizing phase loss in the end to end system. The same observation is supported by Figure 3.
Strong results. The proposed end2end model significantly outperforms previous SOTA in terms of objective measures and subject tests. The video demo is very convincing. The model improved spatialization and sound quality.
High novelty. This paper proposes to impose monotonicity and causality to the learned warping function, which incorporates the physics of sound propagation. I am excited to another example of applying domain knowledge to an end-to-end model. The model includes two novel components: the neural warp network compensates the errors from geometry warp, the temporal convolution works as a post processing module to account for reverberation and other effects. Ablation study shows both components are critical.
To be improved:
The caption for Figure 4(a) seems to be incomplete.
It would be good to include a table to compare the proposed model with baselines in terms of model size and inference speed.","9: Top 15% of accepted papers, strong accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Neural Synthesis of Binaural Speech From Mono Audio","Keywords: binaural audio, sound spatialization, neural sound synthesis, binaural speech, speech processing, speech generation","This paper presents a neural network-based model to generate binaural audio given a single-channel audio and positions of source/listener & their angles. The authors developed a dataset of binaural audio, which will be released on acceptance.
Technical details and model architecture are available in the body of the paper, whereas additional details such as baseline DSP-based approach, proof, and dataset are available in the appendix. The model was evaluated using the dataset developed in this work. A demo video demonstrating the capability of the model is also provided as a supplementary material.
There are a few parts need to be addressed. (1) it is unclear why DTW-based warping is required. IIRC the warpfield here can represent not only a shift but also other monotonic & causal such as repeating. If there is only delay between left and right, just having a shift is enough isn't it? It is great if the authors can explain the motivation to use warpfield more clearly. (2) The use of hyperconvolution is an interesting idea. The equation 5 uses conditional temporal convolution. However, audio generative models such as WaveNet uses a different architecture; gated convolution. The gating mechanism can give additional non-linearity and so I'm wondering if you can evaluate the performance of hyperconvolution against gated convolution. (3) too large confidence intervals in Table 4. Although there were many evaluations, the confidence intervals were pretty large and there were overlaps among them (e.g., small overlap between DSP and ""ours"" in cleanliness, large overlaps between spatialization and realism between DSP and ours). With this result it is difficult to claim that there was a significant improvement over the baseline system. Please check your results and design the experiment more carefully to figure out whether there is any significant difference between them. Conducting side-by-side comparision is one possiblity.
Comments:
This paper claims that it works in real time but no information about speed such as real-time factor & hardware specification are provided.
Sampling rate information is not explicitly provided in the experiment section.
0.6 MOS difference is large, not ""a bit"".
Modern WaveNet models often use mixture-of-logistics (refer to Parallel WaveNet paper for details) as output rather than mu-law to achieve better quality.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","The paper introduces a novel dataset condensation technique that generates synthetic samples (images) by matching model gradients with those obtained on the original input samples (images). The authors also show that these synthetic images are not architecture dependent and can be used to train different deep neural networks. The approach is validated on several smaller datasets like MNIST, SVHN and CIFAR10. This work is well-motivated and the methodological contributions convincing. All reviewers were enthusiastic and indicated that there were no flaws in this work. The rebuttal clarified outstanding questions and made the paper stronger.","The paper introduces a novel dataset condensation technique that generates synthetic samples (images) by matching model gradients with those obtained on the original input samples (images). The authors also show that these synthetic images are not architecture dependent and can be used to train different deep neural networks. The approach is validated on several smaller datasets like MNIST, SVHN and CIFAR10. This work is well-motivated and the methodological contributions convincing. All reviewers were enthusiastic and indicated that there were no flaws in this work. The rebuttal clarified outstanding questions and made the paper stronger.",""
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","According to the reviews, we polished the paper in the following two aspects:
To further clarify the mini-batch sampling process, we have added several notations and comments in Algorithm 1, and also give more details and discussion in Supplementary - Implementation Details - Dataset Condensation - Paragraph 2.
To justify the effectiveness and robustness of the proposed gradient distance metric, we added the ablation study on the gradient distance metric in the supplementary material - Further Analysis - Ablation study on gradient distance metric.","",""
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","##########################################################################
Summary:
The paper proposes a novel dataset condensation technique that generates synthetic samples by matching model gradients with those obtained on the original input dataset. This technique is investigated empirically on several smaller datasets like MNIST, SVHN and CIFAR10. Two applications to continual learning and neural architecture search (NAS) are also explored and show some promising results.
##########################################################################
Reasons for score:
Overall, I vote for accepting this paper. The technique is intuitive and well-justified. Experimental results seem to suggest that it produces a synthetic set that compares favorably to those obtained using alternative methods. Also, additional applications of this technique to continual learning and NAS appear to be quite promising.
##########################################################################
Pros:
The paper is well written. The core idea is arrived at systematically and is carefully explained.
The paper does a good job referencing prior work (with most papers that I knew of being included) and empirical results obtained by the authors appear to compare very favorably to this existing prior work. I do not think that presented empirical results are exhaustive, but they are definitely very promising.
##########################################################################
Cons.
I did not see any major problems with the paper. But wanted to make a few comments that could potentially be addressed:
I found a sentence ""Note that each real and synthetic batch sampled from T and S contains samples from a single class and the synthetic data for each class are separately updated at each iteration"" a bit confusing. Specifically, does this mean that all samples in T and S have the same label? If so, does this mean that in the case when we have only one sample per class, S contains essentially the same input sample (with or without image augmentations depending on the experiment)?
While S is built to match the gradients on the original architecture, it is a little counterintuitive that such a small training set would not cause dramatic overfitting on other (possibly ""heavier"") architectures. Do we need to use multiple synthetic sets in practice, or rely on heavy data augmentation to avoid overfitting? Or just limiting the number of training steps would be enough? It would be interesting to see a more detailed discussion of this aspect. While I am encouraged by NAS results, this remains one of my concerns.
It would be interesting to see results on larger datasets like ImageNet or at least CIFAR-100. I understand that this exploration would be quite computationally intensive, but this would make the results much more convincing.
##########################################################################
Questions during rebuttal period:
Please address and clarify the cons above. (I will update the score depending on the authors reply.)
#########################################################################
Post-rebuttal.
Thanks for a detailed response that clarified some questions and concerns that I previously had. I think the updated paper is stronger and I am inclined to raise the score to 8.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","The paper presents a method for generating synthetic datasets from the large realworld datasets. The CNN trained on such synthetic dataset supposed to have similar accuracy on the realworld data, as trained on the original one. The benefit of a such procedure is reduced model training time and storage space (for data).
The method is built on the idea that the gradients of the network being trained on the real images should be similar to gradients, which were obtained by the training on the synthetic images.
The method is validated on MNIST, SVHN, FashionMNIST and CIFAR-10 on several different architectures: MLP, AlexNet, VGG-like and ResNet architectures.
Moreover, the paper compares the proposed method vs many other baselines, e.g. methods, which select ""representative"" image from the dataset (coreset methods), as well as Dataset Distillation and cGAN.
Overall I like the paper a lot. The method is well-motivated, shows good (sota) results and also often (for MNIST, SVHN, FashionMNIST) produces human-recognizable examples, although there is no term/regularization directly encouraging this.
The paper also studies how architectural choices like normalization, pooling, etc. influence the generated samples and how samples generated for the architecture A are suitable for training architecture B.
I don't see any major weakness in the paper.
Questions and comments.
In Figure 5, IMO it is bad practice to fit linear regression into blob-like data. (minor)
Have you tried to add some term, encouranding the diversity for the different synthetic samples belonging to the same class?
Post-rebuttal.
The rebuttal didn't raised any concerns and made the paper even stronger, thus I am keeping my score.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","We agree that fitting a line to such data may not be very informative... We followed the previous work...
If previous papers proposed some poor practice, the follow-up works should correct it, not repeat and reinforce it.","",""
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","Summary: This paper tackles the challenging dataset condensation problem. The goal is to learn to synthesize a small dataset, so that a neural network trained on the small synthetic dataset can have similar performance as a network trained on the full dataset. The proposed method tackles the problem by gradient matching. The proposed method achieves state-of-the-art performance, and shows promising results on two other downstream tasks, continual learning and neural architecture search.
Strength:
Comparing to existing approaches, the proposed method is efficient and effective, achieving the state-of-the-art performance
The authors use the synthetic dataset for two other downstream tasks and achieve promising results
The authors conduct extensive experiments to study and analyze the proposed method
The synthetic dataset trained on one architecture can be also used to train any other networks with different architectures, which makes to method more applicable
Weakness:
In Section 2.3 “Gradient matching loss”, the authors claim that the proposed distance metric is “a better distance”. It is probably better to use experiments or results to support this claim.
Other comments:
Currently, the loss for the target task is cross-entropy loss only (Figure 1(b)). I wonder if this method can be used for other loss functions as well. Also, I wonder if the proposed method can be used for self-supervised tasks as well. Can the authors comment on these?
It seems that the authors do not have any special designs to make the condensed synthetic dataset cross-architecture generalizable. I wonder why the proposed method has such a good cross-architecture generalization.
In Figure 3, it seems that the performance on CIFAR10 saturates quickly as the number of images per class grows. Just curious, will the relative performance eventually reach 80~90% as in other datasets? If so, then what is the data ratio?
---- Post-rebuttal comments----
Thanks for the response. After reading other reviews' comments and the rebuttal, I think this paper is in a good shape now. Thus, I am willing to increase my score to 8 and recommend acceptance.","8: Top 50% of accepted papers, clear accept","3: The reviewer is fairly confident that the evaluation is correct"
"Dataset Condensation with Gradient Matching","Keywords: dataset condensation, data-efficient learning, image generation","Dear authors, congratulations on this interesting work. I have looked at the code you released in https://github.com/VICO-UoE/DatasetCondensation
And you define the hyperparams only for the dataset sizes that appear in the paper (up to 50 ipc), would it be possible to provide the hyperparams for the results reported in this comment? (with 50, 100, 200, 500, 1000 condensed images per class).
Thank you in advance!","",""
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set. This substantially improves upon existing work. The proposed method is well supported both with theory and experiments. All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion. The determinantal point process might not be one of the most popular topics in the ICLR community today but certainly is relevant.","This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set. This substantially improves upon existing work. The proposed method is well supported both with theory and experiments. All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion. The determinantal point process might not be one of the most popular topics in the ICLR community today but certainly is relevant.",""
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","Dear reviewers,
Thank you very much for your reviews. The authors have given concrete responses to the concerns raised by reviewers. Please acknowledge if you are satisfied with the response or speak up if you still have remaining concerns.","",""
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","This paper presented a novel kernel decomposition for nonsymmetric determinantal point processes, which enables linear time of inference and learning w.r.t. the cardinality of the ground set M. This is a significant improvement over previous arts and makes NDPP practical in relatively large datasets. The theoretical of the paper is solid and supportive to the main claim of the paper. This paper is well written and easy to follow. Even for readers without much theoretical background, this paper is still moderately friendly since the logic and insight are clear. I vote this paper a strong acceptance, except for some minor concerns as follows:
The authors employed a way to simplify the kernel decomposition below Theorem 1. However, it is unclear what the impact of this simplification is to the exactness of learning or inference. It would me a plus if the authors can give some theoretical analysis on the gap between such simplification and
P0+
.
I notice the authors provide both learning and inference procedures for NDPP. It seems these two procedures can formulate a way to learn latent NDPP in an Expectation-Maximization fashion. I would suggest the authors give some discussion about the feasibility of such integration, and this can be an interesting direction.
In general, I enjoy reading this paper and think this paper is insightful.","9: Top 15% of accepted papers, strong accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","I have read the response from the authors and still believe the paper is worth for a strong acceptance. In general, I understand that finding the theoretical gap is very difficult and the corresponding workload can lead to another paper. For the 2nd point, I was just curious if there's any possibility of extending the proposed framework to an EM fashion in terms of NDPP. Personally, I think finding an efficient optimization method for EM with NDPP can be an interesting topic. Thank the authors for their response.","",""
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","Nonsymmetric determinantal point processes (NDPPs) received some attention recently because they allow modeling of both negative and positive correlations between items. This paper developed scalable learning and MAP inference algorithms with space and time complexity linear in ground set size, which is a huge improvement compared to previous approaches. Experimental results show that the algorithms scale significantly better, and can roughly match the predictive performance of prior work.
This is a well written paper and I recommend its acceptance. Scalable learning and MAP inference algorithms are important for the application of the NDPPs model, which seems promising compared with its symmetric counterpart in experiments.
I have some (minor) comments listed below.
In Lemma 1, the result is only proved for skew-symmetric matrices with even rank. Does it hold for odd rank matrices? This is important to support the claim that the new decomposition covers the
P0+
space.
Equation (3) uses notation
λi
, which is already used in Lemma 1. This could cause confusion.
In the paragraph after Theorem 1, it is proposed to set
B
=
V
and relax
C
. Is this used in Section 4? If not, I would suggest moving it to the experiments section, and adding some comparison in Table 2 to show the impact of this simplification.
I cannot quite understand the last sentence before Lemma 2. What can be computed in
O(K2)
time?
The footnote in Table 1 might cause confusion because it can be mis-interpreted as a square.
In G.1, the first sentence after equation (13), do you mean when
M
is odd or when
ℓ
is odd?
In equation (24),
X
should be
BTXB
The inverse of
C
appears in the gradient of
Z
. Is
C
guaranteed to be invertible in the learning algorithm? And how are
V
,
B
,
C
initialized in the algorithm?
In equation (31), please double check if we need the reciprocal in the denominator.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","Thanks for the feedback and the revisions to the paper. I am satisfied to the response. Congratulations on the great work!","",""
"Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes","Keywords: determinantal point processes, unsupervised learning, representation learning, submodular optimization","This paper propose a decomposition for non-symmetric determinantal point process (NDPP) kernels (M*M) which reduces the requirements of storage and running to linear in cardinality (M). Additionally, they derive a NDPP maximum a posteriori inference algorithm that applies to both their proposed kernel and the previous work (NDPP). In their experiments, they show both learning kernels and the MAP inference for subset selection on real-life datasets.
Pros:
○ This paper is well-written and easy to follow.
○ The author provide sufficient calculation process and relevant proofs of scalable NDPP.
○ For the existing problems of traditional DPP method is time consuming, need large memory and could not apply to large set, the scalable NDPP really solves them (e.g., this method could run on Instacart and Million Song datasets). I think it is practical.
Major Concern:
○ I have some doubts about the authenticity of the experimental results in Table 2, for the following reason: in the previous work, i.e., [1] Learning Nonsymmetric Determinantal Point Processes. Gartrell et al. Neurips2019. The results under average MPR have signifcant difference with same datasets and same hyperparameter settings. However, in NDPP paper, for Amazon: Apparel, the MPR of sym dpp is 77.42±1.12, MPR of nonsym DPP is 80.32±0.75, for uk retail, the MPR of sym dpp is 76.79±0.6, MPR of nonsym DPP is 79.45±0.57. In this paper, for Amazon: Apparel, the MPR of sym dpp is 62.63±1.81, MPR of nonsym DPP is 72.2±3.07, for uk retail, the MPR of sym dpp is 69.95±1.32, MPR of nonsym DPP is 74.17±1.37. There is a huge gap btw these two versions. I would like to know what causes this gap.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","Thanks for your attention to our work! I hereby discuss some issues on your tech report and the logit attack itself one by one.
A1. First of all, the importance weighting (IW) implementation we have released on github is just one of possible implementations of GAIRAT. There are many other ways to map the number of PGD steps to instance weights, and the number of PGD steps is only a rough approximation of the geometry property we would like to model, i.e., the non-linear distance from an instance to the class boundary it belongs to. Unlike IW for distribution shifts where we know the optimal weights, we have no idea that which IW map is the best, at least theoretically.
A2. Hence, the IW map itself may be a hyperparameter depending on the task --- the data, the loss, the model, the optimizer, and the set of future attacks. Technically, if we consider flatter IW maps, GAIRAT becomes closer to the standard AT; indeed, GAIRAT with possible IW maps can be regarded as a strictly general case of the standard AT since GAIRAT is just AT equipped with IW.
A3. Moreover, even if the IW map is fixed rather than tuned, other hyperparameters such as weight decay, learning rate, and the choice of the ""best epoch"" should be tuned on validation data, where the validation data are attacked by some stronger test-time attack instead of the weaker training-time attack. Note that AT is much more sensitive to changes of hyperparameters than natural training without simulating the attack; see ""Bag of Tricks for Adversarial Training"" (ICLR 2021: https://openreview.net/forum?id=Xb8xvrtB8Ce).
Then, let me talk about the logit attack itself.
B1. The community knew the attack long time ago and knew how to cancel its effect by actively scaling the weights and biases in the layer where soft-max serves as the activation. If the logit values will be multiplied by 10, the weights and biases will be divided by 10. Then the attacker can argue that the logit values will be multiplied by 100, and so on and so forth... This is an endless game. Even though the attacker can multiply the logit values by a very large number like 1 million, as long as the defender first divide the weights and biases by a much larger number like 1 billion, the robust accuracy can be as close to the natural accuracy as possible (not close to 100%). Therefore, in recent years, almost no paper has played this endless game, since the protocol of experimenting AT algorithms is that neither attacker nor defender is allowed to actively scale the logit values.
B2. Furthermore, the central philosophy of AT is to simulate the future attacks. Certainly, we would like to consider the diversity of attacks, but if we are only allowed to simulate a single attack, we will simulate the strongest attack that is still computationally affordable. By ""computationally affordable"", I meant we cannot simulate PGD-100 even when we know that the attacker will use PGD-100, but we can simulate PGD-10 with eps=16/255 when we know that the attacker will use either PGD-10 or PGD-100 with eps=16/255 rather than eps=8/255 on CIFAR-10. Similarly, multiplying the logit values by 10 is computationally affordable, and if doing so makes the attack stronger than before, we can simulate and should simulate the stronger attack during training.
B3. In my humble opinion, science and engineering are quite different. Kaggle newcomers know that models should be ensembled in practice, but almost no NeurIPS, ICML, or ICLR paper did that unless the paper works on ensemble learning itself. This is science vs engineering. In science, we don't consider all factors for the ease of ablation study. We cannot conclude that it is hard to believe top ML researchers still don't know ensemble learning after it has been proposed for 40+ years. There is significant difference between knowing a thing, knowing how to properly use a thing, and knowing when/why should/shouldn't use a thing.
Thanks again for your attention to our work, and hope my explanations above clarified some of your concerns.","",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","This paper easily breaks GAIRAT by a simple logic scaling attack
https://arxiv.org/abs/2103.01914","",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","Dear All,
we are the authors of the report here. Thanks to Ming to bring our report up and thanks to Jingfeng for his answer. First of all, we believe GAIRAT is a valuable method and the rationale behind makes sense. I came up with something similar but then after some time, I discovered at my own expense that was subject to gradient masking technique related to scaling the logits. When I read GAIRAT, it was interesting and I had the hunch that it could be prone to this attack and we tested it out and wrote the short report.
We were not aware the logit scaling attack was shown firstly by Carlini [1], our attempt was inspired by Section 4 of the AA paper [2]. Thank you, Ming, for bringing this to our attention.
In light of this, we are going to update our report by citing [1] as well. We also are aware only now by looking at the GitHub issues that the follow-up paper [3] shows the same result; despite that, we believe is important to remind why the accuracy goes down (and not just show that it does) related to the gradient masking produced by defenses that scale the baseline CE loss. We hope that this can raise awareness in our community.
Best Regards,
[1] https://arxiv.org/pdf/1607.04311.pdf
[2] https://arxiv.org/pdf/2003.01690.pdf
[3] https://arxiv.org/pdf/2102.07327.pdf","",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","Thanks for your response.
I would say logic scaling attack is as simple, basic, and important as PGD. Anyone can make any NN near 100% robust under vanilla PGD by only changing the temperature (e.g. 0.001) of cross-entropy loss. And it is a long-lasting pitfall when evaluating the robustness. It's hard to believe that the community still knows little about the logic scaling attack in 2021 (five years after it was originally proposed).
See the following paper https://arxiv.org/pdf/1607.04311.pdf
@Nicholas Carlini","",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","The paper proposes an insightful study on the robustness and accuracy of the model. It was hard to simultaneously keep the robustness and accuracy. A few works tried to improve accuracy while maintaining the robustness by investigating more data, early stopping or dropout. From a different perspective, this paper aims to improve robustness while maintaining accuracy.
There are some interesting findings in this paper, which could deepen our understanding of adversarial training. For example, the authors conducted experiments with different sizes of the network in standard training and adversarial training. The capacity of an overparameterized network can be sufficient for standard training, but it may be far from enough to fit adversarial data, because of the smoothing effect. Hence given the limited model capacity, adversarial data all have unequal importance. Though this technique is simple and widely studied in traditional ML, it is an interesting attempt in adversarial ML and the authors provide extensive experimental results to justify its effectiveness.
In the authors' responses, the concerns raised by the reviewers have been well addressed. The new version becomes more complete by including more results on different PGD steps and the insights on designing weight assignment function. Also, the authors gave an interesting discussion on enough model size for the adversarial training, though it is still kind of an open question. I would thus like to recommend the acceptance of this paper.","The paper proposes an insightful study on the robustness and accuracy of the model. It was hard to simultaneously keep the robustness and accuracy. A few works tried to improve accuracy while maintaining the robustness by investigating more data, early stopping or dropout. From a different perspective, this paper aims to improve robustness while maintaining accuracy.
There are some interesting findings in this paper, which could deepen our understanding of adversarial training. For example, the authors conducted experiments with different sizes of the network in standard training and adversarial training. The capacity of an overparameterized network can be sufficient for standard training, but it may be far from enough to fit adversarial data, because of the smoothing effect. Hence given the limited model capacity, adversarial data all have unequal importance. Though this technique is simple and widely studied in traditional ML, it is an interesting attempt in adversarial ML and the authors provide extensive experimental results to justify its effectiveness.
In the authors' responses, the concerns raised by the reviewers have been well addressed. The new version becomes more complete by including more results on different PGD steps and the insights on designing weight assignment function. Also, the authors gave an interesting discussion on enough model size for the adversarial training, though it is still kind of an open question. I would thus like to recommend the acceptance of this paper.",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","This paper evaluates against 40 iterations of PGD, where the strongest result is then +7% accuracy. By repeating this to 5 random restarts, the increase goes down to +4.2% accuracy. What happens if you run for more (several hundred) iterations of gradient descent? Does the effect size continue to decrease?
Note there is a difference between N iterations of gradient descent repeated M times, and N*M iterations of gradient descent. It is not always clear how to perform this tradeoff, but in many cases in the past 40 iterations of PGD has not been sufficient to converge.
For example, it may help to introduce a plot that shows accuracy as a function of PGD steps, as done for example in Figure 1 of Madry et al. 2017. Note that here the stop at 100 because the attack has (mostly) converged---you may have to try more if things have not converged by 100 iterations.","",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","Summary: The paper focused on the sample importance in the adversarial training. The authors firstly revealed that over-parameterized deep models on natural data may have insufficient model capacity for adversarial data, because the training loss is hard to zero for adversarial training. Then, the authors argued that limited capacity should be used for these important samples, that is, we should not treat samples equally important. They used the distance to the decision boundary to distinguish important samples and proposed geometry-aware instance-reweighted adversarial training. Experiments show the superiority over baselines.
Pros:
The finding on insufficient model capacity is very interesting. The following motivation for GAIRAT is intuitive and well explained.
The authors proposed a realized measurement to compute the distance to the decision boundary. This is inspiring for a series of decision-based work.
The experiments demonstrate the effectiveness of the proposed method.
Cons:
Treating data differently has been investigated in related work like MART and MMA. The authors should discuss the difference from these methods.
The capacity analysis provides a very good perspective to analyze adversarial training, however, the explanations in Figure 2 are a little bit weak.
The weight function of Eq. (6) lack some intuitive explanations. Why such a formula? Why choose these constants?
PGD steps are also investigated in CAT and DAT papers. The authors should also discuss the difference to them.
The experiments should compare with some baselines considering the example difference, such as MART, MMA.
The evaluations should test some modern white-box attacks, like auto-attack, only PGD is not convincing. Besides, Black-box attacks should be tested for a complete evaluation and checking the obfuscated gradients.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","This paper focuses on adversarial learning. It improves the robustness while keeping the accuracy. To achieve this point, the authors find that adversarial data should have unequal importance, which naturally brings geometry-award instance-reweighted adversarial training (GAIRAT).
Pros:
The paper has strong novelty in philosophy level. The common belief is that robustness and accuracy hurt each other. However, this paper shows that the robustness can be improved while keeping accuracy. As far as I know, this point has never been explored before.
The paper is well motivated and easy to follow. First, the authors use Figure 1 to illustrate the GAIRAT, which explicitly gives larger weights on the losses of adversarial data. The authors use two toy examples in Figure 3 to explain GAIRAT more. Second, the whole logic of this paper is easy to follow. For example, after explaining motivations of GAIRAT, we can clearly see the objective function of GAIRAT and its realization.
The paper is sufficiently justified in experiments. For example, PGD-200 has been used to verify the robustness of GAIRAT. From my personal opinion, this result is quite strong. Moreover, the authors upgrade their method by incorporing FAT and verify the robustness of GAIR-FAT.
Cons: 1.In the top right panel of Figure 10, the SVHN experiments have a period of increasing robustness training error for GAIRAT. Could you explain this?
2.Although authors show that model capacity is not enough in adversarial training, how large the DNN should be enough? What do you think?","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","I read through the responses and other reviewers' comments. The authors have addressed my questions and I support acceptance.","",""
"Geometry-aware Instance-reweighted Adversarial Training","Keywords: Adversarial robustness","This paper challenges the common belief of the inherent tradeoff between robustness and accuracy. Instead of recent methods improving accuracy while maintaining robustness, this paper proposes a geometry aware instance reweighed adversarial training (GAIRAT) method to improve robustness while maintaining accuracy.
Pros: 1 The direction---improving robustness while maintaining accuracy---is novel and interesting.
Specifically, several papers are challenging the inherent tradeoff, e.g., using more data [1], utilizing early stopped PGD [2], and incorporating dropout [3]. This paper still challenges the inherent tradeoff. However, different from [2,3] improving accuracy while maintaining robustness, this paper goes the other direction. To my knowledge, this is the first paper to explore this direction.
[1] Understanding and Mitigating the Tradeoff Between Robustness and Accuracy, ICML 2020 [2] Attacks Which Do Not Kill Training Make Adversarial Learning Stronger, ICML 2020 [3] A closer Look at Accuracy vs. Robustness, NeurIPS 2020
2 This paper has made two conceptual improvements. a) This paper explicitly argues that the overparameterized networks that have enough model capacity in standard training suffer from the insufficiency in adversarial training (though many studies have already shown AT needs the large model). b) This paper argues that under limited model capacity, adversarial data should have unequal importance. Unequal data's treatment was explored in the traditional ML methods several years ago, but it is rare in deep learning at this moment.
3 The proposed GAIRAT method is effective, indeed increasing robustness while retaining accuracy. The experiments are comprehensive over different network structures, datasets and attack methods. The experiments in the appendix provide much useful information.
Cons: 1.The design of weight assignment function in Section 3.3 seems heuristic. Would you explain some principles on assigning instance dependent weights?
2.In Figure 4, the GAIRAT method can relieve undesirable robust overfitting. Would you explain more about this? For example, why the robust overfitting exists in standard adversarial training? how/why your GAIRAT methods relieve it?","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","Hi authors,
Thanks for this interesting work for facilitating Few-shot Learning with data augmentation techniques in deep feature space. I'd like to point a (possibly) missing related work, which may be worthwhile to be discussed in the paper.
The implicit semantic data augmentation technique (ISDA, published in NeurIPS 2019, https://arxiv.org/abs/1909.12220) also performs data augmentation using deep features. They establish a zero-mean Gaussian distribution for each class and sample infinite semantic directions from it to augment training samples. The variance of the distribution is also estimated using intra-class Statistics. However, ISDA does not focus on Few-shot Learning and does not introduce techniques like Distribution Calibration like this paper.
I believe that citing ISDA won't devalue yours at all. If you want, I will be happy to discuss further.","",""
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","Hi Shuo,
Thanks for the great work and congratulations on getting an oral presentation session for your paper. I'd suggest a potential related work, Model-Agnostic Boundary-Adversarial Sampling for Test-Time Generalization in Few-Shot Learning (2020, https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2307_ECCV_2020_paper.php). It fine-tunes the feature extractor and thus the feature space solely in the test time by generating samples adversarial to classification boundaries, and it operates independently of training and without learning to create samples. If you're willing to update your paper, it would be grateful to see our work discussed in it.
Thank you!","",""
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","Hi authors,
Thanks for your great work. In fact, I have read one paper ""Leveraging the Feature Distribution in Transfer-based Few-Shot Learning"" which is posted on the arxiv half years ago, and it has almost the same idea as your paper. I wonder if this paper is the conference version of that preprint paper. Or can you tell me the relationship or difference between these two versions?
Thank you!","",""
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","This paper proposes a novel and powerful data augmentation strategy for few-shot learning, producing convincing improvements over current approaches. The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily resolved, with the results remaining strong. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.","This paper proposes a novel and powerful data augmentation strategy for few-shot learning, producing convincing improvements over current approaches. The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily resolved, with the results remaining strong. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.",""
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","We really appreciate all three reviewers for their valuable comments.
We’ve uploaded a revised draft incorporating reviewer feedback. Below is a summary of the main changes:
The experiments on various backbones are included in Table 5. (R3 & R4)
The experiments on various baseline models are included in Table 6. (R4)
We also set new state-of-the-art performance on a larger dataset, tieredImageNet, in Table 3. (R3)
We clarify the values of hyper-parameters on all three datasets in Section 4.1.3, and update the Figure 5 to provide more analysis. (R4)
We visualize the feature distribution of randomly selected base classes and novel classes, as well as the distribution of novel classes after Tukey’s transformation in Figure 6 (in Appendix). (R1)
We really hope our responses and revisions address all reviewers’ concerns!","",""
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","Summary
The paper proposes a method to calibrate the underlying distribution of a few samples in the few-shot classification scenario. The idea is to estimate a feature distribution of a few samples of a novel class from base class distributions. The authors assume that every dimension in the feature vector follows a Gaussian distribution. Based on the observation that the mean and variance of the distribution with respect to each class are correlated to the semantic similarity of each class, base class distribution can be transferred to the novel class distribution. After distribution calibration, features can be generated from the calibrated distribution and the generated features are used to train classifiers. SVM and logistic regression classifier are used to verify the approach on the mini-imagenet and CUB datasets.
Pros
The idea can be applied to any types of feature extractors or classifiers when the Gaussian distribution assumption holds.
The proposed approach shows competitive performance on two benchmarks, mini-imagenet and CUB.
Extensive ablation studies verify hyper-parameter settings and their sensitivities.
Cons
Many hyper-parameters are involved in the approach. ( Turkey’s transformation parameter lambda, top-k base distributions, dispersion parameter alpha, number of generated features)
Some hyper-parameter setting should be different depending on the dataset. (dispersion parameter alpha)
The paper uses the previous work (Mangla et al. 2020) as a baseline and applies their approach on top of it. While theoretically the approach can be applied to any types of feature extractors, only one baseline improvement is shown in the paper.
Rating
I give marginally below the threshold. The paper shows good performance and also claims the approach can be applied on top of any classifiers or feature extractor. The general applicability and effectiveness are the strength of the paper. However, the proposed approach is only verified on one baseline approach (Mangla et al. 2020). More empirical data would strengthen the claim of the paper.
Questions
The approach outputs calibrated distribution, i.e., a mixture of Gaussian distributions. These distributions can be directly used to classify query samples by calculating the likelihood of the sample on each distribution. How does likelihood classification perform compared to the retrained classifiers (SVM or RL)? The optimal values for top-k parameter and alpha parameter are different depending on the dataset. Are optimal values for lambda and the number of generated features same or different depending on the dataset? Did the authors use the same lambda and number of generated features for both mini-imagenet and CUB experiments? The authors only applied the proposed method on one baseline approach. Is the approach effective for more variety of backbone networks and losses?
Feedback
Table1 shows that there is a correlation between feature distributions and semantic similarities. It would be interesting to see how the distribution calibration performs on each class depending on the similarity level of top-k base distributions.
The empirical evidence is not sufficient to claim the approach applies to general network architectures and few-shot learning approaches. Experiments with more baseline approach to show the effectiveness of the idea is recommended.","7: Good paper, accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","Q3: How does likelihood classification perform compared to the retrained classifiers (SVM or LR)?
A3: Thanks for this comment! We have included the result using maximum likelihood in Table2 and Table 3 as below:
Methods miniImageNet 1-shot tieredImageNet 1-shot
Maximum Likelihood with DC 66.91
±
0.17 75.92
±
0.60
SVM with DC 67.31
±
0.83 77.93
±
0.12
Logistic Regression with DC 68.57
±
0.55 78.19
±
0.25
We found it can achieve competitive performance while training a SVM / LR classifier using the samples from the calibrated distribution can further improve the performance.
Q4: Table 1 shows that there is a correlation between feature distributions and semantic similarities. It would be interesting to see how the distribution calibration performs on each class depending on the similarity level of top-k base distributions.
A4: Thanks for this comment! We found that the higher similarities between the retrieved base class distribution and the novel class ground-truth distribution, the higher the performance improvement our method will bring as shown in Table 9:
Novel class Top-1 base class similarity Top-2 base class similarity DC improvement
malamute 93% 85% ↑21.30% acc
golden retriever 85% 74% ↑18.37% acc
ant 71% 67% ↑9.77% acc
We really hope our response addresses your concern. If you have any other questions, we are very happy to continue discussions!
[A] Vinyals et al., Matching Networks for One Shot Learning, NeurlPS 2016 [B] Snell et al., Prototypical Networks for Few-shot Learning, NeurlPS 2017 [C] Rusu et al., Meta-Learning with Latent Embedding Optimization, ICLR 2019 [D] Liu et al., An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning, ECCV 2020 [E] Chen et al., A Closer Look at Few-shot Classification, ICLR 2019","",""
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","Summary:
This paper identifies the problem of biased distributions in few-shot learning and proposes to fix it. In few-shot learning, only a few samples per class are available; this makes estimating the class distribution difficult. The paper proposes a distribution calibration algorithm that makes use of the meta-train class distributions to calibrate the few-shot class distributions. Once calibrated, more samples are drawn from this distribution to learn a classifier that generalizes better. This approach does not require additional learnable parameters and can be (potentially) built on-top of any pre-trained feature extractor. Empirical results show that this approach achieves state-of-the-art results on Mini-ImageNet and CUB.
Pros:
This paper identifies and tries to tackle an important problem in few-shot learning - estimation of the class distribution. Due to the limited number of samples, this problem is difficult, but important for few-shot learning. The proposed algorithm is simple and effective in tacking this problem.
As opposed to other related works, the proposed algorithm does not have any learnable parameters. It makes use of the features obtained for the meta-train and few-shot samples.
Cons:
One of the claims of the paper is that the proposed algorithm is pre-trained feature extractor agnostic. However, there are no experiments to validate this claim. Consider adding feature extractors trained in different ways.
Some of the popular few-shot datasets were not included in the experimental section, namely Tiered-ImageNet [1] and Meta-Dataset [2]. The former is a larger dataset than the ones used, and the latter provides cross-domain results for the approach. The proposed algorithm assumes that statistics from the meta-train classes would transfer to the few-shot classes, which would be tested more thoroughly in the cross-domain setting.
Clarifications:
In the feature space, it is intuitive that the means of similar classes would be correlated. Is there a justification for why this is true for the variances?
Tukey's Ladder of Powers transformation is used only on the few-shot samples and not the meta-train samples. Is there a reason for that? If the pre-trained feature extractor is trained using a certain metric (cosine distances for example), I would imagine transforming all the features would be beneficial.
The backbone used in the experiments is trained using a supervised and self-supervised loss. What are the results without the self-supervised loss?
How many samples are drawn from the calibrated distribution for the numbers in Table 2?
Notes:
The performance of few-shot learning algorithms has been traditionally evaluated using the averaged accuracy over multiple tasks, but that is not the only way to do it. Look at [3] for details.
[1] Mengye Ren et al. Meta-Learning for Semi-Supervised Few-Shot Classification. [2] Eleni Triantafillou et al. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. [3] Guneet S. Dhillon et al. A Baseline for Few-Shot Image Classification.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Free Lunch for Few-shot Learning: Distribution Calibration","Keywords: few-shot learning, image classification, distribution estimation","This paper presents a simple and intuitive data augmentation method for few shot image classification. The proposed method assumes the feature distribution of a single class to be gaussian. Based on this assumption, the proposed method samples features from a gaussian distribution that is constructed using the statistics of the similar base classes. Combined with logistic regression, the proposed method achieves strong performance on two standard few-shot image classification benchmarks. This submission also carries out an ablation study on several design choices, which I really appreciate.
The submission is well-written and clear. The proposed method is novel and can inspire future augmentation-based methods in few-shot image classification.
I have a few more requests/ablations that I am curious to see.
Looking at Figure 4, the 1-shot accuracy with only 1 retrieved class is already very strong. Instead of sampling from the ""calibrated"" distribution, can we simply retrieve examples from the nearest class? Namely, find the nearest class, randomly sample some examples, and use their features to augment the novel classes. This ablation should make clear what additional value the sampling procedure adds.
In equation (6), how important is it to include the novel class feature into the mean? Can we simply do \mu_prime = \sum_{i \in S_N} \mu_i / k?
This method makes an important assumption that the feature distribution is gaussian. How well does this assumption hold in practice? Can the authors provide some analysis of the feature distribution? Ideally both before and after Tukey’s Ladder of Powers transformation.","7: Good paper, accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","This paper proposes a model for learning disentangled representations by assuming the slowness prior over transitions between two frames. The model is well justified theoretically, and evaluated extensively experimentally. The results are good, and all reviewers agree that this paper is among the top papers they have reviewed. For this reason, I am pleased to recommend this paper for an Oral.","This paper proposes a model for learning disentangled representations by assuming the slowness prior over transitions between two frames. The model is well justified theoretically, and evaluated extensively experimentally. The results are good, and all reviewers agree that this paper is among the top papers they have reviewed. For this reason, I am pleased to recommend this paper for an Oral.",""
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","We would like to thank again all reviewers for their valuable inputs and positive feedback, summarizing our paper as “very topical” (R1), “full of convincing arguments” (R2), containing “solid experiments and well-presented results” (R3) and “well written and easy to read” (R4). Moreover, their suggestions have led to a number of exciting new results and updates:
L2 transition prior experiment (R1, R3): With this rotationally symmetric prior the model becomes non-identifiable and performs worse. Thus, as predicted by our theory, we need a sparse transition prior to achieve good disentanglement.
Natural dependencies experiment (R3): We have added a visualization that shows the complex dependencies between natural factors and demonstrated that breaking these dependencies (by shuffling) has no clear effect on our model performance.
Delta t experiments (R4): We have clarified the effect of temporal separation in KITTI Masks and demonstrated that sampling pairs with larger temporal separation increases the model’s disentanglement performance.
Improved proof intuition (R1, R2): Thanks to the reviewer’s feedback we have managed to make the proof sketch and intuition more accessible to a wide audience, which is very important to us.
Furthermore, we have retrained our models to include static transitions (R3), which did not significantly influence our results; demonstrated that our datasets are not ""cheating"", as multiple factors change in most transitions (R3); added clarifications about the LAP dataset and the limitations of current metrics (R4); and clarified the significance of our work (R2). Finally, we made several modifications throughout the paper and appendix to improve the clarity of our arguments based on points of discussion in these reviews.","",""
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","This paper introduces the SlowVAE to model transitions (of position, size, etc) in single-object videos. A Laplace prior conditioned on the latents at the previous step is used to learn the transitions, which the authors argue are naturally sparse.
My biggest concern regarding this work is the construction of datasets: the authors effectively “consider pairs of images [x_{t-1}, x_t], which differ effectively in only a few factors of variation due to the sparse prior.” While the marginal distributions of factor transitions may no doubt be sparse in natural settings, I would guess the sparsity is not independent across factors. For example: a video where a camera moves sporadically will show the position factors and size of an object changing sparsely but simultaneously. Could you augment Fig 1 and appendix H to show the joint distributions in addition to the marginals?
Moreover, the LAP procedure has the following rejection rule: “if all factors remain constant (no transition), the sample is rejected as the pair would not result in any temporal learning signal.” Surely there needs to be some static pairs if you want to be close to natural videos?
Given KITTI is the only dataset you show results on whose transitions are natural (albeit not 3D-natural; see caveat in question 2 below), it would be nice to see more evidence how the SlowVAE would generalize and cope in scenarios different from the ones you’ve constructed. If the LAP procedure indeed samples from marginal transition statistics rather than the joint distributions, I don't believe the samples would resemble ""natural transitions"". Learning disentangled representations from data which is engineered to show changes in one generative feature at a time seems like cheating because we usually don't have easy access to such data (unless an agent actively interacts with an object).
Strengths:
Good literature review
Comparison to relevant SOTA models, linking to prior work on disentanglement as well as nonlinear ICA, on a variety of metrics.
Solid experiments and well-presented results
Questions:
Have you tried working with sequences rather than pairs of images? I assume your ELBO can be readily extended to sequential data. Are there additional challenges in working with sequences?
Do you think there's room for bias in the natural statistics you computed on Youtube-VOS and KITTI-MOTS? For instance, from the fact that object masks are projections of 3D objects onto 2D frames? Consider the bicycle example (row 1) at https://youtube-vos.org/. In 3D, the position of the bicycle changes smoothly as the cyclist carries it. But in 2D there is a noticeable jump in the center of mass of the bicycle's mask (relative say to the body of the rider). Could this possibly explain why you found a heavier-tailed Laplace distribution to be a good fit for the transitions in Youtube-VOS and KITTI-MOTS?
How do natural transition statistics look like in multi-object datasets? How do you envision extending your work to tackle those?
In 3.3, you suggest alpha = 1 helps break the rotational symmetry of the ELBO by making axis-aligned representations optimal. Could you substantiate this claim? Theorem 1 does not immediately suggest an identifiability advantage for alpha = 1.
On the Natural Sprites dataset, why is it that discrete transitions give SlowVAE an advantage over PCL (MCC score 52.6 versus 50.2 and all other metrics) whereas continuous transitions are disadvantageous (MCC score 49.1 versus 51.7)? Why work with discrete transitions at all?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","Thanks for your quick turnaround to allow for discussion. The (slightly) increased MCC score on permuted Natural Sprites and your plots of pairwise natural transitions do lend credibility to the concern that SlowVAE's disentanglement hinges on observing transitions in generative factors independently (e.g. one or a few at a time). Here is my full argument:
From my reading, SlowVAE does no better than baseline models on natural transitions from Youtube-VOS (which are only deployed in Natural Sprites). Is that fair to say? I do appreciate there is a performance gain on KITTI Masks; the top-3 latent traversals do look great. But there isn't enough information in the results on KITTI Masks at the moment (e.g. comparisons with PCL and Ada-GVAE) to make a compelling case for SlowVAE. Beta-VAE isn't a fair baseline for a temporal model. Moreover, there is only one score and one number (no average over different runs) in Figure 5.
That prompts the conclusion that to show a performance gain, you are relying on independently sampling transitions (per factor) via the UNI and LAP sampling schemes. Your results on SmallNORB and Cars3D prompt the same conclusion. (Could you also clarify what's the value of k i.e. the number of factors which change simultaneously in your UNI sampling for dSprites? And I assume LOC == UNI in Tables 15 and 16?)
Unfortunately, the independence assumption in the UNI and LAP sampling schemes is totally inadequate when combined with sparse marginal distributions like the Laplace, an upper bound on how many factors can change at the same time, or a prohibition on static objects (i.e. no transition). These assumptions lead directly to the cheating via demonstration we discussed earlier.
To make a convincingly case for SlowVAE, I would advise doing at least one of the following:
focus on natural experiments (and perhaps leave all artificial-transition datasets for the appendix). Maybe you could apply Youtube-VOS transitions to MPI3D?
add ablations to justify your model, especially the choice of prior (e.g. add comparisons with α = 1 and 2) and the effect of parameters like γ and λ
I look forward to reviewing the effectiveness of SlowVAE independently of the artificial-transition datasets.","",""
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","Summary:
The paper addresses the problem of disentangling the underlying generative factors from data with a particular focus on dynamic natural data. It provides evidence that transitions of objects in natural movies can be characterised by temporally sparse distributions. A novel proof based on a sparse prior on temporally adjacent observations is provided, allowing to recover true latent variables up to permutations and sign flips, and improving the disentangling performance over existing methods. Two new datasets with measured natural dynamics are also proposed to enable evaluation on more realistic scenarios.
##################################################################
Strengths:
The paper addresses the important problem of unsupervised learning of disentangled representations. Overall, it is well written and easy to read.
A new framework of non-linear ICA is introduced, showing evidence to the hypothesis that natural scenes have sparse temporal transitions.
Two new benchmarks of natural and unconstrained data are presented which would enable future works on more realistic scenarios.
The paper provides extensive experiments and comparisons to relevant baselines, including both qualitative analysis and quantitative results using different metrics.
##################################################################
Weaknesses:
The existing datasets present in DisLib are considered unnatural since they assume data is i.i.d and define a prior on the number of factors to be changed. The datasets presented in this work represent incrementally more realistic scenarios where sparse transitions are imposed in addition to data with natural continuous generative factors and data with transitions from unstructured natural videos. In the Laplace transitions dataset, the rate of the Laplace prior λ is sampled from a uniform distribution λ ∼ U (1, 10) which may result in changes in many factors (when λ is small). Although this allows to have fair comparison with Locatello et al., (2020), this setting may sometimes result in non-sparse transitions between image pairs which is not aligned with the statistics of natural transitions as discussed throughout the paper. This would not be expected if one does not read Appendix 4. Hence, it would be good to clarify this particular point in the main paper.
In the Natural Sprites dataset, pairs are produced by only varying the position and scale (with real transitions extracted from the YouTube measurements) while color, shape, and orientations are fixed. While fixing the color and shape can be understood to follow natural transitions of objects, it is unclear why the orientation is not varied in this case?
For the Kitti masks dataset, continuous natural transitions are considered in all underlying factors. Table 2 shows a clear improvement when the temporal distance between sampled frames ∆t = 5 compared to ∆t = 1. What would be the effect of further varying this parameter?
From Figure 4, authors raised up an interesting observation for the rotation factor, where SlowVAE shows three sinusoidal oscillations with different frequencies matching the three distinct rotational symmetries of the shapes present in the dataset. However, from the MCC measurements, we can notice that all methods including SlowVAE struggle in disentangling the rotation factor. Here, it would be interesting to further discuss this limitation.
##################################################################
Final update: Authors addressed all my comments in the rebuttal making significant improvements to the revision. I've increased my score.","9: Top 15% of accepted papers, strong accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","Summary
This paper introduces a novel VAE-based model with the aim to improve unsupervised disentangling of latent factors in visual data. This model differs from previous disentangling models in that it takes short (2-frame) videos as input instead of static images. The model is equipped with a Laplace prior over the dynamics of the video to help it align its representation to axes of sparse temporal dynamics. The intuition is that the temporal dynamics of natural visual stimuli vary sparsely according to some choice of factors, and that choice of factors is exactly what “disentangled” should refer to. The authors show that their model achieves better disentangling than previous static-image methods according to a number of metrics.
Pros
The interpretation of disentangling as a basis in which the distribution of temporal dynamics of video is sparse is valuable. Previous approaches to disentangling have been plagued by non-identifiability, and this new interpretation of disentangling is a natural and operational solution. I fully agree with the authors that temporal information is essential for representation-learning, and hope this paper can help accelerate the field in that direction.
The paper is clear and well-written.
The experiments are very thorough in terms of comparisons to previous models and evaluation with previous metrics.
The application of the Mean Correlation Coefficient (MCC) as a metric for disentangling is good --- I think it is simpler and clearer than many existing disentangling metrics.
The latent embedding plots are a nice way to visualize latent representations. While they don’t show what effects non-matched generative factors have on the latent coordinates, they offer valuable information about the latent embedding that is complementary to the commonly used latent traversals.
Cons
My biggest suggestion is to include an ablation study. Aside from PCL (which isn’t variational so lives in a different world), there are no existing disentangling models on videos to fairly compare the authors’ model to. Consequently, it is very important to perform ablation experiments. More specifically, after reading the paper I have a burning question: How important is the Laplace prior over transitions? In other words, can the model work with just a KL regularization between the posteriors for consecutive timesteps? I think it’s quite possible the answer is “yes” --- simply having a KL regularization instead of the Laplace prior should disentangle better than static-image VAEs because the diagonal posterior will be pressured to align with the transition dynamics. So I wouldn’t be surprised if the model does quite well with just a KL instead of the Laplace, and that would be a simpler model with two fewer hyperparameters. So please do this experiment --- regardless of the outcome, the results will be very valuable for readers considering using your model.
Aside from that, I have only a couple minor suggestions:
Figure 2 is a notationally confusing. You use z subscripts to indicate time in the lower part of the figure, but in the top part of the figure z subscripts indicate component index. Maybe make the component index a superscript, or at least make the z in the bottom part of the figure bold so the boldness distinguishes vector from scalar.
Perhaps make a note that the MCC metric doesn’t work well for discrete latents like shape in dSprites. For example, in Figure 4 the SlowVAE permutes the shape ordering (e.g. as compared to PCL) and gets a low MCC score for that, but that low score is a drawback of the metric, not the model. This isn’t unique to MCC --- most other metrics (except MIG) would fall for this too. But perhaps for discrete latents (ones where we don’t care about the ordering of a discrete set of values), MCC can take the highest over all permutations. I don’t think it’s necessary to do this, but perhaps you can add a sentence that mentions it so readers understand why the shape score is low.
Conclusion:
Overall, I recommend this paper to be accepted. It is very topical, since disentangling has recently been receiving increasing attention, and by incorporating temporal cues into disentangling in VAEs this paper could help steer the field in a productive new direction. I only hope the authors do the suggested ablation experiment (I think practitioners would appreciate those results).","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","The paper starts with an observation that temporal transitions in sequences of natural images are sparse, which is supported by data collected from two big datasets (youtube-vots and kitti-mots). This suggests using a sparse prior for temporal transitions of latent variables when modelling naturalistic scenes. The authors then introduce SlowVAE, a model for temporal independent component analysis (ICA), that depends on such a sparsity prior, and prove that latent variables are identifiable under this model up to permutation and sign flips, which, they say, is stronger than any previous result. Additionally, this work introduces a number of datasets of increasing complexity (and similarity to natural datasets) for testing disentanglement as well as it performs a large scale and detailed evaluation of the introduced model.
The paper is very well written and full of convincing arguments. The related work section (#2) is very well thought-through and extensively describes links to the relevant literature. The model (sections 3.1 and 3.3) is well described--I especially liked section 3.4. which describes assumption made by the theory, and how these assumptions can be violated in practice, which is then followed by extensive experimentation showing that the theory can work even when assumptions are validated. It would be of a great benefit to the community if a similar section was present in other papers.
I concede that I did not understand the proof sketch in sec 3.2 nor the corresponding figure 2, and I did not the full proof in the appendix, therefore I cannot speak to the correctness of the identifiability claim.
I have two remarks about eq. 4.
if \gamma != 0, this is not an ELBO, strictly speaking, which is contrary to what the text suggests.
We can read that the mean \mu(x_{t-1}) is used as a ""single sample"" to approximate the last term of that equation. Strictly speaking, this can lead to biased estimates of that term, specifically when the mapping from z_{t-1} to the statistics of p(z_t | . ) is non-linear. Additionally, in high-dimensional distribution, the mode itself is a very unlikely sample. Why is this ok to take the mean in the context of this equation?
The evaluation is extensive (two baselines, one of SOTA from ICA and disentangled representations literature, and 14 datasets). Figure 4, which shows plots of true generative factor vs matched latent variable, is superb. It is a really good method of visualising disentanglement--I agree with the authors that this is a much better way then showing latent traversals.
The paper seems to be very good, though I am no expert on ICA. I strongly recommend acceptance. The reason I am not giving a higher score is that the significance to the community seems not that high, but please correct me if I am wrong.
Some further questions:
in sec 3.1. you write ""we assume that the noise [in g] is modeled indirectly as a latent variable"". Does that mean that g is itself a latent-variable model?
The appendix includes a ""broader impact"" statement, which strongly suggests that this paper was previously rejected from NeurIPS. May I ask what the main points of criticism were?
Some further remarks:
In the first paragraph of intro you write ""although untrue in the literal sense"". Why is that? It seems true to me.
""this problem"" a few lines below the above is unclear.
Table 2 appears BEFORE Table 1 in the text. This is very confusing!
It is unclear what ""Natural"" in Table 1 refers to. I assumed that is refers to the introduces ""Natural Sprites"" dataset, but maybe I am wrong?
A hyphen in latex should be typed as ""---"" and there should be no spaces between the hyphen and the surrounding text; you have one e.g. in the conclusion.
UPDATE: T`he authors' response addressed all my remarks. I've increased the score.","9: Top 15% of accepted papers, strong accept","3: The reviewer is fairly confident that the evaluation is correct"
"Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding","Keywords: disentanglement, independent component analysis, natural scene statistics","Given the response, I agree now that the paper is potentially highly significant. The new proof intuition and updated figure both do a much better job at explaining how the proof works. I've increased my score.","",""
"Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs","Keywords: Generative Adversarial Network, 3D Reconstruction","The paper proposes to use pre-trained 2D (i.e., image) GANs as a mechanism for recovering 3D shape from a single 2D image. The work demonstrates impressive results on not only human and cat faces, but also cars and buildings. The method is demonstrated with qualitative results and quantitative results on multiple datasets and tasks.
The reviewers were persuaded by the novelty and ""neatness"" of the idea (and the AC is in agreement) as well as the results. At submission time, there were some concerns with experimental details. For instance, there was a question of how carefully the settings have to be tuned (always a concern with unsupervised methods) as well as an overarching concern about the initialization and whether the method will work on less clean data. The reviewers (and the AC) seem to think that these have been sorted out in discussion.
All three reviewers were in favor of acceptance and the area chair is inclined to agree with the reviewers. In particular, the AC finds the work interesting and compelling. While there is an updated version already uploaded during the discussion, the AC encourages the reviewers to double check all the questions from the reviewers and include the answers from the discussion into the camera ready (even these results are in the appendix).","The paper proposes to use pre-trained 2D (i.e., image) GANs as a mechanism for recovering 3D shape from a single 2D image. The work demonstrates impressive results on not only human and cat faces, but also cars and buildings. The method is demonstrated with qualitative results and quantitative results on multiple datasets and tasks.
The reviewers were persuaded by the novelty and ""neatness"" of the idea (and the AC is in agreement) as well as the results. At submission time, there were some concerns with experimental details. For instance, there was a question of how carefully the settings have to be tuned (always a concern with unsupervised methods) as well as an overarching concern about the initialization and whether the method will work on less clean data. The reviewers (and the AC) seem to think that these have been sorted out in discussion.
All three reviewers were in favor of acceptance and the area chair is inclined to agree with the reviewers. In particular, the AC finds the work interesting and compelling. While there is an updated version already uploaded during the discussion, the AC encourages the reviewers to double check all the questions from the reviewers and include the answers from the discussion into the camera ready (even these results are in the appendix).",""
"Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs","Keywords: Generative Adversarial Network, 3D Reconstruction","Summary
This paper studies an interesting inverse-graphics problem. It proposed a novel method to learn 3D shape reconstruction using pre-trained 2D image generative adversarial networks. Given an image containing one single object of interest, it first predicts the graphics code (e.g., viewpoint, lighting, depth, and albedo) by minimizing the reconstruction error using a differentiable renderer. The next step is to render many pseudo samples by randomization in the viewpoint and lighting space, while keeping the predicted depth and albedo fixed. A pre-trained 2D image GAN is further used to project the pseudo samples to the learned data manifold through GAN-Inversion. Finally, these projected samples are added to the set for the next round optimization. Experimental evaluations have been conducted on several categories including face, car, building, and horse.
Comments
Overall, this is a very interesting paper with good presentations, promising experimental results, and solid quantitative comparisons with the previous work. Reviewer would like to point out the potential weakness of the paper as follows.
W1: Though impressed by the results (especially the proposed method works for horse and building), reviewer suspects the paper only works in a very simplified setting: (1) the GAN was previously trained on a large amount of 2D images of a single category with many variations in identity, viewpoint, and lighting; (2) the initialization (or step 1 in Section 3.1) step seems very critical to the overall performance; and (3) viewpoint and lightning randomization seems have to be hand-tuned. Reviewer would like to see the discussions on the underlying assumptions more explicitly. In addition, reviewer would like to know how does the method generalize to “dirty” data: people with sunglasses, people with noticeable earrings, people partially occluded by wavy long hair, and people with a side view (looks like the input has to be a frontal face image). Same question applies to those non-convex shapes: a convertible car or a car with the window open. Reviewer suspects the method in the current form cannot handle them well.
W2: Some important experimental settings are neither presented nor clarified. For example, it is not clear what is the difference between “Ours (3D)” and “Ours (GAN)”, which should be clarified. For image editing (see Figure 6 and Figure 8), reviewer sees a noticeable change in background color sometimes (e.g., second row in Figure 6). It would be good to give a very detailed explanation of the image editing process (e.g., what’s the input and output format in each stage). As ellipsoid was used to initialize the face shape, reviewer would like to know what was the initialization for other categories such as building and car (see Figure 10).
W3: It would be good to report the time spent on the computation and optimization and how it is compared to the baselines in Table 1 and Table 2. This is very important metric to report as a fair comparison to the previous work.
== Post-rebuttal Comments ==
I am raising my score from 7 to 8, as author responses addressed my comments well (especially answer to W1 and the Figure 13) than expected.","8: Top 50% of accepted papers, clear accept","5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
"Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs","Keywords: Generative Adversarial Network, 3D Reconstruction","Pros:
This is the first work that attempts to reconstruct 3D shape from 2D image in an unsupervised way using GANs. The idea is neat: Use networks to predict four 3D parameters and use GAN to generate / synthesize the images corresponding to a set of parameters. Then these synthesized images can be used as pseudo ground truth to train the 3D parameter network.
The experiments are comprehensive to support the effectiveness of the proposed pipeline. 2 tasks are evaluated: 3D shape reconstruction and object-aware image manipulation. On 3D shape reconstruction, performances are reported on two datasets and demonstrated it outperforms SoTA method by a large margin. On image manipulation, the visualization results look reasonable and visually better than previous method.
Cons / Questions:
The authors claim in the introduction section that this proposed method has advantage over previous method as it doesn't assume symmetry of the instance. But in this proposed method, a symmetrical ellipsoid is used as the shape prior. Is this a stronger implicit assumption than the symmetry assumption? Is there any experiment to explore how the shape prior affects the models' training results? E.g. what if a non-convex shape prior is provided, what if asymmetrical prior is provided?
In the introduction section, the authors use `'building' as an example to show that previous method's symmetry assumption cannot work. But in the experiments and comparison with previous works, all data used are symmetrical: human face, animal face, cars, etc. Visualizations on buildings are shown in appendix but there is no quantitative analysis or comparing with current SoTA method.
How is the generator initialized? As the generator is always fixed during training, I assume the generator is using some pre-trained network? If so, can we still call this network fully unsupervised?","7: Good paper, accept","3: The reviewer is fairly confident that the evaluation is correct"
"Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs","Keywords: Generative Adversarial Network, 3D Reconstruction","This paper proposes an iterative method that jointly estimates viewpoints, light directions, depth, and albedo from single images, by projecting intermediate renderings to the nautral image manifold. Intuitively, the method works by generating, with pre-trained GANs, multiple views of the same object under different lightings, and then inferring 3D shapes from those variants. The key idea is to use pre-trained 2D GANs to make such data generation photorealistic. The authors also demonstrate 3D edits, such as 3D rotation and relighting, that one can perform after running their model.
I like this paper because (1) it presents the novel idea of ""generating"", by GAN inversion, photorealistic multi-view, multi-light data of the given real object, from which the 3D shape can then be estimated; (2) extensive evaluations were performed to demonstrate the high quality achieved; and (3) one can perform 3D edits, such as 3D rotation and relighting, on top of the model outputs. Having explicit 3D understanding for relighting makes a lot of sense to me, and this paper presents a new angle of doing so by GAN inversion.
In terms of drawbacks, this paper would benefit from the following experiments or clarifications: (1) How crucial is the size of the dataset for GAN pretraining? For example, if the dataset is small, not covering many of the face poses, I imagine the GAN projections may not always look realistic, thereby causing the shape estimation to degrade. Such failure cases or studies should be shown so that the reader understands what impact the dataset bias or size has on the final results. An example would be a plot of shape reconstruction error w.r.t. the dataset size or face pose coverage. (2) How robust is the algorithm to the shape initialization (ellipsoid shape)? More importantly, what about its location -- what if the ""off-the-shelf scene parsing model"" fails so that the ellipsoid is placed off the main object? If the model fails because of bad initializations, what do the failure modes look like? Are the shapes completely garbage, or something that looks like the initialization? (3) Why is the viewing direction parameterized in R6 (I presume the start and end XYZs)? Shouldn't it be in S2, just like the light direction? If they are in R6, then results on zooming in/out should be included. Otherwise, there seems to be no point in defining them in R6.","8: Top 50% of accepted papers, clear accept","4: The reviewer is confident but not absolutely certain that the evaluation is correct"